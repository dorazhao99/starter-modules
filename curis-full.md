# CURIS Full
## Interdisciplinary K-12 CS Education Modules
> Only 57.5% of U.S. public high schools offer at least one foundational computer science (CS) course. Schools serving economically disadvantaged students face barriers in establishing CS programs. This ongoing project tis to create short, 1-2 week interdisciplinary modules that integrate foundational CS concepts with another subject, preferably at the middle and high school levels. I.e., given a non-CS subject at a high school (e.g., History, English, Math, Chemistry, etc.), we are creating teaching modules that will help teach CS concepts in the context of the other course.

Compensation Type: paid
Contact Professor: Christopher Gregg
Field of Research: software_engineering
Prereqs
CS106A
Recommended Background
This project is going to involve curriculum development, so you should be willing to draft interesting and engaging materials for a middle school or high school class. There is also opportunity to work on the web page for the courses, as well.
Office Hours
Tuesdays 2:30pm-3:30pm


## Complexity Theory
> Research in complexity theory

Compensation Type: paid
Contact Professor: Li-Yang Tan
Field of Research: theory_of_computation
Prereqs
254 and 254B
Recommended Background
254, 254B, 261, and 265
Office Hours
By appointment


## Tactile-enabled dexterous in-hand manipulation
> In the evolving domain of robotic dexterous manipulation, the absence of tactile feedback has long been a bottleneck, hindering the robot's ability to perform contact-rich in-hand manipulations with the finesse and adaptability exhibited by the human hand. This project investigates the integration of state-of-the-art tactile sensors (e.g. GelSight, DIGIT360) with multi-finger hands (e.g. LEAP hand, Allegro hand) for tactile contact sensing and pose estimation. The goals of this project are:
1. Establish a tactile-enabled dexterous hand platform that is durable and affordable 
2. Apply algorithms developed at TML to demonstration tactile-enabled dexterous in-hand manipulation

Compensation Type: paid
Contact Professor: Karen Liu
Field of Research: robotics
Prereqs
Experience in programming and CAD.
Recommended Background
Familiarity with concepts from robotics (CS123, CS223A), dynamics and control (ME161, ENG105), optimization (EE364A).
Office Hours
CoDa E344, by appointment.


## Integrating Machine Learning and Physics-Based Methods for Protein Engineering and Discovery
> The exponential growth in sequencing technologies and artificial intelligence has revolutionized our access to protein sequences and predicted structures. However, the fundamental challenge in protein engineering remains: identifying and characterizing proteins with specific functional properties for research and applications. This creates a classic needle-in-a-haystack problem that demands sophisticated computational approaches.

While machine learning (ML) has shown tremendous promise in protein function prediction, these tools face inherent limitations based on their training data. Novel protein functionalities, which often lead to groundbreaking discoveries, typically exist in low-data regimes where ML models struggle to make accurate predictions. Traditional physics-based approaches, including molecular dynamics and quantum mechanical (QM) calculations, offer potential solutions to these data limitations. However, these methods present their own challenges: they can introduce systematic biases based on problem formulation, require deep domain expertise, and are computationally expensive at scale.

This project focuses on developing an innovative hybrid approach to characterize, mine, and engineer light-sensitive proteins by combining custom-trained ML models with established classical and quantum mechanical simulation tools. The applicant will work alongside lab members and international collaborators to optimize our existing ML-QM pipeline. Key responsibilities include:

- Developing and optimizing automation workflows to streamline the integration of ML predictions with physics-based calculations
- Creating robust data engineering solutions to handle diverse protein datasets
- Utilizing data analysis and visualization to extract insights from computational results
- Collaborating with team members at Stanford and international partners to refine and validate computational approaches

This research project offers a unique opportunity to work at the intersection of artificial intelligence, computational chemistry, and protein engineering, while contributing to the development of next-generation tools for protein discovery and design.

Compensation Type: paid
Contact Professor: Ron Dror
Field of Research: biocomputation, scientific_computing, ai
Prereqs
- Strong background in Python and Shell/Bash programming
- Familiarity with structural biology and quantum chemistry concepts
- Familiarity with machine learning concepts
Recommended Background
- Background in molecular dynamics (MD) and quantum mechanical simulations (QM/MM)
- Background in machine learning model development and applications (including classical models, LLMs, GNNs)
- Familiarity with parallelism and distributed systems
Office Hours
Feb 12-13, 8-9:30 AM, Zoom
Feb 17, 2-4 PM, Zoom


## 4D World Modeling with Data-Driven 4D Objects
> The 4D physical world we inhabit encompasses diverse types of 4D objects, including pets, animals, plants, and man-made objects. The ability to animate and manipulate these 4D objects is crucial for creating digital environments that mirror the real world with high levels of detail and realism. In this project, we study the important problem of developing representations for 4D objects from foundation models, video generative models, and in-the-wild data, with potential applications in robotics, embodied AI, and metaverse.

Compensation Type: paid
Contact Professor: Jiajun Wu
Field of Research: ai, vision, robotics
Prereqs
CS231a/CS131/CS231n or similar
Recommended Background
Stanford students interested in this research project should have a strong background in 3D Computer Vision (CS 231a/CS131/CS231n) and Machine Learning (CS236/CS229). Past experience in Computer Graphics / Robotics / CAD is a plus. Additionally, proficiency in programming (especially in PyTorch), familiarity with popular deep learning frameworks, and strong self-motivation are required.
Office Hours
By appointment.


## Developing and Evaluating Language Models for Clinical Applications
> Language models are rapidly being deployed into the clinic. The electronic health record vendor EPIC is integrating GPT-4 into administrative workflows, and startups are deploying tools for ambient documentation. Furthermore, a recent survey of UK physicians found that one in five respondents already use generative AI tools in their clinical practice. Despite this growing adoption, current models face significant limitations in handling clinical data.

- Longitudinal and temporal reasoning: Current LLMs struggle to represent and reason over longitudinal and temporal patient data.
- Bias and fairness: Models have demonstrated disparities in diagnosis and treatment recommendations across demographic groups.
- Evaluation challenges: There remains a critical need for scalable and clinically meaningful evaluation methods.

Potential projects include:
1) Developing scalable tools to assess the faithfulness, comprehensiveness, fairness, and clinical utility of language models in healthcare.
2) Designing methods to measure and reduce the impact of biased training data on downstream clinical tasks.
3) Improving retrieval-augmented generation (RAG) for clinical applications by building effective retrievers for longitudinal EHR data.
4) Creating reliable methods for longitudinal EHR summarization to generate faithful and verifiable clinical summaries that support decision-making

Compensation Type: paid
Contact Professor: Emily Alsentzer
Field of Research: biocomputation, ai
Prereqs
- Proficiency in Python and SQL
- Background in machine learning (ML) and/or natural language processing (NLP) through coursework (e.g., CS224N, CS221, and/or CS229) or experience
- Interest in tackling healthcare challenges through ML/NLP and enthusiasm for exploring clinical data and workflows
- Ability to communicate findings and progress effectively
- Self-driven mindset with an eagerness to actively seek out resources, ask questions, and advance project goals
Recommended Background
- Prior experience with Huggingface libraries and/or finetuning language models 
- Nice to have: Prior experience working with electronic health record (EHR) data
Office Hours
By appointment


## Knowledge Discovery Platform
> This project aims to develop an advanced knowledge discovery system that seamlessly integrates free-text data (e.g., research papers, articles, reports) with structured knowledge sources like databases and knowledge graphs to deliver comprehensive, insightful answers to user queries. Leveraging large language models (LLMs) alongside knowledge representation techniques, the system will extract, link, and synthesize information from both unstructured and structured datasets. Beyond answering straightforward questions, the system will uncover novel insights, identify hidden patterns, and suggest potential research directions. By automating key tasks such as literature reviews, hypothesis generation, and data analysis, this platform will significantly streamline and enhance the research process.

The project’s core focus is on building a user-facing platform that harnesses the power of LLMs while optimizing the integration of structured data. This will involve not only developing the underlying models but also implementing robust engineering solutions to ensure the system is scalable and user-friendly. We are looking for a highly motivated student to commit to this project starting in the spring quarter.

Relevant Works:
- SUQL (https://arxiv.org/abs/2311.09818)
- LOTUS (https://lotus-data.github.io/)
- DocETL (https://www.docetl.org/)

Compensation Type: paid
Contact Professor: Monica Lam
Field of Research: ai, software_engineering
Prereqs
CS 224N/CS224V or equivalent
Software engineering skills
Recommended Background
- Have experience with LLMs, including prompting and fine-tuning techniques.
- Be prepared to tackle essential software engineering tasks such as debugging complex issues, managing edge cases, writing comprehensive tests, and developing a minimum viable product (MVP) frontend.
Office Hours
Thursday, 6 Feb, (2-3pm) - Gates 315
Monday, 11 Feb, (2-3pm) - Gates 323
or by appointment


## Generative Models
> Generative models are a class of machine learning algorithms that learn to produce new data samples by capturing the underlying distribution of existing datasets. They have garnered significant attention due to their ability to generate text, code, images, video, etc. In this research project, we aim to investigate the theoretical foundations and practical implementations of next generation generative model architectures.

Compensation Type: paid
Contact Professor: Stefano Ermon
Field of Research: ai, ai
Prereqs
Stanford students interested in this research project should have a strong grounding in foundational mathematics, particularly linear algebra, calculus, and probability theory. A solid understanding of core machine learning concepts—such as supervised learning, optimization, and neural network architectures—is also essential for diving into the nuances of generative models. Additionally, proficiency in programming (especially in Python) and familiarity with popular deep learning frameworks like PyTorch is required.
Recommended Background
CS236
Office Hours
tbd




## Programmable Dialogue Agents with LLMs
> Creating reliable task-oriented conversational agents presents significant challenges due to:
i) Limited control over LLMs.
ii) Inability of LLMs to effectively follow multiple instructions simultaneously.
iii) Unreliable outputs, especially during extended conversational turns.

To address these issues, we developed Genie, a programmable framework for building reliable task-oriented conversational agents capable of handling complex user interactions and accessing knowledge efficiently. Genie (powered by GPT-4) outperforms agents built using GPT-4 with function calling across three diverse domains.

While Genie delivers impressive performance, it occasionally misinterprets user commands, resulting in incorrect outputs (semantic parsing errors). Improving Genie's semantic parsing—its ability to understand user commands—is the focus of this project.

There are two main directions for improvement:
1) Enhancing Semantic Parsing with Feedback:
In coding agents, LLMs often receive feedback on generated code using tools like linters or code execution. However, providing feedback to Genie agents is more complex due to potential side effects during execution and the need for conversational agents to remain quick and responsive. This project will explore effective feedback mechanisms to improve the accuracy of semantic parsing for Genie.


2) Automating Data Generation for Semantic Parsing:
Creating a semantic parser for Genie currently requires a formal representation of user commands and well-curated few-shot examples. This process can be challenging for non-technical users unfamiliar with concepts like dialogue states and dialogue acts. This project aims to develop a system that simulates diverse user conversations and automatically generates dynamic examples to enhance Genie's semantic parser.

Compensation Type: paid
Contact Professor: Monica Lam
Field of Research: ai, programming_languages, software_engineering
Prereqs
Completion of courses like CS 224N or CS 224V (or equivalent experience) is strongly recommended.
Recommended Background
An ideal candidate would have experience or interest in the following areas: Semantic parsing, LLM agents, Conversational agents

Be prepared to tackle essential software engineering tasks such as debugging complex issues, managing edge cases, writing comprehensive tests, and developing a minimum viable product (MVP) frontend.
Office Hours
Thursday, 6 Feb, (2-3pm) - Gates 315
Monday, 11 Feb, (2-3pm) - Gates 323
or by appointment


## Community-Centered Design for Healthcare and Creativity
> BACKGROUND
Many design approaches focus primarily on centering the end “user.” However, this might miss the opportunity to better understand the opinions of other stakeholders in the community and broader society that are also being impacted by the technology. We are running a series of collective speculation workshops to understand ethical implications of current/future technology (such as in-home sensors, generative AI art, etc.) across the health care and creativity domains with the aim to design tools that better consider a wider range of user perspectives.

Projects include:
- methodology design: analyzing past workshop experiences to iterate on workshop design to be more inclusive
- system design: analyzing artifacts/results from past workshops to inform design and implementation of a system/tool/intervention
- workshop facilitation: running a workshop with artists (especially those who use more manual practices) to understand positive and negative implications of generative AI on their art

POSITION
Roles vary depending on project. Some will be more focused on design, others more focused on development. There are many ways in which you can contribute to these projects, and we’ll do our best to adapt the role to focus on areas/topics you’d be excited to work on. We hope that all students will also have an interest in learning more about the research process (e.g., reviewing related literature, paper writing, user study analysis, etc.).

CONTACT
If you are interested/have any questions, feel free to reach out to Jane E (ejane@stanford.edu). Please include your resume/CV, transcript, and a short paragraph explaining any relevant background/reason for interest in the project.

Compensation Type: paid
Contact Professor: James A Landay
Field of Research: hci
Prereqs
Will have ideally taken CS147 or equivalent
Recommended Background
The strongest applicants will have demonstrated interest in one or more of the following areas:
- interest in the domain: healthcare (older adult physical health, caregivers, medical communication, sensors and privacy, etc.) or creativity (various art approaches, generative AI, etc.)
- familiarity with running user studies
- design thinking methods
- qualitative data analysis
- web dev/coding on new platforms
Office Hours
Feb 10, 3-4pm in CoDa, location TBD (or by appointment through email if this doesn't work)


## Using LLMs to Understand Security Advice At Scale
> Companies keep failing to protect people’s data from attackers. Despite increased attention and investment in security, headlines still report a steady stream of data breaches and ransomware attacks. Enterprise security teams are overwhelmed, with too much to do to prevent compromises. They are also bombarded with advice from governments, security companies, insurance providers, peer forums, and more — advice that can be confusing and contradictory.

Our goal is to understand the disagreements and different priorities in available enterprise security advice documents, so that the community can better help companies cut through the noise. This project involves using LLMs to sort and compare large amounts of domain-specific advice text.

Compensation Type: paid
Contact Professor: Zakir Durumeric
Field of Research: security, ai
Prereqs
- Experience with LLM prompt engineering
- Basic Python fluency
- Interest in learning about security
Recommended Background
The strongest applicants will have one or more of the following:

- Demonstrated creativity and persistence with LLM prompting and pipelining
- Experimental design and data analysis skills
- Familiarity with security (CS 155 or other background knowledge)
Office Hours
Monday 2/10 2-3pm


## Quantum LDPC Codes
> An important question in quantum computing is how to enable fault-tolerant computation: that is, how can we protect our computation from errors?  One approach is to encode data with a quantum error-correcting code, analogous to the classical error correcting codes used to protect classical communication and computation.  To go along with such a code, we also need a mechanism to perform computation on top of that encoded data.

This project will investigate a class of quantum error-correcting codes called quantum LDPC CSS codes, which have been the subject of much recent exciting work. The goal is to look at a few concrete instantiations of codes from this broad family, and compute the set of fault-tolerant gates that they support. This computation could be purely formal (proof-based) or computer-assisted.

Compensation Type: paid
Contact Professor: Mary Katherine Wootters
Field of Research: algorithms, theory_of_computation
Prereqs
A solid background in linear algebra and discrete math (MATH 61DM). You should also be fairly comfortable with abstract algebra (at the level of MATH 120) to be able to understand the algebraic constructions we will consider.
Recommended Background
No prior knowledge of quantum computing or quantum physics is required, but it may be helpful if you have seen some quantum computing before (e.g., CS259Q).  Programming experience is optional but basic familiarity could be useful for calculations.  Experience with error correcting codes (e.g., via CS250/EE387) would also be helpful but is not required.
Office Hours
Monday Feb 10, 2-3pm, CoDa W216, or by appointment.


## Building Collective Knowledge for LLMs
> Large language models (LLMs) demonstrate impressive general capabilities due to their training on vast Internet datasets, but they lack access to user-specific knowledge. For example, while an LLM can provide generic advice about being an undergraduate, it cannot offer institution-specific guidance about what courses to take or what dining hall to eat at. Although retrieval-augmented generation and fine-tuned models offer technical solutions, they do not address the fundamental sociotechnical challenge: how users can effectively author and access the contextual data these models require. To address this limitation, we are building Knoll, a system that allows users to create and incorporate custom knowledge modules into their interactions with commercial LLMs including OpenAI's ChatGPT and Anthropic's Claude.

We're looking for a student to join our research team and contribute to helping build Knoll.  You'll design features for the web application to integrate with different commercial LLMs. This isn't just a theoretical project – Knoll is live and actively used by real users in the broader community! You'll get hands-on experience running user tests and playing a crucial role in our iterative development process.

Compensation Type: paid
Contact Professor: Michael Bernstein
Field of Research: hci, ai
Prereqs
- Experience with React.js 
- Experience programming in Python and Javascript
Recommended Background
- Interest or background in UI/UX Design 
- Interest in large language models
- The following courses are helpful, but not required: HCI courses like CS 147, 247, 347, or 278; AI courses like CS 221, 224N, or 124.
Office Hours
February 11: 2-3PM at CoDa E316


## Language Learning
> New advancements in XR and AI technology allow for the generation of interactive digital narratives that can support experiential and personalized learning experiences. We design and develop one such tool, applied to the educational domain of Language Learning. We are looking for interns to explore implementing 1) novel ways of using AR to improve immersion and learning outcomes 2) AI-based features that focus on supporting users in co-creating immersive simulations based on their learning goals.

Compensation Type: paid
Contact Professor: James A Landay
Field of Research: hci, ai
Prereqs
Familiarity with programming
Recommended Background
Computer Science
Office Hours
Tues / Thurs : 2p - 3p


## Integrated Sound Rendering for Computer Animation
> Physics-based simulation and rendering enable artists to create stunning 3D animations, yet these animations remain inherently silent. The goal of this project is to contribute to an integrated "sound rendering" engine capable of automatically synthesizing physically-based, synchronized sound effects for a variety of animated phenomena (e.g. vibrating rigid bodies, fluids, virtual instruments). Students will have the option to explore several parts of the pipeline, from the theoretical modeling of sound sources themselves, to software integration with existing graphics tools (Houdini, Unreal, etc.). For more details, see https://graphics.stanford.edu/papers/waveblender/)

Compensation Type: paid
Contact Professor: Doug James
Field of Research: graphics, scientific_computing
Prereqs
Familiarity with C++
Recommended Background
Any of the following is helpful, but not required:
- Graphics: CS 248, CS 348
- Parallel programming: CS 149, familiarity with CUDA
- Digital audio: MUSIC 220, MUSIC 320
- Linear algebra, differential equations
Office Hours
CoDa E308, by appointment


## Evaluating Bayesian Optimization and Uncertainty Quantification for Medicinal Chemistry
> Developing pharmaceuticals requires optimizing a wide range of properties, such as binding affinity, pharmacokinetics, toxicity/side effect profile, and blood/brain barrier penetration. Machine learning and physics-based computational methods are widely used to predict these properties, but despite enormous development effort these predictions often differ substantially from experimental measurements. Frequently the experimental measurements themselves also have significant noise, making it challenging to understand the true property values. Thus, uncertainty quantification (UQ) of  these property predictions has the potential to be tremendously helpful in concentrating effort on relevant parts of the search space. 

Bayesian Optimization (BO) is a family of algorithms that relies on UQ to optimize under uncertainty. The molecular property prediction literature is conflicted about the utility of UQ and BO, with some examples of successful uses of BO and other datasets on which uncertainties are so poorly estimated that BO is unhelpful. 

This project seeks to better understand the landscape of uncertainty quantification in molecular property prediction. When does it work and when does it not? Can we develop reliable models and ultimately improve prediction?

Compensation Type: paid
Contact Professor: Ron Dror
Field of Research: biocomputation, ai
Prereqs
Strong background in Python programming
Experience with machine learning and optimizing ML models
Experience reading research papers and drawing actionable conclusions from them
Recommended Background
Introductory biochemistry or organic chemistry
Office Hours
Friday 14 February 2-3 pm

Zoom
Join from PC, Mac, Linux, iOS or Android: https://stanford.zoom.us/j/93940889936?pwd=IaEcAFPGwK5UWpHQD54gt3piZf8jiH.1
    Password: 835490

Or iPhone one-tap (US Toll): +18333021536,,93940889936# or +16507249799,,93940889936#

Or Telephone:
    Dial: +1 650 724 9799 (US, Canada, Caribbean Toll) or +1 833 302 1536 (US, Canada, Caribbean Toll Free)
          
    Meeting ID: 939 4088 9936
    Password: 835490
    International numbers available: https://stanford.zoom.us/u/aDBJksr9B

    Meeting ID: 939 4088 9936
    Password: 835490
    SIP: 93940889936@zoomcrc.com
    Password: 835490


## Robot Data Collection through Crowdsourcing
> We are looking for an undergraduate student to work on a research project in Prof. Dorsa Sadigh's group, ILIAD (Intelligent and Interactive Autonomous Systems Group). We will focus on the problem of teaching new tasks to a robot assistant that can benefit humans around it as it learns new tasks. One way that a human can teach a new task to a robot is by providing teleoperated demonstrations, which the robot can learn from via imitation learning methods. However, collecting teleoperated demonstrations often requires expertise and is tedious, and can therefore be challenging to scale.

In this project, we will focus on making the process of collecting demonstrations more scalable and engaging by appealing to principles of crowdsourcing and incentive design. Specific research topics may include one of the following:
(1) How can we design platforms that incentivize participation from non-experts to teach robots via demonstrations? For example, can we design interactive, game-like interfaces that make teaching a robot intuitive and fun, even for people without robotics experience?
(2) Can we develop methods and interfaces for the robot to give real-time performance feedback to help users provide higher quality data?

Our ultimate goal is to deploy our system on real robots in our lab. You will work collaboratively with PhD students and postdocs with the possibility for publication at the end of the project. We encourage students from all backgrounds to apply.

Compensation Type: paid
Contact Professor: Dorsa Sadigh
Field of Research: robotics, ai
Prereqs
Students should have an interest working at the intersection of robot learning and human-robot interaction. Experience with Python programming, machine learning (e.g., CS229, CS221), and a deep learning framework such as PyTorch is required.
The following are a huge plus:
- Experience in reinforcement learning or imitation learning (e.g., CS224R)
- Experience working on significant programming projects
Recommended Background
CS 229, CS 221
Office Hours
Feb 5, Feb 12: 2pm-3pm Gates 216.


## Molecular Simulation and Machine Learning for Peptide Design
> Wegovy and Ozempic are recent blockbuster drugs for treating diabetes, obesity, and other conditions. They are also peptides: small proteins. 

This CURIS project will be involved in early-stage design of a peptide to treat leukemia. The peptide will target a protein-protein interaction that is involved in cell cycle progression and requires optimization of a number of properties. We will do this using a combination of physics-based molecular dynamics (MD) simulations and machine learning (ML). What specifically you will do will depend on project progress leading up to the summer, but you will have the opportunity to contribute to designing real compounds that will be tested experimentally.

Compensation Type: paid
Contact Professor: Ron Dror
Field of Research: biocomputation, scientific_computing, ai
Prereqs
Strong programming experience in Python, and prior experience working with scientific datasets
Paper-reading experience
Recommended Background
These are not required but are helpful:
- Biochemistry or medicinal chemistry background
- Experience setting up conda environments and working on a cluster
Office Hours
Friday 14 Feb 2-3 pm

https://stanford.zoom.us/j/93940889936?pwd=IaEcAFPGwK5UWpHQD54gt3piZf8jiH.1
    Password: 835490

Or iPhone one-tap (US Toll): +18333021536,,93940889936# or +16507249799,,93940889936#

Or Telephone:
    Dial: +1 650 724 9799 (US, Canada, Caribbean Toll) or +1 833 302 1536 (US, Canada, Caribbean Toll Free)
          
    Meeting ID: 939 4088 9936
    Password: 835490
    International numbers available: https://stanford.zoom.us/u/aDBJksr9B

    Meeting ID: 939 4088 9936
    Password: 835490
    SIP: 93940889936@zoomcrc.com
    Password: 835490


## Distributed Debugging
> Legion is a programming model for distributed, parallel machines that provides a higher-level abstraction than traditional high performance programming frameworks.   This project will investigate building an interactive debugger for Legion, similar in functionality and "feel" to gdb, that takes advantage of these abstractions to allow users to inspect and understand the runtime behavior of a distributed program.

Compensation Type: paid
Contact Professor: Alex Aiken
Field of Research: programming_languages, distributed_systems
Prereqs
CS143 or CS149
Recommended Background
Strong C++ programming skills are necessary for this project.
Office Hours
Thurdsays, 3-4, CoDa E462


## Browser JavaScript Execution
> We are seeking CURIS students to assist in designing and implementing systems for efficient and privacy-preserving JavaScript execution. Tasks may include conducting literature reviews of past measurement studies, integrating browser APIs, implementing a browser extension, designing system architectures, analyzing JavaScript execution, and designing mechanisms to detect and mitigate online tracking. Ideal candidates should have an interest in web technologies, privacy, and system design.

Projects may include:
- The design and implementation of a system to block online tracking JavaScript at the function level.
- Aiding in the design and implementation of a from-scratch web scraping framework.

Compensation Type: paid
Contact Professor: Zakir Durumeric
Field of Research: networking, security, programming_languages
Prereqs
- Taken a basic security course (CS155 or equivalent) AND/OR a basic networking course (CS144 or equivalent). Networking course preferred.
- Familiarity and experience with JavaScript fundamentals
- Experience in programming and system design
- Willingness to contribute 40 dedicated hours a week
Recommended Background
- A basic understanding of web development and browser APIs
- A high-level understanding of browser JavaScript execution
Office Hours
By appointment. Email me!










## Measuring and Circumventing The Great Firewall of China
> Are you interested in understanding the latest Internet censorship techniques and developing solutions to help millions of users bypass them? Join the Empirical Security Research Group (ESRG) lab this summer to work on cutting-edge projects related to the Great Firewall of China (GFW) and other real-world censorship systems.

Our past research has successfully uncovered multiple censorship mechanisms developed and deployed by the GFW of China, helping millions of users bypass censorship (https://censorbib.nymity.ch/pdf/Alice2020a.pdf , https://gfw.report/publications/usenixsecurity23/en/ ).

Within this theme, we offer multiple research opportunities, including:

* Using side channels to fingerprint and reveal insights into the Great Firewall of China.
* Evaluating the adoption and effectiveness of Encrypted Client Hello (ECH) in anti-censorship.
* Designing and developing censorship circumvention systems that are already in use by millions of users in China, Iran, and Russia daily.

The most important thing to us is understanding your interests and expectations—we aim to match you with a project tailored to your skills and passions while supporting your growth, success, and, most importantly, doing cool and impactful research!

Compensation Type: paid
Contact Professor: Zakir Durumeric
Field of Research: security, networking, software_engineering
Prereqs
* Background knowledge in networking through CS144 or equivalent
Recommended Background
* It is helpful, but not required, to have some background in network security (e.g., CS155 or equivalent).
Office Hours
Location: CoDA W338 

Time slots:

Monday 3-4 pm (Feb. 3, Feb 10)
Tuesday 3-4 pm (Feb. 4, Feb 11)
Wednesday 2-3 pm (Feb 5 only)
Thursday 2-4 pm (Feb 6, Feb 13)




## Writing Performant GPU Kernels with Language Models
> Kernel programming is crucial for unlocking the full performance of GPUs but is highly challenging due to the need for deep expertise in hardware, parallelism, and memory optimization. Can LLMs help or be better than humans at writing Kernels? Building on KernelBench, a benchmark for evaluating LLMs' kernel generation capabilities, the research will focus on using reinforcement learning and feedback loops to improve kernel quality iteratively. In this role, you will conduct experiments to evaluate what types of feedback are most effective for improving the performance of language models in generating GPU kernels and how to make LLMs use feedback more effectively. You will also implement reinforcement learning and feedback mechanisms to iteratively optimize kernel quality. Additionally, you may help in expanding KernelBench with new tasks and metrics to better assess LLM capabilities. Interested students should send Anne Ouyang (aco@stanford.edu) a resume/CV, transcript, and short paragraph explaining their relevant background.

Compensation Type: paid
Contact Professor: Azalia Mirhoseini
Field of Research: ai, software_engineering, programming_languages
Prereqs
We are looking for highly motivated students who have a strong deep learning and reinforcement learning background (CS229, CS224N, CS234, etc.) with proficiency in PyTorch, familiar with ML and agentic systems (CS229S, CS329A). Basic knowledge of GPU programming.
Recommended Background
It is recommended to have prior experience with kernel programming for a deeper understanding and effective implementation.
Office Hours
By appointment




## Reasoning Language Models
> Recent large language models such as OpenAI o1/o3 and DeepSeek R1 have demonstrated remarkable performance on math and coding tasks by generating long chains-of-thought before producing their final answer. They explore many paths to a solution and refine their own outputs, in a manner more closely resembling how humans solve challenging problems than previous models. Inspired by these successes, we will consider new methodologies to advance the reasoning capabilities of generative models.

Compensation Type: paid
Contact Professor: Tatsunori Benjamin Hashimoto
Field of Research: ai
Prereqs
N/A
Recommended Background
A strong math background (linear algebra, calculus, probability) and familiarity with deep learning frameworks like PyTorch. Experience with probabilistic ML (e.g., CS 228, CS 236) and GPU/kernel programming (e.g., CS 149, CS 229S, CS 336) is a plus.
Office Hours
Friday Feb 7, 10-11am, Gates 358
Tuesday Feb 11, 4-5pm, Gates 304




## Interactive Visual World Generation and Rendering
> The project aims to develop a fully automatic pipeline of interactive dynamic world generation with realistic renderings. It will leverage language models to generate high-level abstractions of interactive worlds in the form of programs, and visual generative models as renderers that take in the compositional world structures and output realistic renderings. The full pipeline will open possibilities for agent training and user interactions.

Compensation Type: paid
Contact Professor: Jiajun Wu
Field of Research: ai, vision, programming_languages
Prereqs
Applicants should be familiar with deep learning techniques and basic mathematics concepts in linear algebra and probability. Proficiency in python and PyTorch is required. Familiarity with 3D geometry and neural rendering is a plus.
Recommended Background
CS231a, CS231n
Office Hours
By appointment.




## GPU Accelerating Partitioning Computations within the Legion Runtime System
> To distribute computations across multiple nodes and multiple GPUs, data must be partitioned across the compute units in the machine. Computing the necessary partitions of distributed data can be complex when the structure of the partitions are data dependent. The Legion runtime system is capable of representing and computing these data dependent partitions, but currently using CPUs -- this project would focus on developing GPU-based algorithms to speed up data partitioning process.

Alternate projects are also possible, such as porting an existing scientific application to Legion to scale it clusters of GPUs. Additionally, if a student has an application they are interested in scaling to clusters of GPUs, we can discuss how to scope a project around that.

Compensation Type: paid
Contact Professor: Alex Aiken
Field of Research: distributed_systems, programming_languages, ai
Prereqs
CS149 or other parallel programming equivalent
Recommended Background
An ideal candidate would have some background with parallel computation, especially with programming in CUDA and C++.
Office Hours
Wednesdays, 2-4PM, Gates 494
Thursdays, 2-4PM, Gates 494








## Automated Detection of Silent Data Corrupts and Concurrency Bugs
> Project 1: Detecting Multithreading Errors: An Instruction Duplication-Based Approach

Multithreading errors occur when multiple threads within a program attempt to access and modify shared data simultaneously and they are notoriously difficult to detect. This is especially concerning in critical software like the OpenSSL library, where race conditions and improper synchronization can compromise security and reliability. Traditional debugging tools may not catch these issues because they only manifest under specific execution scenarios, making them rare but high-impact when they do occur. This project aims to explore a duplication-based detection for multithreading errors in OpenSSL. We are going to use an approach inspired by ITHICA, an Intra-Thread Instruction Checking Approach originally designed for identifying hardware faults and adjusting it to detect these pernicious errors!

Key research questions we care about are:
* How frequently do multithreading bugs occur in OpenSSL? Reproduce reported OpenSSL race conditions and investigate under which conditions they appear.
* Can we apply duplication strategies to detect these issues? Explore how code duplication (redundant execution) can help identify them.
* What are the distinguishing characteristics of multithreading bugs? Identify unique signs of race conditions and ways to distinguish them from false positives.

Recommended Background:
* C++ programming. 
* (Optional) Experience with multithreading or parallel computing.
* (Optional) Familiarity with LLVM Intermediate Representation.


Project 2: Realistic Silent Data Corruption (SDC) Simulator

Most software assumes that processors execute instructions correctly, but large-scale cloud providers are increasingly observing silent data corruptions (SDCs)—errors where computation silently produces incorrect results. Meta, for example, recently reported cases where “a simple computation like 1.1^53 resulted in the wrong answer (0 instead of 156.24), leading to missing rows in their database.” At hyperscale, SDCs occur at rates as high as one fault per thousand devices, making them a serious reliability concern.

Current fault models fail to accurately simulate these real-world errors. In this project, we aim to develop a fault injector/simulator inspired by existing open-source tools but uniquely designed to model SDCs based on real-world data collected by our group from actual faulty hardware. This project will enable faster and controlled experimentation, allowing us to explore important research questions that would be infeasible to test directly on hardware.

Some key experiments this simulator will enable include:
* Impact of different optimization levels (O0, O1, O2, etc.) on SDC occurrence.
* Effects of specific compilation flags on error resilience.
* How running stress microbenchmarks alongside workloads influences fault behavior.

By building a realistic SDC fault injector, this project will provide a valuable tool for studying processor reliability without the need for large-scale hardware testing.

Recommended Background:
* C++ and/or Python programming. 
* (Optional) Familiarity with LLVM Intermediate Representation.


Project 3: Instruction Diversity for Detecting Silent Data Corruptions (SDC) using Program Synthesis Tools

Most software assumes that processors execute instructions correctly, but large-scale cloud providers are increasingly observing silent data corruptions (SDCs)—errors where computation silently produces incorrect results. Meta, for example, recently reported cases where “a simple computation like 1.1^53 resulted in the wrong answer (0 instead of 156.24), leading to missing rows in their database.” At hyperscale, SDCs occur at rates as high as one fault per thousand devices, making them a serious reliability concern.

This project explores a new approach to detecting SDCs by diversifying instruction execution. Instead of simply duplicating the same operation, we generate alternative but functionally equivalent instruction sequences and compare their outcomes. As a simple example, a multiplication could be replaced with repeated addition. To achieve this, we will design alternative instruction sequences using Rosette, a program synthesis tool. This method helps catch hardware faults that consistently miscompute specific operations but remain undetected when executing the same instruction twice.

Recommended Background:
* Confidence in C++ or Python 
* Experience with symbolic execution or program synthesis (optional but beneficial)
* CS 257 or 357S (optional but beneficial)

Compensation Type: paid
Contact Professor: Caroline Trippel
Field of Research: architecture_and_hardware, compilers, formal_methods_and_verification
Prereqs
See above per project.
Recommended Background
See above per project.
Office Hours
By appointment. Contact Ioanna Vavelidou: vavel@stanford.edu.




## Scaling LLM Test-Time Compute to Supercharge Learning
> Recent work (e.g. chain-of-thought, o1/o3, r1) has demonstrated that scaling test-time compute can significantly improve the reasoning capabilities of large language models when solving particular problems of interest. However, this existing line of work only focuses on one aspect of how humans scale test-time compute. A completely separate but equally-important setting is when someone reads a very difficult textbook chapter and reflects on that material for a long time in order to internalize the knowledge (without necessarily thinking of a concrete problem). The purpose of this project is to explore this idea of how we can scale “test-time compute for learning”.
We can make progress towards this overall research goal through a variety of potential projects, such as improving LLM architectures to allow for better FLOP scaling (e.g. with looping and bidirectionality), new training objectives that explicitly boost in-context-learning, and pursuing recent new approaches like test-time training. Students will be able to pursue any of these concrete project ideas, or self-direct their own project in this area.
This role requires a full-time commitment (40+ hours/week). Interested students should email Jordan Juravsky (jbj@stanford.edu) for more information.

Compensation Type: paid
Contact Professor: Azalia Mirhoseini
Field of Research: ai
Prereqs
CS224N, CS229S, CS329A or equivalent
Recommended Background
Have worked with LLMs.
Office Hours
Please email  Juravsky (jbj@stanford.edu) for more information.




## Provenance for language models
> The goal of this project will be to develop mechanisms for establishing the provenance of a language model. Examples of possible research questions include:

1. Can we determine whether a language model was trained independently of another language model (versus being distilled and/or fine-tuned from the other model)?

2. Can we determine the data on which a language model was trained and/or attribute the predictions/behavior of a model back to certain parts of its training data?

Compensation Type: paid
Contact Professor: Percy Shuo Liang
Field of Research: ai
Prereqs
None.
Recommended Background
Prior background with deep learning frameworks (e.g., Pytorch) and a strong math background.
Office Hours
2/7/25, 4pm, virtual (Zoom link TBD)






## Theory
> Have you enjoyed your theory classes and would like to work on a research problem that nobody knows the answer to, instead of a homework problem? A few theory professors are jointly offering this project for students interested in research in theoretical computer science topics. We will tailor the project to your interests and choose a topic of mutual interest to work on. Possible topics include: approximation algorithms that cut corners for hard optimization problems by returning (slightly) suboptimal solutions, or big data algorithms that cut corners when the input is too big to fit into memory.

Compensation Type: paid
Contact Professor: Moses Charikar, Aviad Rubinstein, and Aaron Sidford
Field of Research: algorithms
Prereqs
CS 161 and/or CS 154
Recommended Background
The project is appropriate for advanced undergraduate students. Students with a stronger background in theoretical computer science will get priority.
Office Hours
Aviad: Wed 2/12, 2:30-3:30, CoDa W250


## The Impact of Implied versus Expressed Emotions on Social Media
> Research has demonstrated that emotions can spread rapidly on social media, with negative emotions often spreading more quickly and broadly than positive ones. However, the majority of prior studies have focused on assessing these emotions using sentiment analysis tools like VADER, which are typically dictionary-based and primarily capture expressed emotions—those explicitly communicated through words, such as “I’m happy.” This approach overlooks the subtle nuances of implied emotions, which are suggested or inferred through indirect language, choice of words, or literary devices like metaphor and irony. For instance, a statement like "I wish my paper got accepted" implies sadness and disappointment without directly stating it. 

In this study, we plan to investigate how implied and expressed emotions differ in their spread and impact on social media. We will collect and analyze social media content from platforms such as Reddit, using large language models to distinguish between implied and expressed emotions. By conducting statistical analyses, we will investigate how these different types of emotional content influence users' subsequent behaviors, such as engagement and emotional responses. The anticipated contribution of this research is to provide deeper insights into the complexities of emotional contagion on social media, improving our understanding of online interactions and potentially informing strategies for managing emotional spread.

Compensation Type: paid
Contact Professor: Michael Bernstein
Field of Research: hci, ai
Prereqs
The ideal candidate is someone who is excited to work on projects at the intersection between HCI, NLP, and AI! Bonus points if you are interested in understanding the behavioral outcomes of conversational exchanges on social media platforms.

The following courses are helpful, but not required: HCI courses like CS 147, 247, 347, or 278; AI and NLP courses like CS 221, 224N, 124, 336 or any other equivalent courses.
Recommended Background
This project involves both technical development and field experiment design. A solid foundation in at least one of the following areas is  recommended:

(1) Experiences with data science skills, such us collecting and analyzing data from social media APIs using Python.
(2) Experiences with LLM-based applications and evaluating machine learning models.
Office Hours
Wed 2/059:30am-10:30am, CoDa E316
Wed 2/12 9:30am-10:30am, CoDa E316






## Wireless Sensing System for Environment Monitoring
> Camera traps are widely used to monitor wildlife in remote forests across California. However, their biggest limitation is that they rely on battery power and lack wireless connectivity. As a result, scientists must frequently visit sites to replace batteries and retrieve SD card data, making long-term deployments difficult and limiting coverage.

This project aims to develop a fully **self-sustainable, wireless camera trap** that connects to satellites for data transmission. The system will be designed to operate indefinitely using energy harvesting techniques, eliminating the need for frequent maintenance.

The project consists of three key components:

1. Hardware Development: Designing an energy-efficient camera system equipped with sensors, a microprocessor, and a low-power communication modem.
2. Energy Harvesting: Implementing renewable energy solutions such as solar cells or microbial fuel cells to power the system.
3. Edge Computing for Wildlife Detection: Developing an ultra-lightweight image processing pipeline that runs on the embedded device to detect and classify wildlife efficiently.

We are looking for undergraduate students who are passionate about building real-world applications, enjoy solving open-ended engineering challenges, and aren’t afraid to get their hands dirty. By joining this project, you will gain hands-on experience in embedded systems, wireless communication, and energy-efficient machine learning while working on a system with real-world scientific impact.

Compensation Type: paid
Contact Professor: Zerina Kapetanovic
Field of Research: architecture_and_hardware, networking
Prereqs
CS 106
Recommended Background
No prior experience is required, but having familiarity with any of the following areas will make it easier to get started. If you have such experience, please highlight it in your application.
1. Embedded systems development, PCB design
2. Wireless communication protocols
3. Low-power electronics and energy harvesting
4. Computer vision
Office Hours
Tue/Thu 11AM-12PM CoDa E460






## Encoding spatial logics into social media design
> This project aims to encoding spatial and relational logics into social media design (e.g., via creating new feeds and feed algorithms, adding annotations, manipulating user networks, etc). In other words, the idea is to make the design of social media reflect different user intuitions to do with closeness of relationship and physical location. For instance, notions of inner and outer networks, the idea of “running into” people by coincidence, notions of landmarks, and more.

Project conceptualization is in early stages now, so the project basis could change significantly by the start of CURIS. The CURIS student will play a significant role in defining both the project and their role on it. This project will employ a combination of methods (experimental, design, building) with the goal of developing a theoretical basis for the idea, designing and building a tool for social media experience (e.g. on Discord, Slack, Bluesky), and designing and running an evaluation. The CURIS student may play an active role in all of these steps, working closely with a PhD student. Project will likely go beyond the summer but intern participation beyond the CURIS period is negotiable.

Compensation Type: paid
Contact Professor: Michael Bernstein
Field of Research: hci
Prereqs
Enthusiasm for HCI and an interest in social media/online behavior. This will be a good fit for people interested in combining social science (psychology, communication, sociology) insights and theory with technology and design, and less of a good fit for those who want to do purely technical (i.e., coding) or purely experimental/qualitative work. We'd like to see some demonstrated HCI interest.
Recommended Background
HCI background and experience with web programming are desirable but not required (be sure to mention any experience in your application!).
Office Hours
CODA 3rd Floor open area by stairs - Feb 10, 10-11am

Students may request to add a new time, but that will be made available to the whole group, not one-on-one.






## AI-Powered Universal Code Verification: Translating Any Code to Formal Proofs with Synthetic Data for AGI
> What if every piece of code ever written—across all programming languages—could be automatically verified for correctness, safety, and reliability?

This project is a first step toward that vision. We are developing a self-optimizing AI pipeline that can take code from any language (Python, Verilog, Coq, Rust, etc.) and automatically translate it into a formally verifiable proof in Lean 4 or Dafny.

🔹 Why is this revolutionary?
✅ Verifying All Code, Everywhere:

Software today is prone to bugs, security vulnerabilities, and logical errors because most code is never formally verified.
By automating translation to verifiable formal logic, we enable a future where all code can be mathematically proven correct.
✅ AI That Improves Itself:

The system will not just translate, but continuously optimize itself using DSPy (Declarative Self-Optimizing Programs).
Instead of a static translation model, our AI learns the best way to translate and verify code over time.
✅ Bridging the Gap Between Code and Proofs:

Current AI models generate code but fail to verify its correctness.
This project ensures AI can both generate code and verify that it works—solving a fundamental problem in AI reasoning and safety.
✅ Scalability to Any Language:

Unlike manually crafted translation systems, this AI-driven pipeline can adapt to any programming language and fine-tune itself for different verification targets.
🤖 What is DSPy?
DSPy (Declarative Self-Optimizing Programs) is an advanced AI framework that automates and optimizes AI reasoning pipelines. Instead of relying on hand-engineered prompts and fine-tuning, DSPy:

Learns how to best structure queries and optimize translation from code to proof.
Generates and refines high-quality synthetic training data to improve formal verification performance.
Adapts to different LLMs (Llama, Gemma, Claude, GPT-4o) for state-of-the-art performance.
🎯 The End Goal?
🔹 An AI system capable of translating and verifying all existing and future code.
🔹 A self-improving pipeline that continuously enhances its ability to formalize and verify programs.
🔹 A major leap toward AI that not only writes but also verifies and improves software correctness.

Compensation Type: paid
Contact Professor: Sanmi Koyejo
Field of Research: ai, formal_methods_and_verification, ai
Prereqs
Strong background in machine learning (PyTorch, JAX, TensorFlow).
Familiarity with formal verification (Lean, Coq, Isabelle, Dafny, TLA+).
Experience with LLMs, DSPy, or synthetic data generation is a plus but not required.
Proficiency in Python and at least one other programming language (Rust, Coq, Verilog, etc.).
Recommended Background
Coursework or research in AI reasoning, NLP, theorem proving, or program synthesis.
Experience working with large-scale datasets and model fine-tuning.
Knowledge of automated formal verification tools.
Office Hours
e-mail me by appointment and follow this: https://brando90.github.io/brandomiranda/prospective-collaborations.html


## The Well-being of Social Support Providers on Social Media
> With the rise of online social interactions, understanding their impact on psychological well-being has become increasingly important. While it is established that receiving online social support can enhance the well-being of recipients, the effects on those providing support remain under-explored. This study aims to fill this gap by investigating how offering social support on platforms like Reddit and Twitter affects the well-being of the support providers themselves. We propose an A/B test experiment to determine whether engaging in supportive online behavior can improve providers’ well-being, assessed through self-reported surveys. To facilitate this, we will develop a Chrome extension that uses large language models like ChatGPT to identify posts in need of support. Participants in the experimental group will be prompted to offer support to several posts daily. We will measure any changes in their well-being through pre- and post-experiment surveys. This research seeks to uncover the potential psychological benefits of digital altruism, offering valuable insights into how structured online support initiatives can foster positive psychological states and encourage prosocial engagement on social media.

Compensation Type: paid
Contact Professor: Michael Bernstein
Field of Research: hci, ai
Prereqs
The ideal candidate is someone who is excited to work on projects at the intersection between HCI, NLP, and AI! Bonus points if you are interested in building AI interventions for social media platforms.

The following courses are helpful, but not required: HCI courses like CS 147, 247, 347, or 278; AI and NLP courses like CS 221, 224N, 124, 336 or any other equivalent courses.
Recommended Background
This project involves both technical development and field experiment design. A solid foundation in at least one of the following areas is  recommended:

(1) Experiences with Python and web development (e.g, HTML, CSS, JavaScript, React).
(2) Experiences with LLM-based applications and evaluating machine learning models.
(3) Experiences with collecting and analyzing data from social media APIs using Python.
Office Hours
Wed 2/059:30am-10:30am, CoDa E316
Wed 2/12 9:30am-10:30am, CoDa E316


## Algorithmic Fairness
> Machine learning and data analysis have enjoyed tremendous success in a broad range of domains. These advances hold the promise of great benefits to individuals, organizations, and society as a whole. Undeniably, algorithms are informing decisions that reach ever more deeply into our lives, from news article recommendations to criminal sentencing decisions to healthcare diagnostics. This progress, however, raises (and is impeded by) a host of concerns regarding the societal impact of computation. A prominent concern is that these algorithms should be fair.  Unfortunately, the hope that automated decision-making might be free of social biases is dashed on the data on which the algorithms are trained and the choices in their construction: left to their own devices, algorithms will propagate -- even amplify --  existing biases of the data, the programmers, and the decisions made in the choice of features to incorporate and measurements of "fitness'' to be applied.

We explore the foundations of Algorithmic Fairness.

Compensation Type: paid
Contact Professor: Omer Reingold
Field of Research: theory_of_computation, algorithms, ai
Prereqs
Most likely, the project will explore a theory perspective on Algorithmic Fairness and thus theory (or at least math) background will be useful. We will also consider students with experimental/ML background for an experimental investigation of the theory oriented approach.
Recommended Background
We will suggest papers for reading prior to the summer
Office Hours
February 12, 1:30-2:30
https://stanford.zoom.us/j/97645916777?pwd=KkKwUcSPQBIdOrfKxFyaNH6tEHJJvm.1


## Evaluating Human-Agent Collaboration
> The future belongs to humans and AI agents partnering together to solve different tasks. Prior works have released agentic benchmarks, however, there does not exist any agentic testing environments to evaluate how humans and AI agents can collaborate together while maintaining autonomy and control.  This is largely caused by the high cost associated with setting up such human-AI interaction environments, and unknown factors involved that can mimic naturalistic human-AI collaboration. In this work, we aim to develop an agentic testing environment that supports humans and agents collaborating together to complete diverse workflows, and a human-in-the-loop LLM-based automatic evaluation paradigm to evaluate the effectiveness of such human-AI collaboration.

Compensation Type: paid
Contact Professor: Diyi Yang
Field of Research: ai, hci, ai
Prereqs
We're looking for a self-motivated student to join our research team and contribute to the project’s various aspects including but not limited to data processing, idealization, and experimentation. Your main project will be on creating better evaluation or training methods for general- or specific-domain LLM agents to collaborate with humans.
Recommended Background
Solid experiences with large language models, training and evaluating machine learning models; students who have taken CS221/CS224N/CS336 or any other equivalent courses
Office Hours
Feb 6th, 10:30-12pm PT, Gates 3A open area
Feb 13th, 10:30-12pm PT, Gates 3A open area




## Value Elicitation for Community-Centered Design Feedback
> While we have various paradigms to help center communities’ needs and experiences within design, we also need evaluation tools that gauge when systems are and are not working for diverse communities. 

To do this, we are developing new design methods to evaluate communities’ experiences with design, as well as the values that they connect to these experiences. We hope that these methods can augment existing feedback mechanisms to advance systems that work across a range of communities, rather than for a privileged subset.

In this role, you will help to develop, pilot, launch and analyze a survey-based feedback mechanism for community-centered design evaluation. This work may include, but is not limited to, literature reviews, survey development, qualitative analysis, and quantitative analysis.

Compensation Type: paid
Contact Professor: Michael Bernstein
Field of Research: hci
Prereqs
Applicants should have a demonstrated interest in one or more of the following areas:

1) design methods and/or community-specific design (e.g.: social computing, accessible UI, user studies, etc.)
2) quantitative data analysis and related software (e.g.: Python, R, statistics, etc.)
3) qualitative data analysis (e.g.: grounded theory, thematic coding, interviewing)
Recommended Background
Will have ideally taken CS147 or equivalent
Office Hours
February 10th, 3-4 pm, CoDa 3rd Floor (near room E314)
February 17th, 3-4pm CoDa 3rd Floor (near room E314)




## Creation of a new introductory computer science / electrical engineering class
> In this project, you'll help create a new introductory computer science + electrical engineering class that introduces students to computer science in a way that emphasizes hands-on interactivity with the real world. This class will probably be piloted as an IntroSem, then eventually offered as an alternative to CS 106A that feeds into CS 106B and intro EE classes. If hyper-successful, one could imagine publishing the course materials beyond Stanford and trying to pilot it at the high-school level or at other universities.

Here's the tentative plan as we envision it: On day 1, we give every student a toy xylophone and start with understanding its eight notes from a musical, physical, and signals perspective, in pure analog. We can ask, "how would you get these signals, seemingly continuous and infinite, into a computer?" That leads to a discussion about sampling and the Nyquist-Shannon sampling theorem, which is an amazing and counterintuitive result.

I'd like the students to start writing programs in a simple language where you can learn the entire syntax in one class (so not C++ and not Python -- probably WebAssembly in its text format) that runs on their own cell phones and computers and gives immediate feedback. I'd love to give them a great development-and-learning environment. WebAssembly is so structured and syntax-minimal that I think we can give them an editor that makes it impossible to have a syntax error or a type/validation error, and lets them visualize and debug the execution of the code in place, and tells them how many milliseconds a function takes to execute on sample inputs. (I'd really like them to experience programming with this immediate connection to the real world and real-world performance.)

In the tentative plan, the students will write a program to listen to samples from the microphone and determine "is the low C on the xylophone playing right now or not" in real time, running on their own cell phone or computer. And then "which of the eight notes of the toy xylophone is most likely to be playing now," again in real time. From that, we can talk about how these eight symbols can convey up to 3 bits of information apiece. The students will devise a code that maps the 26 letters of the alphabet onto the 8 notes of the xylophone (without repeating any note twice in a row), and write a decoder for it. For the midterm "exam," we'll give them a sentence in English and have them "play" it on the xylophone, and their cell phone should print out the letters one-by-one as they play.

Now that they've done that, we'll ask the students to write another program that does what they just did by hand -- take an English sentence and "play" it as synthesized audio. From that point, the students can make a little network where one person's cell phone sends the data (over audio) and another one receives it, and then we can ask them crank up the speed faster and faster where it can't really be made by hand on a xylophone anymore and it's just bits flying back and forth from one student's phone to another, and we can talk about another of Shannon's amazing results: that there is no necessary tradeoff between the communication rate and the error rate (as long as the information transfer rate is below the threshold "capacity"). Which was a very surprising result in 1948 and has arguably enabled the entire information age, digital video compression, etc.

We'd have a lot of work to do to make this class possible -- somebody has to try to do these programming assignments (as if they were a student) and verify that it's really possible, and we'll create the structured editing/development environment where we feel proud of putting first-time programmers in front of it and introducing them to computer programming in this way. And none of these plan is set in stone -- we'll have to adapt it based on whatever we learn or whatever proves infeasible. But I am excited to attempt it!

Compensation Type: paid
Contact Professor: Keith Winstein
Field of Research: software_engineering, programming_languages, ai
Prereqs
Interest in CS education

Programming experience and maturity

Some experience with Web programming

Willingness to learn WebAssembly

Ideally, CS 242 or some experience with the idea of structured editing and software engineering environments
Recommended Background
"A Digital Signal Processing Primer" (by Ken Steiglitz) or similar

WebAssembly text format specification: https://webassembly.github.io/spec/core/text/index.html
Office Hours
February 4, 4 p.m., CoDa W342


## Hardware Memory Systems Verification
> Project 1: Formally Verifying Heterogeneous Cache Coherence

Heterogeneity (i.e. integrating diverse types of hardware into a single system) is an indispensable element of modern hardware design. Further, there are programmability and performance benefits in having these different hardware components share memory in a fine grained way. Such fine-grained memory sharing necessitates the development and verification of a cache coherence protocol that can accommodate diverse hardware components. However, implementing cache-coherent shared memory in heterogeneous systems is challenged by memory consistency model (MCM) mismatches among these compute elements: what the system-wide MCM should be and how it should be enforced are not well-defined. To this end, we recently developed MemGlue [FMCAD24], a cache coherence protocol for heterogeneous systems whose key insight is to implement the C11 MCM as the system-wide MCM of any MemGlue-enabled system.

As part of this work, we developed a manual proof of correctness that MemGlue does correctly uphold the C11 MCM. However, in order to feel confident that the manual proof is correct, we are seeking a CURIS student to mechanize this proof in a proof assistant. For this project, you will fully define the MemGlue protocol as described in its state transition tables within the language of a proof assistant. You will then define the C11 MCM within your proof assistant. Finally, you will develop a proof that no program outcome that is forbidden by the C11 MCM will be allowed by the MemGlue protocol, thereby proving that MemGlue does in fact uphold the C11 MCM.

Recommended Background:
* The core of this project is developing a mathematical proof of correctness.
* Students should be familiar with the concept of mathematical proof.
* Students with prior experience using a proof assistant (e.g. Coq, Lean, Isabelle) or prior experience programming in a functional language (e.g. Haskell, OCaml) will be best suited for this project, although this is not necessary.
* A basic knowledge of memory consistency models will be helpful, but is not required.

Prerequisites/Preparation:
* CS106 and CS103
* Reading the MemGlue paper prior to the start of the project


Project 2: Formal Verification of Hardware Queues

To improve instruction and memory level parallelism, modern hardware systems—such as processors, network switches, and accelerators—commonly use priority queue structures to buffer in-flight instructions, memory requests, and other data.

The correctness of these hardware designs thus relies on the queues' implementation. For example, queues must ensure data integrity, prevent duplication, and maintain correct order—either FIFO or based on specified priorities—for tasks like correctly enforcing the execution order of memory instructions (critical for parallel program correctness).
 
Over the past two decades, much of the work on queue verification, particularly in distributed systems, has relied on interactive theorem proving. While this approach scales well, it demands significant manual effort to encode abstractions and invariants. In contrast, hardware verification methods, such as model checking, are more automated but struggle to scale with common queue components like memory arrays and pointers.

We are seeking CURIS students to:
1) Explore and identify abstractions and invariants useful for queue verification through theorem proving.
2) Apply these invariants to model-check hardware queue implementations.
3) Design invariant synthesis procedures to enable scalable hardware verification.

Recommended Background
- Background knowledge in logic/proof through CS 103, or (even better) CS 257 or CS 357S
- It is helpful, but not required, to have some background in theorem proving or functional programming

Prerequisites
- CS154 Introduction to the Theory of Computation, or CS106B Programming Abstractions and CS103 Mathematical Foundations of Computing or equivalent.

Compensation Type: paid
Contact Professor: Caroline Trippel
Field of Research: architecture_and_hardware, formal_methods_and_verification, ai
Prereqs
See above per project.
Recommended Background
See above per project.
Office Hours
By appointment. 
Project 1: Contact Rachel Cleaveland: rcleavel@stanford.edu
Project 2: Contact Yao Hsiao: yaohsiao@stanford.edu


## Systematic Reviews of the Scientific Literature using LLMs
> Systematic reviews are a cornerstone of the scientific literature. They are used for synthesizing research findings, designing guidelines, and informing science-guided policies. They are particularly common and impactful in the health sciences, education, and psychology. However, current methods of conducting systematic reviews are labor-, time-, and resource-intensive. The systematic review process typically entails researchers reviewing hundreds or thousands of abstracts and extracting data from dozens of articles. Current large language models (LLMs) are capable of extracting content and information from text - and some image - data, but the extent to which they are capable of systematically and consistently extracting information from scientific studies and providing comprehensive and unbiased information is unknown. We are developing and researching the capacity of LLMs to systematically and reliably screen and review scientific studies. We are focusing on LLM capabilities that involve grounding, reproducibility, trustworthiness, and RAG. We are looking for a summer CS intern to join our team.

Compensation Type: paid
Contact Professor: Eran Bendavid
Field of Research: ai, databases
Prereqs
- Python, including scripting APIs
- Preference for completing several CS200-level courses such as AI principles (CS221), deep learning (CS230), or NLP (CS224)
-
Recommended Background
Interest in AI and language models, and a desire to develop tool for scientists
Office Hours
Thursdays 3pm, Encina Commons 102




## Extending the Lean-SMT Tactic
> See URL

Compensation Type: paid
Contact Professor: Clark Barrett
Field of Research: formal_methods_and_verification
Prereqs
See URL
Recommended Background
See URL
Office Hours
Mondays 11-12am, CoDa W334 or by appointment






## Prototyping novel cardiovascular sensors
> Wearable devices that measure signals from our bodies have become commonplace. Today's smartwatches only scratch the surface of the possibilities for continuous health sensing. We are designing and testing new sensors and algorithms to measure the cardiovascular system non-invasively. We hope this technology can help people better diagnose and manage common health issues like high blood pressure.

In this role, you will do hands-on work to help design, fabricate and test novel wearable cardiovascular sensors. This work may include 3D printing molds, casting silicone, assembling circuits, running bench-top tests, and assisting in running studies to test the sensors with human subjects.

Compensation Type: paid
Contact Professor: James A Landay
Field of Research: hci, biocomputation
Prereqs
1. Familiarity with Python
Recommended Background
The strongest applicants will have demonstrated interest in one or more of the following areas:
1. Human biology (e.g. anatomy, physiology, especially cardiovascular)
2. Printed circuit board design (e.g. KiCAD)
3. Electronics design (e.g. breadboarding, soldering)
4. Laboratory experience (e.g. prior research wet lab role)
Office Hours
- Feb 7, 4-5pm, Coda building (room TBD)
- Feb 10, 11am-12pm, Coda building (room TBD)




## Next-Generation Graph Transformers for Relational Deep Learning
> Mentor: Charilaos Kanatsoulis
Next-Generation Graph Transformers for Relational Deep Learning
Keywords: Relational Deep Learning, Graph Transformers, Data Processing, Information Retrieval

The recent advances in natural language processing using large language models have opened unprecedented opportunities in the field of deep learning. Foundation models and transformer architectures have seamlessly extended to various domains, including image generation, biology, and medicine, handling diverse data types. This expansion has stimulated the integration of such models into graph-related tasks. Although graph transformers and graph attention mechanisms have been introduced, they are not fully harnessing the inherent structure of relational graphs. In this project, we aim to address this gap by developing a graph transformer for relational deep learning and reasoning. 

Our goal is to build this model from scratch, which includes:

- data collection and processing: database collection / synthetic database generation 
- model design: design of graph attention mechanisms and positional encodings that harness the unique patterns of relational graphs
- experimentation with various databases, including but not limited to e-commerce platforms, medical databases, sports databases
 - model evaluation and extension to retrieved augmented generation tasks. 

We will develop a robust, computationally efficient, and versatile architecture capable of adapting to a range of tasks, including but not limited to information retrieval, answering complex queries, drug repurposing, and product recommendation. The successful deployment of our model could have significant implications in database management, social network analysis, and biology.

We are looking for highly motivated students who have a background in machine learning, and deep learning (e.g., CS224W, CS 230, CS231N etc.). Strong coding skills and proficiency in PyTorch are required. Experience with distributed computing and data processing is a plus.

Compensation Type: paid
Contact Professor: Jure Leskovec
Field of Research: ai, databases, scientific_computing
Prereqs
Strong coding skills and proficiency in PyTorch.
Recommended Background
Machine learning and deep learning (e.g., CS224W, CS 230, CS231N etc.)
Office Hours
Friday February 7, 4-5 pm, Gates 392
Friday February 14, 3-5 pm, Gates 392


## Bayesian inference for cardiovascular sensing
> Wearable devices that measure signals from our bodies have become commonplace. Today's smartwatches only scratch the surface of the possibilities for continuous health sensing. We are designing and testing new sensors and algorithms to measure the cardiovascular system non-invasively. We hope this technology can help people better diagnose and manage common health issues like high blood pressure.

In this role, you will apply statistical inference techniques to encode biological domain priors into bio-signal inference algorithms. Techniques may include Sequential Monte Carlo and Variational Bayesian Inference.

Compensation Type: paid
Contact Professor: James A Landay
Field of Research: biocomputation, ai, hci
Prereqs
Proficiency in Python (e.g. CS 106A)
Introductory probability (e.g. CS 109)
Demonstrated experience with one or more of the following:
1. Expectation maximization
2. Variational methods
3. Monte Carlo methods
Recommended Background
The strongest applicants will have demonstrated interest in one or more of the following areas:
1. Human biology (e.g. anatomy, physiology, especially cardiovascular)
2. Signal processing (e.g. Fourier analysis, filter design)
3. Numerical methods
4. Optimization
Office Hours
- Feb 7, 4-5pm, Coda building (room TBD)
- Feb 10, 11am-12pm, Coda building (room TBD)


## Making Language Models More Concept-Aware
> Concepts are fundamental to human cognition, yet current large language models (LLMs) primarily operate at the level of tokens rather than conceptual abstractions. Our research explores methods to make LLMs more concept-aware, improving their alignment with human intuition and reasoning.

One key approach we developed (discussed in RQ3 of this paper: https://aclanthology.org/2023.findings-emnlp.877.pdf) is a post-processing method that restructures model outputs to better capture conceptual relationships. The original implementation was built on BERT (an encoder-based model), and I have also partially implemented it using T5 but never completed it. This internship focuses on adapting and extending this method to decoder-based models (e.g., LLaMA, GPT), demonstrating that the approach generalizes across architectures while preserving its benefits. 


Key Tasks for the Intern
* Implement the post-processing method on a state-of-the-art decoder-based LLM (e.g., LLaMA, GPT) while adapting the method to accommodate the structural differences between encoder and decoder models. 
* Run human evaluation experiments to assess whether the method improves conceptual alignment and robustness.

Why This Matters?
Our previous results indicate that post-processing concept structures improves model robustness and better matches human intuition. If we can successfully extend this method to decoder-based models, it will further support the argument that concept-aware processing is a generalizable improvement for LLMs, with potential applications in reasoning, planning, and decision-making.

Compensation Type: paid
Contact Professor: Dan Jurafsky
Field of Research: ai, hci
Prereqs
Stanford students interested in this research project should have a strong background in NLP and Deep Learning (CS 224N) and Machine Learning (CS 236/CS 229). Past experience in NLP / Deep Learning is a plus. Additionally, proficiency in programming (especially in PyTorch), familiarity with popular deep learning frameworks, and strong self-motivation are required.
Recommended Background
This project is ideal for students with:

* Experience in deep learning and NLP, particularly transformer architectures. 
* Familiarity with decoder models (e.g., LLaMA, GPT). 
* Proficiency in Python and deep learning frameworks (e.g., PyTorch, TensorFlow). 
* Interest in cognitive modeling, human-like AI reasoning, and model interpretability.
The project involves non-trivial adaptation work, requiring creativity and a deep understanding of LLM architectures. If you're interested in pushing the boundaries of concept-aware AI, we'd love to hear from you!
Office Hours
Friday Feb 7, 10:30-11:30am, GATES 323
Tuesday Feb 11, 1-2pm, GATES 323
Thursday Feb 13, 10-11am, GATES 323

Please email if you’re planning to come for headcount purposes. Thanks!


## Interactive narrative-driven learning experiences in the real world
> The Smart Primer research team is developing narrative-driven experiences that engage students to learn in the real world. We design and create educational experiences where students can interact with and learn about the physical world around them, mediated by a mobile or tablet device that intelligently adapts the story based on the students' inputs, preferences, and physical environment.

We are looking for student researchers this summer to help with developing our mobile app (built in Unity/C#), refining our LLM architecture, and/or running a user study of our app.

Compensation Type: paid
Contact Professor: James A Landay
Field of Research: hci
Prereqs
Strong programming experience, demonstrated interest in education, and willingness to work with children (our likely audience)
Recommended Background
The strongest applicants will have experience in at least one of the following areas: 
(1) Unity development (experience in developing with AR in Unity is a plus)
(2) LLM prompting (experience developing larger systems that do more than just prompt LLMs is a plus)
(3) Human-computer interaction techniques and principles, e.g. CS 147 or CS 247x series
(4) Educational software development, including relevant personal projects
Office Hours
TBD


## Grounding and Transfer for Generalist Robot Policies
> Training policies that leverage large amounts of data to operate in a diverse range of environments is a promising approach for generalist robots. Although many prior works have involved scaling up the breadth and diversity of robot demonstration data, the resulting models still face challenges in adapting to slight scene variations such as different spatial locations, unseen objects, and different lighting conditions. 

In this project, we will think more deeply about the type of grounding robot policies need. We aim to investigate how different forms of generalization can be achieved, and how better data and objectives can improve performance across diverse environments. In particular, we aim to get answers for these three key research questions:
1. What type of generalization is actually happening for large-scale robot policies?
2. Are there better sources of data and objectives than traditional imitation learning from (state, action) demonstration data to achieve better generalization benefits?
3. Can we develop new grounding objectives to unlock better transfer from cross-embodiment robot data? (i.e. data from different robots, humans, etc.)

You will work with PhD students in ILIAD (Intelligent and Interactive Autonomous Systems Group) with Professor Dorsa Sadigh. The project involves both simulation-based research and hands-on work with real-world robots in the lab.

Compensation Type: paid
Contact Professor: Dorsa Sadigh
Field of Research: robotics, ai
Prereqs
CS 221, CS 229
Recommended Background
Students should have an interest working with real-world robots. Experience with Python programming, machine learning (e.g., CS229, CS221), and a deep learning framework such as PyTorch is required.
The following are a huge plus:
- Experience in reinforcement learning or imitation learning (e.g., CS224R)
- Experience working on significant programming projects
- Experience working with robots in simulation or the real world
Office Hours
February 10, 1pm-2pm Gates 210


## An Extensible Platform for Asking Research Questions of High-Speed Network Links
> Network and security researchers routinely need to analyze network traffic. Past research leveraged network traffic analysis to study censorship, circumvent endpoint blocking, identify sources of unencrypted traffic, track the spread of malware, understand botnets, study protocol deployments, and more. On the other hand, threat actors with significant resources also deploy large-scale network traffic analysis for systemic censorship and surveillance.

As network speeds have increased, researchers – operating with limited resources – have lost the ability to easily ask sophisticated research questions of reassembled, parsed network traffic. The Empirical Security Research Group (ESRG) lab is building an open-source framework tool that aims to allow researchers to conduct scoped network traffic analysis without contributing to broad censorship or surveillance use-cases. 

We are seeking a CURIS student to contribute to experimental evaluation and/or open-source development. We expect the CURIS student to make a meaningful contribution to and be a supporting author on a paper that we will submit.

Examples of possible projects could include:
* Designing and running reproducible experiments to evaluate our implementation
* Developing applications to demonstrate complex, real-world use-cases for network traffic analysis
* Implementing requested features and fixes, such as multi-core callback scheduling, database integration, optimized memory usage for UDP traffic, parallel RegEx search, handling encapsulated traffic, etc.
* Developing functionality testing infrastructure to maintain the open-source codebase

Compensation Type: paid
Contact Professor: Zakir Durumeric
Field of Research: networking, security, operating_systems
Prereqs
* Systems programming experience at the level of at least one advanced systems course (e.g., CS144, CS140) and/or work experience
* Background knowledge in networking through CS144 or equivalent
* Experience programming in Rust OR willingness to work with us in the spring (for 3 units of academic credit or, if eligible and pre-arranged, federal work-study funding)
* It is helpful, but not required, to have some background in network security (e.g., CS155 or equivalent).
Recommended Background
See prerequisites.
Office Hours
Tuesday 2-3pm (2/4 only) - CoDA E303 
Wednesday 10-11am (2/5, 2/12) - CoDA E303
Or arranged by email


## Capturing Crowd Social Human Interactions with Dynamic Surroundings
> Goal & Description
Develop a scalable pipeline for capturing multi-person interactions in dynamic environments, integrating human-human and human-object interactions across indoor and in-the-wild settings, to support research in AI-driven social behavior modeling and human-object collaboration.

Research Background and Motivation
Understanding human interactions in diverse and dynamic environments is crucial for advancing AI-driven perception, robotics, and behavior modeling. Many real-world scenarios involve not only human-human interactions but also extensive human-object interactions. These interactions occur in both structured (indoor) and unstructured (in-the-wild) settings, requiring AI systems to interpret and predict complex social behaviors.
Existing datasets primarily focus on single-person or dyadic interactions with objects, limiting their applicability to broader, more complex interactions involving multiple participants. Moreover, most available datasets are constrained to controlled indoor environments, lacking the diversity needed for real-world deployment. Real-world scenarios involve varying object affordances, unpredictable environmental factors, and complex collaboration dynamics that demand extensive data collection beyond controlled settings.
To bridge this gap, we propose a dataset that captures crowd social human interactions with dynamic surroundings, covering both human-human and human-object interactions across indoor and in-the-wild scenarios. This dataset will include rich annotations detailing object manipulations, group dynamics, and spatial relationships in diverse environments. By providing large-scale, real-world interaction data, our dataset will enable AI models to generalize better to complex, multi-agent object interaction tasks, supporting advancements in robotics, scene understanding, and behavior prediction.

Related Work
Modeling multi-person interactions requires capturing spatial-temporal dependencies between individuals and objects. Existing methods primarily focus on two-person interactions, addressing tasks such as reaction synthesis [1, 2] and interaction generation [3, 4]. While some methods extend to multi-person interactions [5, 6], they remain limited to motion generation and do not support broader multi-modal tasks.
Several datasets provide multi-person motion sequences, including CMU-Panoptic [7] and 4DAssociation [8]. However, many lack detailed textual descriptions, limiting their usability for multimodal AI research. Human-object interaction datasets such as BEHAVE [9] and Something-Something [10] provide insights into human-object interactions but often lack complex multi-agent dynamics. Our dataset aims to address these gaps by capturing large-scale human-human and human-object interactions across diverse environments, supporting AI-driven social behavior modeling and collaboration.

Methodology
1.	Capture Setup
Setting up a multi-sensor capture environment equipped with various modalities, including cameras, IMUs, and infrared-based motion capture systems.
2.	Static Camera&Senser Synchronization
Ensuring precise alignment and synchronization of multiple static cameras and sensors to achieve accurate multi-view data acquisition.
3.	Multi-Modality Calibration
Aligning different 3D systems into a unified coordinate frame by calibrating various sensing modalities across multiple sources.
4.	Multi-Instance Segmentation and Video Tracking
Employing state-of-the-art video segmentation algorithms, such as SAM2, to accurately segment video frames and track multiple individuals in crowded scenes.
5.	ID Matching
Implementing robust identity association techniques to ensure consistent tracking of individuals across frames and camera perspectives.
6.	Triangulation and SMPL Parameter Fitting
Reconstructing 3D human motion by triangulating key points and fitting SMPL parameters for precise body pose and shape estimation.
7.	Iteratively Object 6D pose Fitting
Refining object pose estimation through iterative optimization techniques to achieve accurate 6D pose reconstruction.

References
1.	Ghosh, Anindita, et al. "Remos: 3d motion-conditioned reaction synthesis for two-person interactions." European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.
2.	Xu, Liang, et al. "ReGenNet: Towards Human Action-Reaction Synthesis." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
3.	Liang, Han, et al. "Intergen: Diffusion-based multi-human motion generation under complex interactions." International Journal of Computer Vision (2024): 1-21.
4.	Javed, Muhammad Gohar, et al. "InterMask: 3D Human Interaction Generation via Collaborative Masked Modelling." arXiv preprint arXiv:2410.10010 (2024).
5.	Shan, Mengyi, et al. "Towards open domain text-driven synthesis of multi-person motions." European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.
6.	Fan, Ke, et al. "Freemotion: A unified framework for number-free text-to-motion synthesis." European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.
7.	Joo, Hanbyul, et al. "Panoptic studio: A massively multiview system for social motion capture." Proceedings of the IEEE international conference on computer vision. 2015.
8.	Zhang, Yuxiang, et al. "4D association graph for realtime multi-person motion capture using multiple video cameras." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.
9.	Bhatnagar, Bharat Lal, et al. "Behave: Dataset and method for tracking human object interactions." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.
10.	Goyal, Raghav, et al. "The" something something" video database for learning and evaluating visual common sense." Proceedings of the IEEE international conference on computer vision. 2017.

Compensation Type: paid
Contact Professor: Ehsan Adeli
Field of Research: ai, vision
Prereqs
•	Strong programming skills in Python
•	Experience with deep learning frameworks (e.g., PyTorch, TensorFlow)
•	Familiarity with computer vision and convex optimization
•	Basic knowledge of data collection and preprocessing
•	Ability to work with multi-modal data (e.g., video, text, motion)
Recommended Background
Prior experience with human motion modeling or 3D/4D reconstruction
•	Understanding of multi-person and human-object interaction modeling
•	Coursework or research in computer vision, machine learning, or deep learning
•	Experience with data annotation tools and large-scale dataset curation
•	Experience using large language models (LLMs) (e.g., GPT) and designing effective prompts for tasks such as data augmentation, annotation, or interaction modeling
Office Hours
Feb 12, 2-3pm, Gates 280


## Distributed Library Development in Legion
> The goal of this project is to create a usable partitioning library for graph and mesh data structures for applications developed in the Legion runtime system. The project would entail using the METIS/ParMETIS libraries in conjunction with Legion to develop a friendly user-facing library that external projects may use.

Compensation Type: paid
Contact Professor: Alex Aiken
Field of Research: scientific_computing, distributed_systems
Prereqs
Parallel computing experience (something like CS149) would be good.
Recommended Background
Background in parallel or distributed programming is desired.
Office Hours
Wednesdays, 2-4PM, Gates 494
Thursdays, 2-4PM, Gates 494


## From Monolith to Mosaic: Amplifying Creative Divergence with Pluralistic AI
> As large language models become ever more popular, worrisome patterns are emerging: what happens when models produce fluent but bland writing, or when they produce arguments that sound plausible but don’t align with our views? We’re trending towards a future of AI that allows users to efficiently carry out more tasks than ever—but only in the same way as everyone else. How could powerful modern AI systems not lead users to efficiently converge, but empower them to creatively diverge—to maintain and even amplify their unique voice? 

In this project, we will be exploring how we can promote more divergent thinking through many pluralistic, specialized AI models rather than a monolithic, generic model. We will build interactive systems for everyday, non-technical users to create and orchestrate custom AI teams to amplify their creative vision. For example, a young writer could set up a team of their favorite short story authors and get their take on early story ideas; a researcher could build a team of professors trained in different experimental methods to review their paper draft; or student could assemble a team of foundational figures in psychology to debate about the student’s essay topic. We will explore the broad design space of methods for: instantiating and selecting AI team members, determining how these AI teams collaborate, and demonstrating novel interactions that can be unlocked once we have these dynamic, personalized teams of AI models. The project will develop both new interfaces and new algorithms—all in service of improving everyday users’ expressivity and sense of ownership over AI systems.

Compensation Type: paid
Contact Professor: Michael Bernstein
Field of Research: hci, ai
Prereqs
This project will involve both the development of new algorithms and technical approaches to instantiate our models, as well as the creation of new interfaces and visualizations for our interactive system. Thus, we recommend a solid foundation of experience in at least one of these areas:
(1) Experience with Python, computational notebooks, and LLM-based applications
(2) Experience with web development (ex: tools like React or Svelte, Flask, HTML/CSS/JavaScript)
Recommended Background
The ideal candidate is someone who is excited to work on projects at the intersection between HCI and AI! Bonus points if you are interested in topics like democratizing AI development or building tools for thought.
The following courses are helpful, but not required: HCI courses like CS 147, 247, 347, or 278; AI courses like CS 221,  224N, or 124.
Office Hours
Wed 2/12 10:30-11:15am, CoDa 3rd floor near stairs


## Using AI / Smart Devices to Help Patients with Early Alzheimer's Disease
> Alzheimer's Disease affects millions of people around the world, and many patients in the early stages of the disease have significant memory deficiencies. One common memory concern is in remembering conversations -- often, after a conversation many details of the conversation are lost.  This project will involve writing a mobile application to leverage AI transcription abilities to provide live transcription of conversations. Ideally, the application would be able to differentiate between multiple parties in discussion, and be able to provide an easy interface for a user to see the transcription.  We will also try to work with the Stanford Alzheimer’s Disease Research Center to test the application with real patients in the early stages of the disease.

Compensation Type: paid
Contact Professor: Christopher Gregg
Field of Research: ai, hci, software_engineering
Prereqs
CS106B, preferably some iOS programming background. CS147L would also be worthwhile.
Recommended Background
You should be willing to help write an iOS application for voice transcription, so a solid programming foundation is necessary. Former iOS programming is not required, but you must be willing to spend time learning SwiftUI and iOS programming in general.
Office Hours
Tuesdays 2:30pm-3:30pm


## Measuring and Understanding Implications of Human-like AI
> As language models are increasingly imbued with anthropomorphic (perceived as human-like) traits like personas, politeness, and unique personalities, scholars have raised concerns about how such anthropomorphic system behaviors may lead to harmful outcomes, such as users overrelying on generated text or developing emotional dependence on LLM systems. Potential projects include (1) measuring and understanding adverse impacts of anthropomorphic AI on user agency and how these effects vary across populations and (2) developing NLP methods to steer LLMs to make them have appropriate levels of human-likeness.

Compensation Type: paid
Contact Professor: Dan Jurafsky
Field of Research: ai, hci
Prereqs
N/A
Recommended Background
You have demonstrated interest in one or more of the following areas:
- LLM alignment
- AI ethics
- Human-AI interaction
- Research informed by humanistic theories, social science, or other interdisciplinary perspectives

The following are also recommended:
- Proficiency in Python and familiarity with deep learning frameworks like PyTorch - Experience with survey design, user studies, and/or statistical analysis
- Completion of CS200-level courses such as deep learning (CS230) or NLP (CS224n)
Office Hours
Friday Feb 7, 2-3pm, Gates 304,
Friday Feb 14, 2-3pm, Gates 304,
or arranged by email (myra1@stanford.edu)
Please email if you’re planning to come for headcount purposes. Thanks!


## Improving Drug Safety and Selectivity with Machine Learning
> Many therapeutic drug treatments work by changing the activity of a specific protein in the human body that is involved in a disease. However, protein targets responsible for different biological functions may be highly similar or closely related evolutionarily. Often, this similarity results in drug molecules having off-target activity, which can cause serious side-effects, reduced efficacy, or prevent a drug's medical use altogether.

Our goal is to develop a machine learning model that can incorporate experimental information about molecules that are known to bind to closely related protein targets in order to better predict compounds that are selective for one target over another. We will use a large labeled dataset of protein-small molecule interactions and 3D structural information about our related target proteins to improve our ability to make predictions about protein selectivity, and design more selective and safer drug therapies for the future.

In this project, you will learn how to use current state-of-the-art tools for computational drug discovery, and learn about applying machine learning to computational structural biology. You will experiment with applying different machine learning methods, and learn how to evaluate model results in the context of drug discovery and biology. And hopefully, you'll learn cool things and have fun.

If you're interested in biology, machine learning, or drug discovery, apply! Outside of Python experience, no other specific coursework is required, and feel free to contact me with any questions.

Compensation Type: paid
Contact Professor: Ron Dror
Field of Research: biocomputation, ai, scientific_computing
Prereqs
CS106A and CS106B, previous experience / familiarity with Python, and some previous high school biology and chemistry background.
Recommended Background
No hard requirements, but a plus if you've taken CS 279 or BIOMEDIN 214.
Office Hours
2 - 3 PM on Tuesday, February 11th - Virtual

Virtual Link: https://stanford.zoom.us/j/9141916849?pwd=S1VZeTFrOWJwY0gxVEFGdUlqeXhRUT09


## UniFormal – The Universal Code-to-Formal Verification Benchmark for AGI
> 🚀 Vision: Creating the Standard for AI-Driven Formal Verification
AI benchmarks like MMLU (Massive Multitask Language Understanding) and HumanEval (Python Code Generation) have driven AI research forward by setting clear, measurable goals for model performance.

Now, we propose UniFormal—a benchmark that will define the next frontier of AI progress:
🔹 Translating arbitrary code + natural language into formally verifiable proofs in Lean 4 or Dafny.
🔹 Evaluating AI’s ability to generate structured, correct, and logically provable programs.
🔹 Becoming the gold standard for AI-driven formal verification.

🛠 Benchmark Structure: What Does UniFormal Measure?
UniFormal will test an AI’s ability to translate code (and optionally natural language) into formally verified representations. It will contain:

1️⃣ Code-to-Proof Translation (Code → Lean 4 / Dafny)
✅ Input: Raw code from various languages (Python, Verilog, Rust, Coq, etc.).
✅ Task: Translate into formally verifiable Lean 4 or Dafny code.
✅ Metric:

Correctness (Pass@K) – Does the generated proof successfully verify?
Faithfulness (CE/TFA) – Does the output preserve the original intent of the code?
Compactness – Does the AI generate unnecessarily complex proofs?
2️⃣ Code + Docstring to Formalization (Code + NL → Lean 4 / Dafny)
✅ Input: Code snippets + English explanations (docstrings, inline comments).
✅ Task: Convert into formally correct Lean 4 / Dafny proofs.
✅ Metric:

Mathematical Validity – Do the generated theorems actually hold?
Information Completeness – Does the output include all necessary assumptions?
Reasoning Complexity – Can the AI handle deeper reasoning tasks?
3️⃣ Natural Language to Proof (NL → Lean 4 / Dafny)
✅ Input: Pure natural language descriptions of a theorem or software behavior.
✅ Task: Generate a correct and verifiable formal representation in Lean 4 / Dafny.
✅ Metric:

Semantic Alignment – Does the proof match the original statement?
Formal Correctness – Does the theorem compile and validate?
Expressiveness – Can the AI handle ambiguous or under-specified inputs?
🧪 Benchmark Dataset: Where Will We Get Data?
To ensure high-quality, diverse training and evaluation data, UniFormal will source from:

🔹 Public Repositories (GitHub, CodeParrot, The Stack) – Extracting real-world Python, Rust, and Verilog code.
🔹 Mathematical Libraries (Lean's Mathlib, Coq's Stdlib) – Gathering verified proofs.
🔹 Theorem Proving Challenges (IMO, Putnam, MATH) – Converting math problems into formal statements.
🔹 Existing AI Formalization Datasets – Incorporating examples from MetaMath, ProofNet, and LeanStep.

🔥 Why UniFormal Will Drive AI Progress
📌 For the First Time: AI Performance on Formalization Will Be Measurable!
📌 A New Gold Standard for AI Reasoning, Bridging Code & Formal Proofs.
📌 Encourages AI Researchers to Improve LLMs for Verifiable Code Generation.
📌 Creates a Structured Roadmap Toward AI That Can Verify the World’s Code.

UniFormal isn’t just a benchmark—it’s the foundation for AI-powered formal verification at scale.

Compensation Type: paid
Contact Professor: Sanmi Koyejo
Field of Research: ai, formal_methods_and_verification, programming_languages
Prereqs
To participate in this project, students should have:
✅ Strong programming skills in Python and at least one additional language (e.g., Rust, Coq, Verilog, Lean 4, Dafny).
✅ Background in machine learning (ML) and deep learning (PyTorch, JAX, TensorFlow).
✅ Experience with large language models (LLMs) and NLP techniques (DSPy knowledge is a plus).
✅ Familiarity with formal methods and theorem proving (Lean, Coq, Isabelle, Dafny, TLA+).
✅ Basic knowledge of program verification or automated reasoning.
Recommended Background
The following experience is not required but will be beneficial:
🔹 Coursework or research in AI reasoning, NLP, theorem proving, or program synthesis.
🔹 Familiarity with prompt engineering, DSPy pipelines, or LLM fine-tuning.
🔹 Experience working with large-scale datasets for machine learning training.
🔹 Knowledge of formal logic, symbolic reasoning, or proof assistants.
🔹 Understanding of program synthesis and formal verification frameworks.

Students from diverse backgrounds (ML, formal methods, NLP, or theoretical CS) are encouraged to apply. 🚀
Office Hours
send me a message and follow these instructions: https://brando90.github.io/brandomiranda/prospective-collaborations.html


## Building the Conversational AI Agent’s Virtual Body: A Preliminary Multi-Modal Dataset
> Goal & Description
Develop a robust pipeline for capturing dyadic conversational interactions with multi-modal data sources, including text, audio, video, and motion, to facilitate research in AI-driven conversational agents.

Research Background and Motivation
Human interactions are inherently social, involving complex multi-agent collaborations across multiple modalities—language, audio, motion, and vision (RGB). Effectively modeling these multi-modal interactions is crucial for advancing conversational AI, virtual reality (VR), human-computer interaction, and robotics.
	•	VR and Gaming: Multi-modal social interactions enhance immersion, allowing AI characters to interpret and respond to verbal and non-verbal cues.
	•	Human-Computer Interaction: Synchronizing motion and audio with dialogue improves the realism and effectiveness of AI assistants.
	•	Robotics: Understanding multi-modal human interactions enables AI agents to navigate human environments more naturally.

Recent AI advancements have significantly improved text-based multi-agent collaboration, with frameworks like ChatDev and Generative Agents simulating agent conversations. However, these approaches focus primarily on textual interactions, overlooking speech intonation, gestures, and spatial relationships, which are vital for real-world communication. Human interactions are inherently multi-modal, requiring AI models that integrate audio, motion, and visual signals to comprehend and generate lifelike social interactions.
To bridge this gap, we propose a preliminary dataset designed to capture multi-agent interactions across multiple modalities (language, audio, motion, RGB). This dataset will serve as a foundation for training and evaluating AI models, equipping them with the ability to understand and replicate realistic human interactions in diverse scenarios. By providing synchronized multi-modal data, our dataset will facilitate advancements in conversational AI, multi-modal learning, and interactive simulation.

Related Work
Recent advancements in multimodal AI have enabled motion generation from various inputs, including textual descriptions [1, 2] and audio cues [3, 4]. Among these, text-to-motion generation has gained prominence, with diffusion-based models [5, 6] and vector-quantized approaches [7, 8] showing promising results. However, most existing works focus on single-human scenarios and lack multi-agent interaction modeling.
For human-audio interaction, works such as AudioCLIP [9] and Speech2Motion [10] explore generating human motion from speech, while MAESTRO [11] focuses on speech-motion correspondences. However, these methods are limited to single-human interactions. Our dataset aims to bridge this gap by capturing dyadic interactions with synchronized text, audio, video, and motion data, facilitating research in AI-driven conversational agents.

Methodology
1.	Capture Setup
Establishing a comprehensive motion capture (MoCap), audio, and video recording system.
2.	Interactive Conversation Dataset Collection
Capturing naturalistic multi-person interactions to build a diverse and high-quality dataset.
3.	Multi-Modal Interactive Speech-Motion-Language Model Development
Developing an integrated model that aligns audio, speech, motion, and language for enhanced understanding and generation of human interactions.

References
1.	Ghosh, Anindita, et al. "Synthesis of compositional animations from textual descriptions." Proceedings of the IEEE/CVF international conference on computer vision. 2021.
2.	Lin, Angela S., et al. "Generating animated videos of human activities from natural language descriptions." Learning 1.2018 (2018): 1.
3.	Chen, Changan, et al. "The Language of Motion: Unifying Verbal and Non-verbal Language of 3D Human Motion." arXiv preprint arXiv:2412.10523 (2024).
4.	Dabral, Rishabh, et al. "Mofusion: A framework for denoising-diffusion-based motion synthesis." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.
5.	Zhou, Wenyang, et al. "Emdm: Efficient motion diffusion model for fast and high-quality motion generation." European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2024.
6.	Zhang, Mingyuan, et al. "Motiondiffuse: Text-driven human motion generation with diffusion model." arXiv preprint arXiv:2208.15001 (2022).
7.	Guo, Chuan, et al. "Momask: Generative masked modeling of 3d human motions." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
8.	Zhang, Jianrong, et al. "Generating human motion from textual descriptions with discrete representations." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.
9.	Guzhov, Andrey, et al. "Audioclip: Extending clip to image, text and audio." ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022.
10.	Shi, Mingyi, et al. "It Takes Two: Real-time Co-Speech Two-person's Interaction Generation via Reactive Auto-regressive Diffusion Model." arXiv preprint arXiv:2412.02419 (2024).
11.	Chen, Zhehuai, et al. "Maestro: Matched speech text representations through modality matching." arXiv preprint arXiv:2204.03409 (2022).

Compensation Type: paid
Contact Professor: Ehsan Adeli
Field of Research: ai, vision
Prereqs
•	Strong programming skills in Python
•	Experience with deep learning frameworks (e.g., PyTorch, TensorFlow)
•	Familiarity with computer vision and convex optimization
•	Basic knowledge of data collection and preprocessing
•	Ability to work with multi-modal data (e.g., video, text, audio, motion)
Recommended Background
•	Prior experience with human motion modeling or 3D/4D reconstruction
•	Understanding of multi-person and human-object interaction modeling
•	Coursework or research in computer vision, machine learning, or deep learning
•	Experience with data annotation tools and large-scale dataset curation
•	Experience using large language models (LLMs) (e.g., GPT) and designing effective prompts for tasks such as data augmentation, annotation, or interaction modeling
Office Hours
Feb 12, 2-3pm, Gates 280


## AI Agents for 3D Content Creation
> We seek to create AI agents that can evaluate the user experience of platformer video games. To that end, we have created a large and diverse population of AI agents, each constrained by a unique scene perception or motor skills limitation inspired by typical mistakes of human video game players. We use reinforcement learning to train a policy for each agent that plays as well as possible given the agent's limitations. By comparing the performance of different agents on different games, we are able to make comparative assessments about game difficulty, as well as understand how specific limitations impact performance on specific types of challenges.

We are looking for students to expand this result in two ways. First, we want to use modern AI techniques to increase the capabilities of our agents so that they can play a wide variety of platformer games. Second, we want to investigate how our agents can be used to develop novel tools for use in 3D content creation.

Compensation Type: paid
Contact Professor: Kayvon Fatahalian
Field of Research: graphics, ai
Prereqs
Students should have programming experience in C++ or Python.
Recommended Background
Students interested in the AI side of the project should have experience in Reinforcement Learning to the level of CS 234 or CS 224R (though these classes are not required). Students interested in content creation should foremost be passionate about game development---experience using a 3D engine (Unreal Engine, Unity, etc.) is preferred but not required.
Office Hours
Wednesday, 2pm-3pm, CoDA E370


## Exploring How to Improve Design Processes with Reinforcement Learning
> This project aims to investigate the role reinforcement learning can play within the design process.  

We often think about the theory of design as a cyclical refinement of creating some notion of a prototype, and evaluating how well the prototype works, then trying to come up with a new prototype based on what we've done. It seems  that reinforcement learning methods also follow a similar process. But is it better than people? Worse? Can technical methods benefit from human-centered methods?

This project will be in the early stages of development.  Interested students will have a substantial impact on the trajectory of the project and work closely with a PhD student.  It will involve a combination of methods including human-subject experiments, experimental design, graphic design, systems building, and some machine learning theory.  The project will likely continue on beyond the summer but intern participation beyond the CURIS period is not required.

Compensation Type: paid
Contact Professor: Michael Bernstein
Field of Research: hci, ai
Prereqs
This project is best suited for people who are interested in human-centered design and generally interested in HCI. Human-AI-Interaction is part of the project, but this is a project deeply motivated from a human-first perspective.
Recommended Background
Students with a background with web programming or game development experience are desirable but not required.  Additionally, taking a course in machine learning (CS229) and/or reinforcement learning (CS234)  is beneficial. A majority of the machine learning will be focused on theory.
Office Hours
Tue Feb 11 and Thurs Feb 13 from  10-11:30a
