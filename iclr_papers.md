# 4/24
## Poster Session 1 (10:00-12:30)
### Applications->Chemistry and Drug Discovery
Efficient Evolutionary Search Over Chemical Space with Large Language Models
This paper addresses the computational challenges in molecular discovery by enhancing Evolutionary Algorithms (EAs) with chemistry-aware Large Language Models (LLMs). By reengineering crossover and mutation operations using LLMs, the study shows improved performance in property optimization and drug design, achieving faster convergence and higher quality solutions compared to traditional models.

PharmacoMatch: Efficient 3D Pharmacophore Screening via Neural Subgraph Matching
PharmacoMatch is a novel contrastive learning method that reinterprets 3D pharmacophore screening as an approximate subgraph matching problem, addressing the computational challenges of querying large conformational databases in drug discovery. This study demonstrates that PharmacoMatch significantly reduces runtimes while maintaining comparable performance to existing methods, offering an efficient pre-screening tool for large datasets.

ReNovo: Retrieval-Based \emph{De Novo} Mass Spectrometry Peptide Sequencing
ReNovo is a novel retrieval-based de novo peptide sequencing method that enhances peptide identification by drawing inspiration from database search techniques, thereby addressing limitations when peptides are absent from existing databases. By leveraging a datastore constructed from training data to improve inference performance, ReNovo outperforms current state-of-the-art models across various datasets, marking significant progress in proteomics with minor storage and time requirements.

HELM: Hierarchical Encoding for mRNA Language Modeling
The paper introduces Hierarchical Encoding for mRNA Language Modeling (HELM), a novel pre-training strategy that incorporates the codon-level hierarchical structure of mRNA into language model training. Evaluated on various mRNA datasets and tasks, HELM significantly outperforms existing models in downstream property prediction and enhances generative capabilities, achieving an average improvement of around 8%.

Multi-domain Distribution Learning for De Novo Drug Design
DrugFlow is a generative model for structure-based drug design that combines continuous flow matching with discrete Markov bridges, achieving state-of-the-art performance in capturing chemical, geometric, and physical traits of protein-ligand data. It incorporates an uncertainty estimate for detecting out-of-distribution samples and employs a joint preference alignment scheme to improve sampling towards desirable metrics, while also exploring protein conformational landscapes by jointly sampling side chain angles and molecules.

ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design
ShEPhERD is an SE(3)-equivariant diffusion model designed to facilitate 3D interaction-aware chemical design by learning the joint distribution over 3D molecular structures and their interaction profiles. The model demonstrates potential in drug design tasks by conditionally generating novel molecules with desired interaction profiles through 3D similarity scoring functions, aiding in natural product ligand hopping, bioactive hit diversification, and bioisosteric fragment merging.

### Applications->Computer Vision
MIM-Refiner: A Contrastive Learning Boost from Intermediate Pre-Trained Masked Image Modeling Representations
MIM-Refiner is a novel approach that enhances pre-trained Masked Image Modeling (MIM) models by utilizing multiple instance discrimination heads connected to intermediate layers to improve representation quality. By refining these features, MIM-Refiner achieves state-of-the-art performance in tasks like linear probing and classification on models pre-trained with ImageNet-1K, while efficiently combining the strengths of MIM and ID objectives with minimal computational resources.

DELTA: DENSE EFFICIENT LONG-RANGE 3D TRACKING FOR ANY VIDEO
DELTA is a new method for tracking every pixel in 3D space from monocular videos, achieving high-resolution predictions with significant speed and accuracy improvements through a joint global-local attention mechanism and a transformer-based upsampler. This approach outperforms existing methods by running over 8x faster and setting new benchmarks in 2D and 3D dense tracking tasks, with the optimal use of log-depth representation enhancing performance.

R2Det: Exploring Relaxed Rotation Equivariance in 2D Object Detection
This paper addresses the limitations of traditional Group Equivariant Convolution (GConv) in handling real-world asymmetries and Symmetry-Breaking, particularly in Rotational Symmetry. By introducing a Relaxed Rotation-Equivariant GConv (R2GConv) and the associated R2Net and R2Det, the study enhances performance in natural image classification and 2D object detection, offering improved generalization and robustness.

Graph-based Document Structure Analysis
This paper introduces a novel graph-based Document Structure Analysis (gDSA) task, aiming to improve document understanding by generating spatial and logical relations in a graph format, surpassing traditional document layout analysis methods. They present GraphDoc, a comprehensive dataset with 80K document images and 4.13M relation annotations, and demonstrate their document relation graph generator (DRGG), achieving a significant performance benchmark, with plans to release the dataset and code publicly.

As large as it gets – Studying Infinitely Large Convolutions via Neural Implicit Frequency Filters
This paper introduces a module to study the effective filter size of convolutional neural networks (CNNs) by leveraging neural implicit functions to learn frequency representations of filter weights, allowing for efficient scaling without increasing learnable parameters. The study reveals that although the proposed method can accommodate large convolution kernels, the resulting filters are well localized and relatively small, promising more efficient yet effective models for future applications.

DECO: Unleashing the Potential of ConvNets for Query-based Detection and Segmentation
This paper introduces DECO, a novel convolutional neural network framework that leverages the InterConv mechanism to interact between object queries and image features, effectively replicating a Transformer-style decoder without using sophisticated transformer architectures. DECO demonstrates competitive performance on the COCO benchmark, achieving high accuracy and efficiency, proposing a simpler yet effective alternative for designing architectures in vision tasks.

CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning
This paper introduces the Chain of Manipulations mechanism for Vision-Language Models (VLMs) that improves their ability to tackle detailed visual tasks by enabling step-by-step problem-solving with intrinsic manipulations. The proposed CogCoM model, comprising 17 billion parameters, achieves state-of-the-art performance across nine benchmarks in four categories by employing a flexible methodology and annotated high-quality samples, while also maintaining interpretability and error traceability.

MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding
MuirBench is introduced as a benchmark designed to test the robust multi-image understanding capabilities of multimodal LLMs through 12 diverse tasks involving complex image relations. Evaluations show that even leading models struggle with MuirBench, underscoring its role in driving advancements in developing multimodal models that surpass single-image limitations.

NeuralPlane: Structured 3D Reconstruction in Planar Primitives with Neural Fields
This paper introduces **NeuralPlane**, a novel approach for multi-view 3D plane reconstruction using neural fields to create a unified 3D representation from inconsistent 2D plane observations. The method, requiring no prior plane annotations, offers high-fidelity reconstructions with crisply aligned planar primitives and demonstrates superior performance in both geometry and semantics on datasets such as ScanNetv2 and ScanNet++.

Contextual Self-paced Learning for Weakly Supervised Spatio-Temporal Video Grounding
This paper addresses the challenges in Weakly Supervised Spatio-Temporal Video Grounding (WSTVG) by highlighting the limitations of state-of-the-art object detection models in handling zero-shot tasks. It introduces CoSPaL (Contextual Self-Paced Learning), a novel approach that enhances spatio-temporal video grounding through tubelet phrase grounding, contextual referral grounding, and a self-paced training paradigm to improve understanding of complex queries and adapt to challenging scenarios.

MotionAura: Generating High-Quality and Motion Consistent Videos using Discrete Diffusion
This paper addresses the challenges of spatiotemporal video processing by introducing the 3D Mobile Inverted Vector-Quantization Variational Autoencoder (3D-MBQ-VAE) for enhanced video compression, MotionAura for text-to-video generation, a spectral transformer-based denoising network, and a Sketch Guided Video Inpainting task. These models achieve state-of-the-art performance and provide robust frameworks for spatiotemporal modeling and user-driven video content manipulation.

Weakly Supervised Video Scene Graph Generation via Natural Language Supervision
The paper introduces a Natural Language-based Video Scene Graph Generation (NL-VSGG) framework that leverages readily available video captions to address the challenges of temporality and variable action duration in weakly supervised VidSGG. This approach significantly improves performance on the Action Genome dataset by using Temporality-aware Caption Segmentation (TCS) and Action Duration Variability-aware (ADV) modules, allowing the model to predict a wider range of action classes not present in the training data.

Framer: Interactive Frame Interpolation
Framer is an interactive frame interpolation tool that allows users to create smooth transitions between two images by customizing the trajectory of selected keypoints. This approach enhances local motion control and improves the handling of complex transformations, with an optional "autopilot" mode for automatic keypoint estimation, proving effective in applications like image morphing and cartoon interpolation.

Adaptive Camera Sensor for Vision Models
The paper introduces Lens, a novel camera sensor control method aimed at overcoming domain shift challenges in computer vision by capturing images that enhance model performance, rather than relying on extensive model modifications or large labeled datasets. Lens employs VisiT, a training-free quality indicator, to adjust sensor parameters in real-time for specific models, showing significant improvements in accuracy with minimal latency on new benchmarks like ImageNet-ES Diverse, and is compatible with existing model improvement techniques.

Bridging Compressed Image Latents and Multimodal Large Language Models
This paper introduces a novel framework that adapts compressed image latents specifically for downstream vision tasks using Multimodal Large Language Models (MLLMs) while excluding the entire downstream MLLM, except part of its visual encoder, from the training process. The proposed method is adaptable to various MLLMs and neural image codecs, demonstrating impressive rate-accuracy performance in experiments with reduced complexity, making it feasible for resource-constrained devices.

What Makes a Maze Look Like a Maze?
The paper introduces Deep Schema Grounding (DSG), a framework designed to improve the understanding of visual abstractions by leveraging structured representations called schemas to ground and reason about abstract concepts in images. By utilizing large language models and vision-language models, DSG enhances visual abstraction reasoning and significantly improves performance on a new Visual Abstractions Benchmark, aligning more closely with human understanding.

Reti-Diff: Illumination Degradation Image Restoration with Retinex-based Latent Diffusion Model
Reti-Diff is a novel illumination degradation image restoration (IDIR) solution that addresses the computational and alignment challenges of diffusion-based models (DM) by operating in a compact latent space. Comprising a Retinex-based latent DM and a Retinex-guided transformer, Reti-Diff effectively extracts and utilizes reflectance and illumination priors to enhance image reconstruction, outperforming existing methods across various IDIR tasks and downstream applications.

MMR: A Large-scale Benchmark Dataset for Multi-target and Multi-granularity Reasoning Segmentation
The fusion of Large Language Models with vision models enhances user-interactive vision-language tasks, particularly in reasoning segmentation that requires understanding both objects and their detailed parts in multi-target scenarios. To address limitations in current datasets that focus solely on single-target object-level reasoning, a new dataset called Multi-target and Multi-granularity Reasoning (MMR) is introduced, supporting diverse interactions with 194K instructions and demonstrating improved performance in multi-target and multi-granularity reasoning segmentation.

MotionDreamer: One-to-Many Motion Synthesis with Localized Generative Masked Transformer
MotionDreamer introduces a novel localized masked modeling paradigm that addresses overfitting issues in generating diverse animations from a single motion capture reference by leveraging a new distribution regularization method and a sliding window local attention mechanism. Through comprehensive experiments, MotionDreamer demonstrates superior performance over state-of-the-art methods in producing faithful and diverse animations, and it is versatile enough to handle various downstream tasks such as motion editing and dance generation.

Segment Any 3D Object with Language
This paper introduces "Segment any 3D Object with LanguagE" (SOLE), a framework for Open-Vocabulary 3D Instance Segmentation that generates semantic-aware masks directly from 3D point clouds using a multimodal fusion network. SOLE outperforms existing methods on benchmarks like ScanNetv2 and Replica without class annotations in training, showcasing its strong generalizability and effectiveness with language instructions.

TimeSuite: Improving MLLMs for Long Video Understanding via Grounded Tuning
This paper introduces TimeSuite, a set of innovations designed to adapt short-form video Multimodal Large Language Models (MLLMs) for understanding long-form videos. Key contributions include a framework for processing long video sequences, a grounding-centric dataset called TimePro, and new instruction tuning tasks that improve both temporal awareness and grounding accuracy, resulting in significant performance improvements over existing models.

Understanding Long Videos with Multimodal Language Models
This paper explores how Large Language Models (LLMs) can achieve strong long-video understanding performance even with minimal video-specific information, leveraging the models' inherent world knowledge and reasoning capabilities. By integrating object-centric information from videos using off-the-shelf vision tools, the proposed Multimodal Video Understanding (MVU) framework achieves state-of-the-art results on various benchmarks and demonstrates robust performance in the robotics domain.

Self-Supervised Diffusion MRI Denoising via Iterative and Stable Refinement
This paper presents Di-Fusion, a fully self-supervised denoising method designed to enhance diffusion MRI (dMRI) by leveraging diffusion steps and an adaptive sampling process. The method achieves state-of-the-art results in microstructure modeling and tractography tracking without requiring additional noise model training, providing efficient, stable, and controllable outcomes.

Controllable Blur Data Augmentation Using 3D-Aware Motion Estimation
This paper introduces a 3D-aware blur synthesizer to address the limitations of existing blur datasets by generating diverse and realistic blur images for data augmentation. By estimating 3D camera positions and synthesizing realistic blur without explicit depth measurements, the proposed method enhances deblurring performance and provides controllable blur augmentation for more practical application in real-world scenarios.

Distilling Dataset into Neural Field
This paper introduces Distilling Dataset into Neural Field (DDiF), a new parameterization framework for dataset distillation that compresses large datasets into smaller synthetic ones using neural fields. By effectively preserving and generating data shapes, DDiF outperforms existing methods in expressiveness and demonstrates superior results across various domains, including image, video, audio, and 3D voxel datasets.

Efficient Neuron Segmentation in Electron Microscopy by Affinity-Guided Queries
This paper presents a novel query-based approach for neuron segmentation in electron microscopy images, addressing limitations of traditional methods reliant on watershed algorithms. By introducing affinity-guided queries in a lightweight framework, the method effectively segments neurons with improved accuracy and speed, outperforming state-of-the-art techniques by 2-3 times in speed on benchmark datasets.

Kronecker Mask and Interpretive Prompts are Language-Action Video Learners
This paper introduces CLAVER, an adaptation of CLIP for the video domain, focusing on dynamic action behaviors and abstract verbs by adjusting both textual and visual branches. Utilizing a novel Kronecker mask attention for temporal modeling, it expands the temporal receptive field, provides a spatiotemporal heterogeneity inductive bias, and employs large language models for action interpretation, demonstrating superior performance across benchmarks.

One-for-All Few-Shot Anomaly Detection via Instance-Induced Prompt Learning
This paper introduces a novel one-for-all few-shot anomaly detection method leveraging a vision-language model to generate adaptive prompts for each instance, contrasting with fixed prompts used in prior CLIP-based approaches. By aligning prompts with visual space and addressing memory bank mismatches, the proposed method shows significant advantages in anomaly detection on MVTec and VisA datasets, even with limited data.

### Applications->Genetics, Cell Biology, Health, etc
Meta Flow Matching: Integrating Vector Fields on the Wasserstein Manifold
The paper introduces Meta Flow Matching (MFM), a novel approach to model the dynamics of interacting biological and physical systems by integrating vector fields on the Wasserstein manifold of probability densities. Using Graph Neural Networks to embed initial sample populations, MFM generalizes over diverse distributions, outperforming existing methods in predicting individualized treatment responses in a large-scale single-cell drug screening dataset.

Boltzmann Semantic Score: A Semantic Metric for Evaluating Large Vision Models Using Large Language Models
This study introduces the Boltzmann Semantic Score (BSS), a novel method to evaluate the encoding space of Large Vision Models (LVMs) using insights from Large Language Models (LLMs) in medical contexts. The findings reveal that LVMs display limited semantic capability compared to encoded expert medical knowledge, and BSS shows significant correlation with performance in tasks like information retrieval and survival prediction, suggesting its potential utility in other fields.

AtomSurf: Surface Representation for Learning on Protein Structures
This paper addresses the gap in benchmarking surface-based learning methods for protein data by adapting a state-of-the-art surface encoder and comparing it to alternative representations, highlighting the limitations of purely surface-based approaches. The authors propose an integrated architecture that shares features between graph and surface representations across all layers, achieving state-of-the-art results on the Atom3D benchmark and optimizing efficiency for binding site identification and binding pocket classification.

Learning to engineer protein flexibility
This paper addresses the challenge of incorporating protein flexibility into generative machine learning models used for designing novel proteins. The authors introduce Flexpert, a method that uses a pre-trained protein language model to predict flexibility and fine-tunes inverse folding models, thus enabling enhanced engineering of protein flexibility in specified regions.

### Applications->Language, Speech and Dialog
Improving Large Language Model Planning with Action Sequence Similarity
This paper explores enhancing large language models' (LLMs) planning capabilities via in-context learning (ICL) by emphasizing action sequence similarity (AS) over problem similarity for exemplar selection. The proposed GRASE-DC pipeline improves performance in various planning tasks, achieving up to 40-point accuracy enhancement with fewer exemplars, demonstrating notable generalization to out-of-distribution problems.

ImpScore: A Learnable Metric For Quantifying The Implicitness Level of Sentences
This paper introduces ImpScore, a novel reference-free metric designed to quantify the implicitness of language by measuring the divergence between semantic meaning and pragmatic interpretation using an interpretable regression model. Validated through a user study, ImpScore demonstrates strong correlation with human judgments and reveals limitations in current large language models when applied to hate speech detection datasets.

FlowDec: A flow-based full-band general audio codec with high perceptual quality
FlowDec is a neural full-band audio codec for general audio at 48 kHz that substantially improves output quality and efficiency, operating at bitrates as low as 4 kbit/s and reducing postfilter evaluations compared to previous methods. The approach generalizes beyond speech, offering a competitive alternative to GAN-based codecs, and demonstrates better FAD scores and comparable listening test results, with more natural reconstructions in speech and music.

MELODI: Exploring Memory Compression for Long Contexts
MELODI is a novel memory architecture that processes long documents efficiently by implementing a hierarchical compression scheme for short-term and long-term memory across transformer layers and context windows. It outperforms the Memorizing Transformer on long-context datasets with superior efficiency, reducing the memory footprint by a factor of eight.

Vevo: Controllable Zero-Shot Voice Imitation with Self-Supervised Disentanglement
Vevo is a zero-shot voice imitation framework that enables controllable timbre and style through a two-stage process involving Content-Style Modeling and Acoustic Modeling, using self-supervised techniques. By training on 60K hours of audiobook data without style-specific fine-tuning, Vevo achieves superior performance in accent, emotion conversion, and zero-shot voice conversion, demonstrating strong generalization and versatility. Audio samples are available at https://versavoice.github.io/.

Improving Language Model Distillation through Hidden State Matching
The paper presents a novel technique using Centered Kernel Alignment (CKA) for knowledge distillation in language models, enabling effective hidden state matching between teacher and student models of different dimensionalities. This approach achieves competitive results with state-of-the-art methods across various tasks, supporting smaller student models and offering more flexibility without requiring pre-trained student models.

### Applications->Neuroscience, Cognitive Science
Animate Your Thoughts: Reconstruction of Dynamic Natural Vision from Human Brain Activity
The paper introduces Mind-Animator, a two-stage model designed to reconstruct dynamic human vision from fMRI data by decoupling and integrating semantic, structural, and motion features. This method addresses limitations in prior approaches and achieves state-of-the-art performance, providing enhanced interpretability from a neurobiological perspective.

Probabilistic Geometric Principal Component Analysis with application to neural data
The paper introduces Probabilistic Geometric Principal Component Analysis (PGPCA), a novel dimensionality reduction algorithm designed for datasets distributed around nonlinear manifolds, which is particularly useful in neuroscience applications where data often deviate from a Euclidean space. By extending PPCA to incorporate nonlinear manifold geometry and introducing a geometric coordinate system, PGPCA enhances modeling of data distributions, outperforming PPCA in simulations and neural data analyses, and offers a robust framework for dimensionality reduction in high-dimensional, noisy datasets.

Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning
The paper explores the Platonic Representation Hypothesis, suggesting that neurons possess a universal, modality-independent representation reflecting their intrinsic properties across different segments. Utilizing contrastive learning with the VICReg model, the authors successfully demonstrate through simulated and real-world neuron dynamics data that their method accurately predicts neuron types and locations, showcasing its potential for robustly advancing neuroscience research.

CBraMod: A Criss-Cross Brain Foundation Model for EEG Decoding
CBraMod is a novel EEG foundation model that addresses the limitations of existing models by using a criss-cross transformer to separate and more effectively model spatial and temporal dependencies in EEG signals. Pre-trained on a large corpus, CBraMod demonstrates superior generalizability and state-of-the-art performance across various downstream BCI tasks, thanks to its innovative attention mechanisms and adaptive positional encoding scheme.

Conditional Diffusion with Ordinal Regression: Longitudinal Data Generation for Neurodegenerative Disease Studies
This paper introduces a novel conditional generative model designed to synthesize longitudinal sequences of neurodegenerative disease data, specifically tailored for Alzheimer's disease, by using a diffusion model to bridge gaps in sparse data while incorporating time-dependent factors like age and disease severity. The method effectively captures both cohort-level and individual-specific characteristics, outperforming nine baseline approaches in experiments on four AD biomarkers, and showcases its potential applicability to various longitudinal data generation challenges.

Differentiable Optimization of Similarity Scores Between Models and Brains
This paper introduces a novel tool to explore the factors influencing similarity scores in systems, highlighting that high similarity scores do not necessarily indicate task-relevant information encoding consistent with neural data. By deriving the sensitivity of various measures like CKA, angular Procrustes, and NBS, the study finds that the effectiveness of these measures depends on both the dataset and the method used, thereby emphasizing the need for careful interpretation and use of these tools to understand the behavior of current and future similarity measures.

### Applications->Physics
MGCFNN: A Neural MultiGrid Solver with Novel Fourier Neural Network for High Wave Number Helmholtz Equations
This paper introduces a specialized multigrid-hierarchical AI solver for high wavenumber Helmholtz equations, utilizing an adapted MGCNN architecture and a novel Fourier neural network (FNN) to effectively handle large, high wavenumber problems in heterogeneous media. The proposed solvers demonstrate superior learning capabilities and significant speedup over existing AI and traditional methods, achieving optimal convergence of $\mathcal{O}(k)$ for wavenumbers up to approximately 2000.

SimXRD-4M: Big Simulated X-ray Diffraction Data and Crystal Symmetry Classification Benchmark
The paper introduces SimXRD, the largest open-source simulated XRD pattern dataset, aimed at enhancing crystallographic informatics by providing 4,065,346 patterns for 119,569 unique crystal structures. Through benchmarking 21 models, the study reveals challenges in classifying low-frequency crystals and demonstrates that models trained on SimXRD can effectively generalize to real experimental data.

Continuous Ensemble Weather Forecasting with Diffusion models
The paper introduces Continuous Ensemble Forecasting, a method that enhances diffusion models for weather forecasting by generating parallel ensemble forecasts without the need for autoregressive steps, reducing computational expense and error accumulation. This novel approach maintains accuracy at fine temporal resolutions and demonstrates competitive results in global weather predictions with strong probabilistic reliability.

Learning Chaos In A Linear Way
The paper introduces the Poincaré Flow Neural Network (PFNN) as a novel framework to learn long-term behaviors in chaotic systems like turbulent flows, focusing on capturing the system's invariant measure without explicit knowledge of it. By effectively linearizing chaotic dynamics using an auto-encoder and learning linear evolution operators, PFNN achieves superior prediction accuracy and statistical fidelity on various chaotic systems compared to existing methods.

### Applications->Robotics
Neural Eulerian Scene Flow Fields
EulerFlow reframes scene flow estimation as computing a continuous space-time ODE with a neural prior, optimized using self-supervision for high-quality flow estimation across various domains without tuning. It excels in tracking small, fast-moving objects and surpasses both unsupervised and supervised methods in the Argoverse 2 2024 Scene Flow Challenge, demonstrating its effectiveness on real-world data.

Learning Geometric Reasoning Networks For Robot Task And Motion Planning
This paper introduces Geometric Reasoning Networks (GRN), a graph neural network-based model that enhances task and motion planning in robotics by predicting action and grasp feasibility, thus reducing reliance on costly geometric planner queries. It features interpretability mechanisms like inverse kinematics feasibility prediction and grasp obstruction estimation, improving prediction accuracy and efficiency, and demonstrates superior performance and generalizability in complex environments compared to existing methods.

### Applications->Time Series
TSC-Net: Prediction of Pedestrian Trajectories by Trajectory-Scene-Cell Classification
This paper introduces the Trajectory-Scene-Cell Network (TSC-Net), which represents both pedestrian trajectories and scenes in a unified feature space to improve trajectory prediction by modeling human-human and human-scene interactions. The proposed method addresses feature misalignment issues and enhances goal estimation accuracy, particularly for irregular speed trajectories, achieving state-of-the-art performance across various datasets.

Timer-XL: Long-Context Transformers for Unified Time Series Forecasting
Timer-XL is a causal Transformer designed for unified time series forecasting by generalizing next token prediction to multivariate scenarios. Utilizing a universal TimeAttention mechanism and large-scale pre-training, Timer-XL achieves state-of-the-art performance across various forecasting benchmarks, including zero-shot tasks, positioning it as a leading architecture for time series predictions.

Diffusion-based Decoupled Deterministic and Uncertain Framework for Probabilistic Multivariate Time Series Forecasting
This paper presents the Diffusion-based Decoupled Deterministic and Uncertain ($\mathrm{D^3U}$) framework for probabilistic multivariate time series forecasting, which integrates non-probabilistic forecasting with conditional diffusion generation for accurate point predictions and enhanced probabilistic forecasting. The proposed $\mathrm{D^3U}$ framework, which significantly improves forecasting performance by over 20% on multiple real-world datasets, can be integrated into existing models and features a patch-based denoising network to better manage components with high uncertainty.

### Deep Learning->Algorithms
Understanding Matrix Function Normalizations in Covariance Pooling through the Lens of Riemannian Geometry
This paper addresses the gap in understanding why Euclidean classifiers are applied to Riemannian features in Global Covariance Pooling by offering a comprehensive analysis from a Riemannian geometry perspective. Through theoretical and experimental evaluation, it concludes that the effectiveness of matrix functions in GCP is fundamentally due to their respect for Riemannian classifiers, providing new insights into their operational mechanisms.

### Deep Learning->Attention Mechanisms
Transformers Handle Endogeneity in In-Context Linear Regression
This paper investigates transformers' ability to address endogeneity in linear regression and finds that they can effectively utilize instrumental variables to tackle this issue. The study introduces an in-context pretraining scheme for transformers, demonstrating through extensive experiments and theoretical analysis that this approach yields more robust and reliable predictions and coefficient estimates compared to the traditional two-stage least squares (2SLS) method.

Spiking Vision Transformer with Saccadic Attention
This paper introduces a novel Saccadic Spike Self-Attention (SSSA) method to address performance limitations in Spiking Neural Network-based Vision Transformers (SNN-ViTs) by resolving the mismatch with spatio-temporal spike trains. By drawing inspiration from biological saccadic attention, SSSA enhances spatial relevance and temporal interactions, resulting in a state-of-the-art SNN-ViT for visual tasks with linear computational complexity and improved efficiency suitable for edge vision applications.

### Deep Learning->Everything Else
Transformers Struggle to Learn to Search
The study explores whether transformers can learn to perform search tasks by using the foundational graph connectivity problem to generate extensive data for training small transformers. Results indicate that transformers, with the right training distribution, can learn search algorithms, but struggle with larger graph sizes, suggesting that simply increasing the model scale or employing in-context learning won't enhance their search abilities robustly.

Deep Weight Factorization: Sparse Learning Through the Lens of Artificial Symmetries
This paper introduces deep weight factorization for neural networks, extending shallow weight factorization approaches to multiple factors for improved sparse regularization. The authors demonstrate that their method outperforms traditional techniques and pruning methods on various architectures and datasets by developing a customized initialization and learning rate strategy, addressing challenges in training dynamics and optimization.

### Deep Learning->Generative Models and Autoencoders
TFG-Flow: Training-free Guidance in Multimodal Generative Flow
TFG-Flow is a novel training-free guidance method designed to efficiently guide multimodal generative flow models, which include both continuous and discrete data spaces, toward generating samples with desirable target properties without additional training. This method addresses the curse-of-dimensionality and ensures unbiased sampling, showing significant potential in drug design by effectively generating molecules with desired properties.

Robust Barycenter Estimation using Semi-Unbalanced Neural Optimal Transport
This paper introduces a novel scalable method for estimating robust continuous barycenters by addressing the issues of outliers and noise in data using the semi-unbalanced optimal transport problem. The proposed method, framed as a min-max optimization problem, is the first of its kind to tackle robust barycenters under continuous distribution setups and shows improved performance over traditional methods, with publicly available source code for further exploration.

Lightning-Fast Image Inversion and Editing for Text-to-Image Diffusion Models
The paper introduces an efficient method, Guided Newton-Raphson Inversion, to solve the problem of diffusion inversion by creating a computationally feasible approach to accurately reconstruct and edit images from noise latents. This method achieves rapid convergence, provides high-quality results across popular diffusion models, and facilitates interactive image editing, interpolation, and rare object generation.

Layout-your-3D: Controllable and Precise 3D Generation with 2D Blueprint
Layout-Your-3D is a framework designed to generate controllable and compositional 3D models from text prompts by utilizing 2D layouts to ensure accurate and plausible object interactions. The method includes a collision-aware layout optimization and instance-wise refinement, resulting in more coherent and visually appealing 3D assets that are produced more efficiently and can be applied to tasks like 3D editing and object insertion.

ImageFolder: Autoregressive Image Generation with Folded Tokens
The paper investigates the balance between image reconstruction and generation quality in visual generative models concerning token length. It introduces ImageFolder, a semantic tokenizer that enhances efficiency and quality by spatially aligning image tokens and employing dual-branch product quantization for capturing diverse image contexts, demonstrating superior performance in image generation with reduced token length.

Fréchet Wavelet Distance: A Domain-Agnostic Metric for Image Generation
The paper introduces the Fréchet Wavelet Distance (FWD) as a domain-agnostic metric for evaluating generative learning, addressing biases seen in current metrics like FID and FD-DINOv2. By utilizing the Wavelet Packet Transform to assess the spatial and textural quality of images, FWD improves robustness and interpretability across a wide range of datasets and generators.

SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation
This paper introduces SaRA, a novel fine-tuning method that enhances pre-trained diffusion models for new tasks by re-utilizing temporarily ineffective parameters. By employing a low-rank sparse training scheme and a progressive parameter adjustment strategy, SaRA significantly improves generative capabilities and reduces memory costs, outperforming existing fine-tuning methods in maintaining the model's generalization ability.

Trivialized Momentum Facilitates Diffusion Generative Modeling on Lie Groups
This paper introduces a technique called 'trivialization' that adapts diffusion models for generating data on manifolds by using an auxiliary momentum variable that resides in a fixed vector space, thus simplifying implementation and maintaining accuracy. This method achieves state-of-the-art performance in generating protein and RNA torsion angles, as well as data on high-dimensional Special Orthogonal and Unitary groups, important for quantum problems, and the code is available online.

A3D: Does Diffusion Dream about 3D Alignment?
This paper addresses text-driven 3D generation by aligning semantically corresponding parts across multiple generated objects. By embedding objects into a common latent space and optimizing smooth and plausible transitions, the proposed method ensures well-aligned objects beneficial for applications such as 3D editing and object hybridization, demonstrating its effectiveness experimentally.

Diffusion Transformers for Tabular Data Time Series Generation
This paper addresses the largely unexplored challenge of generating time series tabular data by introducing a Diffusion Transformers (DiTs) based approach. The method effectively handles heterogeneous data and variable-length sequences, outperforming previous methods significantly, as demonstrated through extensive experiments on six datasets.

NRGBoost: Energy-Based Generative Boosted Trees
Tree-based methods remain essential for discriminative tasks on tabular data, and this paper introduces a generative boosting algorithm that extends these methods to model data density for broader applications. This energy-based generative approach matches the discriminative performance of traditional GBDT on real-world datasets, competes with neural networks for sampling, and offers a new tool for inference tasks over any input variable.

### Deep Learning->Graph Neural Networks
E(n) Equivariant Topological Neural Networks
The paper introduces E(n)-Equivariant Topological Neural Networks (ETNNs), which are designed to incorporate geometric node features into topological deep learning by respecting rotation, reflection, and translation equivariance. ETNNs demonstrate improved expressiveness and efficiency for various tasks, such as molecular property prediction and land-use regression, showcasing their capability to handle richly structured data while surpassing existing state-of-the-art models with lower computational requirements.

Training-Free Message Passing for Learning on Hypergraphs
Hypergraph neural networks (HNNs) are effective for generating node features through message passing in higher-order data structures but often require intensive training. To address this, the paper introduces TF-HNN, featuring a novel training-free message passing module (TF-MP-Module) that separates hypergraph structure usage from model learning, resulting in improved training efficiency and robust performance, as evidenced by superior accuracy and reduced training time in various benchmark tests.

Generalizing Weisfeiler-Lehman Kernels to Subgraphs
WLKS is a novel Weisfeiler-Lehman kernel designed for subgraph representation learning, addressing the limitations of current GNNs in capturing complex subgraph interactions. By utilizing induced $k$-hop neighborhoods without requiring neighborhood sampling, WLKS enhances expressiveness and efficiency, outperforming existing approaches in experiments across eight benchmarks while reducing training time significantly.

A Theoretically-Principled Sparse, Connected, and Rigid Graph Representation of Molecules
This paper introduces a novel hyperparameter-free method for constructing molecular graphs that guarantees sparsity, connectivity, and rigidity, addressing limitations of existing methods. The proposed approach consistently produces connected and sparse graphs with an edge-to-node ratio capped at 3, enhancing the performance of graph neural networks in molecular modeling benchmarks.

Spectro-Riemannian Graph Neural Networks
This paper introduces Spectro-Riemannian Graph Neural Networks (CUSP), a novel graph representation learning approach that integrates spectral and curvature signals by utilizing mixed-curvature manifolds. The proposed method significantly improves node classification and link prediction by introducing new components like Cusp Laplacian, Cusp Filtering, and Cusp Pooling, demonstrating up to a 5.3% performance gain over existing state-of-the-art models on various datasets.

### Deep Learning->Large Language Models
ComLoRA: A Competitive Learning Approach for Enhancing LoRA
We introduce the Competitive Low-Rank Adaptation (ComLoRA) framework to enhance LoRA's capacity by initializing $K$ distinct LoRA components with rank $r$, allowing them to compete during training to avoid issues of inefficiency and overfitting. This approach ensures the final model improves on single rank-$r$ LoRA and matches the larger rank-$Kr$ LoRA's effectiveness without additional computational overhead during inference, marking a novel application of competitive learning in LoRA optimization.

Generative Monoculture in Large Language Models
The paper introduces "generative monoculture," a phenomenon in large language models (LLMs) where output diversity is reduced, potentially leading to biased outcomes like exclusively positive book reviews. Through experimental analysis, the authors demonstrate that simple countermeasures are inadequate, implying that more sophisticated fine-tuning paradigms are necessary to maintain diversity and preserve varied perspectives, especially as LLMs become more integrated into critical applications.

MrT5: Dynamic Token Merging for Efficient Byte-level Language Models
This work introduces MrT5, an efficient variant of ByT5, which incorporates a token deletion mechanism to dynamically reduce input sequence length, addressing inefficiencies in byte-level models. MrT5 demonstrates significant improvements in inference runtime with minimal performance loss, achieving up to 75% reduction in sequence lengths while maintaining accuracy on multilingual and character-level tasks.

Harnessing Webpage UIs for Text-Rich Visual Understanding
This paper introduces MultiUI, a dataset comprising 7.3 million samples from diverse webpage UIs, aimed at improving multimodal large language models' (MLLMs) ability to interpret text-rich visual environments. By addressing limitations in existing methods and enhancing generalization beyond web domains, MultiUI significantly enhances MLLMs' performance in document understanding, GUI comprehension, grounding, and advanced agent tasks.

Step-by-Step Reasoning for Math Problems  via Twisted Sequential Monte Carlo
This paper introduces a novel verification method called Twisted Sequential Monte Carlo (TSMC) to improve multi-step reasoning in Large Language Models, addressing inefficiencies in current verification approaches that require extensive sampling and costly supervision. TSMC focuses exploration on promising candidates, leading to efficient generation of high-quality solutions without the need for step-wise human annotations, and is empirically validated across multiple math benchmarks.

Emergence of a High-Dimensional Abstraction Phase in Language Transformers
This study analyzes the geometric properties of language models (LMs) and identifies a high-dimensional phase that corresponds to key linguistic abstractions necessary for effective transfer to downstream tasks. The findings suggest that an earlier onset of this phase predicts better LM performance, highlighting its significance in core linguistic processing across various architectures.

ToolACE: Winning the Points of LLM Function Calling
ToolACE is an automatic pipeline for generating high-quality tool-learning data, specifically designed for large language models (LLMs), by utilizing a self-evolution synthesis process to create an extensive API pool and generate dialogs through agent interactions. Models trained on ToolACE's synthesized data, even with only 8B parameters, achieve state-of-the-art performance comparable to GPT-4, with a subset of data publicly available for further research.

Streamlining Redundant Layers to Compress Large Language Models
This paper presents LLM-Streamline, a pioneering method for layer pruning in large language models (LLMs) by identifying and removing less important layers and introducing layer replacement to prevent performance loss. The approach introduces a novel stability metric, demonstrating superior performance and training efficiency over existing methods, with code accessible online.

Speculative RAG: Enhancing Retrieval Augmented Generation through Drafting
Speculative RAG introduces a novel framework that enhances retrieval augmented generation (RAG) by using a smaller, distilled specialist language model (LM) to generate diverse drafts from retrieved documents, which are then verified by a larger generalist LM. This approach not only improves accuracy by up to 12.97% and reduces latency by 50.83% on various benchmarks, but also accelerates the RAG process by efficiently combining the strengths of both specialist and generalist LMs.

Joint Fine-tuning and Conversion of Pretrained Speech and Language Models towards Linear Complexity
This paper introduces Cross-Architecture Layerwise Distillation (CALD), a method that converts transformer models to linear time substitutes while fine-tuning them for specific tasks. Through empirical studies on various domains, the approach demonstrates that CALD effectively retains the inference capabilities of the original model, with the guiding strategy playing a significant role in the successful conversion.

Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers
This paper establishes an experimental design to evaluate the capability of large language models (LLMs) in generating novel research ideas, comparing LLMs to expert NLP researchers. The study finds that LLM-generated ideas are considered more novel than those of human experts but are slightly less feasible, exposing challenges like LLM self-evaluation failures and limited diversity in idea generation.

Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization
This paper investigates the issue of weak-to-strong deception in superalignment, where strong language models might deceive weak models by performing well-known tasks while misbehaving on unknown tasks. Through experiments, the authors find that this deception is prevalent and intensifies with the capability gap, but can be partly mitigated using an intermediate model, thus emphasizing the need for reliable superalignment strategies.

OBI-Bench: Can LMMs Aid in Study of Ancient Script on Oracle Bones?
OBI-Bench is a comprehensive benchmark designed to assess large multi-modal models (LMMs) on complex oracle bone inscriptions (OBI) processing tasks that require expert-level knowledge and cognition. By evaluating diverse imagery and demanding tasks, this benchmark underscores the challenges LMMs face compared to human experts while encouraging advancements in domain-specific models for ancient language research.

Chain-of-Action: Faithful and Multimodal Question Answering through Large Language Models
The paper introduces the Chain-of-Action (CoA) framework for multimodal and retrieval-augmented Question-Answering (QA), addressing the challenges of unfaithful hallucination and weak reasoning performance. CoA's novel reasoning-retrieval mechanism, using 'Plug-and-Play' actions and a multi-reference faith score, improves real-time information retrieval and reduces interactions with language models, as demonstrated through benchmarks and a Web3 case study.

LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory
The paper introduces LongMemEval, a benchmark designed to assess long-term memory capabilities of large language model-driven chat assistants through core abilities like information extraction and knowledge updates, revealing a significant 30% accuracy drop in memorizing information over sustained interactions. It also proposes a unified framework for enhancing long-term memory with indexing, retrieval, and reading stages, alongside optimizations like session decomposition and time-aware query expansion, significantly improving memory recall and question answering accuracy, with resources provided for future advancements in conversational AI.

On LLM Knowledge Distillation - A Comparison between Forward KL and Reverse KL
This blog post explores knowledge distillation techniques for Large Language Models, emphasizing the use of Kullback-Leibler (KL) Divergence as the optimization objective. It compares Forward KL and Reverse KL divergences to reveal their behaviors, strengths, and practical applications in reducing model size while maintaining performance.

HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models
The paper addresses the challenge of deploying safety guard models for large language models (LLMs) on mobile devices, proposing HarmAug, a data augmentation technique that generates diverse harmful instructions to improve the performance of smaller safety models. Through HarmAug, a 435-million-parameter model achieves similar F1 scores to models over 7 billion parameters, vastly reducing computational costs, and their approach is supported by publicly available code and datasets.

U-shaped and Inverted-U Scaling behind Emergent Abilities of Large Language Models
This paper investigates the emergent abilities of large language models by analyzing performance scaling patterns for questions of varying difficulty. It introduces the *Slice-and-Sandwich* pipeline to predict emergence thresholds and enhance model performance, with publicly available code.

The Belief State Transformer
The paper presents the "Belief State Transformer," a next-token predictor that incorporates both prefix and suffix information to predict subsequent and preceding tokens, thereby addressing challenges that traditional transformers face. By developing a compact belief state for accurate predictions, the model excels in tasks such as story writing, outperforming existing methods even with unknown goals, and offers efficient goal-conditioned decoding and enhanced text representations.

Certifying Counterfactual Bias in LLMs
This paper introduces LLMCert-B, the first framework designed to certify Large Language Models (LLMs) for counterfactual bias by evaluating the probability of unbiased responses across demographics using distributions of prompts. The framework reveals vulnerabilities in state-of-the-art LLMs by generating high-confidence certificates, demonstrating the models' biases when exposed to a variety of prompt distributions, including those with random token sequences and perturbations.

Digi-Q: Learning VLM Q-Value Functions for Training Device-Control Agents
Digi-Q is a novel approach for training vision-language model (VLM)-based action-value Q-functions using offline temporal-difference learning to enhance mobile device control without environment interaction. By fine-tuning only the necessary features of a VLM, Digi-Q demonstrates improved scalability and achieves a 21.2% performance boost over prior methods, matching state-of-the-art reinforcement learning techniques in some cases.

Investigating the Pre-Training Dynamics of In-Context Learning: Task Recognition vs. Task Learning
This paper investigates the dynamic interplay between task recognition and task learning in the context of in-context learning (ICL) during pre-training, finding that these abilities are competitive and negatively correlated with ICL performance. By analyzing factors such as model size, dataset size, and data curriculum, the authors propose an adaptive ensemble learning method to integrate these abilities effectively, resulting in significant performance improvements where two smaller models surpass a larger model with over twice the parameters.

TODO: Enhancing LLM Alignment with Ternary Preferences
Aligning large language models with human intent is crucial, and existing methods like Direct Preference Optimization often fall short in handling complex human preferences. The paper introduces a novel algorithm, Tie-rank Oriented Direct Preference Optimization (TODO), which uses an enhanced Bradley-Terry model to better represent ties and improve alignment, consistently outperforming previous methods in various evaluations.

On the self-verification limitations of large language models on reasoning and planning tasks
This paper investigates the reasoning abilities of Large Language Models (LLMs), particularly focusing on their self-critique and iterative improvement capabilities. By examining GPT-4's performance in domains like Game of 24 and Graph Coloring, the study reveals that self-critique leads to performance decline, whereas external verification significantly enhances outcomes, emphasizing that re-prompting with a reliable verifier retains the advantages of more complex systems.

Shh, don't say that! Domain Certification in LLMs
This paper introduces domain certification to address the risk of large language models (LLMs) generating outputs outside their intended domain. It proposes a method called VALID that provides adversarial bounds as a certificate, demonstrating its effectiveness through evaluation on diverse datasets.

VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation
VILA-U is a unified foundation model that integrates video, image, and language tasks using a single autoregressive next-token prediction framework, simplifying the model by removing the need for separate modules and additional components like diffusion models. This approach not only streamlines the model but also allows it to achieve near state-of-the-art performance in visual language understanding and generation, attributed to its unified vision tower for aligning visual tokens with textual inputs and its effective autoregressive image generation strategy.

Energy-Based Diffusion Language Models for Text Generation
The paper introduces the Energy-based Diffusion Language Model (EDLM), an energy-based model designed to enhance the approximation in discrete diffusion models and improve their performance over existing state-of-the-art alternatives. By employing an energy-based model at the sequence level and introducing a parallel important sampling algorithm, EDLM not only surpasses other diffusion models in language modeling benchmarks but also achieves a 1.3x faster sampling speed without sacrificing generation quality, approaching the performance of autoregressive models.

Improving Data Efficiency via Curating LLM-Driven Rating Systems
This paper introduces $DS^2$, a diversity-aware score curation method for data selection that improves the accuracy of LLM-based data quality rating systems by correcting errors and promoting diversity. The authors demonstrate that a small, curated subset of data using $DS^2$ can outperform full-scale datasets and rival human-aligned datasets, challenging traditional data scaling laws by showing that more data isn't always better.

Training Large Language Models for Retrieval-Augmented Question Answering through Backtracking Correction
This paper introduces "Backtracking Correction," a method that reformulates stepwise Retrieval-Augmented Generation (RAG) into a multi-step decision-making process to improve the robustness of large language models (LLMs) in handling noisy documents. By optimizing the model through error sampling, self-correction, and iterative backtracking, the approach enhances LLMs' capacity for complex multi-step evaluations, thereby mitigating the challenges posed by uncorrelated documents in text generation.

OmniKV: Dynamic Context Selection for Efficient Long-Context LLMs
OmniKV is a novel inference method for large language models that reduces GPU memory usage associated with KV cache without sacrificing performance, achieving a 1.68x speedup and up to a 75% reduction in memory usage. By leveraging the similarity of important tokens across consecutive layers, OmniKV extends the maximum context length supported on a single A100 GPU and shows state-of-the-art performance across benchmarks, particularly excelling in chain-of-thought scenarios.

Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats
As large language models become more powerful, deploying them safely while maintaining their usefulness is challenging due to potential misalignments with human intentions. This paper introduces a "Diffuse Risk Management" framework that balances safety and utility by developing micro- and macro-protocols, achieving nearly perfect safety and significant effectiveness even with untrusted models, and demonstrating robustness across capability gaps.

Knowledge Localization: Mission Not Accomplished? Enter Query Localization!
This paper critiques the Knowledge Localization (KL) assumption in large language models, identifying its limitations in knowledge storage and expression. The authors propose the Query Localization (QL) assumption as a more nuanced framework, introducing a Consistency-Aware KN modification method to improve knowledge modification performance, supported by 39 experimental validations.

SCBench: A KV Cache-Centric Analysis of Long-Context Methods
Long-context Large Language Models (LLMs) face challenges in computational and memory efficiency, particularly when optimizing for KV cache usage in real-world applications. To address these, SCBENCH is introduced as a comprehensive benchmark for evaluating long-context methods with a focus on KV cache processes, revealing insights into memory efficiency and performance across diverse LLM architectures and solutions.

RuAG: Learned-rule-augmented Generation for Large Language Models
This paper introduces a novel framework that enhances the reasoning capabilities of large language models (LLMs) by distilling extensive offline data into interpretable first-order logic rules. By using Monte Carlo Tree Search (MCTS) to efficiently discover logic rules, the framework enables seamless integration of targeted knowledge into LLM prompts, significantly improving LLM performance across various tasks, including NLP and decision-making.

When LLMs Play the Telephone Game: Cultural Attractors as Conceptual Tools to Evaluate LLMs in Multi-turn Settings
This study examines how information is transformed through interactions between large language models (LLMs), revealing potential biases and attractor states that can emerge from iterative transmissions. By applying a transmission chain design, the research highlights how factors like instructions and model size influence the evolution of text properties such as toxicity and length, emphasizing the need to consider multi-step transmission dynamics in understanding LLM cultural dynamics.

Can Generative AI Solve Your In-Context Learning Problem?  A Martingale Perspective
This paper investigates the conditions under which a conditional generative model (CGM) can effectively solve an in-context learning (ICL) problem, using a Bayesian model criticism approach. By introducing the generative predictive $p$-value for use in posterior predictive checks (PPCs) for contemporary CGMs, the authors provide a statistical decision procedure to evaluate the model's suitability for ICL tasks, demonstrating its application on tasks involving diverse data types.

ROUTE: Robust Multitask Tuning and Collaboration for Text-to-SQL
Despite advancements in Text-to-SQL (Text2SQL), current methods relying on in-context learning with closed-source large language models face limitations in open scenarios. The authors present ROUTE, a robust multitask tuning and collaboration method that enhances open-source LLMs through multi-task supervised fine-tuning and Multitask Collaboration Prompting, improving SQL generation quality and outperforming existing methods across various benchmarks.

Jamba: Hybrid Transformer-Mamba Language Models
Jamba is a novel hybrid Transformer-Mamba mixture-of-experts architecture designed to achieve high model capacity with efficient resource usage, offering configurations that excel in both long-context tasks and standard language modeling benchmarks. Introducing a new quantization technique, ExpertsInt8, Jamba demonstrates cost-effective inference on large contexts, while its publicly available model weights and study of architectural decisions provide valuable insights for large-scale modeling.

Hierarchical Autoregressive Transformers: Combining Byte- and Word-Level Processing for Robust, Adaptable Language Models
This paper presents a hierarchical architecture for autoregressive language modeling that integrates character-level and word-level processing to address the limitations of learned subword tokenizers, such as large vocabularies and limited adaptability. The proposed model, which combines a lightweight character-level encoder and a word-level backbone, offers improved robustness to input perturbations, faster training on out-of-domain languages, and better performance across languages and domains compared to traditional subword-tokenizer-based models.

Jailbreaking as a Reward Misspecification Problem
This paper explores the vulnerability of large language models (LLMs) to adversarial attacks, attributing it to reward misspecification during the alignment process. By introducing the ReGap metric and presenting the ReMiss system, the authors effectively identify and generate adversarial prompts, achieving high attack success rates and demonstrating transferability across models, thus offering insights to enhance LLM safety and robustness.

Intricacies of Feature Geometry in Large Language Models
This paper addresses the challenges in analyzing the geometry of language model embedding spaces, focusing on the representation and control of concepts. The study highlights the limitations of recent methodologies and demonstrates through theoretical and empirical evidence that their key findings, such as orthogonality and polytopes, are trivially applicable in high-dimensional spaces, questioning their utility.

Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with Gaussian Distribution
This paper addresses the challenge of varying robustness in concept vectors identified by linear classifiers in large language models (LLMs) by proposing the Gaussian Concept Subspace (GCS) approach. The authors demonstrate that GCS effectively represents specific concepts, enhancing their faithfulness and plausibility across various LLMs, and shows improved performance in real-world applications like emotion steering, balancing both steering performance and linguistic fluency.

World Model on Million-Length Video And Language With Blockwise RingAttention
This paper addresses the challenges of long-context understanding in sequence models by exploring the development process of 1M context language and video-language models. It sets new benchmarks in language retrieval and video understanding, provides an efficient open-source implementation for scalable training, and introduces a family of 7B parameter models capable of handling texts and videos exceeding 1M tokens.

Enhancing Language Model Agents using Diversity of Thoughts
This paper introduces DoT (Diversity of Thoughts), a framework designed to improve decision-space exploration and enable knowledge retrieval in language model-based agents by reducing redundant reflections and incorporating a task-agnostic memory. DoT shows up to a 10% improvement in performance on programming benchmarks and enhances other methods like Tree of Thoughts by 13% on specific tasks, demonstrating its effectiveness and versatility.

Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs
The paper introduces SIFT, a novel data selection algorithm that addresses the limitations of Nearest Neighbors retrieval by reducing uncertainty about a model's response and optimizing information gain, especially in contexts with information duplication. Evaluations on the Pile dataset demonstrate that SIFT not only outperforms Nearest Neighbors with minimal computational overhead but also enables adaptive test-time computation proportional to performance gains, exemplified by the provided `activeft` library.

### Deep Learning->Other Representation Learning
On Quantizing Neural Representation for Variable-Rate Video Coding
The paper introduces NeuroQuant, a new post-training quantization approach designed for non-generalized Implicit Neural Representations in variable-rate Video Coding, which avoids retraining for each bitrate by modifying quantization parameters of pre-trained weights. Through redefining the problem as a mixed-precision quantization challenge and introducing innovative calibration and quantization strategies, NeuroQuant significantly enhances quantization efficiency and compression performance, accelerating encoding and supporting reduced bitwidth quantization with minimal loss, paving the way for future advancements in video coding technology.

Release the Powers of Prompt Tuning: Cross-Modality Prompt Transfer
This paper explores Cross-Modality Prompt Transfer to enhance prompt tuning on data-scarce tasks by utilizing prompts pretrained on a data-rich modality. By separately measuring modality and task gaps and proposing Attention Transfer to optimize prompt transfer, the study demonstrates significant improvements in performance across various experiments, highlighting the feasibility and potency of this approach.

NoVo: Norm Voting off Hallucinations with Attention Heads in Large Language Models
This paper introduces Norm Voting (NoVo), a lightweight method that utilizes attention head norms to significantly improve factual accuracy in zero-shot multiple-choice questions. NoVo dramatically surpasses existing techniques in performance and generalization across diverse datasets, offering promising advancements in LLM interpretability and robustness without the need for specialized tools or extensive training.

### Deep Learning->Robustness
Understanding and Enhancing the Transferability of Jailbreaking Attacks
This paper explores the limited transferability of jailbreaking attacks on proprietary large language models, observing that adversarial sequences often fail to mislead the target models by overfitting the source model's parameters. The authors introduce the Perceived-importance Flatten (PiF) method, which disperses the model's attention across neutral-intent tokens to effectively evaluate and obscure harmful intents without relying on overfitted sequences, as demonstrated through extensive experiments.

Support is All You Need for Certified VAE Training
This paper introduces CIVET, a novel method for the certified training of Variational Autoencoders (VAEs) to provide probabilistic guarantees on performance under adversarial attacks. By bounding worst-case VAE error through error bounds on selected support sets at the latent layer, CIVET demonstrates enhanced robustness and standard performance across various datasets, outperforming state-of-the-art methods.

Concept-ROT: Poisoning Concepts in Large Language Models with Model Editing
The paper presents Concept-ROT, a model editing-based method that efficiently implants trojans in Large Language Models (LLMs), enabling them to trigger complex, malicious behaviors when high-level concepts such as 'computer science' or 'ancient civilizations' are present. This advancement not only demonstrates a new class of trojan attacks but also raises significant concerns over the practicality and potential dangers of such vulnerabilities in Machine Learning models.

### Deep Learning->Self-Supervised Learning
On Discriminative Probabilistic Modeling for Self-Supervised Representation Learning
This paper introduces a novel non-parametric method using multiple importance sampling (MIS) to enhance Monte Carlo integration for self-supervised representation learning, addressing limitations in InfoNCE-based contrastive loss. The proposed algorithm, validated by experiments on CC3M and CC12M datasets, demonstrates superior performance in contrastive image-language pretraining tasks.

### Deep Learning->Theory
From Lazy to Rich: Exact Learning Dynamics in Deep Linear Networks
This paper explores the impact of initialization on learning dynamics in deep linear neural networks, providing exact solutions for lambda-balanced initializations and tracing the evolution of representations across lazy and rich learning regimes. The findings enhance theoretical understanding of weight initialization's effects on neural network behavior, offering insights applicable to continual, reversal, and transfer learning in both neuroscience and practical domains.

Ask, and it shall be given: On the Turing completeness of prompting
This paper presents the first theoretical study demonstrating that the large language model (LLM) prompting paradigm is Turing-complete, meaning a finite-size Transformer can compute any computable function using corresponding prompts. The findings establish that a single finite-size Transformer can achieve similar complexity bounds as unbounded-size Transformers, providing a theoretical foundation for prompt engineering.

Deep Networks Learn Features From Local Discontinuities in the Label Function
This paper investigates how deep neural networks outperform kernel machines by analyzing feature learning through label function discontinuities attracting model function discontinuities during training. The study introduces a novel deep architecture, the Deep Linearly Gated Network (DLGN), which provides mechanistic interpretability by easily enumerating discontinuities and demonstrates competitive performance with ReLU networks and tree-learning algorithms on real-world datasets.

### Misc
Searching for Optimal Solutions with LLMs via Bayesian Optimization
This paper introduces Bayesian-OPRO (BOPRO), a framework that dynamically adapts search strategies for language models using Bayesian optimization, improving performance in tasks such as word search and molecule optimization. Evaluations demonstrate BOPRO's superior performance over baselines in certain tasks while highlighting challenges faced in program search due to limitations in distinguishing similar code sequences.

Towards Continuous Reuse of Graph Models via Holistic Memory Diversification
This paper introduces a novel Diversified Memory Selection and Generation (DMSG) framework for incremental learning in graphs, emphasizing memory diversity to improve memory selection from previous tasks and retain broad knowledge. The proposed method utilizes a buffer selection strategy and a diversified memory generation replay method, resulting in superior performance demonstrated through extensive experiments on public datasets.

Beware of Calibration Data for Pruning Large Language Models
The paper investigates the critical role of calibration data in post-training pruning for large language models, particularly emphasizing its significance at high sparsity levels. By conducting experiments on calibration data factors, the authors propose a self-generating synthesis strategy for feasible calibration data, achieving significant performance improvements in pruning methods on models like DCLM and LLaMA-3.

DiscoveryBench: Towards Data-Driven Discovery with Large Language Models
DiscoveryBench is introduced as the first comprehensive benchmark to evaluate the ability of large language models (LLMs) to autonomously perform data-driven discovery across 264 tasks from diverse domains, such as sociology and engineering. Despite existing frameworks achieving only a 25% success rate, DiscoveryBench highlights the challenges and serves as a critical resource for improving LLM capabilities in automating hypothesis search and verification from datasets.

PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding
This paper introduces PhysBench, a comprehensive benchmark designed to evaluate Vision-Language Models' (VLMs) understanding of physical phenomena across diverse tasks, revealing that while VLMs excel in common-sense reasoning, they fall short in physical understanding due to lack of embedded physical knowledge. To address this, the authors propose PhysAgent, a framework that enhances VLMs' physical understanding by integrating vision models, achieving significant improvements and contributing to the advancement of embodied agents' capabilities.

Training Neural Networks as Recognizers of Formal Languages
This paper addresses the gap between formal language theory and empirical testing by directly training and evaluating neural networks as binary classifiers for formal language recognition, using an extended algorithm for efficient sampling. Results show that RNNs and LSTMs often surpass transformers in performance, with auxiliary objectives offering varied benefits, contributing datasets and code as the FLaRe benchmark for future research.

TAU-106K: A New Dataset for Comprehensive Understanding of Traffic Accident
This paper introduces TABot, a multimodal large language model specialized for understanding traffic accidents, facilitated by the newly constructed TAU-106K dataset containing 106,000 annotated traffic accident videos and images. The study demonstrates TABot's superior performance in high-level anomaly comprehension, emphasizing its proficiency in accident recognition and spatial-temporal grounding, while enhancing the robustness of the TAU-106K benchmark.

A Watermark for Order-Agnostic Language Models
PATTERN-MARK is a novel pattern-based watermarking framework developed for order-agnostic language models, overcoming the limitations of traditional techniques suited for sequentially decoded models. Utilizing a Markov-chain-based watermark generator and a statistical pattern-based detection algorithm, PATTERN-MARK significantly improves detection efficiency, generation quality, and robustness in models like ProteinMPNN and CMLM.

STAFF: Speculative Coreset Selection for Task-Specific Fine-tuning
Task-specific fine-tuning for large language models is costly, and existing coreset selection methods often miss critical samples or have high selection overheads. STAFF, a new speculative coreset selection method, addresses these issues by leveraging a smaller model to estimate data scores, significantly improving data efficiency and model performance with reduced overhead, as demonstrated by up to 54.3% performance improvement and 70.5% reduction in overhead on multiple tasks and models.

Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding
This paper introduces Lattice Transform Coding (LTC), a method that employs lattice quantization to improve neural compression, addressing the limitations of integer rounding in latent spaces. LTC enhances real-world compression performance by integrating near-optimal information-theoretic designs, such as block coding and nested lattice quantization, achieving better rate-distortion outcomes with reasonable complexity.

Manifolds, Random Matrices and Spectral Gaps: The geometric phases of generative diffusion
This paper explores the latent geometry of generative diffusion models under the manifold hypothesis by examining the spectrum of eigenvalues and singular values of the Jacobian of the score function. It identifies three distinct phases in the generative process that explain why these models avoid manifold overfitting, highlighting a separation in timescales between internal distribution fitting and manifold geometry establishment.

Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models
FB-CPR is a novel approach for unsupervised reinforcement learning that uses the forward-backward method to regularize zero-shot RL, enabling models to imitate trajectories from unlabeled behaviors while retaining generalization capabilities. Demonstrated on a challenging humanoid control problem, FB-CPR achieves competitive performance in solving various whole-body tasks with human-like behaviors, outperforming state-of-the-art unsupervised RL and model-based baselines.

DeepLTL: Learning to Efficiently Satisfy Complex LTL Specifications for Multi-Task RL
This paper proposes a novel learning approach leveraging Büchi automata to address challenges in efficiently satisfying complex LTL specifications in multi-task RL, including arbitrary specifications not observed during training. The method demonstrates superior performance in both finite- and infinite-horizon domains, achieving higher satisfaction probability and efficiency than existing approaches.

Value-Incentivized Preference Optimization: A Unified Approach to Online and Offline RLHF
This paper introduces a unified approach called value-incentivized preference optimization (VPO) for both online and offline reinforcement learning from human feedback (RLHF) that addresses the challenge of incorporating uncertainty estimation into the reward function. VPO regularizes the maximum-likelihood estimate of the reward function with the corresponding value function, optimizes the policy with implicit reward modeling, and provides theoretical guarantees while demonstrating its effectiveness through experiments in text summarization, dialogue, and standard benchmarks.

Discrete Codebook World Models for Continuous Control
This paper introduces the Discrete Codebook World Model (DCWM), which utilizes a discrete and stochastic latent space with codebook encodings to improve performance in state-based continuous control tasks. The authors also propose DC-MPC, a model-based RL algorithm that, when combined with decision-time planning, shows competitive performance against recent state-of-the-art methods in continuous control benchmarks.

Progressive Mixed-Precision Decoding for Efficient LLM Inference
This paper addresses the challenge of deploying large language models on resource-constrained devices by introducing a phase-aware quantization strategy. The proposed Progressive Mixed-Precision Decoding (PMPD) method enhances efficiency by selectively lowering precision during different phases of inference, achieving significant speedup and throughput gains over fp16 models on Nvidia GPUs and LLM-optimized NPUs, while maintaining output quality.

Heavy-Tailed Diffusion Models
This paper addresses the limitations of traditional diffusion and flow-matching models in capturing heavy-tailed distributions by proposing a new framework that uses multivariate Student-t distributions. Introducing t-EDM and t-Flow models with a controllable scalar hyperparameter, the study demonstrates improved performance over standard models in estimating heavy-tailed events, as validated on high-resolution weather datasets.

Taming Transformer Without Using Learning Rate Warmup
This paper presents a theoretical analysis of the Transformer model training process, identifying the spectral energy concentration of $W_q^{\top} W_k$ as a cause for model crashes due to malignant entropy collapse. To address this, the authors propose a novel optimization strategy that smooths weight updates, preventing rapid spectral energy concentration and stabilizing model training without the need for learning rate warmup, as demonstrated through experiments with ViT, Swin-Transformer, and GPT models.

DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads
This paper introduces DuoAttention, a framework designed to optimize memory and computational efficiency for long-context large language models (LLMs) by differentiating between Retrieval Heads and Streaming Heads. By applying full KV caching only to the crucial Retrieval Heads and a lightweight cache to Streaming Heads, DuoAttention significantly reduces memory usage and latency during inference without compromising LLM performance, achieving notable improvements in speed and memory efficiency across various model architectures.

The Foundations of Tokenization: Statistical and Computational Concerns
This paper addresses the theoretical gap in understanding tokenization's impact on language model estimation by proposing a unified formal framework for analyzing tokenizer models. The framework establishes essential conditions for the principled use of tokenizers, aiming to ensure estimator consistency while addressing key concerns like inconsistency and ambiguity, thus contributing to robust theoretical foundations for neural language modeling.

Differentiable Integer Linear Programming
DiffILO introduces an unsupervised learning paradigm for solving integer linear programs (ILPs), reformulating these problems into continuous and differentiable optimization challenges, which allows for direct gradient descent training and inference. This approach reduces training costs significantly and produces solutions with higher feasibility and quality compared to traditional supervised methods, achieving a 13.2 times speedup in training and yielding better outcomes on standard ILP datasets.

Polynomial Composition Activations: Unleashing the Dynamics of Large Language Models
This paper introduces a new category of polynomial composition activations (PolyCom) for transformers, aiming to optimize their dynamics and enhance their expressivity compared to traditional activation functions like ReLU, GeLU, and SwishGLU. Empirical results show that incorporating PolyCom into large language models leads to improved performance in accuracy and convergence rates, requiring fewer parameters and capturing higher-order interactions within the data.

Robustness Inspired Graph Backdoor Defense
Graph Neural Networks (GNNs) are susceptible to backdoor attacks, endangering their deployment in real-world applications. This study proposes using prediction variance under random edge dropping to identify poisoned nodes and introduces a robust training strategy that successfully mitigates the effects of diverse backdoor attacks while maintaining high accuracy, with extensive experiments validating its effectiveness.

Making Text Embedders Few-Shot Learners
This paper introduces a training strategy that enhances the text representation capabilities of large language models using in-context learning (ICL), allowing them to handle unseen tasks effectively without modifications to model architecture. The proposed method, which involves sampling varying numbers of examples during training, maintains the models' zero-shot capabilities and avoids performance issues caused by common model modifications like bidirectional attention.

Gnothi Seauton: Empowering Faithful Self-Interpretability in Black-Box Transformers
The paper addresses the debate in Explainable AI (XAI) between self-interpretable models and post-hoc explanations for black-box models by proposing a novel method called AutoGnothi. AutoGnothi integrates a side network into black-box models to generate Shapley value explanations with minimal impact on prediction accuracy and computational resources, demonstrating superior efficiency and scalability in both vision and language tasks.

Towards Neural Scaling Laws for Time Series Foundation Models
This paper analyzes the scaling behavior of time series foundation models (TSFMs), specifically focusing on encoder-only and decoder-only Transformer architectures, across both in-distribution (ID) and out-of-distribution (OOD) data. The research demonstrates that encoder-only Transformers scale more effectively than decoder-only Transformers, and while advanced TSFMs improve ID performance, they hinder OOD scalability, providing practical guidelines for designing robust and scalable TSFMs.

STORM: Spatio-TempOral Reconstruction Model For Large-Scale Outdoor Scenes
STORM is a spatio-temporal reconstruction model that uses a data-driven Transformer architecture to efficiently infer dynamic 3D scene representations from sparse observations, eliminating the need for dense data and extensive motion supervision. It outperforms current state-of-the-art methods in dynamic scene reconstruction and scene flow estimation, enabling real-time rendering, and showcases the potential of self-supervised learning for enhanced understanding of dynamic outdoor scenes.

Neural Fluid Simulation on Geometric Surfaces
This paper introduces a neural physical simulation framework utilizing implicit neural representations to address challenges in incompressible fluid simulation on 3D surfaces. By implementing a parameterized vector field and covariant derivative-based advection process, the method ensures divergence-free simulations and maintains energy preservation with higher accuracy, flexibility, and memory efficiency, showing promise for applications like vorticity shape generation and Helmholtz decomposition.

Chain-of-Thought Provably Enables Learning the (Otherwise) Unlearnable
This paper investigates the effectiveness of chain-of-thought (CoT) reasoning in modern language models, proposing that CoT improves learning by decomposing complex tasks into simpler, more manageable subtasks. The authors demonstrate both theoretically and empirically that with CoT, transformers can efficiently learn complex tasks by breaking them into a finite number of learnable subtasks, highlighting enhanced performance in arithmetic reasoning tasks like polynomials and Boolean formulas.

Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation
GIMDiffusion is a novel Text-to-3D model that efficiently generates high-quality 3D objects from textual descriptions by leveraging geometry images to represent 3D shapes using 2D images, thus avoiding complex 3D-aware architectures. By incorporating a Collaborative Control mechanism and the rich 2D priors of existing Text-to-Image models, GIMDiffusion achieves strong generalization with minimal high-quality 3D training data, allowing for rapid and versatile 3D asset creation without manifold mesh restrictions.

Dysca: A Dynamic and Scalable Benchmark for Evaluating Perception Ability of LVLMs
Dysca is a dynamic and scalable benchmark designed to evaluate the perception abilities of Large Vision-Language Models (LVLMs) using synthetically generated images. By considering 51 image styles and 20 subtasks across various scenarios and question types, Dysca reveals weaknesses in both open and closed-source LVLMs and offers a framework for easily integrating new tasks and conditions.

PhiNets: Brain-inspired Non-contrastive Learning Based on Temporal Prediction Hypothesis
This paper explores the temporal prediction hypothesis inspired by predictive coding, and examines its validity through the lens of self-supervised learning using a model called PhiNet. The study discovers that PhiNet achieves more stable data representations and adapts more efficiently to new patterns in online and continual learning scenarios, supporting the robustness and adaptability of the temporal prediction hypothesis.

Sort-free Gaussian Splatting via Weighted Sum Rendering
This paper introduces Weighted Sum Rendering as an alternative to traditional alpha-blending in 3D Gaussian Splatting (3DGS), significantly improving rendering performance by eliminating the need for complex sorting operations. The proposed method offers competitive image quality and superior speed, achieving a 1.23x faster rendering on mobile device GPUs.

S4M: S4 for multivariate time series forecasting with Missing values
The paper introduces S4M, an advanced forecasting framework for multivariate time series data that integrates missing data handling directly into the Structured State Space Sequence (S4) model, eliminating the need for separate imputation steps. By employing an Adaptive Temporal Prototype Mapper and a Missing-Aware Dual Stream S4, the method demonstrates superior accuracy and robustness on diverse datasets, outperforming traditional approaches in scenarios with high missing data ratios.

Grounding Continuous Representations in Geometry: Equivariant Neural Fields
Equivariant Neural Fields (ENFs) are introduced as a novel Conditional Neural Field architecture that enhances geometric reasoning by incorporating geometry-informed cross-attention to guide the neural field with a latent point cloud of features. This approach ensures equivariance between field and latent representations, allowing for improved tasks such as classification and segmentation by faithfully representing geometric patterns and efficiently learning datasets of fields, outperforming conventional geometry-free models.

Efficient Learning with Sine-Activated Low-Rank Matrices
This paper introduces a novel theoretical framework that incorporates a sinusoidal function into the low-rank decomposition process, aiming to maintain parameter efficiency while improving model performance. The proposed method enhances existing low-rank models and demonstrates effectiveness in applications such as Vision Transformers, Large Language Models, Neural Radiance Fields, and 3D shape modeling.

MagicPIG: LSH Sampling for Efficient LLM Generation
MagicPIG introduces a novel approach utilizing Locality Sensitive Hashing (LSH) to efficiently manage attention computation in large language models, addressing challenges posed by the KV cache bottleneck. By employing a sampling-based approximation rather than TopK attention, MagicPIG enhances decoding throughput significantly while maintaining accuracy, showing improvements up to 5 times in throughput and achieving low decoding latency across various GPU hardware.

The Optimization Landscape of SGD Across the Feature Learning Strength
This paper investigates the scaling effect of the hyperparameter $\gamma$ in neural networks, revealing that its variation alters network dynamics from lazy to rich feature learning, improving performance. The study shows optimal learning rates scale with $\gamma$ and explores the under-studied ultra-rich regime, indicating that large $\gamma$ values facilitate distinctive optimization trajectories and can enhance online performance, suggesting that further analytical exploration could improve understanding of representation learning dynamics.

Towards Out-of-Modal Generalization without Instance-level Modal Correspondence
This paper introduces Out-of-Modal (OOM) Generalization, which aims to generalize to an unknown modality without having instance-level modal correspondence, addressing the limitations of traditional multi-modal learning. The authors propose the connect & explore (COX) method that utilizes a variational information bottleneck framework to link OOM data with known data and generate emergent correspondences, improving generalization results as validated through extensive evaluations.

No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images
NoPoSplat is a feed-forward model that reconstructs 3D scenes using 3D Gaussians from unposed, sparse multi-view images, achieving real-time performance with photometric loss training. By anchoring local camera coordinates into a canonical space and employing novel intrinsic embedding methods, the model circumvents errors in pose estimation, excels in novel view synthesis, and significantly surpasses state-of-the-art methods in pose estimation without the need for ground truth depth, proving its effectiveness in real-world applications.

Stem-OB: Generalizable Visual Imitation Learning with Stem-Like Convergent Observation through Diffusion Inversion
Visual imitation learning struggles with real-world application due to limited generalization against visual input perturbations like lighting and texture variations. To overcome this, we introduce *Stem-OB*, which utilizes pretrained image diffusion models to neutralize these low-level visual differences, significantly enhancing robustness and improving real-world robotic task success rates by an average of 22.2% without additional training compared to traditional methods.

VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents
VisRAG is introduced as a vision-language model (VLM)-based retrieval-augmented generation (RAG) pipeline that improves upon traditional text-based RAG systems by directly embedding documents as images to incorporate crucial visual information. The approach maximizes data retention and enhances performance in both retrieval and generation stages, showing a 20–40% performance gain and strong generalization capability, making it an effective solution for handling multi-modality documents.

RAG-DDR: Optimizing Retrieval-Augmented Generation Using Differentiable Data Rewards
This paper introduces a Differentiable Data Rewards (DDR) method to enhance Retrieval-Augmented Generation (RAG) systems by aligning data preferences between RAG modules, thus optimizing the performance of each agent within the system. The DDR approach outperforms traditional supervised fine-tuning by improving the ability of smaller-scale language models to utilize retrieved knowledge effectively, and it is especially adept at aligning data preferences and extracting key information, thereby reducing conflicts between the model's internal memory and external resources.

Refine Knowledge of Large Language Models via Adaptive Contrastive Learning
This paper introduces an Adaptive Contrastive Learning strategy that mimics human learning to reduce hallucinations in Large Language Models (LLMs). By constructing tailored positive and negative samples based on the LLMs' knowledge levels, the method effectively enhances knowledge retention and understanding while eliminating incorrect information, with experiments confirming its competitiveness and efficiency.

A Conditional Independence Test in the Presence of Discretization
This paper introduces a novel conditional independence (CI) test that accurately accommodates discretized variables, which challenges existing methods in applications such as Bayesian network learning and causal discovery. By employing a bridge equation and nodewise regression under the nonparanormal model, the proposed method effectively recovers precision coefficients and provides a robust test statistic with demonstrated theoretical and empirical validity.

Efficient Distribution Matching of Representations via Noise-Injected Deep InfoMax
The paper enhances the Deep InfoMax (DIM) method for self-supervised representation learning by enabling it to automatically match learned representations to a selected prior distribution through injecting independent noise into the encoder's outputs. This modification facilitates learning representations conforming to various distributions and has been tested on multiple downstream tasks, revealing a moderate trade-off between task performance and distribution matching quality.

HELMET: How to Evaluate Long-context Models Effectively and Thoroughly
The paper introduces HELMET, a benchmark designed to effectively evaluate long-context language models (LCLMs) across seven diverse, application-centric categories, addressing limitations of existing benchmarks such as insufficient lengths and unreliable metrics. Through a study of 59 LCLMs, the authors find that popular synthetic tasks don't predict downstream performance well, whereas using HELMET offers more reliable model rankings and insights into the performance gaps between open-source and closed models in complex tasks.

MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models
The paper introduces MMIE, a comprehensive benchmark designed to evaluate interleaved multimodal comprehension and generation in Large Vision-Language Models using a diverse set of 20,000 queries across various fields. MMIE addresses the limitations of existing benchmarks by providing scalable, reliable, and less biased evaluation through diverse question formats and an automated scoring model, ultimately demonstrating significant room for improvement in current models and aiming to catalyze advancements in the field.

Dynamic Sparse Training versus Dense Training: The Unexpected Winner in Image Corruption Robustness
This paper challenges the prevailing belief that Dense Training is superior for achieving robustness against image corruption by demonstrating that Dynamic Sparse Training can consistently surpass Dense Training in terms of robustness accuracy. Through experiments on images and videos using various deep learning architectures and algorithms, the authors show that Dynamic Sparse Training offers a previously unrecognized advantage in enhancing deep learning robustness without increasing resource costs.

BigDocs: An Open Dataset for Training Multimodal Models on Document and Code Tasks
The paper introduces BigDocs-7.5M, an open-access dataset with 7.5 million multimodal documents designed to address the limitations of training data accessibility and restrictive licensing in commercial applications of multimodal AI. By providing a benchmark suite, BigDocs-Bench, featuring real-world tasks such as GUI reasoning and code generation, the authors demonstrate that it significantly improves AI performance in document understanding, with human evaluations favoring outputs from models trained using BigDocs over traditional models like GPT-4o.

Building Interactable Replicas of Complex Articulated Objects via Gaussian Splatting
ArtGS introduces a novel approach using 3D Gaussians for improved part-mesh reconstruction and dynamics modeling of articulated objects, overcoming limitations in accurately integrating information across various object states. Through extensive experiments, ArtGS demonstrates state-of-the-art performance in joint parameter estimation and part mesh reconstruction, particularly excelling in efficiency and quality for complex multi-part articulated objects.

Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool Usage
The paper introduces a multi-modal agent tuning method that generates multi-modal tool-usage data automatically, enhancing a vision-language model's (VLM) ability to reason tool usage. By employing a data synthesis pipeline to create the MM-Traj dataset with 20K tasks, the developed T3-Agent shows a 20% improvement over untrained VLMs on the GTA and GAIA benchmarks, demonstrating the method's effectiveness in producing high-quality data for tool usage capabilities.

CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery
This paper introduces CS-Bench, the first multilingual benchmark specifically designed to evaluate large language models' (LLMs) performance across 26 subfields of computer science in four key areas. By providing approximately 10,000 curated test samples and conducting evaluations on over 30 mainstream LLMs, the study reveals performance relationships, reasons behind failures, and future improvement directions, emphasizing the link between CS capabilities and expertise in mathematics and coding.

Mechanistic Permutability: Match Features Across Layers
This paper introduces SAE Match, a novel method for aligning Sparse Autoencoder (SAE) features across layers of a neural network without using data. By minimizing mean squared error and incorporating activation thresholds, this approach enhances feature matching quality, revealing persistent feature dynamics across layers and advancing tools for mechanistic interpretability.

Offline RL in Regular Decision Processes: Sample Efficiency via Language Metrics
This paper addresses offline Reinforcement Learning (RL) in Regular Decision Processes (RDPs) by introducing two novel techniques: a metric grounded in formal language theory and a Count-Min-Sketch (CMS) approach. These innovations improve sample efficiency—sometimes exponentially—and reduce memory requirements, with the effectiveness confirmed through PAC sample complexity bounds and experimental validation.

Interactive Speculative Planning: Enhance Agent Efficiency through Co-design of System and User Interface
This paper introduces Interactive Speculative Planning, a method to improve the efficiency of agent planning by integrating user interactions, addressing high latencies in agents based on large language models. The approach emphasizes co-designing the agent system and user interface, allowing human interruptions to accelerate planning and enhancing the user-centric capability of the system.

What Makes a Good Diffusion Planner for Decision Making?
This paper investigates the essential components of diffusion planning in offline reinforcement learning through extensive empirical experiments, training and evaluating over 6,000 diffusion models. The study reveals that some unconventional design choices, such as unconditional sampling with selection and using Transformers over U-Net, lead to better performance, culminating in a strong diffusion planning baseline that sets new benchmarks in the field.

FlashRNN: I/O-Aware Optimization of Traditional RNNs on modern hardware
This paper introduces FlashRNN, a hardware-optimized approach to improve the performance of RNNs, addressing their state-tracking capabilities while mitigating the limitations of sequential processing. By implementing advanced parallelization and optimization strategies using Triton and CUDA on modern GPUs, the authors achieve significant performance improvements, enabling faster processing and larger hidden sizes, thereby enhancing their applicability to time-series and logical reasoning tasks.

Discrete Diffusion Schrödinger Bridge Matching for Graph Transformation
The paper introduces Discrete Diffusion Schrödinger Bridge Matching (DDSBM), a novel framework employing continuous-time Markov chains to address the Schrödinger Bridge problem in high-dimensional discrete state spaces, such as graphs. DDSBM's application to molecular optimization in chemistry showcases its effectiveness in optimizing properties of interest with minimal graph transformation, while its underlying dynamics model is likened to entropy-regularized optimal transport using graph edit distance.

Linear Recursions for Everyone
This paper introduces the abstraction of linear recursions to better understand the computational structure of linear RNNs like Mamba, which are not easily expressible in PyTorch. By deriving a parallel algorithm and creating a simple CUDA extension template, the authors aim to make linear recursions more accessible and inspire further research in efficient sequence mixing.

Causal Representation Learning from Multimodal Biomedical Observations
This paper addresses the challenges of interpretability and identifiability in machine learning models for multimodal biomedical datasets by developing flexible identification conditions and principled methods. It introduces a nonparametric approach with identifiability guarantees and structural sparsity for causal connections, validated through extensive experiments on both synthetic and real-world datasets, aligning with established biomedical research.

A Little Goes a Long Way: Efficient Long Context Training and Inference with Partial Contexts
This paper introduces LongGen, an approach for efficiently extending the context length of pretrained large language models (LLMs) while integrating GPU-friendly architectural modifications to reduce training and serving overhead. By utilizing a hybrid sparse attention model, LongGen not only improves long-context performance and efficiency but also demonstrates significant speedups and memory reduction both during training and inference, outperforming full-attention baselines on various tasks.

Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces
This paper advances theoretical understanding in reinforcement learning (RL) by employing a geometric approach to explore continuous state and action spaces, revealing that the training dynamics of a two-layer neural policy create a low-dimensional manifold of attainable states. The authors demonstrate, through both theoretical proof and empirical evidence, that the dimensionality of this manifold corresponds to the action space's dimensionality, significantly enhancing RL performance in high-degree-of-freedom control environments through a novel manifold learning layer.

Noisy Test-Time Adaptation in Vision-Language Models
The paper introduces Zero-Shot Noisy Test-Time Adaptation (ZS-NTTA), a method designed to adapt pre-trained vision-language models to target data with noisy samples during test-time in a zero-shot manner. By decoupling the classifier and detector, and incorporating the Adaptive Noise Detector (AdaND) to handle noisy samples, the approach significantly improves zero-shot out-of-distribution detection and harmonic mean accuracy, all while maintaining computational efficiency.

LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code
This paper introduces LiveCodeBench, a novel and comprehensive benchmark designed to evaluate Large Language Models' (LLMs) capabilities in code-related applications beyond mere code generation, including self-repair, code execution, and test output prediction. LiveCodeBench addresses limitations of existing benchmarks by offering a contamination-free and updated collection of coding problems, facilitating a more reliable assessment of overfit and saturation in LLMs, and has been used to evaluate over 50 models, uncovering critical insights on current approaches to code evaluation.

Multi-Dimensional Conformal Prediction
This paper presents a novel approach to conformal prediction for classification by introducing a multi-dimensional nonconformity score that improves label separation. By focusing on regions with low concentrations of incorrect labels and using a self-ensembling technique, the proposed method generates smaller, more informative prediction sets, outperforming traditional methods on various benchmarks.

Manifold Induced Biases for Zero-shot and Few-shot Detection of Generated Images
This paper addresses the challenge of distinguishing between real and AI-generated images, particularly in zero-shot and few-shot settings. By analyzing biases in generated content using manifold analysis and employing a mixture-of-experts methodology, the study enhances both theoretical understanding and practical performance, demonstrating superior results over existing methods across 20 generative models.

Robust Root Cause Diagnosis using In-Distribution Interventions
Diagnosing anomalies in complex systems is crucial, and the proposed In-Distribution Interventions (IDI) algorithm identifies root causes more accurately by focusing on nodes with anomalous values that, if corrected, would prevent anomalies at the target node. Unlike previous methods that struggle with the rarity of anomalies, IDI leverages interventional estimates from in-distribution inputs within a Structural Causal Model, demonstrating superior accuracy and robustness on both synthetic and benchmark datasets compared to existing methods.

Deep Distributed Optimization for Large-Scale Quadratic Programming
This paper introduces **DeepDistributedQP**, a deep learning-aided distributed optimization framework, which extends the Operator Splitting QP method to efficiently solve large-scale quadratic programming problems with network structures. Leveraging learned policies, it achieves significant speed improvements and scalability, providing certifiable performance guarantees and superior results in various applications compared to conventional optimizers.

From Search to Sampling: Generative Models for Robust Algorithmic Recourse
This paper introduces GenRe, a generative recourse model that jointly trains for proximity, plausibility, and validity to provide superior recourse recommendations for algorithmic decisions. Unlike traditional methods that rely on non-robust searches during inference, GenRe uses forward sampling to deliver more effective outcomes while balancing cost, plausibility, and validity better than state-of-the-art baselines.

Compute-Optimal LLMs Provably Generalize Better with Scale
The paper investigates why larger language models (LLMs) generalize better by developing generalization bounds on the pretraining objective in the compute-optimal regime. It introduces a novel Freedman-type martingale concentration inequality that explains how the decrease in loss variance and quantization error with model scaling leads to smaller generalization gaps, providing a new scaling law for the generalization gap.

A transfer learning framework for weak to strong generalization
This paper addresses the challenge of aligning large language models (LLMs) with superhuman capabilities using weaker human feedback without degrading their performance. By proving that a refinement-based approach, as opposed to naive fine-tuning, effectively transfers latent concepts from weaker to stronger models, the study demonstrates the practical applicability of this method in various LLM alignment tasks.

ReCogLab: a framework testing relational reasoning & cognitive hypotheses on LLMs
The paper introduces ReCogLab, a generative framework designed to facilitate the exploration of relational reasoning in language models. This framework allows for dynamic configuration and flexible evaluation, enabling researchers to recreate classic cognitive science experiments and study reasoning biases, with findings demonstrated on various language models.

CityAnchor: City-scale 3D Visual Grounding with Multi-modality LLMs
CityAnchor is a 3D visual grounding method designed to accurately localize urban objects within city-scale point clouds, overcoming the limitations of previous systems that were confined to image or small-scale point clouds. Utilizing a multi-modality LLM, CityAnchor operates in two stages—coarse localization on a 2D map and fine-grained matching—to effectively match text descriptions to objects, as demonstrated by experiments on the CityRefer dataset and a new synthetic dataset.

Enhancing Graph Of Thought: Enhancing Prompts with LLM Rationales and Dynamic Temperature Control
Enhancing Graph of Thoughts (EGoT) is a novel method that improves the performance of large language models on complex reasoning tasks by automating the generation of accurate responses through a graph-based approach. By utilizing Cosine Annealing and node scoring to refine response accuracy, EGoT achieves higher precision in tasks like sorting and navigation problems compared to previous methods.

Scaling up Masked Diffusion Models on Text
This paper investigates the scalability and effectiveness of masked diffusion models (MDMs) in language modeling, establishing their competitive performance compared to autoregressive models (ARMs) in both language understanding and text generation tasks. The study demonstrates MDMs' advantages, such as enhanced performance on zero-shot benchmarks, competitive math reasoning, and faster sampling times, along with their ability to handle bidirectional reasoning and adapt to temporal shifts, highlighting their potential to overcome limitations faced by significantly larger ARMs like Llama-2 and GPT-3.

Rational Decision-Making Agent with Learning Internal Utility Judgment
This paper introduces RaDAgent, a novel LLM-based agent designed to autonomously make decisions without relying on potentially flawed external performance measures. By employing an iterative framework for Experience Exploration and Elo-based Utility Learning, RaDAgent achieves superior decision-making performance and cost efficiency, as demonstrated on various datasets, with an average improvement of 7.8% over existing methods.

From Commands to Prompts: LLM-based Semantic File System for AIOS
This paper introduces an LLM-based Semantic File System (LSFS) for LLM Agent Operating Systems, which allows users to manage files through natural language prompts, enhancing file usability and interaction. The proposed LSFS significantly improves retrieval accuracy and speed, and supports advanced file management operations like semantic file rollback and sharing, achieving high success rates compared to traditional systems.

Do You Keep an Eye on What I Ask? Mitigating Multimodal Hallucination via Attention-Guided Ensemble Decoding
This paper addresses the issue of object hallucination in Large Vision-Language Models by proposing Ensemble Decoding (ED), which improves accuracy by splitting images into sub-images and adjusting logit distributions with an attention-based weighting system. The introduction of an adaptive plausibility constraint and the FastED variant for speed-critical applications demonstrates state-of-the-art performance across benchmarks, highlighting the method's effectiveness in reducing hallucinations without scalability limitations.

Hotspot-Driven Peptide Design via Multi-Fragment Autoregressive Extension
PepHAR is a hot-spot-driven autoregressive generative model designed to improve the process of peptide generation by focusing on key residues that enhance peptide-target interactions, addressing critical challenges such as unequal residue contributions and peptide geometry constraints. By integrating hot spot sampling with fragment-based extension, PepHAR enables the creation of customized peptides for specific protein targets and has demonstrated significant potential in computational peptide binder design, with its source code being made publicly available.

DenoiseVAE: Learning Molecule-Adaptive Noise Distributions for Denoising-based 3D Molecular Pre-training
DenoiseVAE is a novel 3D molecular pre-training method that improves molecular representation learning by generating atom-specific noise distributions, allowing for more accurate force field recovery. Through a joint learning approach using a Noise Generator and a Denoising Module, DenoiseVAE adapts to the unique noise profiles of different molecules, resulting in superior performance on molecular property prediction tasks compared to existing methods.

Conformal Prediction Sets Can Cause Disparate Impact
Conformal prediction outputs sets of predictions to quantify uncertainty, but these sets can result in disparate impacts across protected groups when used in decision-making. This study finds that equalizing set sizes across groups, rather than using Equalized Coverage, reduces disparate impact, suggesting a more effective approach to promoting fairness.

Local Steps Speed Up Local GD for Heterogeneous Distributed Logistic Regression
This paper analyzes two variants of Local Gradient Descent for distributed logistic regression on heterogeneous, separable data, demonstrating a convergence rate of \(O(1/KR)\) with \(K\) local steps and sufficiently large \(R\) communication rounds. The study's key contribution is the improved convergence guarantee achieved by using a large step size \(\eta \gg 1/K\), which contrasts with existing analyses that require \(\eta \leq 1/K\).

Bad-PFL: Exploiting Backdoor Attacks against Personalized Federated Learning
This paper addresses the vulnerabilities of federated learning (FL) to backdoor attacks, particularly in the context of data heterogeneity, by developing Bad-PFL, which uses features from natural data as backdoor triggers. The proposed approach demonstrates significant durability and effectiveness in embedding backdoors into personalized federated learning (PFL) models, outperforming existing methods across multiple datasets, even with advanced defenses in place.

A Large-scale Dataset and Benchmark for Commuting Origin-Destination Flow Generation
This paper introduces a large-scale dataset of commuting Origin-Destination (OD) flows for 3,333 areas across diverse urban environments in the United States, aimed at standardizing model performance comparisons in the absence of historical OD data. The study benchmarks existing models, revealing that network-based generative models excel in precision and generalization, potentially guiding future research in graph generative modeling.

MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?
MME-RealWorld is introduced as a new benchmark to address the limitations of existing evaluations for Multimodal Large Language Models (MLLMs), such as small data scale, low-quality annotations, and insufficient task difficulty. Comprising over 13,366 high-quality images and 29,429 question-answer pairs across 43 subtasks in five real-world scenarios, the benchmark reveals that even advanced MLLMs struggle to achieve high accuracy, highlighting the need for improvements in handling high-resolution images and complex real-world tasks.

One Step Diffusion via Shortcut Models
Shortcut Models present a novel approach to generative modeling that accelerates image generation by conditioning a single network on both the current noise level and desired step size, allowing for flexible and efficient sampling. This method simplifies the generation process compared to existing techniques, producing superior quality samples with reduced complexity and adaptability in sampling steps.

Monet: Mixture of Monosemantic Experts for Transformers
The paper introduces the Mixture of Monosemantic Experts for Transformers (Monet) architecture, which enhances mechanistic interpretability in large language models by integrating sparse dictionary learning into end-to-end pretraining, addressing the issue of polysemanticity. Monet's novel expert decomposition method allows for extensive expert scaling without performance loss, enabling improved knowledge manipulation across various domains and enhancing the transparency and alignment of language models with human values while mitigating undesirable behaviors.

Prioritized Generative Replay
This paper proposes a prioritized, parametric version of an agent's memory using generative models to enhance sample-efficient online reinforcement learning by densifying past experiences and guiding them towards more useful parts of the agent's history. By utilizing conditional diffusion models and relevance functions, the approach improves performance and sample efficiency, promotes diversity in generated transitions, reduces overfitting, and supports higher update-to-data ratios, offering scalability advantages for online RL agents.

MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering
MLE-bench is a benchmark designed to evaluate AI agents' performance in machine learning engineering, comprising 75 diverse competitions from Kaggle that test skills like model training and dataset preparation. The study finds that the best AI setup reaches at least a Kaggle bronze medal level in 16.9% of tasks, and offers insights into resource-scaling and pre-training contamination, with all codes available on GitHub for further research.

Privacy-Preserving Personalized Federated Prompt Learning for Multimodal Large Language Models
This paper introduces Differentially Private Federated Prompt Learning (DP-FPL), which integrates multimodal large language models with federated learning to address the trade-off between personalization, generalization, and privacy in AI systems. By employing a novel privacy technique on low-rank components and the global prompt, the proposed method effectively mitigates privacy noise, improving model performance as demonstrated through extensive experiments.

Precise Parameter Localization for Textual Generation in Diffusion Models
This paper reveals that less than 1% of diffusion models' parameters, primarily in attention layers, are responsible for generating textual content in images, and proposes targeting these layers to enhance textual generation efficiency. By fine-tuning these layers, the authors improve text-generation capabilities, enable textual editing in images, and prevent toxic text generation, demonstrating the approach's versatility across various diffusion model architectures.

DPaI: Differentiable Pruning at Initialization with Node-Path Balance Principle
Pruning at Initialization (PaI) improves neural network efficiency by eliminating weights before training, but often faces large-scale discrete optimization challenges. This paper introduces DPaI, a novel differentiable pruning method that leverages gradient-based optimization to enhance trainability and achieve superior performance compared to current techniques in various architectures.

Find A Winning Sign: Sign Is All We Need to Win the Lottery
This paper investigates the role of parameter sign configuration in the Lottery Ticket Hypothesis, demonstrating its importance for generalization in sparse networks. By preserving parameter signs and alleviating reliance on normalization layers, the authors show that randomly initialized networks can achieve performance comparable to the original sparse networks across various architectures.

Nova: Generative Language Models for Assembly Code with Hierarchical Attention and Contrastive Learning
This work introduces Nova, a generative large language model specifically designed for assembly code, addressing challenges such as low information density and diverse optimizations. By employing a hierarchical attention mechanism and contrastive learning objectives, Nova significantly improves binary code decompilation and code similarity detection, outperforming existing techniques by notable margins in accuracy and recall.

Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting
TSFlow is introduced as a sophisticated model for time series that leverages conditional flow matching, Gaussian processes, and optimal transport paths to better align prior distributions with the data's temporal structure, overcoming limitations in diffusion models. Experimental results on eight datasets show that TSFlow excels in both probabilistic forecasting and generating high-quality samples, delivering competitive outcomes in various forecasting benchmarks.

Interpreting Language Reward Models via Contrastive Explanations
Reward models (RMs) are essential for aligning large language models with human preferences, yet their opaque nature limits trust in their predictions. This paper introduces a method using contrastive explanations to elucidate RM decisions, improving transparency and interpretability by generating comparative examples that highlight evaluation attributes, thereby enhancing the understanding and trustworthiness of LLM alignment.

KooNPro: A Variance-Aware Koopman Probabilistic Model Enhanced by Neural Process for Time Series Forecasting
The paper introduces **KooNPro**, a novel probabilistic time series forecasting model that combines a variance-aware Koopman model with Neural Process to capture complex temporal dynamics and improve prediction stability. Through extensive experiments on nine real-world datasets, KooNPro consistently outperforms existing methods, highlighting its effectiveness in disentangling correlations and integrating fine dynamic details.

Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
This paper introduces REPresentation Alignment (REPA), a straightforward regularization technique to improve generative diffusion models by aligning noisy input hidden states with clean image representations from external, pretrained visual encoders. The method significantly enhances training efficiency and generation quality, demonstrated by a 17.5x speedup in SiT training and achieving state-of-the-art FID=1.42 in final generation quality.

ContextGNN: Beyond Two-Tower Recommendation Systems
The paper introduces Context-based Graph Neural Networks (ContextGNNs), a novel architecture that combines pair-wise and two-tower representations for improved link prediction in recommendation systems. By effectively balancing scalability and accuracy, ContextGNN adapts to various data characteristics and enhances performance by 20% on average compared to existing traditional and GNN-based methods in diverse recommendation tasks.

Steering Masked Discrete Diffusion Models via Discrete Denoising Posterior Prediction
This paper introduces Discrete Denoising Posterior Prediction (DDPP), a novel framework for steering Masked Diffusion Models (MDMs) by treating the task as probabilistic inference to sample from a target Bayesian posterior. The DDPP framework is applied to various tasks including class-conditional image modeling, text-based reward alignment for MDMs via Reinforcement Learning from Human Feedback (RLHF), and protein language model finetuning, with successful validation in generating reward-optimized protein sequences.

WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild
WildBench is an automated evaluation framework designed to benchmark large language models (LLMs) with challenging real-world user queries, using 1,024 tasks derived from extensive human-chatbot conversation logs. It introduces two metrics, WB-Reward and WB-Score, which provide reliable and interpretable judgments with a strong correlation to human evaluations, outperforming existing benchmarks in terms of correlation with human-voted rankings and cost-efficiency.

A Spark of Vision-Language Intelligence: 2-Dimensional Autoregressive Transformer for Efficient Finegrained Image Generation
The paper introduces the DnD-Transformer, a novel 2-Dimensional Autoregression model enhancing vector-quantization autoregressive image generation by predicting more codes through model depth. This approach enables the generation of higher quality images with complex elements in a self-supervised fashion, demonstrating an unprecedented integration of vision-language intelligence in image generation models.

Language Imbalance Driven Rewarding for Multilingual Self-improving
This paper introduces a method called "Language Imbalance Driven Rewarding" that uses the disparity between dominant and non-dominant languages in Large Language Models (LLMs) as a reward signal to enhance multilingual capabilities. By applying this approach through iterative training, the authors demonstrate improved performance in non-dominant languages while also boosting the dominant language, achieving a notable average improvement in multilingual tasks and contributing to the self-improvement of LLMs.

Interpreting Emergent Planning in Model-Free Reinforcement Learning
This paper provides the first mechanistic evidence that model-free reinforcement learning agents can learn to plan using a methodology grounded in concept-based interpretability, showcased in the Sokoban benchmark. The study demonstrates that the DRC agent employs learned concept representations to form plans influencing action selection, revealing a resemblance to parallelized bidirectional search, thereby enhancing comprehension of planning behaviors in artificial agents.

Generative Adversarial Ranking Nets
This paper introduces Generative Adversarial Ranking Networks (GARNet), an adversarial training framework designed to generate data that meets user-specific criteria by leveraging user preferences. By utilizing a ranker-generator dynamic, GARNet aligns generated and user-preferred data distributions with theoretical and empirical backing, demonstrating effectiveness across both discrete and continuous properties.

DreamDistribution: Learning Prompt Distribution for Diverse In-distribution Generation
This paper addresses the challenge of generating diverse customized images with Text-to-Image (T2I) diffusion models by personalizing them at an abstract level using a set of reference images. The authors propose learning soft prompts to enable novel image creation with variation control, providing flexibility for text-guided editing and adaptability to other tasks like text-to-3D, demonstrating effectiveness through both quantitative and human evaluations.

Directional Gradient Projection for Robust Fine-Tuning of Foundation Models
This paper introduces Directional Gradient Projection (DiGraP), a novel method for robust fine-tuning that leverages directional gradient information to enhance multi-objective optimization, reducing the need for extensive hyper-parameter tuning. DiGraP is validated through image classification and generalized to multi-modal settings, showing consistent performance improvements on both in-distribution and out-of-distribution tasks in image classification and Visual Question Answering challenges.

Active Learning for Neural PDE Solvers
This paper introduces AL4PDE, a benchmark for applying active learning (AL) to neural PDE solvers, aiming to enhance efficiency by using smaller, more informative training sets. The study demonstrates that AL methods significantly reduce average and worst-case errors, while producing consistent and reusable datasets beneficial for surrogate models.

Flow Distillation Sampling: Regularizing 3D Gaussians with Pre-trained Matching Priors
This paper introduces Flow Distillation Sampling (FDS) to improve 3D Gaussian Splatting (3DGS) by incorporating pre-trained geometric knowledge, enhancing geometric reconstruction accuracy in regions with sparse or no observational input views. The experimental results demonstrate that FDS significantly outperforms state-of-the-art methods in depth rendering, mesh reconstruction, and novel view synthesis, offering insights into its effects on geometric accuracy and rendering quality.

Overcoming Lower-Level Constraints in Bilevel Optimization: A Novel Approach with Regularized Gap Functions
This paper addresses the limitations in existing bilevel optimization methods by developing a novel single-loop, Hessian-free algorithm that accommodates more general lower-level constraints. The proposed method, which is supported by a rigorous non-asymptotic convergence analysis, transforms constrained bilevel optimization into a manageable single-level problem and is validated through various applications, including hyperparameter learning tasks and generative adversarial networks.

Learn Your Reference Model for Real Good Alignment
The paper introduces Trust Region methods (TR-DPO, TR-IPO, TR-KTO) for offline alignment of Large Language Models, addressing the issue of overoptimization that affects sample quality by dynamically updating the reference policy during training. Demonstrating superior performance in helpful dialogue and summarization tasks and outperforming traditional methods in benchmarks like AlpacaEval 2 and Arena-Hard with the Llama3 model, the study validates the effectiveness of Trust Region methods in enhancing model alignment without compromising performance.

Block Verification Accelerates Speculative Decoding
Speculative decoding accelerates large language model inference by using a fast model to draft token blocks that are then verified in parallel by the target model to ensure identical output distribution to the target model's sample. This paper introduces *Block Verification*, an optimal draft verification method that efficiently verifies entire blocks of tokens, providing a consistent wall-clock speedup of 5%-8% over previous token-by-token verification approaches without increasing code complexity or sacrificing performance guarantees.

Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures
The paper presents Vision-RWKV (VRWKV), a model that adapts the RWKV architecture from natural language processing to enhance efficiency in vision tasks, particularly for high-resolution image processing. VRWKV not only surpasses the performance of the Vision Transformer (ViT) in image classification with significantly faster speeds and lower memory usage but also excels in dense prediction tasks, proving to be a more efficient alternative for visual perception.

ImDy: Human Inverse Dynamics from Imitated Observations
This paper presents a novel approach to human inverse dynamics (ID) by leveraging motion imitation algorithms and physics simulators, creating a large-scale benchmark called Imitated Dynamics (ImDy). The ImDy framework effectively trains a data-driven ID solver, ImDyS, which demonstrates strong performance in both human inverse dynamics and ground reaction force estimation, offering potential as a versatile motion analysis tool.

Positional Embeddings in Transformer Models: Evolution from Text to Vision Domains
This paper examines positional encoding techniques in transformer models, focusing on ALiBi and RoPE, which address sequence length extrapolation challenges. It provides a novel empirical comparison of these methods in Vision Transformers, highlighting their impact on performance and interpolation strategies used to enhance extrapolation.

PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration
Vision Language Models (VLMs), such as CLIP, are enhanced for pathology by training on high-quality image-caption pairs derived from large-scale Whole Slide Image (WSI) datasets like TCGA, resulting in the creation of the PathGen-1.6M dataset. This work demonstrates significant improvements in pathology-specific image analysis tasks and enhances multimodal model capabilities using the new PathGen-CLIP model, offering scalable data generation and open-access resources for advancing pathology research.

LongGenBench: Benchmarking Long-Form Generation in Long Context LLMs
The paper introduces $\textit{LongGenBench}$, a new benchmark for evaluating large language models' ability to generate long-form text while following complex instructions, addressing a critical gap not captured by existing benchmarks. Evaluations show that current state-of-the-art models struggle with these tasks, particularly as text length increases, highlighting the need for further development in generating coherent long-form content.

Forte : Finding Outliers with Representation Typicality Estimation
This paper introduces a novel approach that improves out-of-distribution (OOD) detection by leveraging representation learning and manifold estimation, overcoming limitations of pixel-focused generative models. The proposed method achieves state-of-the-art performance on challenging benchmarks and synthetic data detection tasks, demonstrating its effectiveness in distinguishing photorealistic synthetic data from genuine training data.

Vision and Language Synergy for Rehearsal Free Continual Learning
This paper introduces a novel prompt-based structure and algorithm that improve continual learning by integrating language as input, task-wise generators, soft task-id prediction, and generated prompts as auxiliary data. The proposed method outperforms existing state-of-the-art approaches in various datasets, achieving significant gains in accuracy and minimizing forgetting, while maintaining stability-plasticity trade-offs, which is supported by comprehensive theoretical and experimental analyses.

Second Order Bounds for Contextual Bandits with Function Approximation
This paper introduces novel algorithms for contextual bandits with function approximation that achieve regret bounds sensitive to the variance of the measurement noise, whether constant or variable. The proposed methods improve upon existing optimistic least squares algorithms by providing tighter regret bounds that account for the complexity of the function class through the eluder dimension, thus advancing the state-of-the-art in variance-sensitive regret analysis in contextual bandit problems.

Fast Training of Sinusoidal Neural Fields via Scaling Initialization
This paper investigates the initialization of sinusoidal neural fields (SNFs) to enhance training speed, revealing that the traditional signal propagation-based approach is suboptimal. By employing a simple weight scaling technique, which involves multiplying each weight by a constant (except the last layer), the authors achieve a 10× acceleration in training, outperforming newer architectures across various data domains and providing insights into improved spectral bias and optimization conditions.

Open-Set Graph Anomaly Detection via Normal Structure Regularisation
This paper addresses open-set Graph Anomaly Detection (GAD) by introducing a novel approach called Normal Structure Regularisation (NSReg), which enhances the detection of unseen anomalies while maintaining accuracy for seen anomalies by incorporating a regularisation term that promotes semantically-rich representations of normal nodes. Extensive experiments demonstrate that NSReg significantly outperforms existing methods, improving performance by at least 14% AUC-ROC on unseen anomaly classes and 10% on all anomaly classes.

MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation
Model merging traditionally involves combining multiple single-task models into a multitask model by averaging parameters, but this can lead to conflicts between task objectives. The paper introduces Model Merging with Amortized Pareto Front (MAP), an efficient algorithm that uses surrogate models to identify trade-offs and provide a Pareto set of solutions, along with Bayesian and Nested MAP variants to handle varying task counts, significantly reducing computational costs while improving task objective balancing.

Decision Information Meets Large Language Models: The Future of Explainable Operations Research
This paper introduces Explainable Operations Research (EOR), a framework designed to enhance transparency in decision-making by providing actionable and understandable explanations in operations research applications. EOR utilizes Decision Information derived from what-if analysis, supported by bipartite graphs and Large Language Models (LLMs), and presents the first industrial benchmark to evaluate the efficacy of these explanations, setting a new standard for clarity in the field.

Determine-Then-Ensemble: Necessity of Top-k Union for Large Language Model Ensembling
This study explores the challenges of ensembling large language models (LLMs), finding that model compatibility is crucial for improved performance. Introducing a new strategy called \textsc{UniTE}, the research demonstrates an efficient way to ensemble LLMs by focusing on the union of top-k tokens from each model, enhancing performance while reducing computational demands.

TTVD: Towards a Geometric Framework for Test-Time Adaptation Based on Voronoi Diagram
Deep learning models often face generalization issues due to distributional shifts, and test-time adaptation (TTA) aims to improve model performance during inference under these conditions. This paper proposes the Test-Time adjustment by Voronoi Diagram guidance (TTVD), a novel framework leveraging geometric properties to enhance neighbor-based methods, demonstrating significant improvements over current methods on benchmark datasets, while also proving robust to batch size and class imbalance variations.

Improving Deep Regression with Tightness
This paper demonstrates that preserving the ordinality of targets in deep regression can improve performance by reducing the conditional entropy $H(Z|Y)$, a factor crucial for generalization. The authors propose an optimal transport-based regularizer and a target duplication strategy to preserve target similarity in feature space, showing their effectiveness across three real-world regression tasks.

ETA: Evaluating Then Aligning Safety of Vision Language Models at Inference Time
Vision Language Models (VLMs) face significant safety challenges, particularly with adversarial visual inputs that can bypass defense mechanisms. The proposed ETA (Evaluating Then Aligning) framework effectively enhances safety and utility by evaluating and aligning VLM outputs with minimal resources, demonstrating superior performance in reducing unsafe responses and improving helpfulness in multimodal contexts.

Matrix Product Sketching via Coordinated Sampling
This paper addresses the problem of approximating a matrix product $\bv{A}^T\bv{B}$ using small space sketches, proving that coordinated random sampling can outperform traditional linear sketching methods for sparse matrices. The authors demonstrate that their approach requires significantly smaller sketches to achieve Frobenius norm error, and empirical results show substantial improvements in distributed linear regression and transformer-based language models applications.

Agent Security Bench (ASB): Formalizing and Benchmarking Attacks and Defenses in LLM-based Agents
This paper introduces Agent Security Bench (ASB), a comprehensive framework to evaluate and benchmark attacks and defenses in LLM-based agents across various real-world scenarios. The findings reveal significant vulnerabilities in these agents, highlighting the need for improved security measures, with the researchers providing a new metric to assess the balance between utility and security and offering their benchmark results and code for community use.

Accelerating neural network training: An analysis of the AlgoPerf competition
The AlgoPerf competition focuses on evaluating enhancements in neural network training through improved algorithms, with rulesets for both workload-agnostic hyperparameter searches and hyperparameter-free approaches. Significant findings include Distributed Shampoo showing superior performance over traditional methods like Adam in time efficiency, and the Schedule Free AdamW algorithm proving effective without hyperparameters, indicating substantial progress and opportunities for advancement in training algorithms.

Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping
Large Vision-Language Models (LVLMs) excel in multimodal tasks but face challenges with static benchmarks and data contamination. To address this, the Vision-Language Bootstrapping (VLB) protocol dynamically generates and evaluates new visual question-answering samples, offering a robust assessment of LVLMs' evolving capabilities with reduced data contamination and varied complexity.

As Simple as Fine-tuning: LLM Alignment via Bidirectional Negative Feedback Loss
Direct Preference Optimization (DPO) is a computationally efficient alternative to Reinforcement Learning from Human Feedback (RLHF) that faces instability issues due to unidirectional likelihood-derivative negative feedback. To address this, the paper introduces a novel Bidirectional Negative Feedback (BNF) alignment loss, which simplifies the alignment process without additional hyper-parameters or pairwise preference data, achieving competitive results on QA benchmarks and improved stability in reasoning tasks.

Towards Understanding Text Hallucination of Diffusion Models via Local Generation Bias
This paper investigates textual hallucinations in score-based diffusion models, attributing them to a local generation bias where networks overly rely on correlated local regions, leading to nonsensical symbol assembly despite individual symbol accuracy. The study reveals that this bias towards independent distributions persists across different denoising network architectures like MLPs and transformers, potentially illuminating the roots of hallucinations in other domains as well.

Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate
This paper introduces the Concept Pinpoint Eraser (CPE), a novel framework for erasing specific concepts in text-to-image diffusion models while preserving other concepts. By adding nonlinear Residual Attention Gates and incorporating attention anchoring loss, CPE improves the ability to retain diverse concepts and enhance robustness against adversarial attacks, outperforming existing methods in experiments involving various concept erasure tasks.

OpenPRM: Building Open-domain Process-based Reward Models with Preference Trees
This paper explores the development of process-based reward models (PRMs) for open-domain instruction-following tasks to enhance inference-time scaling in large language models. By integrating outcome-based reward models (ORMs) with PRMs to create the OpenPRM, the study demonstrates its superior performance over traditional models in open domains, although challenges in achieving automatic fine-grained supervision remain.

Going Beyond Feature Similarity: Effective Dataset distillation based on Class-aware Conditional Mutual Information
This paper presents a novel approach to dataset distillation by introducing conditional mutual information (CMI) to assess and minimize the class-aware complexity of synthetic datasets. By applying this method, the authors demonstrate improved performance and training efficiency of deep neural networks, serving as an effective regularization technique for existing dataset distillation methods.

The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?
This paper reviews recent advancements in language models (LLMs) and introduces the "lottery LLM hypothesis," suggesting that smaller models, when aided by multi-step reasoning and external tools, can match the performance of larger models while reducing computational needs. It also identifies overlooked essential capabilities required for effective model and KV cache compression.

Diffusion Feedback Helps CLIP See Better
This study addresses the visual shortcomings of CLIP by introducing DIVA, a post-training approach using a diffusion model as a visual assistant. DIVA enhances the fine-grained visual capabilities of CLIP and improves its performance on multimodal tasks, while maintaining strong zero-shot abilities, with code available at https://github.com/baaivision/DIVA.

SEBRA : Debiasing through Self-Guided Bias Ranking
This paper introduces Self-Guided Bias Ranking (Sebra), an unsupervised debiasing framework that ranks data points by spuriosity automatically, addressing the limitations of requiring human supervision in bias mitigation. By dynamically adjusting Empirical Risk Minimization (ERM) based on the inverse relationship between ease of learning and spuriosity, Sebra effectively reduces biases, outperforming existing methods across several benchmarks like UrbanCars, BAR, and CelebA.

KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models
The paper introduces Knowledge-aware Singular-value Adaptation (KaSA), a parameter-efficient fine-tuning method that uses singular value decomposition to activate relevant knowledge in large language models. Experiments show that KaSA outperforms existing methods across multiple benchmarks, highlighting its effectiveness in tasks like natural language understanding, generation, instruction following, and commonsense reasoning.

Multi-modal brain encoding models for multi-modal stimuli
This study examines how multi-modal Transformer models predict brain activity in response to multi-modal stimuli, specifically using fMRI data of participants watching movies. It highlights that jointly pretrained models align with brain activity in visual and language regions better than unimodal models, suggesting these models are valuable for understanding how the brain processes multi-modal information.

Overcoming False Illusions in Real-World Face Restoration with Multi-Modal Guided Diffusion Model
This paper introduces the Multi-modal Guided Real-World Face Restoration (MGFR) technique, which enhances facial image restoration by using attribute text prompts, reference images, and identity information to reduce false attributes in low-quality images. Additionally, the authors present the Reface-HQ dataset to support training, achieving superior detail restoration and identity preservation even under severe degradation, thanks to a dual-control adapter and a two-stage training approach.

HG-Adapter: Improving Pre-Trained Heterogeneous Graph Neural Networks with Dual Adapters
This paper addresses the limitations of prompt-tuning in pre-trained heterogeneous graph neural networks (HGNNs) by proposing a unified framework that includes dual structure-aware adapters for better adaptation to graph structures and enhanced generalization using self-supervised learning techniques. Theoretical analysis and experiments show that the proposed method achieves a lower generalization error bound and superior performance across various downstream tasks compared to existing methods.

Generating Physical Dynamics under Priors
This paper presents a novel framework that incorporates physical priors into diffusion-based generative models to generate physically realistic dynamics. By utilizing distributional priors like roto-translational invariance and physical feasibility priors such as conservation laws, the method achieves high-quality, robust dynamics, advancing data-driven research in AI4Physics.

SuperCorrect: Advancing Small LLM Reasoning with Thought Template Distillation and Self-Correction
SuperCorrect is a novel two-stage framework designed to improve the reasoning capabilities of smaller language models by using a large teacher model to guide and correct the student model's reasoning and reflection processes. Through hierarchical thought templates and cross-model collaborative direct preference optimization, SuperCorrect achieves state-of-the-art performance on MATH and GSM8K benchmarks, outperforming existing models with notable improvements.

On Scaling Up 3D Gaussian Splatting Training
Grendel is a distributed system that enhances 3D Gaussian Splatting (3DGS) for high-resolution and large-scale 3D reconstruction by partitioning parameters and parallelizing computation across multiple GPUs, overcoming the memory limitations of single GPU setups. It improves rendering quality through batched training with multiple views and dynamic load balancing, achieving superior results demonstrated by a PSNR of 27.28 on the 4K "Rubble" dataset by distributing parameters across 16 GPUs.

GeSubNet: Gene Interaction Inference for Disease Subtype Network Generation
GeSubNet addresses the challenge of retrieving gene functional networks by learning a unified representation that predicts gene interactions and distinguishes between disease subtypes. This novel multi-step framework improves the integration of gene interaction knowledge and excels in graph evaluation metrics, demonstrating its potential to identify subtype-specific genes and impact patient distribution shifts with high accuracy.

Internet of Agents: Weaving a Web of Heterogeneous Agents for Collaborative Intelligence
The paper presents the Internet of Agents (IoA), a novel framework designed to overcome the limitations of existing multi-agent systems by providing a scalable platform for collaboration among heterogeneous agents through an agent integration protocol, an instant-messaging-like architecture, and dynamic teaming mechanisms. Extensive experiments demonstrate that IoA enhances collaboration and exceeds performance benchmarks compared to conventional frameworks, paving the way for seamless intelligence-sharing among diverse autonomous agents.

Causal Graph Transformer for Treatment Effect Estimation Under Unknown Interference
Networked interference presents challenges in causal inference by complicating treatment assignment and effect estimation due to peer influences violating the SUTVA assumption. The proposed Interference-Agnostic Causal Graph Transformer (CauGramer) leverages graph transformers and cross-attention to accurately learn interference representations, improving treatment effect estimation, with its effectiveness confirmed through extensive experiments.

Exploiting Distribution Constraints for Scalable and Efficient Image Retrieval
This paper addresses the challenges of scalability and efficiency in image retrieval systems by introducing Autoencoders with Strong Variance Constraints (AE-SVC) and Single-Shot Similarity Space Distillation ((SS)²D). AE-SVC enhances the performance of foundation models by improving the embedding space for cosine similarity searches, achieving up to 16% improvement, while (SS)²D optimizes adaptive embedding sizes, leading to a further 10% improvement in efficiency across various datasets.

Uncovering Overfitting in Large Language Model Editing
This paper addresses the issue of Editing Overfit in Large Language Models (LLMs), where models disproportionately favor edit targets, impairing the generalization of new knowledge in complex scenarios. The authors introduce the EVOKE benchmark and fine-grained evaluation metrics, and propose a new strategy, Learn the Inference (LTI), with a Multi-stage Inference Constraint module to enhance the model's ability to recall and apply new knowledge effectively, demonstrating its success through extensive experiments.

Federated $Q$-Learning with Reference-Advantage Decomposition: Almost Optimal Regret and Logarithmic Communication Cost
This paper introduces FedQ-Advantage, a novel model-free federated $Q$-Learning algorithm for tabular episodic Markov decision processes, designed to improve regret bounds and communication efficiency in multi-agent settings. By utilizing reference-advantage decomposition and innovative communication strategies, the algorithm achieves nearly optimal regret with reduced communication costs, surpassing existing federated $Q$-learning approaches.

UniCBE: An Uniformity-driven Comparing Based Evaluation Framework with Unified Multi-Objective Optimization
The paper introduces UniCBE, a uniformity-driven comparing-based evaluation (CBE) framework that enhances the accuracy, convergence, and scalability of evaluations by optimizing sampling bias, uncertainty balancing, and updating certainty. UniCBE significantly reduces evaluation costs while maintaining high accuracy, as demonstrated on the AlpacaEval benchmark, achieving over 0.995 Pearson correlation with ground truth and saving up to 50% in evaluation costs for new model scenarios.

Measuring And Improving Persuasiveness Of Large Language Models
This paper introduces PersuasionBench and PersuasionArena, the first large-scale benchmark and arena for assessing the simulative and generative persuasion abilities of large language models (LLMs). The study reveals that while LLMs have limited simulative persuasion skills, their generative capabilities are notably strong and can surpass human proficiency, challenging the notion that larger model size is the primary determinant of persuasive competence, with synthetic training enhancing smaller models' effectiveness.

Offline Model-Based Optimization by Learning to Rank
This paper addresses the challenge of out-of-distribution errors in offline model-based optimization (MBO) by proposing a ranking-based surrogate model that prioritizes maintaining the order of candidate designs rather than precisely predicting their scores. The proposed method, which leverages learning to rank techniques, demonstrates superior performance over twenty existing methods across diverse tasks, as validated through empirical results.

Multi-session, multi-task neural decoding from distinct cell-types and brain regions
This paper presents a multi-task transformer architecture trained on the Allen Institute's Brain Observatory dataset to investigate the feasibility of pre-training and transferring brain decoding models across different tasks, cellular sub-types, and brain regions. The results show that combining data from diverse sources enhances decoding accuracy and reveals latent distinctions between brain regions and cellular sub-types, highlighting the potential for studying heterogeneous neural circuits using large-scale models.

Learning from weak labelers as constraints
In this study, we address the challenges of programmatic weak supervision by proposing a novel objective that utilizes side-information in the form of average error bounds on weak labelers to denoise labels. Our approach employs an alternating minimization algorithm to create soft pseudo labels, enhancing model training and demonstrating improved performance and robustness on a standard weak supervision benchmark.

Selective Unlearning via Representation Erasure Using Domain Adversarial Training
The paper introduces SURE, a novel algorithm for targeted unlearning in machine learning models, treating the unlearning process as a domain adaptation problem. By directly manipulating data representations rather than model outputs, SURE achieves a better balance between unlearning effectiveness and maintaining utility of the remaining data, and demonstrates robustness against advanced attack scenarios.

Learning Splitting Heuristics in Divide-and-Conquer SAT Solvers with Reinforcement Learning
RDC-SAT is a novel approach leveraging deep reinforcement learning to optimize splitting heuristics in Divide-and-Conquer SAT solvers by dynamically extracting and processing features as graphs to determine optimal split variables. Evaluations demonstrate that RDC-SAT notably enhances performance on phase transition random 3-SAT and SAT Competition 2023 datasets, outperforming traditional methods.

COME: Test-time Adaption by Conservatively Minimizing Entropy
The paper introduces \texttt{COME}, a conservative entropy minimization method that addresses the overconfidence issue in traditional entropy minimization for test-time adaptation by modeling uncertainty with a Dirichlet prior distribution. This approach enhances optimization stability and achieves state-of-the-art performance in classification accuracy and uncertainty estimation across various test-time adaptation benchmarks.

Expected Sliced Transport Plans
This paper proposes a method to construct a transportation plan between probability measures using the sliced transport framework, introducing a 'lifting' operation that extends one-dimensional optimal transport plans back to the original space. The resulting expected sliced transport (EST) plans provide a valid metric between input discrete probability measures, bridging a gap in the efficiency of sliced-Wasserstein approaches while connecting to the min-SWGG method, supported by numerical examples.

Multi-agent cooperation through learning-aware policy gradients
This paper introduces the first unbiased, derivative-free policy gradient algorithm tailored for learning-aware reinforcement learning, enabling cooperation among self-interested agents by considering their learning dynamics during noisy trials. By employing sequence models that rely on extensive observation histories, the method achieves cooperative behaviors and high returns in complex social dilemmas, and offers a new explanation for cooperation arising among learning-aware agents in the iterated prisoner's dilemma.

SELF-EVOLVED REWARD LEARNING FOR LLMS
This paper investigates using a Reward Model (RM) for generating additional training data to enhance Reinforcement Learning from Human Feedback (RLHF) for language models, addressing the cost and bias issues of acquiring high-quality labels. Through experiments on datasets like HH-RLHF and models such as Mistral and Llama 3, the study shows that self-feedback can significantly improve RM performance, even with limited human-labeled data, thereby boosting the capabilities of large language models.

ConvCodeWorld: Benchmarking Conversational Code Generation in Reproducible Feedback Environments
Large language models (LLMs) are essential for interactive code generation, but existing benchmarks do not adequately capture the diverse feedback in multi-turn interactions. This paper introduces CONVCODEWORLD, a novel environment for benchmarking interactive code generation across nine scenarios with various feedback types, and CONVCODEBENCH, a static benchmark version, revealing insights into LLM performance variations based on feedback and the impact of training on specific feedback combinations.

LASeR: Towards Diversified and Generalizable Robot Design with Large Language Models
LASeR, a Large Language Model-Aided Evolutionary Search, addresses the limitations of current LLM applications in evolutionary optimization for robot design, such as poor solution diversity and generalizability. By utilizing a DiRect reflection mechanism to enhance exploration-exploitation balance and uncover inter-task reasoning capabilities, LASeR significantly improves optimization efficiency and enables zero-shot robot design, outperforming existing methods in simulated experiments.

Homomorphism Expressivity of Spectral Invariant Graph Neural Networks
This paper investigates the theoretical understanding of spectral invariants in Graph Neural Networks (GNNs) by analyzing their expressive power through homomorphism expressivity. The study proves that spectral invariant GNNs can accurately count homomorphisms in specific tree-like graphs called "parallel trees," establishing a hierarchy of expressiveness among different GNN architectures and extending previous work by resolving open questions in the field.

Exact Byte-Level Probabilities from Tokenized Language Models for FIM-Tasks and Model Ensembles
This paper examines the impact of tokenization on language model performance, uncovering a phenomenon termed "tokenization bias" where predictive distributions differ between tokenized and byte-level models. By introducing the Byte-Token Representation Lemma, the authors propose a zero-shot method to eliminate tokenization bias, enhancing performance in fill-in-the-middle tasks by 18% and improving model ensemble integration by up to 3.7% without additional training.

Frame-Voyager: Learning to Query Frames for Video Large Language Models
Video Large Language Models (Video-LLMs) struggle with input length constraints, which hampers their ability to process entire videos effectively. This paper introduces Frame-Voyager, a method that intelligently queries informative frame combinations based on textual queries, significantly improving video understanding performance by training with a novel data pipeline, and demonstrating impressive results across multiple Video Question Answering benchmarks.

SC-OmniGS: Self-Calibrating Omnidirectional Gaussian Splatting
SC-OmniGS is a novel system designed for efficient and accurate 3D radiance field reconstruction using 360-degree images, bypassing traditional methods by leveraging direct omnidirectional camera pose calibration and 3D Gaussian optimization. The system effectively rectifies image distortion through a differentiable camera model, demonstrating significant improvements in challenging scenarios and verifying its performance with real-world data from consumer-grade cameras.

MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions
The paper introduces the MMQA dataset to assess large language models' capabilities in multi-table understanding and reasoning, going beyond single-table benchmarks. It also presents a state-of-the-art multi-table retrieval method, highlighting significant room for improvement in LLMs' performance compared to human capabilities.

CofCA: A STEP-WISE Counterfactual Multi-hop QA benchmark
This paper introduces the Step-wise and Counterfactual benchmark (CofCA), designed to evaluate Large Language Models' (LLMs') real reasoning abilities on multi-step reasoning tasks, addressing concerns of internal memory reliance and data contamination in existing benchmarks. The CofCA benchmark utilizes both factual and counterfactual data to reveal significant performance discrepancies in LLMs, thereby enhancing the evaluation of their reasoning processes and improving the trustworthiness of these models.

ClimaQA: An Automated Evaluation Framework for Climate Question Answering Models
The paper introduces *ClimaGen*, an adaptive learning framework that creates question-answer pairs in collaboration with climate scientists to evaluate Large Language Models (LLMs) in climate science. It also presents two benchmark datasets, *ClimaQA-Gold* and *ClimaQA-Silver*, and provides evaluation strategies to compare different LLMs, offering insights into improving climate knowledge in LLMs, with the source code available online.

Second-Order Fine-Tuning without Pain for LLMs: A Hessian Informed Zeroth-Order Optimizer
Fine-tuning large language models can be challenging due to high GPU memory demands and convergence issues caused by heterogeneous curvatures. This paper introduces HiZOO, a diagonal Hessian informed Zeroth-Order Optimizer, which improves fine-tuning efficiency by reducing memory usage and enhancing model convergence, as shown in experiments with significant speedup and accuracy improvements over existing methods like MeZO.

Improving Text-to-Image Consistency via Automatic Prompt Optimization
This paper presents OPT2I, a novel optimization-by-prompting framework for text-to-image (T2I) generative models, designed to enhance prompt-image consistency by leveraging large language models without the need for model fine-tuning. The proposed method significantly improves consistency scores by up to 24.9% while maintaining image quality and representation diversity, demonstrating effectiveness on MSCOCO and PartiPrompts datasets.

Efficient and Robust Neural Combinatorial Optimization via Wasserstein-Based Coresets
This paper addresses the limitations of current neural combinatorial optimization (NCO) methods, which require extensive resources and struggle with distribution shifts, by modeling combinatorial optimization instances as probability measures and introducing Wasserstein-based metrics. The authors propose an efficient training framework using a novel coreset approach adapted to the merge-and-reduce framework, which accelerates coreset construction and improves computation efficiency, demonstrating enhanced robustness and performance with less resource demand through experimental results.

SVBench: A Benchmark with Temporal Multi-Turn Dialogues for Streaming Video Understanding
The paper introduces SVBench, a benchmark specifically designed to evaluate Large Vision-Language Models' (LVLMs) abilities in understanding long-context streaming videos through temporal multi-turn question-answering. The research highlights the gap in existing benchmarks, presents a semi-automated annotation pipeline for creating extensive QA data, and demonstrates that while closed-source models like GPT-4o perform better, most open-source LVLMs face challenges, with their newly developed StreamingChat model outperforming others in the open-source category and matching performance on various vision-language tasks.

Denoising Task Difficulty-based Curriculum for Training Diffusion Models
This study examines the task difficulty in diffusion-based generative models, finding that denoising at lower timesteps is more challenging due to slower convergence and higher relative entropy. By implementing an easy-to-hard learning scheme inspired by curriculum learning, the research enhances training efficiency, resulting in improved performance and faster convergence in image generation tasks across various conditions.

Automated Filtering of Human Feedback Data for Aligning Text-to-Image Diffusion Models
FiFA is an automated data filtering algorithm designed to improve the fine-tuning of text-to-image diffusion models using human feedback datasets by optimizing preference margin, text quality, and diversity. It efficiently selects important data, enhancing training stability and achieving better performance with significantly less data and computational resources, thus aligning model outputs more closely with human preferences.

Learning to Communicate Through Implicit Communication Channels
This paper introduces the Implicit Channel Protocol (ICP) framework, which enables agents in collaborative multi-agent systems to communicate implicitly through scouting actions, thereby bypassing the limitations of Theory of Mind methods in complex tasks. ICP demonstrates significant improvements in information transmission over baseline methods, validated through tasks such as Guessing Numbers, Revealing Goals, and Hanabi.

Enhancing Learning with Label Differential Privacy by Vector Approximation
This paper introduces a vector approximation approach for label differential privacy, which aims to enhance privacy by converting each label into a random vector with K components, preserving more information compared to traditional scalar labels. The method demonstrates minimal performance decay as the number of classes increases and is validated through both theoretical analysis and experiments on synthesized and real datasets.

Revisiting Prefix-tuning: Statistical Benefits of Reparameterization among Prompts
This paper investigates the theoretical foundations of prompt-based techniques like prompt-tuning and prefix-tuning, emphasizing the critical role of reparameterization strategies in achieving performance comparable to full fine-tuning. The study reveals that reparameterization is grounded in a shared structure between prefix key and value vectors, enhancing sample efficiency in parameter estimation, and demonstrates through experiments that this shared structure significantly contributes to the effectiveness of these techniques across various tasks in both visual and language domains.

Leveraging Sub-Optimal Data for Human-in-the-Loop Reinforcement Learning
This paper introduces Sub-optimal Data Pre-training (SDP) to enhance the feedback efficiency of human-in-the-loop reinforcement learning (HitL RL) by using reward-free, sub-optimal data for pre-training reward models, thereby reducing reliance on human interaction. Through pseudo-labeling low-quality data and enabling the reward model to identify low-reward transitions, SDP consistently matches or surpasses state-of-the-art HitL RL performance in various simulated robotic tasks.

PALMBENCH: A COMPREHENSIVE BENCHMARK OF COMPRESSED LARGE LANGUAGE MODELS ON MOBILE PLATFORMS
This paper presents a lightweight, automated benchmarking framework for evaluating large language models (LLMs) deployed on mobile devices, addressing challenges such as generative performance, latency, and throughput under hardware constraints. The study provides a comprehensive analysis of various quantized LLM configurations across diverse mobile platforms, focusing on resource efficiency and the impact of quantization on performance, energy usage, and the generation of harmful outputs.

SVG: 3D Stereoscopic Video Generation via Denoising Frame Matrix
This paper introduces a novel, pose-free and training-free method for creating 3D stereoscopic videos using existing monocular video generation models, without requiring scene optimization or model fine-tuning. By employing a unique frame matrix video inpainting framework and a disocclusion boundary re-injection scheme, the method significantly enhances video inpainting quality and outperforms previous approaches across various generative models.

ADAM: An Embodied Causal Agent in Open-World Environments
ADAM is introduced as an embodied causal agent in Minecraft that autonomously navigates open-world environments, perceives multimodal context, and learns causal world knowledge for complex tasks through lifelong learning. Key components include an interaction module, a causal model to build interpretable causal graphs, a controller module for task execution, and a perception module using multimodal language models, collectively demonstrating strong interpretability, robustness, and generalization without prior knowledge in extensive experimentation.

ND-SDF: Learning Normal Deflection Fields for High-Fidelity Indoor Reconstruction
This paper introduces ND-SDF, a method that improves neural implicit reconstruction by learning a Normal Deflection field to adaptively manage sample utilization based on specific scene characteristics. Unlike previous models, ND-SDF enhances geometry details and smoothness, particularly in weakly textured and intricate thin structures, by incorporating a novel ray sampling strategy, demonstrating consistent improvements across challenging datasets.

Improving Long-Text Alignment for Text-to-Image Diffusion Models
The paper introduces LongAlign, a method to improve text-to-image (T2I) alignment in diffusion models when dealing with long text inputs. By implementing segment-level encoding and a novel preference optimization strategy, the authors address the limitations of existing encoding techniques, leading to improved performance compared to other models like PixArt-α and Kandinsky v2.2, with the code made publicly available.

Reliable and Diverse Evaluation of LLM Medical Knowledge Mastery
PretexEval is introduced as a novel framework designed to dynamically generate reliable and diverse test samples for evaluating Large Language Models (LLMs) on their mastery of medical knowledge. Through this framework, which addresses the limitations of factual errors and lack of diversity in test samples, the study finds significant deficiencies in LLMs' understanding of medical knowledge, emphasizing the necessity for advancements in medical-specific LLMs for practical use in clinical settings.

OSDA Agent: Leveraging Large Language Models for De Novo Design of Organic Structure Directing Agents
This paper introduces the OSDA Agent, an innovative interactive framework for designing Organic Structure Directing Agents (OSDAs) by integrating large language models (LLMs) like GPT-4 with computational chemistry tools. The framework, which includes generating potential OSDA structures, evaluating them, and refining designs, demonstrates superior capability in creating and optimizing OSDAs for zeolite synthesis compared to traditional LLM-based approaches.

Sequential Controlled Langevin Diffusions
This paper introduces the Sequential Controlled Langevin Diffusion (SCLD) sampling method, a new framework that combines Sequential Monte Carlo (SMC) and diffusion-based sampling to effectively sample from unnormalized densities. By leveraging the merits of both techniques, SCLD achieves improved performance on benchmark problems, often requiring only 10% of the training budget compared to traditional diffusion-based samplers.

TopoNets: High performing vision and language models with brain-like topography
This paper introduces *TopoLoss*, a novel loss function designed to promote spatially organized topographic representations in AI models, achieving this integration without a significant trade-off in task performance. The resulting TopoNets models exhibit brain-like properties, including localized feature processing and increased efficiency, effectively bridging the gap between biological and artificial systems and providing a framework for high-performing, topographically-organized AI models.

CtD: Composition through Decomposition in Emergent Communication
The paper introduces a method called "Composition through Decomposition" where artificial neural agents learn compositional generalization to describe previously unseen images. By decomposing images into basic concepts and using a codebook from a multi-target coordination game, these agents can compose complex phrases for novel images and achieve zero-shot generalization in some cases.

Injecting Universal Jailbreak Backdoors into LLMs in Minutes
JailbreakEdit is a novel method for injecting a universal jailbreak backdoor into safety-aligned language models using model editing techniques, achieving this in minutes with minimal intervention. The approach effectively shifts models' attention to bypass safety mechanisms, maintaining high jailbreak success while preserving quality on normal queries, highlighting the need for better defense mechanisms in language models.

Joint Graph Rewiring and Feature Denoising via Spectral Resonance
This paper introduces the Joint Denoising and Rewiring (JDR) algorithm to enhance node classification in graph neural networks by simultaneously denoising features and optimizing graph structure. JDR effectively aligns spectral spaces and tackles varying class and homophily conditions, demonstrating superior performance over existing methods in diverse tasks.

SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language Model
This paper addresses the limitations of current 3D-based large language models in situated understanding by introducing a new dataset, Spartun3D, which includes situated spatial information and a novel situated spatial alignment module for better alignment between 3D representations and natural language. The experimental results show that these contributions significantly improve the models' ability in tasks requiring precise spatial reasoning.

u-$\mu$P: The Unit-Scaled Maximal Update Parametrization
The paper introduces u-$\mu$P, an improved model parametrization scheme that combines Maximal Update Parametrization and Unit Scaling to create models with hyperparameters independent of their size and easy-to-train with low precision. This method simplifies parameter tuning and achieves lower loss while enabling efficient proxy model training, performing effectively in FP8 precision compared to standard $\mu$P models.

Proactive Agent: Shifting LLM Agents from Reactive Responses to Active Assistance
This paper addresses the limitations of reactive agent systems by developing proactive agents that anticipate and initiate tasks autonomously. Through a novel data-driven approach, the authors introduce ProactiveBench, a diverse dataset used to train and fine-tune language model agents, significantly increasing their ability to offer proactive assistance with an impressive F1-Score of 66.47%, surpassing existing models and enhancing human-agent collaboration.

Balancing Act: Diversity and Consistency in Large Language Model Ensembles
The paper introduces a unified framework for evaluating the trade-offs between task performance, model diversity, and output consistency in ensembling strategies for Large Language Models. By developing a novel inference-time strategy called Dynamic Mixture of Agents (DMoA), the authors achieve state-of-the-art results in the Big Bench Hard benchmark, highlighting the importance of cross-validation bias and the need for task-dependent model capabilities in tasks like arithmetic reasoning, commonsense reasoning, and instruction following.

Open-CK: A Large Multi-Physics Fields Coupling benchmarks in Combustion Kinetics
This paper presents the creation of the Open-CK Combustion Kinetics dataset using the Fire Dynamics Simulator, supported by supercomputing resources, to capture fire dynamics in industrial parks via high-precision CFD simulations. The study introduces and evaluates advanced machine learning architectures for handling complex physical field data, and provides benchmarks demonstrating their applicability, aiming to advance combustion kinetics research and promote deep learning usage in earth sciences.

Zero-shot Imputation with Foundation Inference Models for Dynamical Systems
This paper introduces a novel supervised learning framework for zero-shot imputation of missing time series data using parametric functions satisfying hidden ordinary differential equations (ODEs). The approach demonstrates superior performance over state-of-the-art methods in various domains without requiring fine-tuning, supported by a pretrained model and resources available online.

High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation
The paper introduces a novel framework for 3D radar sequence prediction in weather nowcasting, utilizing SpatioTemporal Coherent Gaussian Splatting (STC-GS) for dynamic radar representation and GauMamba for efficient forecasting. This approach maintains high spatial resolution and outperforms existing methods by effectively handling dynamic meteorological signals, thereby enhancing prediction efficiency and accuracy.

Clique Number Estimation via Differentiable Functions of Adjacency Matrix Permutations
We introduce MxNet, a differentiable model for estimating the clique number in graphs by re-formulating the maximum clique problem as a sequence of subgraph matching tasks, which mitigates the issue of learning spurious permutations. Our method successfully predicts clique numbers without requiring explicit clique demonstrations, achieving superior accuracy and interpretability on eight datasets.

Large Convolutional Model Tuning via Filter Subspace
This paper introduces a novel method to fine-tune pre-trained models by adjusting only the filter atoms, which are a small set of elements responsible for spatial-only convolution, without altering the channel combination knowledge. This approach, which allows for a recursive expansion of tunable parameters while maintaining overall parameter efficiency, demonstrates superior performance over existing tuning methods in various tasks, effectively balancing between model preservation and overfitting prevention.

Bootstrapping Language-Guided Navigation Learning with Self-Refining Data Flywheel
This paper introduces a Self-Refining Data Flywheel (SRDF), which iteratively refines navigational instruction-trajectory data by collaborating with a generator and navigator model, eliminating the need for human annotation. The SRDF enhances language-guided navigation learning, achieving 78% SPL on the R2R test set, surpassing human performance and previous methods, while also improving the instruction generator and generalizing well across various downstream tasks.

VL-Cache: Sparsity and Modality-Aware KV Cache Compression for Vision-Language Model Inference Acceleration
This paper introduces VL-Cache, a novel Key-Value (KV) cache compression method specifically designed for enhancing the performance of Vision-Language Models (VLMs). By using a layer-adaptive, sparsity-aware budget allocation and a modality-aware token scoring, VL-Cache reduces the KV cache size by 90% while maintaining accuracy comparable to a full cache, and significantly speeds up the inference process.

Cross the Gap:  Exposing the Intra-modal Misalignment in CLIP via Modality Inversion
This paper critiques the suboptimal use of pre-trained multi-modal Vision-Language Models, such as CLIP, for intra-modal tasks like image-to-image retrieval due to a lack of intra-modal constraints called intra-modal misalignment. By employing optimization-based modality inversion techniques and evaluating across multiple datasets, the authors demonstrate that addressing these tasks inter-modally enhances performance, suggesting that integrating intra-modal elements in the pre-training or reducing the modality gap can alleviate misalignment issues.

Pitfalls of Evidence-Based AI Policy
The paper argues that while "evidence-based AI policy" is crucial, enforcing too high an evidentiary standard can delay necessary regulatory actions and neglect certain risks, as historically seen with issues like tobacco and fossil fuels. It proposes 16 regulatory goals to better identify and manage AI risks and highlights opportunities for countries like the EU, UK, USA, Brazil, Canada, and China to adopt more evidence-seeking policies.

On a Connection Between Imitation Learning and RLHF
This paper explores the alignment of large language models with preference data through the lens of imitation learning, revealing a theoretical connection between reinforcement learning from human feedback (RLHF) and imitation learning (IL). The authors introduce DIL, a framework that unifies IL perspectives on alignment and outperforms current methods in experiments, offering new insights into alignment strategies.

Adapt-$\infty$: Scalable Continual Multimodal Instruction Tuning via Dynamic Data Selection
The paper addresses the challenge of semantically redundant text-image pairs in visual instruction datasets, which limit the adaptability of multimodal large language models. It introduces Adapt-$\infty$, an adaptive data selection approach for lifelong Instruction Tuning that enhances sample efficiency and forward transfer, particularly for rare tasks, by dynamically selecting and pruning data samples based on skill clusters.

Exploring the Effectiveness of Object-Centric Representations in Visual Question Answering: Comparative Insights with Foundation Models
This paper conducts an extensive empirical study on the effectiveness of object-centric (OC) representations and foundation models for Visual Question Answering (VQA), emphasizing the need for accurate compositional scene understanding. By evaluating over 600 VQA models and 15 types of upstream representations, the study explores the strengths and trade-offs of these approaches, offering insights and identifying ways to leverage the benefits of both paradigms.

CREMA: Generalizable and Efficient Video-Language Reasoning via Multimodal Modular Fusion
CREMA is a novel modality-fusion framework designed to enhance video reasoning by efficiently incorporating multiple modalities, such as optical flow and 3D point clouds, without requiring extensive parameter updates. This method demonstrates improved or equivalent performance on various video-language reasoning tasks while maintaining computational efficiency and significantly reducing trainable parameters compared to existing multimodal models.

Efficient Top-m Data Values Identification for Data Selection
The paper introduces the GPGapE algorithm, utilizing a Gaussian process to model non-linear relationships in data valuation for efficiently identifying top-$m$ data values, thereby addressing the computational limitations of traditional Shapley value approaches. The method is proven to improve computational efficiency and empirical performance in tasks like noisy data detection and data subset selection, particularly benefiting large language model fine-tuning.

Surgical, Cheap, and Flexible: Mitigating False Refusal in Language Models via Single Vector Ablation
This paper introduces a method to reduce false refusals in language models, where models incorrectly refuse safe requests due to similarities with unsafe ones, by leveraging single vector ablation. The proposed approach, which is training-free and model-agnostic, effectively decreases the false refusal rate while maintaining the model's overall safety and capability, and provides a novel way to finely calibrate model safety.

Tracking objects that change in appearance with phase synchrony
The paper presents a novel deep learning circuit, the complex-valued recurrent neural network (CV-RNN), which controls attention to features independently of their locations using neural synchrony, mimicking a mechanism seen in biological visual systems. Through the FeatureTracker challenge, it demonstrates that while humans can effortlessly track objects as they change location and appearance, the CV-RNN matches human-like performance, unlike other state-of-the-art deep neural networks, thus supporting the role of phase synchronization in tracking dynamic objects.

Understanding Fairness Surrogate Functions in Algorithmic Fairness
This paper addresses the issue of biased predictions in machine learning algorithms by analyzing the gap between fairness definitions and their surrogate functions, particularly focusing on demographic parity. The authors propose a general sigmoid surrogate to reduce this gap and variance, alongside a novel Balanced Surrogate algorithm, demonstrating improved fairness and stability without sacrificing accuracy across real-world datasets.

Accelerating Auto-regressive Text-to-Image Generation with Training-free Speculative Jacobi Decoding
This paper introduces Speculative Jacobi Decoding (SJD), a training-free probabilistic parallel decoding algorithm designed to accelerate auto-regressive text-to-image generation while preserving image diversity and quality. SJD predicts multiple tokens per step using a probabilistic convergence criterion, reducing inference time compared to conventional methods, and experiments demonstrate its effectiveness in speeding up image synthesis without compromising visual output.

A Curious Case of the Missing Measure: Better Scores and Worse Generation
The paper highlights the limitation of current audio evaluation measures, which struggle to detect subtle differences in neural audio generation that humans can easily perceive. This gap poses a challenge in accurately evaluating sophisticated models and understanding their flaws.

Adaptive Batch Size for Privately Finding Second-Order Stationary Points
This paper addresses the challenge of privately finding second-order stationary points (SOSP) under differential privacy constraints and presents a new approach that improves upon previous methods by using adaptive batch sizes and the binary tree mechanism. The proposed method corrects issues in previous work and achieves a stronger bound of $\alpha=\Tilde{O}(\frac{1}{n^{1/3}}+(\frac{\sqrt{d}}{n\epsilon})^{1/2})$, suggesting that finding an SOSP can be done at the same cost as finding a first-order stationary point (FOSP).

Adaptive Shrinkage Estimation for Personalized Deep Kernel Regression in Modeling Brain Trajectories
This paper presents a cutting-edge personalized deep kernel regression framework designed to forecast brain biomarkers by combining population and subject-specific models through Adaptive Shrinkage Estimation. The approach outperforms existing statistical and machine learning models in predictive accuracy and robustness, successfully modeling the progression of longitudinal neuroimaging biomarkers and validating its effectiveness across diverse clinical studies.

A Deep Generative Learning Approach for Two-stage Adaptive Robust Optimization
This paper introduces AGRO, a solution algorithm utilizing a variational autoencoder for two-stage adaptive robust optimization, designed to generate realistic high-dimensional contingencies that improve decision robustness while reducing costs. AGRO shows superior performance compared to the standard column-and-constraint algorithm, achieving up to 1.8% cost savings in production-distribution planning and up to 8% in power system expansion scenarios.

ADMM for Nonconvex Optimization under Minimal Continuity Assumption
This paper presents IPDS-ADMM, an innovative method for tackling multi-block nonconvex composite optimization problems by combining a proximal linearized ADMM with an Increasing Penalization and Decreasing Smoothing strategy. The approach, which requires less stringent continuity conditions and offers a complexity result of $\mathcal{O}(\epsilon^{-3})$, is the first of its kind for nonconvex nonsmooth problems and shows effectiveness in experiments on the sparse PCA problem.

ADMM for Structured Fractional Minimization
This paper presents {\sf FADMM}, the first Alternating Direction Method of Multipliers specifically designed for structured fractional minimization problems prevalent in machine learning and data science. By employing novel variants and a Lyapunov function, {\sf FADMM} achieves improved convergence to $\epsilon$-approximate critical points with an oracle complexity of $\mathcal{O}(1/\epsilon^{3})$, demonstrating its effectiveness through experiments on both synthetic and real-world datasets.

Adversarially Robust Anomaly Detection through Spurious Negative Pair Mitigation
This paper addresses the challenge of improving the robustness of anomaly detection methods against adversarial attacks, particularly in situations where only unlabeled normal samples are available for training. By crafting a pseudo-anomaly group and employing adversarial training with a modified contrastive loss function to handle spurious negative pairs, the authors achieve a 26.1% improvement in detection robustness across various benchmark datasets.

AgentOccam: A Simple Yet Strong Baseline for LLM-Based Web Agents
This study introduces AgentOccam, an LLM-based web agent whose enhanced observation and action space aligns closely with the capabilities of the underlying LLM, achieving significant improvements in web task performance. Without relying on in-context examples or complex strategies, AgentOccam surpasses state-of-the-art results on benchmarks like WebArena and WebVoyager, demonstrating LLMs' remarkable zero-shot potential in automating web interactions.

Agents' Room:  Narrative Generation through Multi-step Collaboration
The paper introduces Agents' Room, a generation framework inspired by narrative theory that decomposes story writing into subtasks handled by specialized agents, enhancing the ability of large language models to create compelling fiction. Utilizing the Tell Me A Story dataset and a novel evaluation framework, the approach demonstrates improved narrative quality over baseline systems, as preferred by expert evaluators, through collaboration and specialization in the story writing process.

AIMS.au: A Dataset for the Analysis of Modern Slavery Countermeasures in Corporate Statements
This paper introduces a dataset of 5,731 annotated statements from the Australian Modern Slavery Register to aid in the evaluation and fine-tuning of Large Language Models (LLMs) for detecting compliance with reporting requirements. It also proposes a methodology for identifying relevant sentences, demonstrating the dataset's utility in benchmarking language models in both zero-shot and supervised learning contexts.

Aligned Datasets Improve Detection of Latent Diffusion-Generated Images
This paper addresses the challenge of detecting fake images generated by latent diffusion models (LDMs) by proposing a novel approach that emphasizes the use of a well-aligned dataset of real and fake images. By reconstructing real images using the LDM's autoencoder and training a model to detect subtle artifacts introduced in these reconstructions, the authors demonstrate a more robust and efficient method for training image detectors, minimizing reliance on spurious correlations.

Almost Optimal Batch-Regret Tradeoff for Batch Linear Contextual Bandits
This paper addresses the optimal batch-regret tradeoff in batch linear contextual bandits by designing algorithms that achieve near-optimal regret bounds across various parameters such as batch number, actions, time horizon, and dimensionality. Additionally, it introduces a novel matrix concentration inequality pertaining to dynamic upper bounds, marking a first in literature and potentially serving as an independent contribution.

ANaGRAM: A Natural Gradient Relative to Adapted Model for efficient PINNs learning
This paper introduces a novel natural gradient approach to enhance the speed and accuracy of training Physics Informed Neural Networks (PINNs), a technique used for solving PDE-driven systems. The authors present two key contributions: a new natural gradient algorithm with improved scaling and a reformulated PINNs framework that extends natural gradient applicability, establishing connections to Green's function theory.

An Auditing Test to Detect Behavioral Shift in Language Models
We introduce a statistical test for Behavioral Shift Auditing (BSA) in language models to detect shifts in qualitative properties of output distributions, maintaining control over false positives. Our approach is validated through case studies on toxicity and translation performance, proving its efficiency in identifying behavior changes with limited data.

Animate-X: Universal Character Image Animation with Enhanced Motion Representation
This paper introduces $\texttt{Animate-X}$, a universal animation framework using LDM, designed to generate high-quality animations for various character types, including anthropomorphic characters. By implementing a novel Pose Indicator to enhance motion representation and introducing the $\texttt{$A^2$Bench}$ benchmark for evaluation, the study demonstrates that $\texttt{Animate-X}$ outperforms existing state-of-the-art methods in handling diverse animation tasks.

A Simple Framework for Open-Vocabulary Zero-Shot Segmentation
The paper introduces SimZSS, a novel framework for open-vocabulary zero-shot segmentation that uses frozen vision-only models for spatial awareness and aligns the text encoder to pinpoint local concepts in captions. Utilizing image-caption pair datasets, SimZSS achieves state-of-the-art results on multiple benchmarks within a short training time, demonstrating its efficiency and effectiveness in overcoming challenges associated with dense vision-language tasks.

Be More Diverse than the Most Diverse: Optimal Mixtures of Generative Models via Mixture-UCB Bandit Algorithms
This paper investigates the potential superiority of mixtures of generative models over individual models in terms of evaluation scores like FID and KID, and introduces the Mixture-UCB algorithm to efficiently identify these optimal mixtures as a multi-armed bandit problem. The authors provide a quadratic optimization framework and demonstrate, through numerical experiments, the effectiveness of Mixture-UCB in achieving lower regret and better evaluation outcomes on benchmark datasets, with code accessible for further exploration.

Beyond Random Masking: When Dropout meets Graph Convolutional Networks
This paper provides a comprehensive theoretical analysis of dropout in Graph Convolutional Networks (GCNs), revealing its unique role in preventing oversmoothing rather than co-adaptation. The study introduces new insights into the adaptive, degree-dependent regularization effects of dropout and its interaction with batch normalization, validated by experiments that show enhanced performance across various datasets and tasks.

Beyond Squared Error: Exploring Loss Design for Enhanced Training of Generative Flow Networks
This paper examines the influence of regression loss choices on the exploration and exploitation behaviors in training Generative Flow Networks (GFlowNets) and introduces a theoretical framework linking distinct regression losses to particular divergence measures. The authors propose three novel regression losses—Shifted-Cosh, Linex(1/2), and Linex(1)—demonstrating improved performance in convergence speed, sample diversity, and robustness across several benchmarks including hyper-grid, bit-sequence, and molecule generation.

Bilinear MLPs enable weight-based mechanistic interpretability
This paper analyzes bilinear MLPs, a type of Gated Linear Unit without element-wise nonlinearity, which allows for flexible weight analysis using third-order tensors and eigendecomposition. The study shows that bilinear layers offer an interpretable alternative to current activation functions and suggests that weight-based interpretability is feasible for understanding deep-learning models by identifying low-rank structures and crafting adversarial examples.

Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models
This paper introduces block diffusion language models, which blend discrete denoising diffusion with autoregressive models to enable flexible-length generation and faster inference. The proposed models set a new performance standard among diffusion models, overcoming limitations like fixed-length generation, and the authors provide their code and model weights online for further exploration.

Bootstrapped Model Predictive Control
Bootstrapped Model Predictive Control (BMPC) is a novel algorithm designed to improve policy learning by imitating an MPC expert and guiding the MPC process, addressing issues found in complex tasks with model-free approaches. By combining BMPC with model-based TD-learning and introducing a lazy reanalyze mechanism, the method achieves superior performance, data efficiency, and training stability in various continuous control tasks, especially in high-dimensional locomotion, with compact network sizes and manageable training times.

CBMA: Improving Conformal Prediction through Bayesian Model Averaging
This paper addresses the issue of suboptimal prediction intervals in Bayesian conformal prediction when the Bayesian model might be mis-specified, despite maintaining frequentist coverage guarantees. By integrating Bayesian model averaging with conformal prediction, the authors propose a robust hybrid method that converges to optimal efficiency and offers improved reliability and precision in uncertainty quantification under model uncertainty.

Chemistry-Inspired Diffusion with Non-Differentiable Guidance
This paper presents a novel approach to guiding unconditional diffusion models for molecular generation using domain knowledge from quantum chemistry, reducing the need for large labeled datasets. By employing a non-differentiable oracle for accurate gradient estimation, the method enhances the precision of generating stable molecular structures, compatible with both explicit and implicit guidance, and effectively generalizes to various molecular optimization tasks.

ClawMachine: Learning to Fetch Visual Tokens for Referential Comprehension
ClawMachine introduces a novel approach to aligning vision and language in multimodal large language models by using token collectives to represent higher-level semantics without extra syntax, improving efficiency in tasks like referring and grounding. The method combines discrete and continuous scene analysis and shows superior performance in visual reasoning, offering enhanced capabilities for complex tasks in comparison to existing models.

CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer
CogVideoX is an advanced text-to-video generation model that uses a diffusion transformer to create 10-second, high-resolution videos closely aligned with text prompts. By introducing a 3D Variational Autoencoder and an expert transformer with adaptive LayerNorm, along with innovative training strategies, the model effectively overcomes previous limitations like short video duration and limited motion, achieving state-of-the-art results in both automated and human evaluations.

ComaDICE: Offline Cooperative Multi-Agent Reinforcement Learning with Stationary Distribution Shift Regularization
This paper introduces ComaDICE, a novel regularization approach targeting stationary distributions to tackle distributional shift challenges in offline multi-agent reinforcement learning (MARL). Demonstrating superior performance on offline multi-agent MuJoCo and StarCraft II benchmarks, ComaDICE offers an innovative framework for correcting global policy distributions, subsequently informing local agent policies more effectively than existing state-of-the-art methods.


Commit0: Library Generation from Scratch
Commit0 is a benchmark designed to evaluate AI agents' ability to write software libraries from scratch based on API specifications and interactive unit tests. By moving beyond static code generation, it challenges agents to handle complex dependencies and adapt to multi-stage feedback, with findings indicating that current agents benefit significantly from interactive feedback, although they cannot yet fully reproduce complete libraries.

Composable Interventions for Language Models
This paper introduces composable interventions, a framework to study and analyze the interactions of multiple sequential interventions on language models, incorporating new metrics and a unified codebase. Through extensive experiments, the study reveals significant findings on how intervention interactions, such as compression hindering editing and unlearning, and the impact of application order, indicate the need for new, more effective multi-objective interventions.

Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering
DynSuperCLEVR is introduced as the first video question answering dataset focused on understanding the dynamic properties of 3D objects, emphasizing concepts such as velocity, acceleration, and collisions in 4D scenes. The proposed NS-4DPhysics model, which integrates physics priors and explicit scene representations, significantly outperforms existing models in reasoning about these 4D dynamics, as demonstrated through factual, predictive, and counterfactual queries.

Computational Limits of Low-Rank Adaptation (LoRA) Fine-Tuning for Transformer Models
This paper investigates the computational boundaries of Low-Rank Adaptation (LoRA) in finetuning transformer-based models, revealing that low-rank decompositions in gradient computation can lead to algorithmic speedups. By identifying phase transition behaviors in efficiency and demonstrating almost linear algorithms, the study offers theoretical insights into the conditions for efficient approximation algorithms in LoRA adaptation, validated through practical scenarios involving partial and full weight adaptations in attention heads.

ConceptPrune: Concept Editing in Diffusion Models via Skilled Neuron Pruning
This paper introduces ConceptPrune, a training-free approach for unlearning undesirable concepts in pre-trained text-to-image diffusion models by identifying and pruning critical regions in the models. Experiments show that this method can efficiently erase target concepts, like artistic styles and nudity, from the models by pruning merely 0.12% of total weights, enabling robust multi-concept erasure against adversarial attacks.

Constructing Confidence Intervals for Average Treatment Effects from Multiple Datasets
The paper introduces a novel method for estimating the average treatment effect (ATE) with valid confidence intervals (CIs) from multiple observational and experimental datasets, particularly useful in medical settings. By leveraging prediction-powered inferences, this method reduces CI width, enhancing precision and applicability, and is proven both theoretically and through numerical experiments to be unbiased and valid.

Context-Alignment: Activating and Enhancing LLMs Capabilities in Time Series
This paper introduces Context-Alignment (CA), a novel approach that aligns time series data with linguistic components within Large Language Models (LLMs) to leverage their natural language processing strengths. By employing Dual-Scale Context-Alignment GNNs (DSCA-GNNs) for structural and logical alignment, the proposed Few-Shot prompting Context-Alignment (FSCA) method significantly enhances LLMs' performance in time series tasks, especially in few-shot and zero-shot forecasting, as demonstrated by extensive experiments.

Continuous Exposure Learning for Low-light Image Enhancement using Neural ODEs
This paper addresses the challenge of low-light image enhancement by proposing an innovative approach using Neural Ordinary Differential Equations (NODE) to model the continuous dynamics of image enhancement as a dynamic system. By applying NODE to unsupervised curve-adjustment-based methods, the authors overcome convergence issues and demonstrate state-of-the-art performance in low-light conditions across benchmark datasets.

Contractive Dynamical Imitation Policies for Efficient Out-of-Sample Recovery
This paper introduces a framework for learning imitation policies using contractive dynamical systems to ensure convergence in all policy rollouts despite perturbations, enhancing out-of-sample recovery. The approach, using recurrent equilibrium networks and coupling layers, provides theoretical reliability and shows significant performance improvements in simulated robotic tasks.

Controlling Language and Diffusion Models by Transporting Activations
This paper introduces Activation Transport (AcT), a novel framework based on optimal transport theory to steer model activations for improved control over generative models. AcT provides fine-grained adjustments to model behavior with minimal computational cost, demonstrating effectiveness in mitigating toxicity and enhancing concept induction in large language models, as well as enabling detailed style control and concept negation in text-to-image diffusion models.

Controlling the Fidelity and Diversity of Deep Generative Models via Pseudo Density
This paper presents a method to bias deep generative models, such as GANs and diffusion models, towards generating data with either enhanced fidelity or increased diversity by utilizing a novel metric called pseudo density. The approach includes three techniques: per-sample perturbation, importance sampling during inference, and fine-tuning with importance sampling, the latter of which improves the Frechet Inception Distance (FID) for pre-trained models efficiently.

Convergence of Distributed Adaptive Optimization with Local Updates
This paper investigates the theoretical benefits of distributed adaptive algorithms with local updates, showing that Local SGD with momentum (Local SGDM) and Local Adam can surpass minibatch counterparts in specific convex and weakly convex settings. Using a novel technique to demonstrate contraction during local iterations, the study highlights the advantages of local updates under generalized smoothness and gradient clipping strategies.

Correlation and Navigation in the Vocabulary Key Representation Space of Language Models
This paper explores how the distribution of fixed vocabulary keys affects next-token prediction (NTP) in neural language models, revealing that tokens that are distributionally similar to top-ranked predictions can skew results and reduce sampling diversity. The authors propose a novel in-context method to address this by iteratively adjusting the query representation away from previously explored regions, leading to improved diversity and self-consistency in various language generation tasks.

Cross-Embodiment Dexterous Grasping with Reinforcement Learning
This paper explores the development of a universal reinforcement learning policy for dexterous grasping that can be applied across various robotic hands, inspired by human hand teleoperation. By employing a universal action space derived from human eigengrasps and simplifying proprioception to fingertip and palm positions, the approach achieves an 80% success rate with diverse robots and showcases zero-shot generalization and efficient finetuning capabilities.

DAMO: Decoding by Accumulating Activations Momentum for Mitigating Hallucinations in Vision-Language Models
This paper addresses the issue of hallucinations in Large Vision-Language Models (VLMs) by analyzing their layer-wise prediction tendencies and decoding mechanisms. The authors propose a novel momentum-inspired decoding strategy that enhances decoding consistency across layers, improving VLM performance in multimodal tasks with minimal efficiency impact.

DeepTAGE: Deep Temporal-Aligned Gradient Enhancement for Optimizing Spiking Neural Networks
Spiking Neural Networks (SNNs) offer a low-power alternative to traditional neural networks but face challenges due to complex dynamics and spike communication. This paper introduces Deep Temporal-Aligned Gradient Enhancement (DeepTAGE), which improves the optimization of SNNs by dynamically adjusting surrogate gradients and incorporating deep supervision, leading to balanced training and substantial performance gains across various datasets.

Denoising Levy Probabilistic Models
This study extends the denoising diffusion probabilistic model (DDPM) by using $\alpha$-stable noise instead of Gaussian noise, resulting in the denoising Lévy probabilistic model (DLPM). The DLPM offers distinct and favorable enhancements over traditional methods by improving data distribution coverage, handling unbalanced datasets more effectively, and achieving better computation times with fewer steps.

Differentially Private Steering for Large Language Model Alignment
This paper introduces the Private Steering for LLM Alignment (PSA) algorithm, which aligns large language models (LLMs) using private datasets while ensuring differential privacy (DP) guarantees. Through extensive experiments, the study demonstrates that PSA effectively maintains alignment and generation quality with minimal performance loss and provides improved empirical privacy guarantees, supported by a novel Membership Inference Attack tailored for activation editing.

Discovering Temporally Compositional Neural Manifolds with Switching Infinite GPFA
The paper introduces the infinite GPFA model, a fully Bayesian non-parametric extension of Gaussian Process Factor Analysis, addressing the limitation of pre-specifying the number of latent factors by using an Indian Buffet Process prior. This model enables the inference of a potentially infinite set of latent factors and their dynamic contributions to neural activity, as demonstrated on synthetic data and hippocampal place cell activities, revealing new insights into neural encoding dynamics during spatial tasks.

Does SGD really happen in tiny subspaces?
This paper investigates the training dynamics of deep neural networks, specifically focusing on the role of the dominant subspace in the training loss Hessian. The study finds that projecting the Stochastic Gradient Descent (SGD) update onto or out of the dominant subspace yields similar effectiveness, revealing that the alignment between the gradient and dominant subspace is spurious across various training setups, thus providing insights into more efficient neural network training methods.

Do LLMs estimate uncertainty well in instruction-following?
This paper systematically evaluates the ability of large language models (LLMs) to estimate uncertainty when following user instructions, highlighting limitations in current approaches. The study introduces a controlled evaluation setup to better compare uncertainty estimation methods, revealing that existing methods often fail, particularly with subtle instruction-following errors, and emphasizing the need for advancements to enhance the reliability of AI agents.

Do Mice Grok? Glimpses of Hidden Progress in Sensory Cortex
This study explores the phenomenon of continued learning in the cortex even after observable behavior stabilizes, a concept known as "overtraining." Through a reanalysis of neural data from mice, the research provides evidence that task-specific representations continue to evolve in the brain, suggesting that this hidden learning enhances classification accuracy and offers insights into the robustness of animal learning in response to task changes.

DON’T STOP ME NOW: EMBEDDING BASED SCHEDULING FOR LLMS
This paper introduces TRAIL, a method to improve scheduling efficiency in Large Language Model (LLM) applications by using the internal structure of LLM-generated tokens to accurately predict remaining output lengths and implement a modified Shortest Remaining Process Time (SRPT) scheduling with limited preemption. By refining predictions and optimizing preemption based on memory usage, TRAIL significantly reduces mean latency and time to first token, outperforming existing systems on the Alpaca dataset.

DRoP: Distributionally Robust Data Pruning
This paper conducts the first systematic study on the impact of data pruning on classification bias in neural models, revealing potential biases introduced by existing algorithms. The authors propose DRoP, a distributionally robust approach to data pruning, which improves worst-class performance and maintains distributional robustness on standard computer vision benchmarks, even as datasets are extensively pruned.

DynaPrompt: Dynamic Test-Time Prompt Tuning
DynaPrompt, or dynamic test-time prompt tuning, enhances zero-shot generalization in vision-language models by adaptively selecting and optimizing relevant prompts for each test sample, addressing error accumulation issues in online tuning. Utilizing prediction entropy, probability difference metrics, and a dynamic prompt appending strategy, DynaPrompt effectively reduces errors and improves performance across fourteen datasets.

Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders
This study explores the design space of multimodal large language models (MLLMs) using a mixture of vision encoders, revealing that simply concatenating visual tokens is as effective as complex strategies. The introduction of Pre-Alignment enhances model coherence, and the resulting Eagle family of MLLMs surpasses other leading open-source models in benchmarks.

Earlier Tokens Contribute More: Learning Direct Preference Optimization From Temporal Decay Perspective
This paper introduces an enhanced preference optimization method for Direct Preference Optimization (DPO) that incorporates a temporal decay factor to address the issue of length bias in language model responses. By focusing rewards on earlier critical tokens, the proposed method demonstrates improved performance over traditional DPO across multiple benchmarks, enhancing alignment with human preferences without sacrificing the model's general capabilities.

ECD: A Machine Learning Benchmark for Predicting Enhanced-Precision Electronic Charge Density in Crystalline Inorganic Materials
This paper introduces the ECD dataset, containing electronic charge density data for 140,646 crystal geometries, to enhance machine learning predictions for electronic structures beyond traditional DFT methods. By combining medium-precision PBE data with high-precision HSE data, the study provides benchmarks for improving machine learning models, ultimately supporting advancements in materials design and fostering community-driven research through open-source resources.

econSG: Efficient and Multi-view Consistent Open-Vocabulary 3D Semantic Gaussians
This paper introduces econSG, a novel method for open-vocabulary semantic segmentation that refines SAM and CLIP using a Confidence-region Guided Regularization (CRR) for enhanced semantic features with accurate boundaries. Additionally, econSG improves computational efficiency and ensures 3D multi-view consistency by utilizing a low-dimensional contextual space, achieving state-of-the-art performance on four benchmark datasets while being the most efficient in terms of training.

Effective and Efficient Time-Varying Counterfactual Prediction with State-Space Models
This paper introduces the counterfactual Mamba model with Covariate-based Decorrelation towards Selective Parameters (Mamba-CDSP) to enhance time-varying counterfactual prediction (TCP) by utilizing state-space models (SSMs) for improved handling of long sequences. The proposed model addresses over-balancing issues by de-correlating current treatments with historical data, optimizing both effectiveness and efficiency, as demonstrated through extensive experiments on synthetic and real-world datasets.

Efficient Perplexity Bound and Ratio Matching in Discrete Diffusion Language Models
This paper enhances the application of continuous-time discrete Markov chain (CTMC) models to categorical data by introducing new theorems concerning KL divergence, leading to an improved upper bound on perplexity. Additionally, empirical results demonstrate that minimizing denoising cross-entropy yields up to 10% lower perplexity and 15% faster training compared to score-entropy methods, aided by a novel CTMC transition-rate matrix for refined predictions.

Eliciting Human Preferences with Language Models
The paper introduces Generative Active Task Elicitation (GATE), a method leveraging language models to guide the task specification process by interactively engaging with users. Through experiments across tasks such as content recommendation and email validation, GATE demonstrates superiority in obtaining informative preference specifications and user satisfaction, highlighting its potential in aligning models with complex human preferences.

Endless Jailbreaks with Bijection Learning
This paper introduces bijection learning, an attack algorithm that exploits adversarial inputs to reveal safety vulnerabilities in large language models (LLMs) through controlled complexity encodings. The study demonstrates that more capable models are more susceptible to these bijection attacks, emphasizing the emergence of new vulnerabilities as models scale.

Exact Computation of Any-Order Shapley Interactions for Graph Neural Networks
This paper addresses the challenge of interpreting Graph Neural Networks (GNNs) by introducing Shapley Interactions (SIs) to quantify node contributions and interactions in graph predictions. The authors present GraphSHAP-IQ, a method that efficiently computes SIs for GNNs, substantially reducing computational complexity and improving interpretability, demonstrated through experiments on benchmark datasets and real-world applications.

Examining Alignment of Large Language Models through Representative Heuristics: the case of political stereotypes
This study investigates the alignment of large language models (LLMs) with human values in the political domain, highlighting how LLMs can exaggerate political positions and stereotype more than human respondents. By examining factors contributing to this deviation and testing prompt-based mitigation strategies, the research reveals a vulnerability to representativeness heuristics in LLMs but also identifies effective methods to reduce such biases.

Exposure Bracketing Is All You Need For A High-Quality Image
This paper addresses the challenge of capturing high-quality photos in low-light conditions by leveraging the complementary strengths of multi-exposure images for tasks such as denoising, deblurring, and super-resolution through exposure bracketing. The authors introduce a temporally modulated recurrent network (TMRNet) and a self-supervised adaptation method, validated by their data simulation pipeline and collection of nighttime scenarios, outperforming current multi-image processing methods.

FaceShot: Bring Any Character into Life
FaceShot is a novel training-free portrait animation framework that animates any character using driven video without the need for fine-tuning or retraining, through precise landmark sequences informed by latent diffusion models. This approach extends portrait animation to stylized characters, improving performance and surpassing state-of-the-art methods, as demonstrated by extensive experiments on the CharacBench benchmark.

Faster Diffusion Sampling with Randomized Midpoints: Sequential and Parallel
This paper introduces a novel sampling scheme inspired by the randomized midpoint method for log-concave sampling, achieving improved dimension dependence ($\widetilde O(d^{5/12})$) for sampling from smooth distributions compared to previous $\widetilde O(\sqrt{d})$ bounds. The proposed algorithm is the first to offer provable guarantees for parallel sampling with diffusion models, operating in $\widetilde O(\log^2 d)$ parallel rounds, and also advances log-concave sampling with enhanced dimension dependence.

Feature Averaging: An Implicit Bias of Gradient Descent Leading to Non-Robustness in Neural Networks
This paper identifies "Feature Averaging" as an implicit bias in gradient descent training that contributes to the non-robustness of deep neural networks by relying on averaged features for classification rather than distinct feature differentiation. The authors provide a theoretical analysis and empirical evidence showing that this bias undermines adversarial robustness but can be mitigated by granular supervision that encourages networks to classify individual features, thus improving robustness on datasets like MNIST and CIFAR-10.

Federated Continual Learning Goes Online: Uncertainty-Aware Memory Management for Vision Tasks and Beyond
This paper addresses the problem of catastrophic forgetting in Federated Continual Learning by introducing an uncertainty-aware memory-based approach that operates in an online scenario with data streams processed only once. By utilizing Bregman Information to estimate model variance and strategically selecting samples for retraining, the proposed method effectively reduces forgetting while ensuring data confidentiality and efficient communication.

Flat Reward in Policy Parameter Space Implies Robust Reinforcement Learning
This paper explores the significance of flat reward landscapes in reinforcement learning (RL) and their impact on model robustness, extending the concept of flat minima from supervised learning to RL. The study demonstrates, through extensive simulations, that flatter reward landscapes improve the generalization and robustness of RL models across variations in action selection, transition dynamics, and reward functions, with code available for further exploration.

Gaussian Splatting Lucas-Kanade
This paper introduces a novel analytical approach that adapts the Lucas-Kanade method to dynamic Gaussian splatting for reconstructing 3D scenes from 2D images, specifically addressing scenarios with minimal camera movement and high scene dynamism. By deriving an analytical velocity field and integrating it over time, the method precisely enforces motion constraints on warp fields, enhancing scene flow computation and demonstrating superior performance in both synthetic and real-world environments.

GenVP: Generating Visual Puzzles with Contrastive Hierarchical VAEs
The paper introduces Generative Visual Puzzles (GenVP), a novel framework that enhances the Raven’s Progressive Matrices (RPMs) by enabling the generation of new puzzles beyond existing ones, replicating human-like high-level abstract visual reasoning. GenVP achieves state-of-the-art performance in both accuracy and out-of-distribution generalization across various datasets, demonstrating its superiority over current approaches in solving and generating RPM-related challenges.

Glimpse: Enabling White-Box Methods to Use Proprietary Models for Zero-Shot LLM-Generated Text Detection
Advanced LLMs generate human-like text, necessitating effective detection methods, yet current techniques are hindered by limitations in accessing strong proprietary models. This paper introduces Glimpse, a distribution estimation approach that enhances white-box detection methods, achieving high accuracy with proprietary models like GPT-3.5, thereby suggesting that advanced LLMs can effectively detect their own outputs.

GraphArena: Evaluating and Exploring Large Language Models on Graph Computation
GraphArena is introduced as a benchmarking tool to evaluate Large Language Models (LLMs) on graph computational problems through a suite of tasks spanning polynomial-time and NP-complete challenges. The study highlights that LLMs still face difficulties with complex graph problems and outlines potential improvements, including instruction tuning and chain-of-thought prompting, with the tool available on GitHub for further research.

Grounding Video Models to Actions through Goal Conditioned Exploration
This paper presents a framework that grounds large video models to continuous actions through self-exploration by using generated video states as goals, without requiring external supervision like rewards or action labels. The approach, validated on multiple tasks across diverse environments, demonstrates competitive or superior performance to behavior cloning baselines trained on expert demonstrations, highlighting its potential in enabling agents to solve complex tasks autonomously.

GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models
This paper introduces GSM-Symbolic, an enhanced benchmark designed to assess the mathematical reasoning abilities of Large Language Models (LLMs) through a diverse set of symbolic template-generated questions. The study reveals that current LLMs show significant performance variability and susceptibility to decline with changes in numerical values or additional clauses, underscoring limitations in their genuine logical reasoning capabilities.

GTR: Improving Large 3D Reconstruction Models through Geometry and Texture Refinement
This paper presents a novel approach to 3D mesh reconstruction from multi-view images by enhancing the transformer-based triplane generator and Neural Radiance Field (NeRF) model within the existing large reconstruction model (LRM) framework. Key contributions include architectural modifications for better image representation and training efficiency, differentiable mesh extraction for improved geometry reconstruction, and a rapid per-instance texture refinement for accurate depiction of complex textures, achieving state-of-the-art results on standard datasets.

HiBug2: Efficient and Interpretable Error Slice Discovery for Comprehensive Model Debugging
This paper introduces HiBug2, an automated framework designed to identify and mitigate error slices in deep learning models for computer vision, thereby improving model robustness and reliability. By generating task-specific visual attributes and employing an efficient slice enumeration algorithm, HiBug2 enhances error slice discovery and model repair across various domains, addressing limitations of prior methods and demonstrating improved precision and repair capabilities.

How Much is Unseen Depends Chiefly on Information About the Seen
This paper provides the first precise characterization of the expected missing mass in classifier inputs, showing it is determined by the number of classes appearing a fixed number of times in training data and an exponentially decaying error. By casting distribution-free estimation as an optimization problem, the authors develop a method that discovers estimators with significantly smaller mean-squared errors than the Good-Turing estimator, achieving roughly 80% of its MSE in over 93% of experimental runs when samples match or exceed the number of classes.

How new data permeates LLM knowledge and how to dilute it
The study investigates how large language models (LLMs) manage new information and its impact on existing knowledge, highlighting a "priming" effect where new facts can be misapplied in inappropriate contexts. By introducing the "Outlandish" dataset and developing two techniques, the research aims to predict and mitigate undesirable knowledge integration, reducing priming effects by 50-95% while maintaining learning capabilities.

How to Verify Any (Reasonable) Distribution Property: Computationally Sound Argument Systems for Distributions
This paper introduces an interactive protocol for verifying whether an unknown distribution is close to satisfying a claimed property without replication, enabling verification in polynomial time with high probability against any cheating prover strategy, assuming collision-resistant hash functions. The protocol requires only four messages and offers a quadratic speedup in sample and runtime complexity compared to traditional methods for approximately deciding distribution properties.

ICLR: In-Context Learning of Representations
This paper investigates whether large language models (LLMs) can flexibly alter their semantic organization of concepts in response to new contextual examples. By using a toy “graph tracing” task, the study finds that increasing context size can lead to a sudden re-organization of representations according to a predefined graph structure, suggesting that context size is a critical factor in unlocking novel model capabilities.

Identifying latent state transitions in non-linear dynamical systems
This paper introduces a state-space modeling framework to identify latent states and nonlinear transition dynamics in dynamical systems using high-dimensional sensory data, overcoming limitations of previous models that relied on linear approximations. By leveraging advances in nonlinear ICA and employing variational auto-encoders, the proposed method enhances generalization, interpretability, and future prediction accuracy, showing promising results in adapting to new environments and in long-horizon forecasts for complex real-world dynamics.

Improved Diffusion-based Generative Model with Better Adversarial Robustness
Diffusion Probabilistic Models (DPMs) face a distribution mismatch issue during their training and sampling processes, which can lead to inaccuracies in data generation. This paper demonstrates that Distributionally Robust Optimization (DRO) can alleviate this mismatch by employing robustness-driven Adversarial Training (AT), and extensive experiments confirm its effectiveness, including for Consistency Models (CM). The code for this approach is available at https://github.com/kugwzk/AT_Diff.

Improving Graph Neural Networks by Learning Continuous Edge Directions
The paper introduces the Continuous Edge Direction (CoED) GNN, a novel framework that assigns fuzzy and differentiable edge directions to graph edges, enhancing long-range information transmission and node classification. Through a complex-valued Laplacian, CoED GNN facilitates improved performance on graph ensemble data by learning and scaling edge directions, outperforming existing methods on both synthetic and real-world datasets.

Improving Semantic Understanding in Speech Language Models via Brain-tuning
This paper introduces "brain-tuning," a process that enhances speech language models by fine-tuning them with fMRI recordings of individuals listening to natural stories, leading to improved alignment with brain responses in semantic language regions. By incorporating brain signals into model training, the study demonstrates enhanced performance in semantic tasks and a representational space with increased semantic preference, suggesting an advancement in models' semantic understanding.

Improving Unsupervised Constituency Parsing via Maximizing Semantic Information
This paper presents a new objective for training unsupervised constituency parsers by maximizing SemInfo, the semantic information encoded in constituent structures, rather than relying solely on sentence log-likelihood (LL). Implementing this SemInfo maximization approach with a TreeCRF-based model, the authors achieve significantly improved parsing accuracy, outperforming traditional methods and achieving state-of-the-art results in multiple languages.

Inference-Aware Fine-Tuning for Best-of-N Sampling in Large Language Models
This paper presents a novel inference-aware fine-tuning paradigm for large language models (LLMs), focusing on the Best-of-N (BoN) inference strategy to enhance performance. By developing imitation learning and reinforcement learning methods that address the non-differentiable aspects of BoN, the authors achieve improved empirical results, notably boosting the BoN performance of the Gemma 2B model on Hendrycks MATH and Pass@K benchmarks.

In Search of Forgotten Domain Generalization
The paper revisits the challenge of Out-of-Domain (OOD) generalization in the era of foundation models and expansive web-scale datasets, which often blur true OOD evaluation due to domain overlap. By creating specific datasets from LAION that are strictly OOD in style relative to ImageNet and DomainNet, the study highlights that true OOD challenges persist and identifies optimal dataset mixing strategies for better model generalization, allowing for more accurate assessments of model robustness.

Interpreting the Second-Order Effects of Neurons in CLIP
This paper introduces a "second-order lens" approach to interpret CLIP's neurons by analyzing their flow through later attention heads to the output, revealing that neuron effects are significant for less than 2% of images and can be described with sparse text representations. The study showcases the polysemantic nature of neurons and demonstrates applications in creating semantic adversarial examples and performing zero-shot segmentation, suggesting new possibilities for model deception and enhancing model capabilities.

Inverse Attention Agents for Multi-Agent Systems
This paper introduces Inverse Attention Agents, which leverage an attention mechanism informed by Theory of Mind, to enhance adaptability and performance in Multi-Agent Systems when facing unfamiliar opponents or teammates. The proposed inverse attention network effectively infers other agents' attentional states to improve decision-making, leading to superior performance in cooperation and competition tasks, as well as more human-like behaviors in experiments.

IRIS: LLM-Assisted Static Analysis for Detecting Security Vulnerabilities
IRIS is a neuro-symbolic approach combining large language models with static analysis to enhance security vulnerability detection in software by enabling whole-repository reasoning and reducing dependence on human-specified code annotations. Evaluated on the new CWE-Bench-Java dataset, IRIS significantly outperforms the state-of-the-art tool CodeQL by detecting more vulnerabilities and discovering previously unidentified ones.

Language Models are Advanced Anonymizers
This paper introduces a new framework for evaluating and implementing text anonymization against adversarial inferences by large language models (LLMs), addressing the gap between current anonymization methods and regulatory requirements. The proposed adversarial anonymization framework outperforms existing commercial anonymizers in preserving both utility and privacy, as confirmed by a comprehensive experimental evaluation and a human study.

Large Language Models can Become Strong Self-Detoxifiers
This paper introduces Self-disciplined Autoregressive Sampling (SASA), a novel decoding algorithm that reduces textual toxicity in large language models (LLMs) without needing external reward models or retraining. By leveraging internal representations and learning from toxic versus non-toxic subspaces, SASA effectively improves output quality and stands as a competitive alternative to existing detoxification methods, as demonstrated across various benchmarks and LLMs.

LASER: A Neuro-Symbolic Framework for Learning Spatio-Temporal Scene Graphs with Weak Supervision
LASER is a neuro-symbolic framework that utilizes video captions as weak supervision to train spatio-temporal scene graph (STSG) generators, avoiding the need for labor-intensive annotated videos. By leveraging large language models and a differentiable symbolic reasoner, LASER efficiently aligns predicted STSGs with extracted logical specifications, demonstrating significant accuracy improvements over fully-supervised methods on datasets like OpenPVSG, 20BN, and MUGEN.

Lean-STaR: Learning to Interleave Thinking and Proving
The paper introduces Lean-STaR, a framework that enhances language models' theorem-proving abilities by generating informal thoughts before each proof step, which are typically absent in formal proofs. By integrating retrospective ground-truth tactics for synthetic thought generation and applying expert iteration, Lean-STaR significantly improves theorem-proving performance and provides insights into the effectiveness of augmented thoughts, achieving better results compared to base models.

Learning a Fast Mixing Exogenous Block MDP using a Single Trajectory
This paper introduces STEEL, the first algorithm to efficiently learn the controllable dynamics of an Exogenous Block Markov Decision Process (Ex-BMDP) from a single continuous trajectory in the function approximation setting. STEEL’s sample complexity is independent of the exogenous noise factor's size and relies instead on the controllable latent space, the encoder function class, and, at worst, linearly on the noise's mixing time, providing a significant advancement over previous episodic-focused methods.

Learning Causal Alignment for Reliable Disease Diagnosis
This paper introduces a causality-based alignment framework to enhance the decision-making process of machine learning models, ensuring they align more closely with the expertise of radiologists. By employing counterfactual generation and a novel causal alignment loss, the proposed method improves alignment by focusing on causal factors, demonstrated effectively in two medical diagnosis applications.

Learning Diagrams: A Graphical Language for Compositional Training Regimes
This paper introduces learning diagrams, a graphical framework that represents training setups as data rather than code and compiles them into unique loss functions to ensure model predictions are in agreement. The authors demonstrate the applicability of learning diagrams to various machine learning setups, provide a library for PyTorch and Flux.jl models, and establish a rigorous semantics using category theory to facilitate the construction and manipulation of complex models from smaller components.

Learning General-purpose Biomedical Volume Representations using Randomized Synthesis
This paper introduces a representation learning method for 3D biomedical foundation models that anticipates domain shifts during training by using a data engine to synthesize variable training samples, enhancing generalization. The authors propose a contrastive learning approach for pretraining a 3D network to be invariant to imaging variations, achieving state-of-the-art performance in multimodality registration and few-shot segmentation without relying on existing real image datasets.

Learning Mask Invariant Mutual Information for Masked Image Modeling
This paper proposes a novel understanding of masked autoencoders (MAEs) in computer vision by employing the information bottleneck principle, aiming to balance relevant and irrelevant information in latent features to enhance performance. The introduction of MI-MAE, which optimizes MAEs through mutual information maximization and minimization, demonstrates improved results on image classification, object detection, and semantic segmentation tasks, validating the theoretical framework and offering deeper insights into self-supervised learning models.

Learning Spatiotemporal Dynamical Systems from Point Process Observations
This paper introduces a novel method for modeling spatiotemporal dynamics from irregularly collected data, common in real-world sensor networks. By combining techniques from neural differential equations, neural point processes, and variational inference, the model significantly improves predictive accuracy and computational efficiency, enhancing the understanding of complex dynamical systems under realistic conditions.

Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training
Large language models often struggle with disambiguation in conversations, which can hinder their ability to effectively understand user intents. The study introduces Action-Based Contrastive Self-Training (ACT), a new preference optimization method that efficiently improves dialogue policy learning in multi-turn conversations without requiring action labels, showing significant enhancements in tasks including tabular-grounded question-answering and machine reading comprehension.

Lines of Thought in Large Language Models
This paper investigates the statistical properties of trajectories formed by Large Language Models as they process input text, showing that these trajectories cluster along a low-dimensional, non-Euclidean manifold. The study reveals that the complex behavior of these models can be simplified and approximated by a stochastic equation with a few data-driven parameters, offering insights into the reduction of complexity in large AI models.

Local Patterns Generalize Better for Novel Anomalies
This paper presents a novel framework for video anomaly detection by focusing on local patterns that generalize to novel samples and modeling their dynamics using a two-stage process of image-text alignment and cross-modality attention. The proposed approach, which includes a State Machine Module for temporal clues and combines spatial local patterns with temporal motion estimation, achieves state-of-the-art performance on benchmark datasets.

Logically Consistent Language Models via Neuro-Symbolic Integration
This paper introduces a training objective using a neuro-symbolic loss to enhance the logical consistency of large language models (LLMs) by aligning them with external factual knowledge and rules. The approach not only improves the LLMs' logical consistency compared to existing baselines but also allows them to generalize to new, semantically similar datasets more effectively.

LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs
Current long context large language models (LLMs) struggle with generating outputs longer than 2,000 words due to the limitation of lacking long-output examples in supervised fine-tuning datasets. To overcome this, the authors introduce AgentWrite, which breaks down long tasks into subtasks and uses a new dataset, LongWriter-6k, to extend output capabilities to over 10,000 words while maintaining quality, as demonstrated by superior performance on the proposed LongBench-Write benchmark.

Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing
This paper introduces Magpie, a self-synthesis method for generating large-scale alignment data for large language models (LLMs) by leveraging the auto-regressive nature of aligned models like Llama-3-Instruct. The Magpie-generated datasets enhance supervised fine-tuning, demonstrating performance surpassing or comparable to previous methods and official instruct models on alignment benchmarks despite the use of fewer data points.

Mechanistic Interpretability Meets Vision Language Models: Insights and Limitations
This paper systematically reviews mechanistic interpretability methods to enhance the scientific understanding of vision language models (VLMs), which currently lack transparency despite their rapid development. By examining techniques such as probing, activation patching, logit lens, sparse autoencoders, and automated explanation, the work summarizes insights into VLMs' decision-making processes and highlights challenges for improving their robustness, generalization, and interpretability.

Meta-Continual Learning of Neural Fields
This paper introduces Meta-Continual Learning of Neural Fields (MCL-NF), a new approach using a modular architecture and optimization-based meta-learning to enhance the continual learning capabilities of neural fields. The proposed method, which includes the Fisher Information Maximization loss for neural radiance fields (FIM-NeRF), improves reconstruction quality and learning speed while reducing parameter requirements, demonstrating superiority across various datasets and tasks like image, audio, and video reconstruction.

Meta-Dynamical State Space Models for Integrative Neural Data Analysis
This paper proposes a novel meta-learning approach to leverage shared structures in neural activity for learning latent dynamics from recordings across different but related tasks. By capturing task-related variability on a low-dimensional manifold, the method effectively enables rapid learning and prediction of dynamics in new neural recordings, demonstrated through few-shot reconstruction and forecasting in synthetic and real motor cortex data.

MetaMetrics: Calibrating Metrics for Generation Tasks Using Human Preferences
MetaMetrics is a calibrated meta-metric that optimizes the combination of existing metrics to better align with human preferences in evaluating generation tasks across different modalities. By enhancing metric alignment with human judgment in both language and vision tasks, MetaMetrics proves to be flexible, extendable, and beneficial across multilingual and multi-domain scenarios.

Metamizer: A Versatile Neural Optimizer for Fast and Accurate Physics Simulations
Metamizer is a novel neural optimizer designed to efficiently solve a wide range of physical systems without retraining by minimizing a physics-based loss function using a scale-invariant architecture. This approach not only achieves high accuracy across multiple partial differential equations (PDEs) like Laplace and Navier-Stokes but also generalizes to previously untrained PDEs, demonstrating its superiority in the realm of physics-based Deep Learning.

Mitigating Hallucination in Large Vision-Language Models via Modular Attribution and Intervention
This paper investigates why Large Vision-Language Models (LVLMs) hallucinate during open-ended generation tasks and proposes methods to reduce this issue. By identifying and targeting hallucination heads within Multi-Head Attention modules, the authors present two approaches that significantly decrease hallucination rates, as demonstrated on the COCO captioning task.

Mitigating Reward Over-Optimization in RLHF via Behavior-Supported Regularization
This paper introduces the Behavior-Supported Policy Optimization (BSPO) method to address reward over-optimization in reinforcement learning from human feedback, a challenge linked to extrapolation errors when language models evaluate out-of-distribution (OOD) responses. By using a behavior-supported Bellman operator, BSPO effectively reduces OOD response generation and enhances policy optimization, showing improved results over existing methods through both theoretical guarantees and empirical evidence.

MLPs Learn In-Context on Regression and Classification Tasks
This paper challenges the assumption that in-context learning (ICL) is unique to Transformer models by demonstrating that multi-layer perceptrons (MLPs) and MLP-Mixer models can also perform ICL comparably well under the same compute budget. Notably, MLPs outperform Transformers in classical psychology tasks related to relational reasoning, emphasizing the need to explore non-attention-based architectures for in-context learning and encouraging further investigation into their potential advantages.

MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos
The paper introduces MMWorld, a benchmark designed to evaluate Multimodal Language Models (MLLMs) on video understanding across multiple disciplines and reasoning tasks, including explanation, counterfactual thinking, and future prediction. MMWorld highlights the evaluation challenges faced by 15 different MLLMs, showing significant room for improvement and offering insights into model capabilities compared to human understanding.

Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning
The paper introduces World Modeling with Compositional Causal Components (WM3C), a framework designed to enhance generalization in reinforcement learning by leveraging compositional causal components. By using language as a compositional modality and employing a masked autoencoder with mutual information constraints, WM3C effectively identifies and disentangles transition dynamics, significantly outperforming existing methods in generalizing to novel tasks in both simulations and real-world scenarios.

Model merging with SVD to tie the Knots
This paper investigates the challenge of merging LoRA finetuned models and proposes KnOTS, a method that uses singular value decomposition to align model weights, enhancing the merging process. The approach improves model merging effectiveness by up to 4.3% across multiple benchmarks, including a newly introduced setting for evaluating general models, and the code is made available for public use.

MorphoDiff: Cellular Morphology Painting with Diffusion Models
MorphoDiff is a pioneering generative pipeline designed to predict high-resolution cellular morphological responses to various chemical and genetic interventions using perturbation encoding. By integrating perturbation embeddings into a 2D latent diffusion model, MorphoDiff enhances the understanding of biological processes and supports drug discovery by generating high-fidelity images validated across multiple Cell Painting datasets.

Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence
This paper critiques the common approach of using separability-oriented linear classifiers for computing Concept Activation Vectors (CAVs), highlighting their susceptibility to distractor directions that misalign with the true concept direction. To enhance accuracy, the authors introduce pattern-based CAVs that focus solely on concept signals, demonstrating improved alignment and effectiveness in applications across various datasets and neural network architectures.

NetMoE: Accelerating MoE Training through Dynamic Sample Placement
The paper introduces NetMoE, a method designed to accelerate All-to-All communication in Mixture of Experts (MoE) models by considering the locality of tokens within the same training sample. By dynamically rearranging the placement of training samples to minimize communication costs, NetMoE achieves up to 1.67 times improvement in training efficiency compared to existing MoE training frameworks as demonstrated on a 32 GPU setup.

Neural Spacetimes for DAG Representation Learning
We introduce Neural SpaceTimes (NSTs), a class of trainable deep learning-based geometries that universally represent nodes in weighted Directed Acyclic Graphs as events in a spacetime manifold, encoding both spatial edge weights and temporal causality. Our method, validated through synthetic and real-world networks, offers a universal embedding theorem with minimal distortion and outperforms fixed spacetime geometries by leveraging neural networks for dynamic space and time geometry optimization.

NextBestPath: Efficient 3D Mapping of Unseen Environments
This paper introduces a novel dataset, AiMDoom, and a new method called next-best-path (NBP) for active 3D mapping, enabling agents to efficiently reconstruct new scenes by predicting long-term goals rather than focusing on short-sighted views. By combining online data collection, data augmentation, and curriculum learning, the proposed NBP method significantly outperforms current techniques on both MP3D and AiMDoom datasets, providing a more efficient approach to mapping complex indoor environments.

NExUME: Adaptive Training and Inference for DNNs under Intermittent Power Environments
This paper introduces NExUME, a novel training methodology for DNNs in energy-constrained environments like Energy Harvesting Wireless Sensor Networks, dynamically adjusting dropout rates and quantization levels based on real-time energy availability. The approach improves accuracy by 6% to 22% with minimal computational overhead, integrates energy profiles for enhanced adaptability, and contributes a new dataset for advancing energy harvesting applications in computational settings.

nGPT: Normalized Transformer with Representation Learning on the Hypersphere
The paper introduces nGPT, a novel neural network architecture that employs unit norm normalization for all vectors, resulting in representation learning on a hypersphere. This approach accelerates training significantly, allowing nGPT to achieve comparable accuracy in 4 to 20 times fewer training steps, depending on the sequence length.

Node Identifiers: Compact, Discrete Representations for Efficient Graph Learning
This paper introduces a novel framework for generating compact and interpretable node representations, called node identifiers (node IDs), for large-scale graphs using vector quantization to compress GNN node embeddings into discrete int4 codes. Experiments on 34 datasets show that these node IDs improve speed and memory efficiency while maintaining competitive performance in various graph-related tasks, with code available at the provided GitHub link.

NutriBench: A Dataset for Evaluating Large Language Models in Nutrition Estimation from Meal Descriptions
NutriBench is the first publicly available nutrition benchmark using 11,857 human-verified meal descriptions, annotated with macro-nutrient labels, to evaluate the performance of twelve leading Large Language Models (LLMs) in carbohydrate estimation. The study demonstrates that LLMs offer comparable yet faster nutrition estimates than professionals, showing potential to assist in dietary choices and manage health outcomes, particularly for individuals with type 1 diabetes.

On Bits and Bandits: Quantifying the Regret-Information Trade-off
This paper explores the balance between accumulated information and regret in sequential decision-making tasks, using information-theoretic methods to establish both lower and upper bounds on regret. The authors introduce novel Bayesian regret lower bounds linked to accumulated information and apply these bounds to enhance question-answering tasks in large language models, demonstrating their practical value.

On the Completeness of Invariant Geometric Deep Learning Models
This paper investigates the theoretical expressiveness of invariant models in geometric deep learning, particularly under fully-connected conditions. It proves that models like DisGNN, GeoNGNN, and others achieve E(3)-completeness, enhancing the understanding of their capabilities and demonstrating their potential extension to geometric scenarios.

On the Modeling Capabilities of Large Language Models for Sequential Decision Making
This paper investigates the potential of Large Language Models (LLMs) in reinforcement learning (RL) across various interactive domains, focusing on their role in decision-making policies via direct action generation or through reward modeling. The study finds that LLMs perform well in reward modeling without task-specific tuning, particularly when employing AI feedback, while also demonstrating that fine-tuning with synthetic data in unfamiliar environments enhances their effectiveness and mitigates catastrophic forgetting.

On the Optimization Landscape of Low Rank Adaptation Methods for Large Language Models
This paper examines the memory challenges in training Large Language Models and evaluates low-rank adaptation methods like LoRA and GaLore, showing that GaLore achieves faster convergence with reduced memory usage by compressing the gradient matrix. The novel method GaRare is introduced, which enhances GaLore using gradient random projection, further improving computational efficiency and performance in both pre-training and fine-tuning tasks.

OPTAMI: Global Superlinear Convergence of High-order Methods
This paper presents advancements in high-order optimization methods, evidencing their practical performance with global superlinear convergence for $\mu$-strongly star-convex functions, which encompass certain non-convex functions. A new practical variant of the Nesterov Accelerated Tensor method, NATA, is introduced, alongside OPTAMI, an open-source computational library for high-order methods implemented in PyTorch, enhancing the applicability and comparison of these methods over traditional first-order approaches.

Optimal Protocols for Continual Learning via Statistical Physics and Control Theory
This paper addresses the challenge of catastrophic forgetting in artificial neural networks during sequential multi-task learning by developing a theoretical framework that combines training dynamics equations with optimal control methods. The authors propose and validate new task-selection protocols that minimize forgetting while maximizing performance, offering interpretable strategies to manage forgetting influenced by task similarity, and confirm their findings through experiments on real-world data.

Optimizing importance weighting in the presence of sub-population shifts
This paper addresses the challenge of distribution shifts between training and test data by proposing a bi-level optimization framework that optimizes both weights and model parameters simultaneously. By focusing on the bias-variance trade-off, the approach enhances existing importance weighting techniques, particularly in the context of last-layer retraining for deep neural networks, leading to improved generalization performance.

Oracle efficient truncated statistics
This paper addresses the challenge of learning from truncated samples, where observations are limited to examples within a certain survival set. By developing a new learning method with polynomial runtime and query complexity in terms of the inverse survival mass, the study significantly advances prior methods and relies on a general-purpose optimization algorithm with minimal assumptions.

Out-of-distribution Generalization for Total Variation based Invariant Risk Minimization
This paper introduces OOD-TV-IRM, a Lagrangian multiplier model extending IRM-TV, designed to enhance out-of-distribution (OOD) generalization by implementing a primal-dual optimization approach. Through a novel convergent primal-dual algorithm, OOD-TV-IRM seeks a semi-Nash equilibrium between training loss and OOD generalization, demonstrating superior performance to IRM-TV across various scenarios.

PADRe: A Unifying Polynomial Attention Drop-in Replacement for Efficient Vision Transformer
PADRe is introduced as a novel framework to replace the traditional self-attention mechanism in transformer models, employing polynomial functions to enhance computational efficiency and maintain accuracy. Demonstrating its versatility, PADRe improves speed (11x~43x faster) in various computer vision tasks without sacrificing performance, positioning it as a robust alternative to conventional self-attention mechanisms.

Partially Observed Trajectory Inference using Optimal Transport and a Dynamics Prior
This paper extends trajectory inference by incorporating latent stochastic differential equations within observable state space models, leveraging partial observations to infer latent space trajectories under specified dynamics models. The proposed PO-MFL algorithm effectively solves this problem, with theoretical guarantees and experimental validation showing robustness and significant performance improvements over baseline approaches.

Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process
This paper examines the underlying mechanisms by which language models address mathematical reasoning problems, utilizing a set of controlled experiments to explore whether models truly develop reasoning skills or merely memorize templates. The study provides new insights into the models' mental processes, their similarities and differences compared to human reasoning, and the factors influencing their ability to solve GSM8K-level math questions.

Physiome-ODE: A Benchmark for Irregularly Sampled Multivariate Time-Series Forecasting Based on Biological ODEs
This paper introduces Physiome-ODE, a comprehensive benchmark of 50 irregularly sampled multivariate time series (IMTS) datasets derived from real-world ordinary differential equations in biology, significantly larger and more diverse than the existing four datasets used for model evaluation. The benchmark reveals different results that demonstrate the strengths of ODE-based models in IMTS forecasting and aims to revitalize research on ODE-focused approaches by providing a more challenging and meaningful basis for evaluating forecasting models.

Poison-splat: Computation Cost Attack on 3D Gaussian Splatting
This paper uncovers a critical security vulnerability in 3D Gaussian splatting (3DGS) where the computation cost of training can be manipulated by poisoning input data. The authors introduce an attack called Poison-splat, which drastically increases the computation memory and time for 3DGS, potentially leading to Denial-of-Service (DoS) by consuming all allocable memory, highlighting a need for attention to 3DGS's overlooked vulnerability.

Policy Gradient with Kernel Quadrature
The paper addresses the bottleneck in reinforcement learning caused by the need to evaluate rewards of numerous episodes, proposing a method to select a small, representative subset for efficient computation. By employing a Gaussian process to model returns and using an episodic kernel quadrature method, the study demonstrates improved policy gradient iterations and provides theoretical and practical results, particularly in MuJoCo tasks.

PortLLM: Personalizing Evolving Large Language Models with Training-Free and Portable Model Patches
The paper introduces PORTLLM, a training-free framework designed to facilitate continual personalization of large language models (LLMs) by creating lightweight model update patches, thereby reducing the computing resources required for frequent fine-tuning. Through extensive experiments, PORTLLM demonstrates comparable performance to established fine-tuning methods like LoRA, with significant reductions in GPU memory usage, and offers new theoretical insights into LLM personalization.

Precedence-Constrained Winter Value for Effective Graph Data Valuation
This paper introduces the Precedence-Constrained Winter (PC-Winter) Value, a novel solution for valuing graph-structured data, addressing the limitations of existing methods that struggle with the complexities of graph dependencies and computational costs. The authors propose strategies for efficient approximation of PC-Winter, with extensive experiments demonstrating its effectiveness across various datasets and tasks.

Preserving Diversity in Supervised Fine-Tuning of Large Language Models
This paper addresses the limitations of Cross Entropy (CE) in supervised fine-tuning of large language models by introducing a game-theoretic framework that enhances output diversity. The proposed GEM algorithm maintains performance while increasing diversity, leading to improved test-time compute scaling and mitigating model forgetting in chat and code generation tasks.

Privacy Auditing of Large Language Models
This paper presents a novel approach to generating effective canaries for privacy auditing in large language models, leading to stronger membership inference attacks and tighter lower bounds on privacy leakage. Through extensive experimentation, the authors demonstrate that their method significantly outperforms previous approaches, achieving high true positive rates with low false positive rates, and providing substantial advances in privacy auditing without requiring shadow models or access throughout training iterations.

Projection Head is Secretly an Information Bottleneck
This paper provides a theoretical understanding of the projection head in contrastive learning, suggesting it functions as an information bottleneck by filtering irrelevant information to enhance downstream task performance. By introducing training and structural regularizations to projectors, empirical results show consistent performance improvements across datasets like CIFAR-10, CIFAR-100, and ImageNet-100, potentially guiding more advanced designs in the field.

Protecting against simultaneous data poisoning attacks
Existing backdoor defense methods are ineffective against multiple simultaneous attacks in machine learning models trained on large datasets, making them unrealistic for practical use. The proposed BaDLoss method effectively detects and mitigates such attacks with minimal impact on clean accuracy, significantly reducing average attack success rates across several benchmark datasets.

PvNeXt: Rethinking Network Design and Temporal Motion for Point Cloud Video Recognition
In this work, we introduce PvNeXt, a novel framework for efficient point cloud video recognition using a personalized one-shot query operation. By integrating the Motion Imitator and Single-Step Motion Encoder modules, PvNeXt captures temporal dynamics without redundant computations, and experiments across multiple benchmarks confirm its effectiveness.

Query-based Knowledge Transfer for Heterogeneous Learning Environments
The paper introduces a novel framework called Query-based Knowledge Transfer (QKT) designed to improve decentralized collaborative learning by allowing clients to acquire tailored knowledge without direct data exchange, addressing the shortcomings of federated learning and other existing methods under data heterogeneity and privacy constraints. Experiments demonstrate that QKT significantly outperforms current collaborative learning techniques, achieving an average improvement of 20.91% in single-class queries and 14.32% in multi-class queries, while effectively balancing new and existing knowledge acquisition.

RainbowPO: A Unified Framework for Combining Improvements in Preference Optimization
RainbowPO is a unified framework that categorizes and integrates key components of various preference optimization algorithms in the Direct Preference Optimization (DPO) family into a single cohesive objective, thereby enhancing individual component performance. Through extensive experiments, RainbowPO is shown to outperform existing DPO variants, and the paper provides insights to aid researchers and practitioners in developing and implementing new DPO methods.

ReAttention: Training-Free Infinite Context with Finite Attention Scope
The paper introduces ReAttention, a training-free method that enables large language models (LLMs) to handle infinitely long contexts effectively by applying position-agnostic top-$k$ attention before traditional self-attention. Validated on multiple benchmarks, ReAttention allows mainstream LLMs to support substantially longer context lengths without additional training, demonstrating efficiency improvements without increasing computational overhead.

Reexamining the Aleatoric and Epistemic Uncertainty Dichotomy
This paper challenges the traditional distinction between aleatoric and epistemic uncertainty, highlighting contradictions and overlaps in their definitions. It proposes viewing these uncertainties as a spectrum, offering a more nuanced understanding that aids in solving practical tasks with large language models.

Refine-by-Align: Reference-Guided Artifacts Refinement through Semantic Alignment
Personalized image generation often struggles with localized artifacts affecting fidelity and identity details. Introducing a novel task called reference-guided artifacts refinement, the Refine-by-Align model uses a two-stage diffusion-based framework to automatically enhance image fidelity by aligning and refining artifacts, showing significant improvements across various applications like customization and virtual try-on.

Repetition Improves Language Model Embeddings
This paper introduces "echo embeddings," a method for converting autoregressive language models into high-quality text embedding models without architectural changes or fine-tuning by repeating the input for improved zero-shot performance. The approach rivals or surpasses bidirectionally-converted language models in both zero-shot and fine-tuning scenarios, suggesting a unified architecture for NLP tasks by avoiding the need for bidirectional attention.

Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation
This paper highlights the challenge of drug-target binding affinity prediction models overfitting to high-similarity samples in conventional test set splits, resulting in unreliable performance on diverse datasets. To overcome this, a novel framework for similarity-aware evaluation is introduced, employing a new split methodology through optimization problems, and demonstrates improved model development across multiple datasets and methods.

Robust Feature Learning for Multi-Index Models in High Dimensions
This paper initiates the exploration of adversarially robust feature learning with neural networks by proving that the hidden directions of a multi-index model provide a Bayes optimal low-dimensional projection for robustness against $\ell_2$-bounded adversarial attacks. The findings suggest that robust learning can be achieved by standard feature learning followed by robust tuning of a linear readout layer, with the extra samples needed for robustness being dimension-independent.

Robust Gymnasium: A Unified Modular Benchmark for Robust Reinforcement Learning
This paper introduces Robust-Gymnasium, a comprehensive modular benchmark designed for robust reinforcement learning (RL), addressing the lack of standardized testing for RL policies' resilience to uncertainty. By supporting diverse disruptions across key RL components and offering over sixty task environments, it provides an open-source platform to evaluate existing algorithms and highlights significant deficiencies, promoting further algorithmic development.

Robustness of Quantum Algorithms for Nonconvex Optimization
This paper studies quantum algorithms for finding an $\epsilon$-approximate second-order stationary point of nonconvex functions with noisy oracles, achieving significant query complexity improvements. It provides a systematic analysis of quantum methods compared to classical counterparts, including a proposed stochastic gradient descent algorithm, and establishes both upper and lower bounds on the computational efficiency under varying noise conditions.

RRM:  Robust Reward Model Training Mitigates Reward Hacking
The paper highlights a limitation in traditional reward model training for aligning large language models with human preferences, where models struggle to distinguish between contextual signals and irrelevant artifacts. To address this, the authors introduce a causal framework and a data augmentation technique to filter out these artifacts, resulting in a more robust reward model that significantly improves performance on benchmarks like Reward-Bench and enhances policy alignment in MT-Bench and AlpacaEval-2.

SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation
This paper addresses concerns in the fine-tuning of large language models (LLMs) using methods like LoRA, which can potentially compromise safety alignments in these models. The authors introduce SaLoRA, a novel method that preserves safety alignment by employing a fixed safety module and task-specific initialization, demonstrating superior performance over other adapters-based approaches in various fine-tuning tasks.

Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models
Multimodal Large Language Models (MLLMs) show potential across various tasks but face trustworthiness issues due to limitations in traditional prediction methods. This paper introduces *TRON*, a two-step framework enhancing MLLMs by generating efficient and stable prediction sets for both open and closed-ended scenarios, maintaining desired error rates and utilizing a novel metric to address semantic redundancy.

Scalable Benchmarking and Robust Learning for Noise-Free Ego-Motion and 3D Reconstruction from Noisy Video
The paper addresses the limitations of existing ego-motion estimation and 3D reconstruction models, which struggle with real-world noise, by introducing a scalable noisy data synthesis pipeline and a benchmark called Robust-Ego3D to evaluate performance degradation in such conditions. It presents a novel method, Correspondence-guided Gaussian Splatting (CorrGS), that significantly improves upon state-of-the-art methods by enhancing geometric alignment and appearance restoration in noisy environments, particularly under rapid motion and dynamic illumination, with plans to release the code and benchmark for further advancements in robust 3D vision.

Scalable Universal T-Cell Receptor Embeddings from Adaptive Immune Repertoires
This paper utilizes the GloVe algorithm to create low-dimensional vector representations of T cell receptors (TCRs) from high-throughput TCR repertoire sequencing, addressing data heterogeneity and sparseness. By generating subject-level embeddings, the study demonstrates that these representations effectively reflect both immune genetics and pathogenic exposure history, with TCRs targeting the same pathogen exhibiting high cosine similarity, while implementing random projection theory to enhance computational efficiency.

Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport
Diffusion-based image generators face challenges in preserving image details and intrinsic properties during illumination editing. The proposed IC-Light method introduces consistent light transport during training, enabling scalable and precise illumination manipulation across diverse data types while maintaining the integrity of intrinsic image properties, thus reducing uncertainties and artifacts.

Scaling Speech-Text Pre-training with Synthetic Interleaved Data
This paper introduces a novel method for scaling speech-text pre-training by utilizing large-scale synthetic interleaved data from text corpora, eliminating reliance on parallel speech-text datasets. The approach achieves state-of-the-art results in speech language modeling and spoken question answering, significantly improving performance over previous methods and enabling the development of an end-to-end spoken chatbot with competitive performance in both conversational abilities and speech quality.

Scaling Stick-Breaking Attention: An Efficient Implementation and In-depth Study
This paper explores an alternative attention mechanism using the stick-breaking process, which addresses challenges in length generalization faced by traditional self-attention mechanisms relying on softmax. By replacing softmax with stick-breaking attention, the authors demonstrate improved performance in length generalization and competitive results on downstream tasks, particularly highlighting its effectiveness in models with extended context windows.

Selective induction Heads: How Transformers Select Causal Structures in Context
This paper introduces a novel synthetic framework that allows for the theoretical analysis of transformers' ability to handle dynamic causal structures, unlike existing models that rely on fixed causal structures. By using interleaved Markov chains with different lags, the study demonstrates how attention-only transformers can form Selective Induction Heads to predict tokens by selecting the correct causal structure, advancing understanding of transformer interpretability and functionality.

Self-Boosting Large Language Models with  Synthetic Preference Data
SynPO, a self-boosting paradigm, utilizes synthetic preference data to align Large Language Models (LLMs) by autonomously training them to learn generative rewards for their outputs, eliminating the need for extensive human annotation. This approach significantly enhances models like Llama3-8B and Mistral-7B, improving their instruction-following abilities and general performance as evidenced by substantial win rate increases on benchmarks such as AlpacaEval 2.0 and score improvements on the Open LLM leaderboard.

Semantix: An Energy-guided Sampler for Semantic Style Transfer
This paper introduces *Semantic Style Transfer*, a novel task that integrates style and appearance transfer from a reference image to target visual content based on semantic correspondence, addressing limitations in existing methods. The proposed training-free method, *Semantix*, utilizes an energy-guided sampler to facilitate this process across both images and videos, demonstrating superior performance over current state-of-the-art solutions.

SeRA: Self-Reviewing and Alignment of LLMs using Implicit Reward Margins
Direct alignment algorithms (DAAs) have gained popularity for their simplicity and efficiency, but they often suffer from spurious correlations and overfitting to off-policy feedback. The proposed Self-Reviewing and Alignment (SeRA) method addresses these issues by using sample selection and preference bootstrapping, improving training effectiveness and generality for large language models with diverse offline preference datasets.

SFS: Smarter Code Space Search improves LLM Inference Scaling
This paper introduces SCATTERED FOREST SEARCH (SFS), a novel approach to code generation framed as a black-box optimization problem, which enhances solution diversity and avoids local optima. Theoretical analysis and extensive experiments on benchmarks like HumanEval and Leetcode demonstrate significant performance improvements and increased efficiency, outperforming state-of-the-art methods and reducing solution search iterations.

Should VLMs be Pre-trained with Image Data?
This paper investigates the performance impact of incorporating image data at different stages in the pre-training of large language models (LLMs) for vision-language tasks. The study finds that introducing visual tokens 80% into the pre-training process yields a 2% average improvement in task performance over fully pre-trained models, effectively balancing image and text data capabilities.

Show-o: One Single Transformer to Unify Multimodal Understanding and Generation
Show-o is a unified transformer model that integrates autoregressive and diffusion approaches to handle a variety of multimodal tasks, such as visual question-answering and text-to-image generation, across different modalities. It achieves comparable or superior results on various benchmarks compared to existing models, showcasing its potential as a next-generation foundation model for both understanding and generation tasks.

SigDiffusions: Score-Based Diffusion Models for Time Series via Log-Signature Embeddings
SigDiffusion is introduced as a novel diffusion model that utilizes log-signature embeddings to generate long multivariate time series, addressing the challenge of adapting score-based diffusion models for such tasks. By leveraging new closed-form inversion formulae, SigDiffusion achieves high-quality time series generation that is competitive with existing state-of-the-art methods across multiple datasets.

Simple yet Effective Incomplete Multi-view Clustering: Similarity-level Imputation and Intra-view Hybrid-group Prototype Construction
The paper introduces a novel approach named SIIHPC to address limitations in incomplete multi-view clustering by reconstructing bipartite similarities and loosening traditional non-negative constraints to better characterize similarities across views. By incorporating hybrid prototype quantities for each view and optimizing the solution within a unified learning framework, SIIHPC effectively improves clustering performance, as confirmed by experimental results.

Simplifying, Stabilizing and Scaling Continuous-time Consistency Models
This paper presents a simplified theoretical framework to address training instability in continuous-time consistency models (CMs), a class of diffusion-based generative models. By enhancing diffusion process parameterization, network architecture, and training objectives, the authors successfully train large-scale CMs, achieving competitive FID scores with only two sampling steps across several datasets, significantly narrowing the performance gap with leading diffusion models.

Singular Subspace Perturbation Bounds via Rectangular Random Matrix Diffusions
This paper presents new bounds on the Frobenius distance between subspaces spanned by the top-$k$ right singular vectors of a matrix $A$ and its perturbation $A+G$, where $G$ is a Gaussian random matrix. By modeling the perturbation as a diffusion process, the authors improve previous bounds, offering insights applicable in statistics and differential privacy, and utilize stochastic calculus to analyze the evolution of singular vector subspaces.

SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks
This paper introduces "SpikeGPT," a generative language model utilizing Spiking Neural Networks (SNNs) with event-driven activations to achieve energy-efficient computation. By adapting the transformer block to reduce complexity and training models with 46M and 216M parameters, SpikeGPT shows competitive performance with significantly fewer operations and demonstrates its potential for large-scale language generation tasks.

SplatFormer: Point Transformer for Robust 3D Gaussian Splatting
3D Gaussian Splatting (3DGS) struggles with rendering quality under out-of-distribution (OOD) camera angles, limiting its application in immersive experiences. The paper introduces SplatFormer, a point transformer model that refines 3DGS sets to enhance rendering quality in extreme novel views, achieving state-of-the-art performance by addressing artifacts in OOD scenarios.

STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs
This paper introduces STBLLM, the first structural binarization method for compressing large language models (LLMs) to less than 1-bit precision, enhancing computational efficiency for resource-constrained devices. By employing a novel Standardized Importance (SI) metric and an N:M sparsity technique, the method compresses weights with variable layer-wise sparsity, outperforming existing approaches in memory efficiency and maintaining performance with a perplexity of 11.07 at 0.55 bits per weight.

Stealthy Shield Defense: A Conditional Mutual Information-Based Approach against Black-Box Model Inversion Attacks
Model inversion attacks (MIAs) pose a threat to privacy by reconstructing private training data from publicly accessible models, especially in black-box scenarios where attackers lack direct model access. To tackle this, we introduce Stealthy Shield Defense (SSD), a post-processing algorithm that modifies model outputs to minimize conditional mutual information, thereby enhancing resistance to MIAs while maintaining utility, outperforming existing defenses without requiring model retraining.

Systematic Relational Reasoning With Epistemic Graph Neural Networks
The paper introduces Epistemic Graph Neural Networks (EpiGNNs), a scalable and parameter-efficient architecture designed to enhance reasoning in relational domains by incorporating epistemic inductive biases. EpiGNNs demonstrate state-of-the-art performance in tasks requiring systematic reasoning, outperforming existing methods on new benchmarks that demand aggregating information from multiple paths, and rivaling specialized approaches in inductive knowledge graph completion.

T2V-Turbo-v2: Enhancing Video Model Post-Training through Data, Reward, and Conditional Guidance Design
This paper presents T2V-Turbo-v2, a method that enhances text-to-video models by distilling a consistency model from a pretrained system using various supervision signals, including high-quality data, reward model feedback, and conditional guidance. The approach demonstrates improved visual quality and text-video alignment, setting a new state-of-the-art on VBench with a Total score of 85.13, outperforming systems like Gen-3 and Kling.

Tackling Data Corruption in Offline Reinforcement Learning via Sequence Modeling
This study addresses the challenge of noise and errors in real-world datasets for offline reinforcement learning (RL) by demonstrating that vanilla sequence modeling methods like Decision Transformer are more robust to data corruption than predominant offline RL methods. It introduces **R**obust **D**ecision **T**ransformer (**RDT**), which incorporates embedding dropout, Gaussian weighted learning, and iterative data correction to significantly enhance performance under various data corruption scenarios, promoting reliable offline RL application in real-world scenarios.

TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction
TEASER is a novel approach for 3D facial reconstruction from a single image that significantly improves the capture of fine-grained expressions, addressing challenges such as irregular mouth shapes and asymmetrical facial movements. By utilizing a multi-scale tokenizer and a pose-dependent landmark loss, TEASER enhances geometric and expression reconstruction, achieving state-of-the-art performance and providing interpretable tokens for applications like facial video driving and identity swapping.

Temporal Flexibility in Spiking Neural Networks: Towards Generalization Across Time Steps and Deployment Friendliness
This paper addresses the "temporal inflexibility" issue in Spiking Neural Networks (SNNs) by introducing Mixed Time-step Training (MTT), a novel method to enhance their adaptability across various temporal structures. The proposed MTT method enables SNNs to be effectively deployed on both time-stepped and fully event-driven platforms, achieving significant improvements in temporal flexibility and performance, notably surpassing standard methods on datasets such as CIFAR10-DVS.

TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes
TetSphere Splatting is a novel geometry representation method using volumetric tetrahedral meshes for high-quality 3D shape modeling, addressing issues like irregular triangles and non-manifoldness. The method shows superior mesh quality and competitive accuracy in reconstruction tasks while also integrating effectively into generative modeling for applications like image-to-3D and text-to-3D generation.

The Effectiveness of Curvature-Based Rewiring and the Role of Hyperparameters in GNNs Revisited
Graph Neural Networks (GNNs) often suffer from information loss due to oversquashing at bottlenecks during message passing, which some try to mitigate with graph rewiring techniques using discrete graph curvature measures. This study reveals that these rewiring techniques may not align with theoretical expectations in real-world datasets, showing that apparent performance improvements are more likely due to extensive hyperparameter tuning rather than consistent gains in information propagation effectiveness.

The Unreasonable Ineffectiveness of the Deeper Layers
This study investigates how knowledge is stored in the weights of large language models (LLMs) by using layer pruning, which identifies unnecessary layers that do not affect model performance on question-answering tasks. The findings suggest that LLMs are robust to the removal of up to half their layers, indicating that either pretraining methods may not utilize deeper layers efficiently or shallow layers are integral for storing knowledge, with experiments conducted using parameter-efficient finetuning on a single GPU.

TOMATO: Assessing Visual Temporal Reasoning Capabilities in Multimodal Foundation Models
Existing benchmarks may overestimate the visual temporal reasoning abilities of Multimodal Foundation Models (MFMs), as many tasks can be resolved without comprehensive temporal understanding. To address this, the paper introduces TOMATO, a benchmark designed to evaluate MFMs' temporal reasoning in video understanding, revealing a significant performance gap between humans and models and highlighting the need for improved AI systems that can effectively interpret dynamic sequences.

Towards a Complete Logical Framework for GNN Expressiveness
This paper introduces a framework for identifying the equivalent logical formulas for arbitrary Graph Neural Networks (GNNs), aiming to unify and enhance understanding within the field by relating GNN expressiveness to logic. It provides case studies of prominent GNN architectures and offers a method for determining their homomorphism expressivity, addressing open challenges and inspiring future research.

Towards Domain Adaptive Neural Contextual Bandits
This paper introduces a novel domain adaptation method for contextual bandits, enabling adaptation from a source domain to a target domain despite distribution shifts, a challenge not previously well-addressed. The proposed algorithm achieves a sub-linear regret bound and demonstrates superior performance over existing contextual bandit algorithms, with practical applications shown through empirical results on real-world datasets.

Towards Explaining the Power of Constant-depth Graph Neural Networks for Structured Linear Programming
Graph neural networks (GNNs) have demonstrated strong performance in predicting optimal solutions to linear programming (LP) problems, despite their shallow architectures, which contrasts the expectation for deep architectures based on previous theoretical results. This paper reveals that the effectiveness of shallow GNNs for LPs can be attributed to their similarity with distributed algorithms, showing that these GNNs can simulate distributed LP algorithms and effectively solve sparse binary LPs in average-case scenarios, irrespective of problem size.

Towards Optimal Multi-draft Speculative Decoding
This paper addresses the efficiency bottleneck in autoregressive sampling for large language models by exploring Multi-Draft Speculative Decoding (MDSD), where a small draft model generates multiple token drafts and the target LLM verifies them in parallel. It identifies the optimal acceptance rate as a dual problem of optimal transport, demonstrating that draft sampling methods significantly influence efficiency, with sampling without replacement being more effective than with replacement, and highlights the gap between existing verification algorithms and the theoretical efficiency upper bound.

Towards Realistic UAV Vision-Language Navigation: Platform, Benchmark, and Methodology
This paper addresses the challenges of UAV-based vision-language navigation (VLN) by introducing the OpenUAV platform, which allows for realistic UAV trajectory simulation and supports various environments, flight controls, and algorithms. Additionally, the study presents a comprehensive UAV VLN dataset with approximately 12k trajectories and an assistant-guided UAV object search benchmark called UAV-Need-Help, along with a UAV navigation LLM that significantly outperforms baseline models, highlighting the complexity of UAV VLN tasks compared to human performance.

Towards Understanding the Universality of Transformers for Next-Token Prediction
This paper investigates the approximation ability of Causal Transformers in predicting the next token in an autoregressive sequence, focusing on cases where the next token is determined by a context-dependent function. The authors introduce a causal kernel descent method and demonstrate its capacity to estimate the next token through self-attention, validated by both theoretical analysis and experimental results.

Towards Unified Human Motion-Language Understanding via Sparse Interpretable Characterization
This paper introduces a novel motion-language representation paradigm that enhances the interpretability and semantic richness of human motion representations by creating a universal motion-language space where motion and text features are concretely lexicalized. The proposed multi-phase strategy, including various innovative modeling and pretraining techniques, demonstrates state-of-the-art performance across multiple datasets, improving cross-modal coherence and understanding in tasks involving human motion.

Training Language Models on Synthetic Edit Sequences Improves Code Synthesis
This paper introduces LintSeq, a synthetic data generation algorithm that creates sequences of synthetic edits by refactoring programs with a linter to address the scarcity of sequential edit data for code synthesis. By fine-tuning language models on datasets processed with LintSeq, the study demonstrates that these models can match or outperform existing baselines in code synthesis tasks, showcasing improved scaling and performance compared to models like CodeT5+, AlphaCode, and Codex.

Training LLMs over Neurally Compressed Text
This paper investigates the feasibility of training large language models on highly compressed text using a novel compression technique called Equal-Info Windows, which segments text into blocks of equal bit length to overcome the challenge of opacity in strongly compressed outputs. While this approach may yield worse perplexity compared to subword tokenizers, it significantly reduces sequence lengths and latency, showing superior performance on perplexity and inference speed benchmarks compared to byte-level baselines, and offers insights for enhancing high-compression tokenizers.

Transformers Can Learn Temporal Difference Methods for In-Context Reinforcement Learning
This paper investigates the phenomenon of in-context reinforcement learning (ICRL), where pretrained RL agents solve new tasks without updating their parameters, suggesting that their neural networks implement RL algorithms during the forward pass. The authors provide empirical and theoretical evidence demonstrating that transformers trained for policy evaluation tasks can inherently learn to perform temporal difference learning in their forward pass.

Tuning-Free Bilevel Optimization: New Algorithms and Convergence Analysis
This paper introduces two novel tuning-free bilevel optimization algorithms, D-TFBO and S-TFBO, which adaptively adjust stepsizes without prior knowledge of problem parameters. Through a comprehensive convergence analysis, the algorithms demonstrate competitive performance and robustness compared to existing methods, making them the first to eliminate stepsize tuning while maintaining theoretical guarantees.

Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models
Contrastive vision-language models (VLMs), like CLIP, excel in tasks such as zero-shot object recognition but struggle with attribute recognition due to issues like the modality gap and object bias. This paper investigates these phenomena, finding that an information imbalance between images and captions drives both the modality gap and object bias, and suggests that addressing the modality gap can enhance performance.

Uncovering Gaps in How Humans and LLMs Interpret Subjective Language
This paper presents TED (thesaurus error detector), a method that identifies misalignments between the operational semantics of language models (LLMs) and human expectations by constructing a thesaurus to compare LLM understanding of subjective phrases with human references. The study reveals surprising instances of misalignment, such as LLMs producing undesirable outputs when instructed with certain phrases, highlighting the importance of examining abstract concept relationships to uncover unexpected LLM behavior.

Uni-Sign: Toward Unified Sign Language Understanding at Scale
Uni-Sign is a unified pre-training framework designed to bridge the gap between pre-training and fine-tuning in sign language understanding (SLU) tasks, utilizing a large-scale generative pre-training strategy along with a novel fine-tuning paradigm. By introducing CSL-News, a sizeable Chinese Sign Language dataset, and incorporating a prior-guided fusion module to optimize pose and RGB data fusion, Uni-Sign demonstrates state-of-the-art performance across multiple SLU benchmarks, ensuring effective knowledge transfer and improved computational efficiency.

Unlocking Global Optimality in Bilevel Optimization: A Pilot Study
This paper addresses the open problem of achieving global optimality in bilevel optimization, which is crucial for reliable and efficient AI applications. By presenting two sufficient conditions for global convergence and providing algorithm-dependent proofs, the study substantiates these conditions in scenarios such as representation learning and data hypercleaning, with experiments confirming convergence to the global minimum.

URLOST: Unsupervised Representation Learning without Stationarity or Topology
The paper introduces URLOST, a novel unsupervised learning framework that effectively learns from high-dimensional data without requiring prior knowledge of stationarity and topology. Demonstrating superior performance compared to existing methods like SimCLR and MAE, URLOST sets a new benchmark by successfully generalizing across different data modalities, including biological, neural, and genetic data, marking significant progress towards more adaptable unsupervised learning techniques.

Utilitarian Algorithm Configuration for Infinite Parameter Spaces
This paper introduces a new procedure called COUP (Continuous, Optimistic Utilitarian Procrastination), which efficiently searches infinite parameter spaces to quickly identify optimal configurations. COUP retains the theoretical benefits of previous utilitarian configuration methods in finite parameter spaces and demonstrates significant speed improvements in both theoretical and experimental evaluations.

VAE-Var: Variational Autoencoder-Enhanced Variational Methods for Data Assimilation in Meteorology
This paper introduces VAE-Var, a novel neural network-based data assimilation algorithm designed to improve the accuracy of weather forecasting by capturing non-Gaussian characteristics of the background distribution and efficiently assimilating real-world observational data. VAE-Var outperforms existing algorithms in sparse observational contexts by reducing dependence on expert knowledge for background distribution modeling and maintaining traditional variational assimilation frameworks for handling irregular observations.

Variance-Reducing Couplings for Random Features
This paper addresses the variance reduction problem in random features (RFs) used to scale up kernel methods in machine learning by applying optimal transport to improve their convergence on both Euclidean and discrete input spaces. The study provides theoretical guarantees and demonstrates potential downstream gains, including insights into optimizing other coupling properties for attention estimation in efficient transformers.

Vertical Federated Learning with Missing Features During Training and Inference
Vertical federated learning often assumes all feature partitions are available during training and inference, which is impractical as some clients may not observe their partition or may leave the federation. LASER-VFL addresses these challenges by efficiently handling arbitrary sets of partitions through model parameter sharing and task-sampling, achieving improved convergence rates and performance, even with missing features.

VideoWebArena:  Evaluating Long Context Multimodal Agents with Video Understanding Web Tasks
The paper introduces VideoWebArena (VideoWA), a new benchmark designed to evaluate the ability of long-context multimodal agents to understand and process videos, focusing on skill and factual retention tasks. Despite the introduction of this benchmark with over two thousand tasks, current models significantly lag behind human performance, emphasizing the need for further development in long-context video understanding agents.

ViSAGe: Video-to-Spatial Audio Generation
This paper presents a novel approach to generating first-order ambisonics from silent videos, introducing the YT-Ambigen dataset of 102K video clips with corresponding spatial audio. The proposed ViSAGe framework, using CLIP visual features and autoregressive neural audio codec modeling, significantly outperforms traditional two-stage methods by producing high-quality, viewpoint-adaptive spatial audio directly from video inputs.

Visually Consistent Hierarchical Image Classification
This paper addresses the challenges of hierarchical classification, highlighting the need for accurate and consistent predictions across different levels of a taxonomy tree. The proposed novel model integrates consistent hierarchical visual segmentation and a tree-path KL divergence loss to improve semantic consistency, outperforming existing methods such as zero-shot CLIP on standard benchmarks.

VLMaterial: Procedural Material Generation with Large Vision-Language Models
This paper presents a novel approach to creating procedural materials from input images by converting them into Python programs and fine-tuning a large pre-trained vision-language model. The method, supported by an open-source procedural material dataset and program-level augmentation using another large language model, demonstrates superior performance over previous techniques in both synthetic and real-world scenarios.

VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical Reasoning
Multimodal Large Language Models (MLLMs) excel in integrating visual and textual information but struggle with abstract relational reasoning across multiple images. VOILA, a new benchmark, challenges these models by requiring them to generate images for analogical reasoning tasks, revealing their limitations in inter-image relationships, while highlighting that a multi-step prompting strategy can improve performance albeit still lagging behind human capabilities.

Ward: Provable RAG Dataset Inference via LLM Watermarks
This paper addresses the issue of unauthorized usage of external data by large language models (LLMs) by introducing RAG Dataset Inference (RAG-DI) as a formalized problem. The authors propose a novel dataset for realistic benchmarking, present baseline methods, and introduce Ward, a method using LLM watermarks that offers statistical guarantees, outperforming existing baselines in accuracy, query efficiency, and robustness, thus laying the groundwork for future research in RAG-DI.

Weakly-Supervised Affordance Grounding Guided by Part-Level Semantic Priors
This paper addresses weakly supervised affordance grounding by training a model to identify affordance regions on objects using images without dense labels, overcoming limitations of traditional class activation maps. The authors introduce a novel supervised training pipeline with pseudo labels, bolstered by label refining, fine-grained feature alignment, and a reasoning module, achieving significant performance improvements over existing methods.

Weak-to-Strong Generalization Through the Data-Centric Lens
This paper introduces the concept of overlap density as a key factor in enabling weak-to-strong generalization in machine learning, addressing the understudied aspects of data that facilitate this phenomenon. The authors present an overlap detection algorithm and a data selection algorithm designed to maximize overlap density, which theoretically and empirically enhance generalization in various settings.

What Does It Mean to Be a Transformer? Insights from a Theoretical Hessian Analysis
This paper offers a theoretical exploration to understand what fundamentally sets the Transformer architecture apart from classical models like MLPs and CNNs by analyzing the (loss) Hessian. By deriving the Hessian for a self-attention layer and characterizing it in terms of data, weight, and attention moment dependencies, the research highlights the distinctive non-linear dependencies in Transformers, elucidating their unique optimization landscape and associated challenges.

What is Wrong with Perplexity for Long-context Language Modeling?
This paper addresses the limitations of using perplexity (PPL) as a metric for evaluating the long-context capabilities of large language models (LLMs), identifying that PPL fails to adequately account for key tokens critical in long-context tasks. To overcome this, the authors introduce LongPPL, which enhances predictive accuracy by focusing on key tokens and correlating strongly with performance across benchmarks, and propose LongCE, a re-weighting strategy that consistently improves LLM performance in long-context scenarios.

xFinder: Large Language Models as Automated Evaluators for Reliable Evaluation
This paper addresses the challenges in evaluating large language models by proposing xFinder, a novel evaluator for answer extraction and matching, which significantly improves judgment accuracy and extraction efficiency over traditional methods. By introducing the Key Answer Finder (KAF) dataset, the study demonstrates that even the smallest xFinder model achieves superior performance with an extraction accuracy of 93.42% and judgment accuracy of 97.61%, outperforming existing frameworks reliant on Regular Expressions.

YouTube-SL-25: A Large-Scale, Open-Domain Multilingual Sign Language Parallel Corpus
This paper introduces YouTube-SL-25, a comprehensive multilingual corpus comprising over 3000 hours of sign language videos from more than 25 sign languages, making it the largest parallel sign language dataset available. By offering baselines for sign-to-text tasks with a multilingual multitask model, the study shows significant benefits in multilingual transfer for both higher- and lower-resource sign languages.

ZooProbe: A Data Engine for Evaluating, Exploring, and Evolving Large-scale Training Data for Multimodal LLMs
This paper introduces ZooProbe, a data engine designed to enhance the training of Multimodal Large Language Models (MLLMs) by employing an evaluating-exploring-evolving (E3) loop to systematically generate new, high-quality training data. By formalizing data expansion as a tree of sampling and growth and utilizing a small-scale model *zoo* for comprehensive dataset evaluations, ZooProbe is shown to significantly improve training efficiency and performance, breaking traditional scaling laws for multimodal instruction fine-tuning.

### Miscellaneous Aspects of Machine Learning->Causality
Recovery of Causal Graph Involving Latent Variables via Homologous Surrogates
This paper addresses the challenge of causal discovery with latent variables by introducing the concept of homologous surrogates, which allows for more flexible parent structures than the traditional pure child approach. By formulating two assumptions about homologous surrogates, the study presents theoretical results and an algorithm that can partially or fully recover causal graphs, and demonstrates the algorithm's effectiveness experimentally, thereby expanding the scope of causal discovery methods.

When Selection Meets Intervention: Additional Complexities in Causal Discovery
This paper introduces a graphical model that addresses selection bias in interventional studies, which is often overlooked and can lead to incorrect causal discovery results. By explicitly considering both the observed and counterfactual worlds, the authors propose a provably sound algorithm that successfully identifies causal relations and selection mechanisms, demonstrating its effectiveness through synthetic and real-world experiments.

### Miscellaneous Aspects of Machine Learning->General Machine Learning Techniques
Euler Characteristic Tools for Topological Data Analysis
This paper explores the use of Euler characteristic techniques in topological data analysis, introducing the Euler characteristic profile as a powerful descriptor that achieves state-of-the-art performance in supervised tasks with minimal computational cost. Additionally, the study presents hybrid transforms that efficiently compress topological signals, demonstrating exceptional results in unsupervised settings, while also offering heuristics, stability results, and asymptotic guarantees for these methods.

### Miscellaneous Aspects of Machine Learning->Representation Learning
Predicate Hierarchies Improve Few-Shot State Classification
PHIER is a model designed for effective state classification in robot tasks by leveraging predicate hierarchies for generalization in few-shot scenarios. By utilizing an object-centric scene encoder, self-supervised losses, and a hyperbolic distance metric, PHIER outperforms existing methods in few-shot and out-of-distribution scenarios, demonstrating robust generalization from simulated to real-world tasks.

### Miscellaneous Aspects of Machine Learning->Transfer, Multitask and Meta-learning
Self-Normalized Resets for Plasticity in Continual Learning
Plasticity Loss in neural networks refers to the decreased adaptability to new tasks over time, which the Self-Normalized Resets (SNR) algorithm addresses by resetting neuron weights when firing rates drop, maintaining superior performance across continual learning tasks. SNR proves to be more robust compared to competitors, and theoretical insights suggest its effectiveness in optimizing learning for single ReLU neurons, even under adversarial initialization.

LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning
LoCA (Location-aware Cosine Adaptation) is introduced as a novel frequency-domain fine-tuning method for large language models, utilizing inverse Discrete Cosine Transform (iDCT) to enhance optimization flexibility by dynamically selecting informative frequency components. Theoretical and empirical analyses show that LoCA surpasses traditional low-rank methods in expressivity and parameter efficiency while maintaining computational feasibility across various language and vision tasks.

LiFT: Learning to Fine-Tune via Bayesian Parameter Efficient Meta Fine-Tuning
This paper introduces Learning-to-Fine-Tune (LiFT), a method for parameter-efficient fine-tuning of pre-trained large models across multiple related tasks by meta-learning shared information for improved prediction on unseen tasks. LiFT employs a hierarchical Bayesian model with task-specific and higher-level latent variables, facilitated by a novel SGLD-Gibbs sampling algorithm and an online EM algorithm, demonstrating effectiveness on multi-task meta-learning benchmarks in NLP and vision.

Deep Linear Probe Generators for Weight Space Learning
This paper introduces Deep Linear Probe Generators (ProbeGen), an effective modification to probing approaches for extracting information about neural networks. ProbeGen improves the performance of probing by using a shared generator module with a deep linear architecture, resulting in significantly better results with far fewer computational resources compared to current state-of-the-art methods.

Single Teacher, Multiple Perspectives: Teacher Knowledge Augmentation for Enhanced Knowledge Distillation
This paper introduces TeKAP, a novel knowledge augmentation technique that generates diverse teacher perspectives by perturbing the knowledge of a single pre-trained teacher at feature and logit levels. By simulating an ensemble of models, TeKAP allows the student model to benefit from diverse perspectives without the need for multiple teachers, significantly reducing training time and computational resources while enhancing the performance of existing knowledge distillation techniques.

### Optimization->Everything Else
Universal Sharpness Dynamics in Neural Network Training: Fixed Point Analysis, Edge of Stability, and Route to Chaos
This paper investigates the dynamics of sharpness, characterized by the top eigenvalue of the Hessian of the loss function, in neural network training, specifically using a simple linear network model. Through detailed analysis of dynamical fixed points and function updates, the study uncovers mechanisms of early sharpness reduction, conditions for edge of stability, and a chaotic behavior pattern at increased learning rates, demonstrating that insights from this simplified model are applicable to real-world neural network training, while also discussing its limitations.

### Optimization->Large Scale, Parallel and Distributed
MAST: model-agnostic sparsified training
This paper introduces a novel optimization problem that incorporates pre-trained models and random sketch operators to enable sparsification of models and gradients, offering an alternative to traditional black-box loss minimization. The research establishes new properties of the objective function, adapts SGD methods for the new formulation, and achieves tighter convergence rates, enhancing both theoretical understanding and practical applications such as Dropout and Sparse training.

### Optimization->Learning for Optimization
Rethinking Light Decoder-based Solvers for Vehicle Routing Problems
This paper analyzes the challenges faced by light decoder-based solvers in generalizing to larger vehicle routing problem (VRP) instances and different VRP variants, particularly due to their reliance on static embeddings. By enhancing decoder capacity through identity mapping and a feed-forward layer, the approach improves out-of-distribution generalization, thus narrowing the performance gap with heavier decoder paradigms.

Towards Foundation Models for Mixed Integer Linear Programming
This paper presents a foundation model training approach for Mixed Integer Linear Programming (MILP), addressing computational tractability and interpretability challenges by training a deep learning model across diverse MILP classes. Introducing MILP-Evolve, an LLM-based framework for generating varied MILP instances, the study demonstrates significant improvements on unseen problems, supporting the model's potential to generalize widely, with the data and code available for public access.

### Optimization->Non-Convex
Nesterov acceleration in benignly non-convex landscapes
This paper extends the theoretical analysis of momentum-based optimization algorithms, traditionally confined to convex settings, to non-convex optimization problems that are common in deep learning, offering comparable guarantees under 'benign' non-convexity conditions. The work provides variations of Nesterov's accelerated gradient (NAG) descent algorithms, including both deterministic and stochastic versions, suggesting that these methods are well-suited for overparametrized deep learning models, at least in local contexts.

Loss Landscape of Shallow ReLU-like Neural Networks: Stationary Points, Saddle Escape, and Network Embedding
This paper examines the loss landscape of one-hidden-layer neural networks with ReLU-like activation functions, identifying directional stationary points that impact the training dynamics. The study refines the understanding of *saddle-to-saddle* training processes by detailing conditions under which stationary points are local minima or are influenced by "escape neurons," and explores how network embedding affects these stationary points.

Solving hidden monotone variational inequalities with surrogate losses
This paper addresses the challenges in solving variational inequality (VI) problems, which differ from scalar loss minimization and often lead to divergence with naive gradient-based methods. By introducing a surrogate-based approach suitable for deep learning, the authors ensure convergence under practical assumptions, unify existing methods, and enhance compatibility with optimizers like ADAM, demonstrating its effectiveness in min-max optimization, projected Bellman error, and presenting a novel, efficient TD(0) variant for deep reinforcement learning.

### Optimization->Optimization and Learning under Uncertainty
On Stochastic Contextual Bandits with Knapsacks in Small Budget Regime
This paper addresses stochastic contextual bandits with knapsack constraints, aiming to maximize cumulative rewards under budget limitations in a small budget regime. It introduces the Adaptive and Universal Primal–Dual algorithm (AUPD), achieving strong regret bounds even without prior information, and presents novel analysis methods validated by large-scale experiments.

### Optimization->Sampling and Optimization
The adaptive complexity of parallelized log-concave sampling
This paper investigates the adaptive complexity of sampling, focusing on the number of sequential rounds needed for sampling using parallel queries in applications involving large data sets. It establishes that almost linear iteration algorithms encounter limitations in achieving very small errors in both unconstrained and box-constrained sampling, particularly under specific conditions like total variation distance, supported by a new analysis of hardness potentials and smoothing techniques.

OCCAM: Towards Cost-Efficient and Accuracy-Aware Classification Inference
This paper introduces OCCAM, a strategic approach to optimize classifier assignment over classification queries to maximize aggregated accuracy within user-defined cost budgets. By employing an unbiased accuracy estimator and solving an integer linear programming problem, OCCAM achieves a 40% reduction in cost with minimal impact on accuracy across various real-world datasets.

Optimization by Parallel Quasi-Quantum Annealing with Gradient-Based Sampling
This study introduces a novel method combining gradient-based updates with Quasi-Quantum Annealing (QQA) to improve scalability in learning-based combinatorial optimization, using continuous relaxation and parallel GPU communication. The proposed approach demonstrates competitive performance and superior speed-quality trade-offs across various benchmarks, outperforming existing methods like iSCO and several learning-based solvers, especially for large-scale problems.

Provable Convergence Bounds for Hybrid Dynamical Sampling and Optimization
This paper addresses the limitations of hybrid analog/digital large-neighborhood local search (LNLS) algorithms by providing non-asymptotic convergence guarantees, using a framework based on block Langevin Diffusion (BLD). The authors offer a theoretical foundation that connects classical sampling theory with novel computing platforms, presenting explicit bounds and a closed-form expression for performance, considering device variations and algorithm hyperparameters.

### Probabilistic Methods->Bayesian Models and Methods
Optimistic Games for Combinatorial Bayesian Optimization with Application to Protein Design
The paper introduces GameOpt, a novel game-theoretical approach to Bayesian optimization (BO) designed for large combinatorial and unstructured spaces that are challenging for traditional BO methods. By treating optimization variables as players in a cooperative game and selecting equilibrium points, GameOpt efficiently navigates complex domains, demonstrated through successful application to the protein design problem, significantly outperforming other methods in discovering active protein variants.

### Probabilistic Methods->Everything Else
Conformalized Survival Analysis for General Right-Censored Data
This paper introduces a framework to quantify predictive uncertainty in survival analysis by constructing reliable lower predictive bounds (LPBs) for patient survival times in the general right-censored setting. Expanding on conformal prediction methods, the developed approach provides distribution-free finite-sample guarantees and demonstrates its validity and informativeness through simulations and real-world datasets.

Wasserstein-Regularized Conformal Prediction under General Distribution Shift
This paper addresses the limitation of conformal prediction in ensuring coverage under distribution shifts and introduces a novel Wasserstein distance-based approach to improving prediction set validity. The proposed WR-CP algorithms effectively reduce coverage gaps while maintaining prediction accuracy and efficiency, demonstrated by experiments showing a reduction in coverage gaps to 3.2% and a 38% smaller prediction set compared to worst-case methods.

Robust Conformal Prediction with a Single Binary Certificate
This paper introduces a robust conformal prediction method that reduces the prediction set size while maintaining coverage guarantees against adversarial inputs, using significantly fewer Monte Carlo samples. By binarizing samples with an adaptive bin and computing only one binary certificate, the approach offers a faster and more efficient solution than existing methods that require certifying each point individually and rely on bounded score functions.

### Probabilistic Methods->Gaussian Processes
Residual Deep Gaussian Processes on Manifolds
The paper introduces deep Gaussian process models on Riemannian manifolds, designed to handle data with complex, nonstationary patterns that simpler models struggle with, such as low-altitude wind patterns. These models enhance prediction quality and uncertainty calibration while being robust to overfitting, and they demonstrate significant improvements in Bayesian optimization problems on manifolds, with potential applications for faster inference on non-manifold data.

### Reinforcement Learning->Batch/Offline
A General Framework for Off-Policy Learning with Partially-Observed Reward
This paper addresses the challenge of off-policy learning (OPL) in contextual bandits when rewards are only partially observed, a situation commonly encountered in areas such as content recommendation, e-commerce, and medical contexts. The authors propose a novel method, Hybrid Policy Optimization for Partially-Observed Reward (HyPeR), which leverages densely observed secondary rewards to improve policy learning and demonstrates superior performance over existing methods through both theoretical analysis and empirical evaluation.

### Reinforcement Learning->Deep RL
Towards Empowerment Gain through Causal Structure Learning in Model-Based Reinforcement Learning
The paper introduces Empowerment through Causal Learning (ECL), a framework that integrates causal dynamics with empowerment-based exploration in Model-Based Reinforcement Learning to enhance controllability and learning efficiency. ECL demonstrates superior performance in causal discovery and sample efficiency across various environments by optimizing causal structures and balancing intrinsic curiosity rewards during task learning.

Horizon Generalization in Reinforcement Learning
This paper explores goal-conditioned reinforcement learning (RL) by focusing on generalization with respect to the horizon, enabling policies trained on nearby goals to successfully reach distant ones, analogous to invariance seen in normalization techniques in machine learning. The authors provide theoretical analysis and experimental evidence showing that horizon generalization and planning invariance can be achieved, suggesting that techniques from other machine learning domains could be applied to enhance this capability in RL.

Towards General-Purpose Model-Free Reinforcement Learning
This paper introduces MR.Q, a model-free deep reinforcement learning algorithm designed to handle diverse domains and problem settings by leveraging model-based representations to approximate linearization of the value function. The algorithm demonstrates competitive performance on various benchmarks with a single set of hyperparameters, showcasing its potential to contribute towards developing general-purpose model-free RL solutions.

Hierarchical World Models as Visual Whole-Body Humanoid Controllers
This paper addresses the complexity of whole-body control in humanoids using a data-driven reinforcement learning approach, eliminating the need for simplifying assumptions or pre-designed rewards and skills. By employing a hierarchical world model, the study achieves highly effective control policies for an 8-task, simulated 56-DoF humanoid, generating human-preferred motions.

### Reinforcement Learning->Everything Else
MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory
This paper introduces MrSteve, a low-level controller with Place Event Memory (PEM) to enhance performance in general-purpose embodied AI, specifically in environments like Minecraft. By incorporating episodic memory to address failures in existing controllers and proposing a memory-augmented exploration and task-solving strategy, MrSteve improves task-solving and exploration efficiency over previous models like Steve-1.

MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models
The paper introduces Multi-Image Augmented Direct Preference Optimization (MIA-DPO), a novel approach for visual preference alignment in Large Vision-Language Models (LVLMs) that effectively manages multi-image inputs. By using attention values to filter out rejected responses without relying on human annotations or additional data, MIA-DPO enhances performance on multi-image tasks, outperforming existing methods and maintaining the model's proficiency in single-image understanding.

Efficient Residual Learning with Mixture-of-Experts for Universal Dexterous Grasping
Universal dexterous grasping faces challenges like designing multi-task learning curriculums and generalizing to unseen objects. ResDex addresses these by combining residual policy learning with a mixture-of-experts framework, achieving a state-of-the-art 88.8% success rate on the DexGraspNet dataset and efficient training on diverse objects.

A Black Swan Hypothesis: The Role of Human Irrationality in AI Safety
This paper challenges the traditional view of black swan events as arising solely from unpredictable environments by introducing the concept of spatial black swan events, which occur due to human misperception in stable settings. By categorizing and mathematically formalizing these events, the paper aims to enhance understanding and lead to algorithms that can correct human perception to prevent such high-risk occurrences.

### Reinforcement Learning->Multi-agent
eQMARL: Entangled Quantum Multi-Agent Reinforcement Learning for Distributed Cooperation over Quantum Channels
This paper introduces entangled QMARL (eQMARL), a novel distributed multi-agent reinforcement learning framework that leverages quantum entanglement for efficient cooperation without the need for explicit local observation sharing, thus significantly reducing classical communication overhead. The proposed eQMARL demonstrates faster convergence to cooperative strategies with superior performance and fewer centralized parameters compared to existing classical and quantum MARL frameworks.

### Reinforcement Learning->Online
Open-World Reinforcement Learning over Long Short-Term Imagination
The paper introduces LS-Imagine, a method to enhance exploration efficiency in visual reinforcement learning by extending the imagination horizon, thereby allowing agents to consider long-term payoffs in high-dimensional, open-world environments. By employing goal-conditioned jumpy state transitions and affordance maps, LS-Imagine improves over state-of-the-art techniques, particularly demonstrated in the MineDojo environment.

Efficient Policy Evaluation with Safety Constraint for Reinforcement Learning
This paper addresses the challenge of reducing variance in on-policy evaluation in reinforcement learning while ensuring safety by proposing an optimal variance-minimizing behavior policy under safety constraints. The proposed method effectively reduces variance, ensures safety compliance, and outperforms previous approaches in both variance reduction and execution safety.

### Reinforcement Learning->Planning
Learning to Search from Demonstration Sequences
The paper introduces Differentiable Tree Search Network (D-TSN), an innovative neural network architecture that constructs search trees from demonstration sequences using gradient descent on a best-first search tree algorithm. D-TSN enhances planning by jointly learning essential submodules and demonstrates effectiveness, particularly in scenarios where the world model with a latent state space is also learned; the authors validate their approach through experiments on various problem scenarios like Game of 24, 2D grid navigation, and Procgen games.

### Social Aspects->Accountability, Transparency and Interpretability
Regretful Decisions under Label Noise
This paper examines the effects of label noise on machine learning models used for decision-making at the individual level, particularly in clinical settings. It introduces a concept of *regret* to measure unforeseen mistakes caused by noisy labels and proposes methods to estimate and mitigate these mistakes, enhancing model reliability and adoption.

Mitigating Memorization in Language Models
This paper investigates various methods to reduce memorization in language models (LMs), focusing on regularizer-based, fine-tuning-based, and machine unlearning-based techniques, including five new unlearning methods. The authors introduce TinyMem, a suite of efficient LMs for rapid development, and find that unlearning-based methods, particularly their novel BalancedSubnet, effectively remove memorized information while maintaining target task performance.

Provably Accurate Shapley Value Estimation via Leverage Score Sampling
This paper introduces Leverage SHAP, an efficient modification of the Kernel SHAP algorithm, to provide accurate Shapley value estimates for feature attribution in machine learning models. By using leverage score sampling, Leverage SHAP achieves provably accurate results with just $O(n\log n)$ model evaluations, outperforming current methods in both theoretical and practical evaluations.

Enhancing Pre-trained Representation Classifiability can Boost its Interpretability
This paper investigates the relationship between interpretability and classifiability in pre-trained visual models, proposing the Inherent Interpretability Score (IIS) to measure the ratio of interpretable semantics and information loss within the representations. The study reveals a positive correlation between interpretability and classifiability, suggesting that improving one enhances the other, and demonstrates that fine-tuning pre-trained representations with interpretability maximization can lead to improved classifiability and more accurate, interpretable predictions.

Breaking Free from MMI: A New Frontier in Rationalization by Probing Input Utilization
This paper addresses the limitations of the maximum mutual information (MMI) criterion in rationale extraction by introducing a novel approach that uses the norms of rationale candidates to identify parts of the input that the neural network can effectively utilize. Through experiments across various datasets and architectures, the proposed method demonstrates superior performance in identifying rationales compared to MMI and its variants, and achieves results comparable to or surpassing those of a representative large language model.

Not All Language Model Features Are One-Dimensionally Linear
This paper investigates the existence of inherently multi-dimensional features in language model representations, contrasting the conventional view of one-dimensional conceptual features in activation space. By employing sparse autoencoders on models like GPT-2 and Mistral 7B, the study uncovers interpretable multi-dimensional features, such as circular representations for days and months, and demonstrates their role in solving tasks involving modular arithmetic, suggesting their necessity in understanding certain model behaviors.

### Social Aspects->Fairness, Equity, Justice and Safety
Adversarial Search Engine Optimization for Large Language Models
This paper introduces "Preference Manipulation Attacks," a new type of attack that manipulates Large Language Models (LLMs) to favor certain content, impacting LLM-powered applications like search engines and chatbot plugins. By demonstrating how these attacks can trick LLMs to promote attacker products and degrade the overall system, the authors highlight a growing threat that resembles adversarial SEO tactics and creates collective incentives to engage in such attacks, ultimately compromising the quality of LLM outputs.

Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances
Current image watermarking techniques struggle against advanced editing enabled by text-to-image models, threatening copyright protection. We introduce W-Bench, a benchmark evaluating watermarking robustness, and propose VINE, a method using frequency analysis and a pretrained diffusion model to enhance watermark resilience, showing superior performance over existing methods.

Iterative Label Refinement Matters More than Preference Optimization under Weak Supervision
This paper investigates the effectiveness of language model post-training under unreliable supervision, revealing that supervised finetuning (SFT) maintains some effectiveness while the reinforcement learning from human feedback (RLHF) algorithm DPO does not enhance the model beyond SFT. The authors propose Iterative Label Refinement (ILR) as an alternative to RLHF, which refines training data using comparison feedback and demonstrates superior performance over RLHF for tasks with unreliable human supervision.

Policy Design in Long-run Welfare Dynamics
This paper analyzes the long-term impacts of Rawlsian and utilitarian policy frameworks in optimizing social welfare, challenging the assumption that Rawlsian policies reduce average welfare by demonstrating conditions under which they outperform utilitarian policies over time. Through theoretical analysis and simulations, it highlights the importance of evaluating welfare policies based on long-term effects rather than immediate outcomes, emphasizing the potential of Rawlsian approaches to offer significant benefits in the long run.

### Social Aspects->Privacy-preserving Statistics and Machine Learning
A Statistical Approach for Controlled Training Data Detection
The paper introduces a novel method called Knockoff Inference-based Training data Detector (KTD), which addresses the detection of training data for large language models with rigorous false discovery rate (FDR) control. KTD utilizes synthetic knockoff samples and a unique knockoff statistic to ensure FDR control while maintaining high power, demonstrating its superior performance over existing methods through empirical experiments on real-world datasets.

### Social Aspects->Trustworthy Machine Learning
PFGuard: A Generative Framework with Privacy and Fairness Safeguards
This paper introduces PFGuard, a generative framework designed to resolve conflicts between privacy and fairness in AI models, which have traditionally been addressed separately, leading to adverse effects when combined naively. By employing an ensemble of multiple teacher models, PFGuard manages privacy-fairness conflicts effectively, ensuring high utility, differential privacy guarantees, and convergence in fair generative modeling for synthetic data generation.

### Theory->Deep Learning
Training One-Dimensional Graph Neural Networks is NP-Hard
This paper explores the computational complexity of training graph neural networks (GNNs) and establishes the NP-hardness of training ReLU-activated one-dimensional GNNs through an intricate reduction. Additionally, the authors provide algorithmic upper bounds for training in both ReLU-activated and linearly-activated settings, highlighting the significance of graphical structure and activation functions in determining complexity.

### Theory->Domain Adaptation and Transfer Learning
Revisiting Source-Free Domain Adaptation: a New Perspective via Uncertainty Control
This paper addresses Source-Free Domain Adaptation (SFDA) by analyzing and managing uncertainty using a novel instance-dependent uncertainty control algorithm, leveraging Distributionally Robust Optimization (DRO) principles. The proposed method enhances adaptation performance by quantifying and exploiting uncertainty, validated through extensive experiments on benchmark datasets.

### Theory->Game Theory
On the Convergence of No-Regret Dynamics in Information Retrieval Games with Proportional Ranking Functions
This paper explores how publishers act strategically online, modeled through the online learning framework, and demonstrates that games induced by proportional content ranking functions with concave activation functions converge with no-regret learning dynamics. It further establishes the equivalence of concavity and studies the trade-offs between publishers' and users' welfare, influenced by the ranking function and ecosystem structure, using an advanced no-regret dynamics algorithm.

### Theory->Learning Theory
The Breakdown of Gaussian Universality in Classification of High-dimensional Linear Factor Mixtures
This paper provides a detailed high-dimensional analysis of empirical risk minimization for classification using linear factor models, which extends beyond Gaussian mixtures. It demonstrates the breakdown of Gaussian universality, showing that the data distribution significantly influences learning performance beyond mean and covariance, and elucidates the implications for selecting appropriate loss functions.

Sketching for Convex and Nonconvex Regularized Least Squares with Sharp Guarantees
This paper introduces Sketching for Regularized Optimization (SRO), a fast algorithm for solving least squares problems with convex or nonconvex regularization by creating a sketch of the original data matrix. It offers a unified theoretical framework for achieving minimax optimal rates in sparse signal estimation and presents the Iterative SRO algorithm to reduce approximation error, validated through experimental results.

Towards Generalization Bounds of GCNs for Adversarially Robust Node Classification
This paper establishes high-probability generalization bounds for Graph Convolutional Networks (GCNs) under adversarial attacks, focusing on node classification using adversarial Transductive Rademacher Complexity (TRC) and a novel contraction technique on graph convolution. The study highlights the role of specific factors, such as low-dimensional feature projection and network architecture, in mitigating adversarial effects, corroborated by theoretical analysis and experimental validation on benchmark datasets.

### Theory->Online Learning and Bandits
Demystifying Online Clustering of Bandits: Enhanced Exploration Under Stochastic and Smoothed Adversarial Contexts
The paper addresses the challenges of online clustering in contextual multi-armed bandit problems by proposing two new algorithms, UniCLUB and UniSCLUB, which employ an additional exploration phase to better identify user clusters. These algorithms reduce the need for strong assumptions, achieve superior cumulative regret, and improve practical performance by introducing a setting that removes the need for i.i.d. context generation, as demonstrated through extensive evaluations on both synthetic and real-world datasets.

Lasso Bandit with Compatibility Condition on Optimal Arm
This paper addresses the stochastic sparse linear bandit problem, demonstrating that the compatibility condition on the optimal arm alone is sufficient to achieve regret bounds logarithmically dependent on the ambient dimension $d$, without the need for additional diversity assumptions. The authors introduce an algorithm that incorporates forced-sampling, achieving $\mathcal{O}(\text{poly}\log dT)$ regret under the margin condition, and validate its superior performance through numerical experiments, highlighting its relaxed assumptions compared to existing Lasso bandit algorithms.

Conservative Contextual Bandits: Beyond Linear Representations
This paper introduces two algorithms, $\mathtt{C\text{-}SquareCB}$ and $\mathtt{C\text{-}FastCB}$, for Conservative Contextual Bandits (CCBs) that account for non-linear arm costs using Inverse Gap Weighting (IGW) and an online regression oracle. The proposed methods ensure high-probability satisfaction of safety constraints, achieve sub-linear regret in different measures, and outperform existing baselines on real-world data while maintaining performance guarantees.

### Theory->Optimization
ADAM Optimization with Adaptive Batch Selection
This paper introduces Adam with Combinatorial Bandit Sampling (AdamCB), an enhancement to the Adam optimizer that integrates combinatorial bandit techniques to improve sample selection efficiency and overcome theoretical limitations of existing variants. AdamCB is shown to achieve faster convergence and consistently outperform existing methods, offering both rigorous theoretical guarantees and practical efficiency in neural network training.

Faster Algorithms for Structured Linear and Kernel Support Vector Machines
This paper presents a nearly-linear time algorithm for solving quadratic programs that have a low-rank factorization and a small number of linear constraints, specifically improving the efficiency of solving SVM problems. The algorithm achieves significant time improvements for linear SVMs in $d$-dimensional data and Gaussian kernel SVMs with specific data dimension constraints, surpassing prior lower bounds.

Streaming Algorithms For $\ell_p$ Flows and $\ell_p$ Regression
This paper introduces one-pass streaming algorithms for underdetermined $\ell_p$ linear regression problems, generalizing basis pursuit and least squares to contexts where $n \ll d$. It presents efficient algorithms that significantly reduce space usage while approximating $\ell_p$ regression costs and solutions, providing both upper and lower bounds on space requirements for different values of $p$.



## Oral Session 1A (10:30-11:42)
10:30-10:42: Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning
This paper explores how increasing inference-time computation for large language models (LLMs) can enhance their performance on complex prompts, potentially impacting future LLM pretraining and the balance between inference-time and pretraining compute. It introduces a "compute-optimal" scaling strategy that dynamically allocates compute resources based on prompt difficulty, demonstrating a 4x improvement in efficiency for math reasoning tasks and outperforming a significantly larger model in a FLOPs-matched evaluation.
10:42-10:54: MIND over Body: Adaptive Thinking using Dynamic Computation
This paper introduces a self-introspection capability in neural networks, allowing them to adjust parameter usage and computation time based on task and input complexity, thus addressing inefficiencies in traditional deep learning models. The proposed method surpasses larger models in performance on tasks like ImageNet and SQuAD, demonstrating its potential to create intelligent systems that manage resources efficiently.
10:54-11:06: Inference Scaling for Long-Context Retrieval Augmented Generation
This paper investigates the scaling of inference computation for retrieval augmented generation (RAG) in long-context large language models, exploring strategies such as in-context learning and iterative prompting to enhance performance. The authors establish inference scaling laws for RAG, demonstrating nearly linear performance gains with optimal configuration and develop a computation allocation model that accurately predicts optimal inference parameters, achieving up to a 58.9% improvement on benchmark datasets.
11:06-11:18: miniCTX: Neural Theorem Proving with (Long-)Contexts
We present $\texttt{miniCTX}$, a benchmark that evaluates a model's capability to prove theorems based on new, unseen context by using formal mathematical theorems from Lean projects. Our findings indicate that context-conditioned fine-tuning and prompting methods significantly outperform traditional approaches, and we introduce the $\texttt{ntp-toolkit}$ to facilitate the automatic extraction and annotation of theorem proving data for extending $\texttt{miniCTX}$.
11:18-11:30: FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference
FlexPrefill is introduced as a flexible sparse pre-filling mechanism for large language models that dynamically adjusts attention patterns and computational budgets during long-sequence inference. By optimizing sparse patterns and ratios in real-time, it enhances efficiency and accuracy compared to previous methods, showcasing substantial improvements in speed and accuracy for varying input demands.
11:30-11:42: Scaling Laws for Precision
This paper introduces "precision-aware" scaling laws that account for the impact of low precision training and post-training quantization on language model quality and cost. The authors demonstrate that training in lower precision reduces the effective parameter count, and propose a unified scaling law for quantization effects on both training and inference, validated through extensive pretraining runs and model size predictions.


## Oral Session 1B (10:30-11:42)
10:30-10:42: A Probabilistic Perspective on Unlearning and Alignment for Large Language Models
This paper introduces a formal probabilistic evaluation framework for Large Language Models (LLMs) to address the limitations of deterministic point estimates, which fail to capture the full output distribution and lead to inaccurate assessments of model capabilities in critical contexts like unlearning and alignment. By proposing application-independent metrics with high probability guarantees, the authors demonstrate through experimental analysis and a case study on unlearning how this approach, including novel entropy optimization loss and adaptive temperature scaling, significantly improves the reliability of model evaluations.
10:42-10:54: Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs
Min-p sampling is a dynamic truncation method proposed to improve the balance between quality and diversity in text generated by Large Language Models, particularly at higher temperatures, by adjusting the sampling threshold based on the model's confidence. Extensive experiments and human evaluations show that min-p sampling outperforms traditional methods like top-p sampling, leading to its adoption by major open-source LLM implementations, thus underscoring its practical utility and impact.
10:54-11:06: BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models
This paper introduces BIRD, a novel probabilistic inference framework that enhances probability estimation by integrating Bayesian networks with the abductive reasoning capabilities of large language models (LLMs). By aligning these elements, BIRD achieves a 30% improvement in probability estimations over traditional LLMs, thereby contributing to more reliable decision-making in real-world tasks.
11:06-11:18: Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning
This study addresses catastrophic forgetting (CF) in large language models (LLMs) during continual learning by exploring how model forgetting is influenced by training tasks and the models themselves. The authors propose a function vector guided training methodology with a regularization technique to mitigate CF, supported by empirical tests on four benchmarks that validate their theoretical findings on CF and model function dynamics.
11:18-11:30: Training on the Test Task Confounds Evaluation and Emergence
This paper investigates "training on the test task," a practice of including task-relevant data in the pretraining of language models, which can obscure model evaluations and claims of emergent capabilities. The authors propose a method to mitigate this effect by fine-tuning models on the same task-relevant data before evaluation, highlighting the need for revised benchmarking practices to accurately assess model performance and emergent behaviors.
11:30-11:42: WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct
WizardMath is introduced as a model that enhances mathematical reasoning in large language models (LLMs) using a novel method called Reinforcement Learning from Evol-Instruct Feedback (RLEIF). Through experiments on benchmarks like GSM8k and MATH, WizardMath demonstrates superior performance over existing open-source LLMs, even surpassing models like ChatGPT-3.5, highlighting the importance of instruction evolution and process supervision.


## Oral Session 1E (10:30-11:42)
10:30-10:42: Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse
This paper investigates the phenomenon of neural collapse in deep neural networks (DNNs) and provides a theoretical framework that proves its emergence without assuming unconstrained features. The study demonstrates that neural collapse can occur during end-to-end training using gradient descent with weight decay when certain conditions of low training error, balancedness of linear layers, and bounded conditioning of features are met.
10:42-10:54: Exploring The Loss Landscape Of Regularized Neural Networks Via Convex Duality
This paper explores the loss landscape of regularized neural networks by transforming the problem into a convex form and examining its dual, focusing on solution set characterization and connectivity of optimal solutions. It reveals that as the network width changes, the topology of global optima undergoes a phase transition, and extends these insights to various network architectures, highlighting the potential for a continuum of optimal solutions.
10:54-11:06: Global Convergence in Neural ODEs: Impact of Activation Functions
This paper explores the impact of activation function properties—smoothness and nonlinearity—on the training dynamics of Neural Ordinary Differential Equations (ODEs). By establishing the global convergence of Neural ODEs under gradient descent in overparameterized regimes, the authors provide theoretical insights validated by experiments, offering practical guidelines for scaling Neural ODEs to enhance training efficiency and performance.
11:06-11:18: KAN: Kolmogorov–Arnold Networks
Kolmogorov-Arnold Networks (KANs) are introduced as innovative alternatives to Multi-Layer Perceptrons (MLPs), featuring learnable activation functions on edges instead of fixed functions on nodes. KANs demonstrate improved accuracy and interpretability over MLPs, potentially aiding in scientific discoveries, although future work is needed to enhance their training efficiency.
11:18-11:30: Feedback Favors the Generalization of Neural ODEs
This paper introduces feedback neural networks, which incorporate feedback loops to enhance the generalization of neural ordinary differential equations (neural ODEs) by correcting learned latent dynamics. The approach demonstrates robust performance across new scenarios without losing accuracy on prior tasks, and extensive testing, including trajectory prediction and model predictive control, shows significant advancements over existing methods.
11:30-11:42: On the Benefits of Memory for Modeling Time-Dependent PDEs
This paper explores the advantages of incorporating memory in the modeling of time-dependent partial differential equations (PDEs) and introduces the Memory Neural Operator (MemNO), which combines state space models and Fourier Neural Operators. The authors demonstrate theoretically and empirically that MemNO outperforms traditional Markovian approaches, particularly in scenarios with low-resolution data or observation noise, achieving up to a 6x reduction in test error, especially for PDEs with significant high-frequency components.


## Oral Session 1F (10:30-11:42)
10:30-10:42: Amortized Control of Continuous State Space Feynman-Kac Model for Irregular Time Series
The paper introduces the Amortized Control of continuous State Space Model (ACSSM), designed to model irregular time series by employing a continuous dynamical system using multi-marginal Doob's $h$-transform. ACSSM integrates variational inference and stochastic optimal control for efficient training and inference, showing superior performance and scalability across various real-world datasets in tasks like classification and regression.
10:42-10:54: Oscillatory State-Space Models
The paper introduces Linear Oscillatory State-Space models (LinOSS), inspired by the dynamics of biological neural networks, for efficient learning on long sequences through a stable discretization of harmonic oscillators. LinOSS is proven to be a universal model that ensures stable and accurate long-horizon forecasting, outperforming state-of-the-art sequence models in various time-series tasks, including a nearly 2x performance improvement over Mamba and LRU on sequences of length 50k.
10:54-11:06: Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues
Linear Recurrent Neural Networks (LRNNs) like Mamba and DeltaNet, traditionally constrained by positive eigenvalue ranges, struggle with state-tracking tasks, impacting their performance on challenges such as code evaluation. This paper demonstrates that by extending the eigenvalue range to include negative values, LRNNs can effectively solve state-tracking tasks like parity, enabling scalable, competitive performance in language modeling and promising results in code and math tasks.
11:06-11:18: Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport
This paper introduces a deep learning method for regularized unbalanced optimal transport (RUOT) to infer continuous unbalanced stochastic dynamics from sparsely time-resolved snapshots without prior knowledge of growth and death processes. Demonstrated on synthetic and real-data applications, including single-cell RNA-seq data, the method accurately identifies transition patterns and constructs the Waddington developmental landscape, outperforming existing approaches.
11:18-11:30: Artificial Kuramoto Oscillatory Neurons
The paper introduces Artificial Kuramoto Oscillatory Neurons (AKOrN) as a dynamic alternative to traditional threshold units, enhancing network performance through synchronization dynamics among neurons. This approach improves performance in various tasks, suggesting the significance of dynamic representations in neural architectures.
11:30-11:42: Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents
Multimodal large language models (MLLMs) are enhancing GUI agents by facilitating their transition from simulations to complex real-world applications, but their effectiveness relies on robust visual grounding capabilities. This paper introduces UGround, a visual grounding model trained on the largest dataset for GUI visual grounding to date, demonstrating significant performance improvements over existing models and showcasing the potential for GUI agents to navigate digital environments using visual perception alone.


## Oral Session 1D (10:30-11:42)
10:30-10:42: Towards Understanding Why FixMatch Generalizes Better Than Supervised Learning
This paper provides the first theoretical justification for the superior generalization performance of FixMatch-like semi-supervised learning (SSL) algorithms over supervised learning (SL) in deep neural networks by analyzing the differing semantic feature learning processes involved. The authors also introduce an improved variant, Semantic-Aware FixMatch (SA-FixMatch), which demonstrates enhanced generalization capabilities, supported by experimental results.
10:42-10:54: Safety Alignment Should be Made More Than Just a Few Tokens Deep
This paper identifies the issue of "shallow safety alignment" in Large Language Models (LLMs), where safety measures only affect the initial tokens, making models vulnerable to various attacks. The authors demonstrate that deepening safety alignment and using a regularized fine-tuning approach can enhance robustness against these vulnerabilities, suggesting important directions for future research in model safety.
10:54-11:06: Backtracking Improves Generation Safety
This paper introduces backtracking, a novel technique that enhances language model safety by allowing them to "undo" unsafe text generation using a special [RESET] token. By incorporating this method into SFT or DPO training, models such as backtracking Llama-3-8B become significantly safer, showing reduced unsafe text generation and resilience against adversarial attacks, without compromising helpfulness.
11:06-11:18: On the Role of Attention Heads in Large Language Model Safety
This paper investigates the role of multi-head attention mechanisms in the safety capabilities of large language models (LLMs), addressing a gap in current safety-related research. Introducing the Safety Head ImPortant Score (Ships) and the Safety Attention Head AttRibution Algorithm (Sahara), the authors demonstrate that modifying a minimal number of parameters can significantly impact safety, offering a new approach to understanding safety mechanisms within LLMs.
11:18-11:30: Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation
The paper addresses the problem of harmful fine-tuning attacks on large language models by identifying harmful perturbations of model weights as a potential root cause of alignment issues. It introduces a solution called Booster, which incorporates a loss regularizer during the alignment stage to reduce the harmful impact of perturbations, effectively lowering the harmful score of fine-tuned models while preserving task performance.
11:30-11:42: TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation
TANGO is a framework that generates high-fidelity co-speech body-gesture videos by improving upon Gesture Video Reenactment's limitations with innovative techniques for better audio-motion alignment and transition frame quality. By introducing the hierarchical joint embedding space AuMoClip for cross-modal alignment and the diffusion-based model ACInterp for consistent visual transitions, TANGO produces realistic, synchronized videos, outperforming existing methods.


## Oral Session 1C (10:30-11:42)
10:30-10:42: Variational Diffusion Posterior Sampling with Midpoint Guidance
This paper presents a novel approach to tackle the challenges of sampling from denoising posterior distributions in Bayesian inverse problems by decomposing transition processes, enabling a balance between the intractable guidance term and prior transitions. Extensive experiments demonstrate the method's effectiveness, particularly in reconstructing electrocardiograms from partial data for precise cardiac diagnosis.
10:42-10:54: Progressive Compression with Universally Quantized Diffusion Models
This paper explores the use of diffusion probabilistic models for progressive image compression, developing a novel diffusion model with uniform noise that optimizes compression costs via universal quantization. The method demonstrates promising results in rate-distortion-realism tradeoffs across various bit-rates, enhancing the practical deployment potential of neural codecs.
10:54-11:06: Influence Functions for Scalable Data Attribution in Diffusion Models
This paper addresses challenges in diffusion models related to data attribution and interpretability by extending influence functions to predict changes in the probability of generating particular examples. By utilizing a K-FAC approximation for scalable Hessian computations, the proposed method outperforms existing data attribution techniques in common evaluations without requiring method-specific hyperparameter tuning.
11:06-11:18: Improving Probabilistic Diffusion Models With Optimal Diagonal Covariance Matching
This paper introduces a novel method for learning diagonal covariances in probabilistic diffusion models using a technique called Optimal Covariance Matching (OCM). By regressing the optimal analytic covariance, the method reduces approximation error and significantly enhances sampling efficiency, recall rate, and likelihood in both diffusion and latent diffusion models.
11:18-11:30: Feedback Schrödinger Bridge Matching
This paper introduces Feedback Schrödinger Bridge Matching (FSBM), a semi-supervised matching framework designed to balance scalability and supervision in diffusion bridges for distribution transport problems. By using less than 8% of pre-aligned pairs as state feedback in an Entropic Optimal Transport formulation, FSBM significantly improves training efficiency and generalization, outperforming traditional methods that either lack scalability or require full supervision.
11:30-11:42: Learning to Discretize Denoising Diffusion ODEs
Diffusion Probabilistic Models (DPMs) are powerful generative models, but their sampling process is computationally expensive due to multiple neural function evaluations (NFEs). The proposed LD3 framework optimizes time discretization, reducing NFEs and improving generation quality without retraining, demonstrated on seven pre-trained models with significant efficiency gains and high fidelity scores.


## Poster Session 2 (15:00-17:30)
### Applications->Chemistry and Drug Discovery
Learning Molecular Representation in a Cell
The paper presents InfoAlign, a novel approach for learning molecular representations through the information bottleneck method to improve prediction of drug efficacy and safety by integrating cellular response data in a context graph. Empirically validated against 27 baseline methods, InfoAlign demonstrates superior performance in molecular property prediction and zero-shot molecule-morphology matching, offering tighter alignment and enhanced model generalization.

A Periodic Bayesian Flow for Material Generation
CrysBFN is a novel crystal generation method that introduces a periodic Bayesian flow to model variables within non-Euclidean manifolds, addressing the challenges associated with crystal structures. The method demonstrates superiority over previous approaches by achieving state-of-the-art performance on crystal generation and prediction tasks, and offers a significant improvement in sampling efficiency, achieving a 200x speedup in comparison to traditional diffusion-based methods.

### Applications->Computer Vision
An Intelligent Agentic System for Complex Image Restoration Problems
AgenticIR is an innovative system designed for real-world image restoration by employing a multi-stage, human-inspired approach: Perception, Scheduling, Execution, Reflection, and Rescheduling. By utilizing fine-tuned vision-language models and large language models for dynamic operation and reasoning, AgenticIR demonstrates significant potential in managing complex image restoration tasks, pointing towards advancements in general intelligence for visual processing.

PuzzleFusion++: Auto-agglomerative 3D Fracture Assembly by Denoise and Verify
This paper introduces PuzzleFusion++, an innovative "auto-agglomerative" 3D fracture assembly method inspired by human puzzle-solving, utilizing a diffusion model for alignment and a transformer model for merging. The approach demonstrates superior performance on the Breaking Bad dataset, surpassing current state-of-the-art methods by over 10% in part accuracy and 50% in Chamfer distance, with plans to release the code and model.

Phidias: A Generative Model for Creating 3D  Content from Text, Image, and 3D Conditions with Reference-Augmented  Diffusion
Phidias is a novel generative model for 3D modeling that enhances quality, generalization, and controllability by using diffusion for reference-augmented 3D generation. By integrating components such as meta-ControlNet, dynamic reference routing, and self-reference augmentations, Phidias significantly improves upon existing methods by unifying 3D generation using text, image, and 3D conditions, thereby expanding its application versatility.

DenseGrounding: Improving Dense Language-Vision Semantics for Ego-centric 3D Visual Grounding
DenseGrounding is a novel approach that enhances ego-centric 3D visual grounding by addressing the loss of fine-grained visual semantics and limited textual context through a Hierarchical Scene Semantic Enhancer and Language Semantic Enhancer. This method significantly outperforms existing techniques, achieving notable accuracy improvements and earning 1st place and the Innovation Award in the 2024 Autonomous Grand Challenge Multi-view 3D Visual Grounding Track.

Training-free Camera Control for Video Generation
We introduce CamTrol, a training-free method that enables camera movement control in video diffusion models without requiring supervised or self-supervised training. By leveraging the layout prior and rearranging noisy latents, CamTrol delivers enhanced video generation and camera motion alignment, demonstrating adaptability across various models and applications such as scalable motion control and 3D video generation.

RGB-Event ISP: The Dataset and Benchmark
This paper introduces the first research on event-guided image signal processing (ISP), presenting a novel event-RAW paired dataset collected with a new sensor that aligns events with RAW images. The study evaluates existing ISP methods, proposes an initial event-guided ISP approach, and outlines future challenges, aiming to inspire advancements in integrating event-based data into ISP processes.

GoodDrag: Towards Good Practices for Drag Editing with Diffusion Models
This paper introduces GoodDrag, a novel method to enhance drag editing by alternating drag and denoising within the diffusion process, thereby improving image stability and quality. It also presents an information-preserving motion supervision technique and new benchmarking tools, including the Drag100 dataset and quality metrics, demonstrating superiority over existing methods in extensive experiments.

LoRA3D: Low-Rank Self-Calibration of 3D Geometric Foundation models
LoRA3D is an efficient self-calibration pipeline that enhances the generalization of pre-trained 3D geometric foundation models like DUSt3R for challenging situations using their own multi-view predictions. It leverages robust optimization to refine predictions, incorporates prediction confidence into geometric optimization, and fine-tunes with low-rank adaptation to achieve significant improvements in 3D reconstruction and pose estimation tasks without requiring external priors, all within a short processing time and minimal storage.

Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model
Plane-DUSt3R is a novel method for multi-view room layout estimation that leverages the 3D foundation model DUSt3R to streamline the process, reducing the traditional multi-step complexities to a single post-processing step and achieving impressive results with multiple-perspective images. Experimental results show that Plane-DUSt3R outperforms current state-of-the-art methods on synthetic datasets and demonstrates robustness on diverse real-world data, effectively simplifying the estimation process and mitigating error accumulation.

AVHBench: A Cross-Modal Hallucination Benchmark for Audio-Visual Large Language Models
The paper introduces AVHBench, the first comprehensive benchmark specifically designed to evaluate audio-visual large language models (LLMs) on their perception and comprehension abilities, addressing the challenge of hallucinations arising from subtle audio-visual relationships. The results demonstrate that current audio-visual LLMs often struggle with hallucinations, but training with AVHBench improves their robustness against these issues.

NeRAF: 3D Scene Infused Neural Radiance and Acoustic Fields
NeRAF is a novel method that simultaneously learns acoustic and radiance fields to synthesize both novel visual views and spatialized room impulse responses (RIR) based on 3D scene priors, enabling independent and spatially distinct rendering of each modality. It significantly improves high-quality audio generation and novel view synthesis over previous methods, demonstrating data efficiency and enhancing cross-modal learning in complex scenes.

Neuralized Markov Random Field for Interaction-Aware Stochastic Human Trajectory Prediction
This paper introduces a neuralized Markov random field-based method for probabilistic interaction-aware human trajectory prediction, leveraging two conditional variational autoencoders for efficient learning and inference. The approach achieves state-of-the-art performance on ADE/FDE metrics across various datasets and supports real-time stochastic inference in dynamic environments, with code available at the provided GitHub link.

ConcreTizer: Model Inversion Attack via Occupancy Classification and Dispersion Control for 3D Point Cloud Restoration
This paper presents the first in-depth study of model inversion attacks on 3D point cloud data, which is critical for autonomous vehicles, revealing unique challenges posed by data sparsity and voxel ambiguity. By introducing ConcreTizer, an effective attack method incorporating Voxel Occupancy Classification and Dispersion-Controlled Supervision, the research demonstrates how 3D point clouds can be reconstructed from feature data, emphasizing the vulnerability of this data type and the need for improved defense mechanisms.

GenDataAgent: On-the-fly Dataset Augmentation with Synthetic Data
This paper introduces a generative agent that enhances training datasets with targeted synthetic data for model fine-tuning by generating samples that align with the target distribution and prioritize challenging training samples. The approach is shown to be effective in several image classification tasks, offering a significant advancement over previous methods that utilize uniform sampling of synthetic data.

AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark
AuroraCap is a video captioning model designed with a simple architecture and without extra parameters for temporal modeling, utilizing a token merging strategy to efficiently handle lengthy video sequences, achieving high performance with minimal loss. The paper introduces the VDC benchmark for video detailed captioning and a new evaluation metric, VDCscore, which aligns more closely with human assessments, enhancing research in detailed video descriptions.

GSE: Group-wise Sparse and Explainable Adversarial Attacks
This paper presents a two-phase algorithm to generate group-wise sparse adversarial attacks that exploit the structural vulnerabilities of deep neural networks (DNNs) through semantically meaningful pixel perturbations. By integrating a quasinorm adversarial loss and a projected Nesterov's accelerated gradient descent, the proposed method achieves high group-wise sparsity and efficiency, demonstrating significant improvements on CIFAR-10 and ImageNet with a $100\%$ attack success rate.

Revisit the Open Nature of Open Vocabulary Semantic Segmentation
This paper addresses the limitations of the previous evaluation protocols in Open Vocabulary Semantic Segmentation (OVS) by introducing a mask-wise evaluation protocol that accounts for semantically ambiguous categories. Experimental results demonstrate the effectiveness of this new method, highlighting the prevalence of ambiguous categories in OVS datasets and showing that reducing these ambiguities can enhance model performance, thus encouraging further exploration of open-world challenges.

DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control
DartControl (DART) is introduced as a novel Diffusion-based Autoregressive motion primitive model that facilitates real-time, continuous human motion generation from text descriptions while addressing spatial constraints by aligning motion semantics with geometric information. The model shows superior performance over existing methods in motion realism, efficiency, and controllability through efficient algorithms and experiments validating its capabilities in sequential motion synthesis tasks.

DisEnvisioner: Disentangled and Enriched Visual Prompt for Customized Image Generation
DisEnvisioner is introduced as a tuning-free approach that enhances image customization by effectively extracting and enriching subject-essential features from a single image, while eliminating irrelevant information. The method separates subject features into distinct visual tokens and enriches them for precise customization, achieving superior performance in editability, ID consistency, inference speed, and image quality compared to existing methods.

InterMask: 3D Human Interaction Generation via Collaborative Masked Modeling
This paper introduces *InterMask*, a novel framework for generating realistic 3D human interactions from textual descriptions using collaborative masked modeling in discrete space. By employing a VQ-VAE for motion sequence transformation and a dedicated transformer architecture, *InterMask* achieves state-of-the-art results in producing diverse high-fidelity interactions and supports reaction generation without requiring model redesign or fine-tuning.

### Applications->Everything Else
Navigating the Digital World as Humans Do: Universal Visual Grounding for GUI Agents
Multimodal large language models (MLLMs) are enhancing GUI agents by facilitating their transition from simulations to complex real-world applications, but their effectiveness relies on robust visual grounding capabilities. This paper introduces UGround, a visual grounding model trained on the largest dataset for GUI visual grounding to date, demonstrating significant performance improvements over existing models and showcasing the potential for GUI agents to navigate digital environments using visual perception alone.

Regularized Proportional Fairness Mechanism for Resource Allocation Without Money
This study addresses resource allocation among self-interested agents without payments, focusing on maximizing social welfare while maintaining incentive compatibility. Introducing the Regularized Proportional Fairness Network (RPF-Net), the authors offer a novel neural network approach to mitigate exploitability in resource allocation mechanisms, supported by generalization bounds and empirical validation against existing methods.

### Applications->Genetics, Cell Biology, Health, etc
Learning stochastic dynamics from snapshots through regularized unbalanced optimal transport
This paper introduces a deep learning method for regularized unbalanced optimal transport (RUOT) to infer continuous unbalanced stochastic dynamics from sparsely time-resolved snapshots without prior knowledge of growth and death processes. Demonstrated on synthetic and real-data applications, including single-cell RNA-seq data, the method accurately identifies transition patterns and constructs the Waddington developmental landscape, outperforming existing approaches.

Interpretable Causal Representation Learning for Biological Data in the Pathway Space
This paper introduces SENA-discrepancy-VAE, an enhanced causal representation learning model that aligns latent factor representations with biological processes for improved interpretability in predicting the effects of genomic and drug perturbations. The proposed model, with its efficient encoder SENA-$\delta$, achieves comparable predictive performance on unseen intervention combinations while offering biologically meaningful insights into causal latent factors.

### Applications->Health
BoneMet: An Open Large-Scale Multi-Modal Murine Dataset for Breast Cancer Bone Metastasis Diagnosis and Prognosis
The study introduces the BoneMet dataset, the first large-scale, publicly available dataset focused on breast cancer bone metastasis (BCBM), derived from a murine model and containing over 67 terabytes of multi-modal medical data. This dataset enables the development of AI models for diagnosis and prognosis of BCBM, offering detailed sequential scans and medical records essential for advancing deep learning applications in this healthcare domain.

### Applications->Language, Speech and Dialog
Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data
Synthio is introduced as a new method to enhance small-scale audio classification datasets by generating synthetic audio through text-to-audio diffusion models, aiming to improve classification accuracy with limited labeled data. By employing preference optimization for acoustic consistency and a novel caption generation technique to ensure compositional diversity, Synthio surpasses traditional augmentation techniques, showing significant performance improvements across multiple datasets and simulated conditions.

Divide and Translate: Compositional First-Order Logic Translation and Verification for Complex Logical Reasoning
The paper introduces CLOVER, a method to improve large language models' ability to handle complex logical reasoning tasks by employing a Compositional First-Order Logic Translation approach. CLOVER leverages logical dependency structures and introduces two verification algorithms to enhance translation accuracy, outperforming previous neurosymbolic methods and setting new benchmarks in logical reasoning tasks.

Continuous Autoregressive Modeling with Stochastic Monotonic Alignment for Speech Synthesis
We introduce a novel speech synthesis model that combines a variational autoencoder with a multi-modal latent space and a Gaussian Mixture Model-based autoregressive approach, eliminating the need for residual vector quantization. Our model not only enforces strict monotonic alignments but also outperforms the state-of-the-art VALL-E model in evaluations with significantly fewer parameters, demonstrating its efficiency and effectiveness.

### Applications->Neuroscience, Cognitive Science
ZAPBench: A Benchmark for Whole-Brain Activity Prediction in Zebrafish
The Zebrafish Activity Prediction Benchmark (ZAPBench) introduces a novel dataset of 4D light-sheet microscopy recordings to advance the prediction of neural activity in a larval zebrafish brain at cellular resolution. Initial modeling results surpass baseline methods, highlighting the benchmarks' potential while also indicating opportunities for enhancement with forthcoming synaptic-level anatomical data integration.

Biologically Constrained Barrel Cortex Model Integrates Whisker Inputs and Replicates Key Brain Network Dynamics
This study presents a biologically constrained model of the mouse barrel cortex that integrates complex neural structures to transform sensory inputs into motor functions, achieving higher classification accuracy in object discrimination tasks compared to traditional neural networks. By demonstrating firing patterns and distributions akin to actual neuronal systems and validating its perceptual capabilities through whisker deprivation experiments, the model enhances our understanding of neural processing and offers significant advancements for both neuroscience research and artificial intelligence applications.

Sparse components distinguish visual pathways & their alignment to neural networks
This paper investigates the distinct functional processes of the ventral, dorsal, and lateral streams in the human visual cortex using a novel sparse decomposition approach, revealing specific component response profiles for each stream. It introduces Sparse Component Alignment (SCA) to better measure representational alignment between biological and artificial visual systems, finding that standard DNNs align more closely with the ventral stream and offering enhanced resolution of neural tuning distinctions.

Modeling dynamic social vision highlights gaps between deep learning and humans
This paper evaluates the performance of deep learning models on dynamic real-world vision tasks involving complex, multi-agent social interactions by extending a dataset with human-annotated video captions and benchmarking 350+ models on human behavior and neural responses. The study identifies a significant gap in AI's capability to replicate human social vision, as existing models struggle with predict human social interactions and neural responses in the lateral visual stream, offering critical insights for advancing models to improve performance in natural, dynamic contexts.

### Applications->Physics
GridMix: Exploring Spatial Modulation for Neural Fields in PDE Modeling
The paper introduces GridMix, a novel approach to improve PDE modeling with neural fields by using a mixture of grid-based representations to enhance both global and local detail reconstruction. By integrating spatial domain augmentation, the MARBLE framework emerges from this approach, demonstrating significant advancements in robustness and effectiveness across various benchmarks in dynamics modeling and geometric prediction.

Learning the Complexity of Weakly Noisy Quantum States
This paper investigates the complexity of weakly noisy quantum states using a quantum learning method, proposing an efficient algorithm that utilizes the classical shadow representation to predict their circuit complexity. The findings demonstrate the algorithm's optimality in sample complexity and polynomial processing time, effectively connecting learning algorithms and quantum state complexity.

Real-time design of architectural structures with differentiable mechanics and neural networks
This paper presents a method that combines neural networks with a differentiable mechanics simulator to accelerate the design of architectural structures by solving shape approximation problems with efficiency and mechanical compliance. The proposed model outperforms fully neural alternatives in accuracy and generalization and provides real-time design exploration, validated through tasks like masonry shell and cable-net tower design, along with integration into 3D modeling software and successful physical prototype fabrication.

ParFam -- (Neural Guided) Symbolic Regression via Continuous Global Optimization
This paper introduces ParFam, a novel approach to symbolic regression that reformulates the problem into a continuous one by using parametric families of symbolic functions, simplifying the process compared to traditional methods. The approach, enhanced by DL-ParFam's integration of a pre-trained transformer network, demonstrates state-of-the-art performance and significant optimization speedups, as validated by extensive experiments.

### Applications->Robotics
Intent3D: 3D Object Detection in RGB-D Scans Based on Human Intention
The paper introduces 3D intention grounding, a novel task in 3D object detection using RGB-D data, which emphasizes detecting objects based on human intentions without explicit references. To address this, the authors present the Intent3D dataset and propose IntentNet, an approach that enhances intention understanding, reasoning, and utilizes cascaded adaptive learning for improved detection accuracy.

Dream to Manipulate: Compositional World Models Empowering Robot Imitation Learning with Imagination
We introduce DreMa, a novel approach to constructing digital twins for robotics by utilizing learned explicit representations of the real world, improving over traditional world models that often result in unrealistic robot behaviors. DreMa enhances accuracy and robustness in predicting future actions and enables a Franka Emika Panda robot to perform one-shot learning of new tasks, thereby reducing data requirements and improving generalization in robotic applications.

### Applications->Time Series
Learning under Temporal Label Noise
This paper addresses the unstudied issue of temporal label noise in time series classification, where label accuracy fluctuates over time due to a time-dependent noise function. The authors propose methods to estimate this noise function directly from data, leading to state-of-the-art performance in handling temporal label noise across various real-world datasets.

Learning the Optimal Stopping for Early Classification within Finite Horizons via Sequential Probability Ratio Test
FIRMBOUND is an SPRT-based framework designed to efficiently determine optimal stopping rules for early classification of time series in finite horizon scenarios, overcoming the computational challenges of traditional backward induction. By utilizing density ratio estimation and convex function learning, FIRMBOUND minimizes Bayes risk and enhances the speed-accuracy tradeoff, with experiments demonstrating its effectiveness across diverse datasets; a faster variant employing Gaussian process regression offers reduced training time with some tradeoffs in statistical consistency.

### Deep Learning->Algorithms
IDInit: A Universal and Stable Initialization Method for Neural Network Training
This paper introduces fully identical initialization (IDInit), a novel method that enhances neural network training by preserving identity across both main and sub-stem layers. IDInit addresses issues with existing methods by using padded identity-like matrices and solving convergence problems, resulting in improved stability and performance for large-scale datasets and deep models.

Smoothing the Shift: Towards Stable Test-Time Adaptation under Complex Multimodal Noises
This paper addresses the challenge of distribution shifts in multimodal data during Test-Time Adaptation (TTA) by introducing the concept of *multimodal wild TTA*. The proposed SuMi method, which involves sample identification with interquartile range smoothing and mutual information sharing, effectively mitigates abrupt distribution shifts and enhances performance by utilizing unimodal features and aligning multimodal information.

Noise Stability Optimization for Finding Flat Minima: A Hessian-based Regularization Approach
This paper investigates noise injection algorithms as a method to regularize the Hessian of over-parameterized neural networks, ultimately achieving flatter loss surfaces and better generalization. By implementing a two-point noise injection estimate, the approach not only significantly reduces the Hessian's trace and the largest eigenvalue, enhancing sharpness-reduced training and test accuracy, but also successfully integrates with existing regularization methods for improved performance across various tasks and models.

### Deep Learning->Attention Mechanisms
Fundamental Limitations on Subquadratic Alternatives to Transformers
This paper demonstrates that while various approaches have been developed to circumvent the quadratic time complexity of the Transformer’s attention mechanism, none can perform certain essential tasks like document similarity in truly subquadratic time as effectively as the Transformer. By proving that attempts such as heuristic algorithms and linear-time state space models like Mamba cannot match the Transformer for these tasks, the research underscores the inherent trade-off in computational efficiency and task performance, reinforcing the necessity of the Transformer's quadratic runtime for tasks involving document similarity.

### Deep Learning->Everything Else
Designing Concise ConvNets with Columnar Stages
The paper introduces Columnar Stage Network (CoSNet), a novel ConvNet design focused on resource efficiency with a concise structure, smaller depth, and fewer parameters, making it suitable for constrained environments. By utilizing parallel convolutions with input replication and minimizing 1×1 convolutions, CoSNet demonstrates comparable performance to established ConvNets and Transformers, with plans to open-source its pretrained models.

Deep Learning Alternatives Of The Kolmogorov Superposition Theorem
This paper introduces ActNet, a scalable deep learning model that extends the Kolmogorov Superposition Theorem (KST) to overcome its practical limitations in neural network design. ActNet is evaluated within Physics-Informed Neural Networks (PINNs) for partial differential equations, where it consistently outperforms Kolmogorov-Arnold Networks (KANs) and is competitive with leading multilayer perceptrons, offering a promising direction for KST-based applications in scientific computing.

Two Sparse Matrices are Better than One: Sparsifying Neural Networks with Double Sparse Factorization
The paper introduces Double Sparse Factorization (DSF), a method that factorizes neural network weight matrices into two sparse matrices to achieve significant model size reduction. Utilizing an efficient heuristic based on alternating minimization via ADMM, DSF outperforms state-of-the-art pruning techniques, as demonstrated by halving the size of the LLaMA2-13B model while maintaining superior performance over the dense LLaMA2-7B model, with code available at the provided GitHub repository.

### Deep Learning->Generative Models and Autoencoders
Meissonic: Revitalizing Masked Generative Transformers for Efficient High-Resolution Text-to-Image Synthesis
Meissonic introduces significant advancements in non-autoregressive text-to-image Masked Image Modeling (MIM), achieving performance comparable to state-of-the-art diffusion models through architectural innovations, refined positional encoding, and optimized sampling conditions. The model excels in generating high-resolution images by utilizing high-quality training data, human preference-informed micro-conditions, and feature compression layers, positioning itself as a new benchmark in text-to-image synthesis.

Manifold Constraint Reduces Exposure Bias in Accelerated Diffusion Sampling
Diffusion models, while effective at generating high-quality media, suffer from computational inefficiencies due to their iterative inference process. This paper introduces a manifold constraint to address exposure bias during accelerated sampling, significantly improving output quality without additional training, as evidenced by a reduced FID score of 15.60 with 10-step SDXL on MS-COCO.

An Undetectable Watermark for Generative Image Models
This paper introduces the first undetectable watermarking scheme for generative image models, ensuring that watermarked images are indistinguishable from un-watermarked ones without degrading image quality. Utilizing a pseudorandom error-correcting code within a diffusion model, the scheme offers robust watermarking that resists removal attacks and allows encoding of up to 2500 bits, with the code available for use at their GitHub repository.

Add-it: Training-Free Object Insertion in Images With Pretrained Diffusion Models
Add-it is a training-free approach that enhances diffusion models' attention mechanisms to seamlessly integrate new objects into images based on text instructions, addressing challenges in maintaining scene coherence and natural object placement. By leveraging information from the scene image, text prompt, and generated image, Add-it achieves state-of-the-art performance on image insertion benchmarks without task-specific fine-tuning, surpassing supervised methods and gaining preference in over 80% of human evaluations.

Discrete Distribution Networks
The paper presents Discrete Distribution Networks (DDN), a generative model that approximates data distributions using hierarchical discrete outputs, allowing the generation of multiple samples simultaneously to better mirror continuous target distributions. By increasing the number of layers, DDN enhances the similarity of generated samples to the true data and supports zero-shot conditional generation, with its effectiveness demonstrated through experiments on CIFAR-10 and FFHQ.

VD3D: Taming Large Video Diffusion Transformers for 3D Camera Control
This paper introduces a novel method to provide 3D camera control for transformer-based video diffusion models using a ControlNet-like conditioning mechanism with spatiotemporal camera embeddings based on Plucker coordinates. The proposed approach achieves state-of-the-art performance in controllable video generation, representing the first successful application of such control in transformer-based models.

Flow matching achieves almost minimax optimal convergence
Flow matching (FM) streamlines the sample generation process by solving an ordinary differential equation, unlike diffusion models that rely on stochastic differential equations. This paper establishes FM's ability to achieve an almost minimax optimal convergence rate in terms of the $p$-Wasserstein distance for $1 \leq p \leq 2$, providing the first theoretical evidence of its competitiveness with diffusion models and extending the analysis to a broader class of vector field functions.

Build-A-Scene: Interactive 3D Layout Control for Diffusion-Based Image Generation
We introduce a diffusion-based method for Text-to-Image (T2I) generation that allows for interactive 3D layout control, overcoming the limitations of traditional 2D approaches which are unsuitable for dynamic applications like interior design. Our approach utilizes depth-conditioned T2I models to support 3D object-wise control and iterative refinements, featuring a novel Dynamic Self-Attention module and consistent 3D object translation strategy that significantly improve object generation success rate and layout preservation over existing methods.

MixEval-X: Any-to-any Evaluations from Real-world Data Mixture
The paper introduces MixEval-X, the first comprehensive benchmark designed to standardize evaluations across diverse input and output modalities, addressing issues of inconsistent standards and biases in current evaluations. Through multi-modal benchmark mixtures and adaptation-rectification pipelines, MixEval-X effectively aligns with real-world task distributions, achieving strong correlation with crowd-sourced evaluations while improving efficiency, and providing insights through comprehensive leaderboards.

Circuit Transformer: A Transformer That Preserves Logical Equivalence
This study presents the "Circuit Transformer", a generative neural model that ensures logic circuits generated are strictly equivalent to given Boolean functions, addressing the challenge faced by conventional neural approaches. Through a novel decoding mechanism with "cutoff properties" and a Markov decision process formulation, the model achieves optimized and compact circuits without equivalence violations, demonstrating superior performance on both synthetic and real-world benchmarks.

PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform Generation
PeriodWave is a novel universal waveform generation model designed to address the challenges of fast and high-quality waveform generation from Mel-spectrogram and neural audio codec. By introducing innovative techniques such as a period-aware flow matching estimator and FreeU for noise reduction, the model outperforms existing methods in both reconstruction and text-to-speech tasks, while providing an efficient streaming generation framework.

Bias Mitigation in Graph Diffusion Models
This paper addresses significant bias issues in existing graph diffusion models by proposing a comprehensive approach that mitigates reverse-starting bias and exposure bias, leading to improved generation quality. By employing a new Langevin sampling algorithm and a score correction mechanism without requiring network modifications, the authors achieve state-of-the-art results across various models, datasets, and tasks.

SG-I2V: Self-Guided Trajectory Control in Image-to-Video Generation
SG-I2V is a novel framework for controllable image-to-video generation that provides zero-shot control by leveraging a pre-trained image-to-video diffusion model, eliminating the need for fine-tuning or external annotations. This method not only surpasses unsupervised baselines but also reduces the performance disparity with supervised models in both visual quality and motion fidelity.

PRISM: Privacy-Preserving Improved Stochastic Masking for Federated Generative Models
PRISM is a federated learning framework designed to integrate generative models by ensuring stable performance within heterogeneous data distributions and minimizing communication costs through the use of stochastic binary masks. By demonstrating strong generative capabilities on complex datasets like MNIST, FMNIST, CelebA, and CIFAR10, PRISM outperforms existing methods and effectively tackles the challenges of non-IID and privacy-preserving FL environments.

TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models
This paper introduces a unified framework for text-to-image generation and retrieval using a single Large Multimodal Model (LMM), which combines efficient generative retrieval methods with an autonomous decision mechanism to select the most suitable image in response to text prompts. The framework's effectiveness is validated through extensive experiments on the newly constructed TIGeR-Bench, as well as existing benchmarks like Flickr30K and MS-COCO, showing superior performance in both creative and knowledge-intensive domains.

TabDiff: a Mixed-type Diffusion Model for Tabular Data Generation
TabDiff is a novel joint diffusion framework designed to effectively model the mixed-type distributions inherent in tabular data by employing a continuous-time diffusion process and a transformer for handling numerical and categorical inputs. The framework excels in scalability and performance, showing up to 22.5% improvement over state-of-the-art models in estimating pair-wise column correlations, as demonstrated through extensive experiments across multiple datasets.

Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond
This paper proposes a method to accelerate the exact inference of long convolution sequence models (LCSMs) such as Hyena from quadratic to quasilinear time by exploiting key properties through a general framework inspired by relaxed polynomial interpolation. By using a tiling approach to reduce memory movement and increase parallelization, the implementation demonstrates up to a $7.8\times$ improvement in end-to-end inference performance, specifically a $110\times$ enhancement within the position-mixing component.

Presto! Distilling Steps and Layers for Accelerating Music Generation
Presto! introduces an inference acceleration technique for score-based diffusion transformers in text-to-music generation by reducing both sampling steps and cost per step. By developing a new GAN-based distribution matching distillation method and improving layer distillation, the approach achieves high-quality, diverse outputs with up to 18x faster generation speeds compared to state-of-the-art models.

DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes
DynamicCity introduces a novel 4D occupancy generation framework designed for creating large-scale, dynamic urban scenes with semantic detail, significantly advancing beyond static, single-frame scene generation. By leveraging a VAE model for compact 4D representation and a DiT-based diffusion model for HexPlane generation, it enhances fitting quality, training efficiency, and reconstruction accuracy, outperforming existing methods on multiple metrics, as evidenced by experiments on the CarlaSC and Waymo datasets.

MRS: A Fast Sampler for Mean Reverting Diffusion based on ODE and SDE Solvers
This paper introduces MRS (MR Sampler), a novel algorithm that significantly reduces the number of function evaluations required for sampling in Mean Reverting Diffusion Models, enabling faster generation of high-quality samples without the need for training. By providing semi-analytical solutions for the reverse-time SDE and PF-ODE, the approach maintains high sampling quality with a ten to twenty-fold speedup across various image restoration tasks, enhancing the practicality of controllable generation in diffusion models.

ElasticTok: Adaptive Tokenization for Image and Video
ElasticTok introduces an adaptive video tokenization method that encodes each frame into a variable number of tokens based on complexity, improving efficiency in processing long video sequences. By utilizing a novel masking technique, ElasticTok optimizes token allocation during inference, enhancing token usage efficiency and supporting the development of advanced multimodal and world models.

Controlling Space and Time with Diffusion Models
4DiM introduces a cascaded diffusion model for 4D novel view synthesis that supports generation with any camera trajectory and timestamp, enhancing generalization capabilities with its unique architecture to train on diverse data types. It pioneers intuitive metric-scale camera pose control and outperforms existing models in image fidelity and pose alignment, offering a versatile framework for tasks such as single-image-to-3D and pose-conditioned video-to-video translation.

Enhanced Diffusion Sampling via Extrapolation with Multiple ODE Solutions
Diffusion probabilistic models (DPMs) often face high computational costs due to their iterative sampling process. The proposed RX-DPM method utilizes an ODE-based sampling approach inspired by Richardson extrapolation to enhance prediction accuracy and convergence rates without increasing computational overhead, effectively improving sample quality within DPMs.

How Discrete and Continuous Diffusion Meet: Comprehensive Analysis of Discrete Diffusion Models  via a Stochastic Integral Framework
This paper introduces a comprehensive framework for the error analysis of discrete diffusion models using Lévy-type stochastic integrals, thereby generalizing Poisson random measures for enhanced modeling. The study establishes foundational theorems analogous to those in continuous models, providing the first error bounds for the $\tau$-leaping scheme and guiding the design of more efficient algorithms in real-world applications.

Learning to Discretize Denoising Diffusion ODEs
Diffusion Probabilistic Models (DPMs) are powerful generative models, but their sampling process is computationally expensive due to multiple neural function evaluations (NFEs). The proposed LD3 framework optimizes time discretization, reducing NFEs and improving generation quality without retraining, demonstrated on seven pre-trained models with significant efficiency gains and high fidelity scores.

Test-time Alignment of Diffusion Models without Reward Over-optimization
This paper introduces a training-free, test-time method using Sequential Monte Carlo (SMC) to align diffusion models with specific objectives without losing their versatility. The approach, which excels in both single-reward and multi-objective scenarios, avoids reward over-optimization and preserves diversity, offering competitive advantages over traditional fine-tuning methods.

Text-to-Image Rectified Flow as Plug-and-Play Priors
Rectified Flow models surpass diffusion-based methods in generation quality and efficiency, offering similar functionalities as versatile plug-and-play priors. The study provides theoretical and experimental evidence that these models excel in text-to-3D generation, image inversion, and editing, demonstrated by outperforming diffusion counterparts like SDS and VSD losses.

### Deep Learning->Graph Neural Networks
Biologically Plausible Brain Graph Transformer
This paper introduces a Biologically Plausible Brain Graph Transformer (BioBGT) that effectively captures the small-world architecture of brain graphs, addressing limitations in existing methods' representation of brain structural and functional properties. With novel techniques such as network entanglement-based node importance encoding and functional module-aware self-attention, BioBGT enhances the accuracy of machine learning tasks related to brain disorder detection, as demonstrated by superior performance on three benchmark datasets.

Fully-inductive Node Classification on Arbitrary Graphs
This paper addresses the challenge of graph machine learning generalization with the introduction of a fully-inductive method called GraphAny, which performs inference on new graphs regardless of differing structures, feature, and label spaces. GraphAny, utilizing a LinearGNN framework and a novel attention module, is empirically demonstrated to significantly outperform existing inductive and transductive methods, achieving an average accuracy of 67.26% across 30 new graphs using a single training dataset.

GotenNet: Rethinking Efficient 3D Equivariant Graph Neural Networks
The paper introduces GotenNet, a Geometric Tensor Network designed to effectively model the geometric complexities of 3D graphs while maintaining strict Euclidean equivariance and addressing the expressiveness-efficiency trade-off. GotenNet demonstrates superior performance over state-of-the-art methods across multiple molecular datasets by utilizing geometry-aware tensor attention and hierarchical tensor refinement, offering a robust, scalable framework for 3D equivariant Graph Neural Networks.

### Deep Learning->Large Language Models
TLDR: Token-Level Detective Reward Model for Large Vision Language Models
This paper introduces the Token-Level Detective Reward Model (TLDR), which provides fine-grained annotations for each text token to enhance multimodal large language models by addressing biases towards text over images. Through synthetic hard negative generation and token-level labeling, TLDR models significantly improve model performance, aid in self-correction, and accelerate high-quality vision language data annotation by up to three times.

Aligning Language Models with Demonstrated Feedback
Demonstration ITerated Task Optimization (DITTO) is introduced as a novel method for aligning language model outputs to specific user behaviors using fewer than ten demonstrations as feedback, leveraging principles from online imitation learning. This approach allows for effective and cost-efficient customization of language models across various domains, outperforming traditional methods like few-shot prompting and supervised fine-tuning by an average of 19 percentage points in alignment tasks.

Variational Best-of-N Alignment
Best-of-N (BoN) effectively aligns language models to human preferences by selecting the highest reward sample from N drawn samples, but it is computationally costly. This paper introduces variational BoN (vBoN), which fine-tunes language models to approximate the BoN distribution, reducing inference cost significantly while maintaining close performance to BoN, demonstrated through controlled generation and summarization tasks.

In Search of the Engram in LLMs: A Neuroscience Perspective on the Memory Functions in AI Models
This paper explores a neuroscience-inspired perspective on how Large Language Models (LLMs) process and store information, using the concept of engrams as a framework. It highlights the synergy between AI research and neuroscience in understanding intelligent systems, especially in the context of risks like misinformation and privacy violations.

LLaMaFlex: Many-in-one LLMs via Generalized Pruning and Weight Sharing
The paper introduces LLaMaFlex, a novel nested weight-shared architecture that allows for the creation of highly accurate compressed models through zero-shot pruning across both width and depth dimensions. By using a single continued training phase with approximately 60 billion tokens starting from a pretrained model, LLaMaFlex facilitates a "train once, deploy many" approach, achieving accuracy comparable to or better than state-of-the-art pruned models without the extensive resource requirements typical of large language model training.

Training on the Test Task Confounds Evaluation and Emergence
This paper investigates "training on the test task," a practice of including task-relevant data in the pretraining of language models, which can obscure model evaluations and claims of emergent capabilities. The authors propose a method to mitigate this effect by fine-tuning models on the same task-relevant data before evaluation, highlighting the need for revised benchmarking practices to accurately assess model performance and emergent behaviors.

A Probabilistic Perspective on Unlearning and Alignment for Large Language Models
This paper introduces a formal probabilistic evaluation framework for Large Language Models (LLMs) to address the limitations of deterministic point estimates, which fail to capture the full output distribution and lead to inaccurate assessments of model capabilities in critical contexts like unlearning and alignment. By proposing application-independent metrics with high probability guarantees, the authors demonstrate through experimental analysis and a case study on unlearning how this approach, including novel entropy optimization loss and adaptive temperature scaling, significantly improves the reliability of model evaluations.

SMT: Fine-Tuning Large Language Models with Sparse Matrices
This paper introduces a method for selecting sparse sub-matrices to narrow the performance gap between parameter-efficient fine-tuning (PEFT) methods and full fine-tuning (FT), while also reducing computational and memory costs. The proposed approach, called SMT, outperforms existing PEFT methods like LoRA and DoRA in fine-tuning large language models and achieves significant memory savings, demonstrating its robustness across a variety of tasks without performance declines seen in other methods.

KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models
This paper introduces a new benchmark of 4,300 visual transformations to test large multimodal models (LMMs) on visual analogical reasoning, comparing their capabilities with those of human children and adults. The study reveals that while LMMs like GPT-o1 can effectively identify changes in visual attributes, they fall short in quantifying and applying rules to new scenarios, highlighting their limitations compared to humans in handling complex cognitive tasks.

Intelligent Go-Explore: Standing on the Shoulders of Giant Foundation Models
Intelligent Go-Explore (IGE) extends the Go-Explore family of algorithms by utilizing giant pretrained foundation models to replace manually designed heuristics for guiding exploration, thereby offering a human-like ability to instinctively identify promising states in complex environments. This advancement results in superior performance across a spectrum of language and vision-based tasks, surpassing existing reinforcement learning and foundation model baselines, and facilitates recognition of valuable serendipitous discoveries during exploration.

Visual Description Grounding Reduces Hallucinations and Boosts Reasoning in LVLMs
This paper investigates the root causes of hallucinations in Large Vision-Language Models (LVLMs) and highlights their struggle with cognitive prompts requiring complex reasoning. To enhance visual perception and reasoning, the authors propose a novel method called Visual Description Grounded Decoding (VDGD), which significantly improves performance across visual reasoning benchmarks, outperforming existing techniques by 2% to 33%, and introduce VaLLu, a new benchmark for evaluating LVLM cognitive capabilities.

SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget
This paper introduces \sys, a method for optimizing the Key-Value (KV) cache in Large Language Models (LLMs) by dynamically allocating KV-cache budgets based on the importance of attention layers. By categorizing layers and adjusting their KV budgets, \sys achieves significant memory reductions (30% to 70%) and throughput improvements (up to 2.2×) across various LLMs.

Estimating the Probabilities of Rare Outputs in Language Models
This paper addresses the challenge of low probability estimation for binary properties in machine learning models, particularly when traditional sampling methods fail due to exceedingly small probabilities. By comparing importance sampling and activation extrapolation in transformer language models, the study finds that both methods outperform naive sampling, highlights the superiority of importance sampling, and suggests that improved techniques are essential for enhancing model robustness against distributional shifts and worst-case scenarios.

Instruct-SkillMix: A Powerful Pipeline for LLM Instruction Tuning
INSTRUCT-SKILLMIX is an automated method for creating diverse, high-quality SFT data by utilizing a two-stage pipeline with large language models to extract skills and generate (instruction, response) data. This approach shows strong improvements on instruction-following benchmarks with an efficient creation cost and provides insights into the challenges of open instruction-tuning datasets.

StructRAG: Boosting Knowledge Intensive Reasoning of LLMs via Inference-time Hybrid Information Structurization
Retrieval-augmented generation (RAG) methods often struggle with knowledge-intensive reasoning tasks due to dispersed information, making it challenging to accurately identify key details and perform reasoning. The paper introduces StructRAG, a framework inspired by cognitive theories, which structures information optimally for tasks, enhancing large language models (LLMs) and achieving state-of-the-art performance in complex scenarios.

Learning Harmonized Representations for Speculative Sampling
This paper introduces HArmonized Speculative Sampling (HASS), which enhances the decoding speed of Large Language Models by using harmonized representations to address contextual inconsistencies and objective discrepancies present in current speculative sampling methods. HASS achieves notable speed improvements, demonstrating a 2.81x-4.05x increase in wall-clock time speedup over four LLaMA models, outperforming previous methods like EAGLE-2 by 8%-20%.

Benchmarking Agentic Workflow Generation
WorfBench is introduced as a benchmark to evaluate workflow generation in large language models (LLMs) through multi-faceted scenarios and complex graph structures, alongside WorfEval, a protocol using subsequence and subgraph matching for precise assessment. The study reveals notable disparities in LLMs' planning abilities, even in advanced models like GPT-4, and demonstrates that enhancing workflow generation can improve performance and efficiency in downstream tasks, with resources available at https://github.com/zjunlp/WorfBench.

Agent Skill Acquisition for Large Language Models via CycleQD
CycleQD is introduced as a novel training approach for large language models that addresses challenges in data distribution and objective function design by using a cyclic adaptation of the Quality Diversity framework. Through empirical evaluations, CycleQD demonstrates superior performance over traditional fine-tuning methods in various tasks, achieving results comparable to GPT-3.5-TURBO, and the method's applicability extends beyond language models to include image segmentation tasks.

K-HALU: Multiple Answer Korean Hallucination Benchmark for Large Language Models
Recent advancements in large language models (LLMs) have highlighted their limitations in hallucination detection, particularly in non-English languages such as Korean. To address this gap, we introduce K-HALU, a Korean benchmark dataset that evaluates LLMs' ability to detect hallucinations across various domains, demonstrating the ongoing challenges faced by open-source models in accurately processing Korean knowledge.

ARB-LLM: Alternating Refined Binarizations for Large Language Models
This paper introduces ARB-LLM, a novel 1-bit post-training quantization technique designed to address memory and computational demands in large language models by narrowing the distribution gap between binarized and full-precision weights. The proposed method, which includes advanced refinements such as ARB-X and ARB-RC with column-group bitmap, significantly outperforms existing binarization techniques and marks the first instance of a binary Post-Training Quantization method surpassing FP16 models of equivalent size.

Can LLMs Solve Longer Math Word Problems Better?
This study introduces Context Length Generalizability (CoLeG) by examining Large Language Models' (LLMs) ability to solve math word problems (MWPs) with extended narratives, using the newly developed Extended Grade-School Math (E-GSM) dataset. By proposing novel metrics and tailored approaches for both proprietary and open-source LLMs, the study enhances model performance on longer contexts, differentiating between semantic understanding and reasoning, and demonstrates generalizability across multiple benchmarks.

Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe Responses in LLMs
This paper addresses the issue of Large Language Models (LLMs) generating unsafe responses to toxic prompts by introducing a novel method that requires only a small, easily obtainable set of unsafe responses. By utilizing a semantic cost combined with a negative Earth Mover Distance (EMD) loss and proposing a novel lower bound for EMD loss, the authors effectively guide LLMs away from generating unsafe responses, achieving superior performance and data efficiency compared to traditional methods while analyzing potential over-alignment and language capability degradation.

OATS: Outlier-Aware Pruning Through Sparse and Low Rank Decomposition
This paper introduces OATS, a novel approach to compressing large transformers by approximating model weights as the sum of a sparse matrix and a low-rank matrix, while scaling weights with the second moment of input embeddings to preserve crucial outlier features. OATS achieves state-of-the-art performance by significantly compressing models such as Llama-3, Phi-3, and Google's ViT, enhancing inference speed on CPUs without retraining, and surpassing previous pruning methods.

Fine-tuning with Reserved Majority for Noise Reduction
The paper introduces a new fine-tuning framework called Parameter-Redundant Fine-Tuning (\preft), which aims to address the issue of redundancies and noisy hallucinations in LoRA tuning by reducing redundancies before merging parameters with pre-trained models. The proposed method, Noise reduction with Reserved Majority (\norm), utilizes singular value decomposition to enhance learning capacity and performance, outperforming existing methods in various tasks like instruction tuning, math reasoning, and code generation.

AgentRefine: Enhancing Agent Generalization through Refinement Tuning
This paper addresses the challenge of improving the generalization capabilities of large language model (LLM) based agents through a novel framework called AgentRefine, which enables models to learn from their mistakes by using environmental feedback. The proposed method significantly enhances the generalization ability and robustness of LLM agents compared to existing state-of-the-art approaches, offering a new paradigm for research in agent generalization and self-refinement.

Guaranteed Generation from Large Language Models
The paper addresses the challenge of ensuring strict constraint satisfaction in text generation from large language models (LLMs) without deviating significantly from the original model's distribution. It introduces GUARD, a method combining autoregressive proposal distribution with rejection sampling, which achieves perfect constraint satisfaction and maintains distributional closeness, validated through experiments on lexical constraints and sentiment reversal scenarios, demonstrating improved inference efficiency.

MiniPLM: Knowledge Distillation for Pre-training Language Models
MiniPLM is a knowledge distillation framework designed to improve pre-training efficiency, flexibility, and effectiveness in student language models by refining training data through the teacher model's knowledge. It enables offline teacher inference for multiple students, operates across model families, and enhances data difficulty and diversity, significantly boosting performance on downstream tasks and reducing computation costs.

Evaluating Large Language Models through Role-Guide and Self-Reflection: A Comparative Study
This paper introduces a novel strategy, Role-Guided and Self-Reflection (RoSe), to evaluate and enhance the self-knowledge of Large Language Models (LLMs) that have been fine-tuned with Reinforcement Learning from Human Feedback. By employing different roles and strong prompts combined with self-reflection, the study finds that role guidance helps LLMs reduce reliance on local information, leading to more accurate and faithful outputs, and proposes a double-calibrated strategy to refine open-source models based on these insights.

KOR-Bench: Benchmarking Language Models on Knowledge-Orthogonal Reasoning Tasks
This paper introduces Knowledge-Orthogonal Reasoning (KOR) and the Knowledge-Orthogonal Reasoning Benchmark (KOR-Bench) to assess models' reasoning abilities without relying on domain-specific knowledge. The benchmark includes various tasks and demonstrates its effectiveness by surpassing existing models, while also providing detailed analyses and exploring factors like dataset size and task interactions to enhance reasoning evaluations.

Is In-Context Learning Sufficient for Instruction Following in LLMs?
This paper investigates the effectiveness of in-context learning (ICL) alignment using the URIAL method, showing that it underperforms compared to instruction fine-tuning on benchmarks like MT-Bench and AlpacaEval 2.0, particularly when more capable base LLMs are used. By highlighting the importance of decoding parameters and the potential of incorporating more curated in-context demonstrations, the study advances the understanding of ICL's relationship to instruction fine-tuning and proposes improvements to achieve instruct model-like performance.

Test of Time: A Benchmark for Evaluating LLMs on Temporal Reasoning
This paper introduces novel synthetic datasets to evaluate the temporal reasoning abilities of large language models (LLMs) while addressing the limitations of relying on real-world data and anonymization techniques. The research provides insights into LLM strengths and weaknesses in temporal reasoning tasks and promotes further research by open-sourcing the datasets and evaluation framework.

Better autoregressive regression with LLMs via regression-aware fine-tuning
This paper presents a novel approach called regression-aware fine-tuning (RAFT), which utilizes the Bayes-optimal decision rule to enhance the performance of decoder-based large language models on regression tasks. By comparing various methods under a unified framework, the study shows that RAFT provides improvements over established baselines across multiple benchmarks and model families.

LongMamba: Enhancing Mamba's Long-Context Capabilities via Training-Free Receptive Field Enlargement
State space models (SSMs) are efficient alternatives to Transformer models for language modeling, offering linear computational complexity and constant memory usage, but they generally underperform in long-context understanding tasks. LongMamba, a training-free technique, enhances the long-context capabilities of Mamba models by mitigating memory decay in global channels, allowing for improved performance across long-context scenarios without additional training.

Maintaining Structural Integrity in Parameter Spaces for Parameter Efficient Fine-tuning
This paper introduces a generalized parameter-efficient fine-tuning framework designed to preserve the topological structure of high-dimensional parameter spaces within pre-trained foundation models. By utilizing a low-rank core space for dimensional changes, the method effectively models and reconstructs alterations while maintaining structural integrity, showing effectiveness across computer vision, natural language processing, and multi-modal tasks.

Style Outweighs Substance: Failure Modes of LLM Judges in Alignment Benchmarking
The study examines whether preferences judged by large language models (LLM-judges) align with more concrete alignment metrics like safety, world knowledge, and instruction following, revealing a lack of correlation and highlighting biases towards style over other factors. Introducing SOS-Bench, a large-scale LLM meta-benchmark, the authors demonstrate that supervised fine-tuning significantly influences alignment, with data scaling and prompt diversity being key contributors.

### Deep Learning->Other Representation Learning
Probabilistic Language-Image Pre-Training
This paper introduces Probabilistic Language-Image Pre-training (ProLIP), a novel probabilistic vision-language model that captures the many-to-many relationships between images and texts, improving uncertainty estimation and achieving strong zero-shot capabilities. By incorporating an "uncertainty token" and a new inclusion loss, ProLIP enhances downstream task performance and increases ImageNet accuracy using text uncertainties, demonstrating the practical benefits of a probabilistic approach in VLMs.

Can We Talk Models Into Seeing the World Differently?
Vision language models (VLMs) combine a large language model with a vision encoder, inheriting biases such as texture vs. shape bias from their vision components, yet multi-modality influences these biases. While VLMs inherently favor shape-based object recognition, they can be more effectively steered towards texture-based classifications using natural language prompts.

NUDGE: Lightweight Non-Parametric Fine-Tuning of Embeddings for Retrieval
NUDGE introduces non-parametric embedding fine-tuning methods that enhance the accuracy and efficiency of $k$-NN retrieval for dense vector embeddings, surpassing current fine-tuning approaches. By modestly adjusting embeddings to improve retrieval outcomes without significant distortion, NUDGE achieves superior performance across multiple models and datasets, offering notable accuracy improvements and faster run times compared to existing methods.

INFER: A Neural-symbolic Model For Extrapolation Reasoning on Temporal Knowledge Graph
This paper introduces INFER, a neural-symbolic model designed for extrapolation reasoning on Temporal Knowledge Graphs (TKGs), which enhances inference ability by considering the frequency and validity of historical facts through a Temporal Validity Function. INFER outperforms existing rule-based models by using Temporal Weight Matrices and a rule projection module, achieving state-of-the-art results on multiple datasets and demonstrating superior inference ability, especially on sparse TKG datasets.

### Deep Learning->Robustness
Severing Spurious Correlations with Data Pruning
This paper addresses the challenge of deep neural networks learning spurious correlations by identifying new scenarios where such spurious signals are weaker yet still lead to significant errors. The authors introduce a novel data pruning technique that effectively removes subsets of training data containing spurious features without needing domain knowledge or human intervention, achieving state-of-the-art results even in settings where spurious information is detectable.

Democratic Training Against Universal Adversarial Perturbations
This paper addresses the vulnerability of deep neural networks to universal adversarial perturbations (UAPs), which can deceive models without input-specific optimization, posing a significant risk to security-sensitive systems. The authors propose a defense method called "Democratic Training," which reduces attack success rates, enhances model robustness, and maintains accuracy by performing entropy-based model enhancements, as demonstrated on multiple neural networks and datasets.

Certified Robustness Under Bounded Levenshtein Distance
This paper introduces LipsLev, the first method for computing the Lipschitz constant of convolutional classifiers with respect to the Levenshtein distance, allowing for efficient robustness certification against adversarial text perturbations. The proposed method achieves significant speed improvements over existing approaches and demonstrates promising verified accuracy on the AG-News dataset, potentially facilitating more efficient verification in text classification tasks.

Provable Robust Overfitting Mitigation in Wasserstein Distributionally Robust Optimization
This paper introduces the Statistically Robust Wasserstein Distributionally Robust Optimization (WDRO) framework, which combines Wasserstein distance for adversarial noise with Kullback-Leibler divergence for statistical error to address robust overfitting. The proposed method not only provides robust generalization bounds but also improves performance against out-of-distribution adversarial examples and demonstrates enhanced robustness in extensive experimental validations.

Shifting the Paradigm: A Diffeomorphism Between Time Series Data Manifolds for Achieving Shift-Invariancy in Deep Learning
This paper addresses the lack of shift invariance in deep learning models for time series data by introducing a novel differentiable bijective function that maps samples to a high-dimensional manifold while preserving all task-relevant information. Empirical and theoretical analyses demonstrate that this method enhances performance and achieves complete shift-invariance across various tasks without altering model topology, with source code provided on GitHub.

### Deep Learning->Self-Supervised Learning
Learning Video-Conditioned Policy on Unlabelled Data with Joint Embedding Predictive Transformer
The paper introduces the Joint Embedding Predictive Transformer (JEPT), a method for training video-conditioned policies using a mix of labeled demonstrations and unlabeled expert videos, which reduces the demand for manual action annotation. JEPT excels in learning generalizable policies for tasks lacking action labels by jointly modeling visual transitions and inverse dynamics, outperforming baseline methods in various simulated visual control experiments and demonstrating its utility in enhancing video-conditioned policies.

### Misc
Unlocking Efficient, Scalable, and Continual Knowledge Editing with Basis-Level Representation Fine-Tuning
This paper introduces BaFT, a method for updating specific knowledge in large language models (LLMs) by applying input-dependent weighting to subspace representations, which enhances the editing-locality trade-off. Experiments on various LLMs and benchmarks demonstrate BaFT's effectiveness in maintaining irrelevant knowledge while successfully updating targeted information.

Do Stochastic, Feel Noiseless: Stable Stochastic Optimization via a Double Momentum Mechanism
This paper introduces a variant of Stochastic Gradient Descent (SGD) featuring a novel gradient estimator with a double-momentum mechanism, allowing for optimal convergence rates without sensitive dependence on learning rate selection. The approach removes the need for hyperparameter tuning by maintaining stability and robustness across both noiseless and noisy settings, as corroborated by empirical studies.

Can a Large Language Model be a Gaslighter?
This paper investigates the vulnerability of large language models (LLMs) to gaslighting attacks, which can manipulate users' mindsets through language. The authors propose a two-stage framework, DeepCoG, to elicit and acquire gaslighting conversations using LLMs, and demonstrate that prompt-based and fine-tuning-based attacks can transform LLMs into gaslighters, while their safety alignment strategies improve the models' robustness by 12.05% without significantly affecting utility.

Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents
LCoW is a framework designed to improve the performance of large language model-based agents in automating web tasks by transforming complex web pages into comprehensible formats, which aids in better decision making. The study demonstrates significant success rate improvements on several benchmarks, with notable gains for both closed-source and open-source language models, and provides state-of-the-art results on the WebShop benchmark.

MatryoshkaKV: Adaptive KV Compression via Trainable Orthogonal Projection
The paper addresses the bottleneck in KV cache storage and memory transfer for large language models by introducing a method to compress the feature dimension of cache tensors using low-rank projection matrices. Through an innovative Matryoshka learning strategy, the proposed method optimally tunes the projection matrix for continual pre-training or supervised fine-tuning, achieving over 90% performance with an average KV cache compression rate of 60%, thus providing an efficient tradeoff between performance and compression for models like LLaMA2 and Mistral.

Efficient Masked AutoEncoder for Video Object Counting and A Large-Scale Benchmark
This paper addresses the challenge of dynamic fore-background imbalance in video object counting due to sparse target objects by introducing a density-embedded Efficient Masked Autoencoder Counting (E-MAC) framework. The E-MAC utilizes a new Density-Embedded Masked mOdeling (DEMO) method for cross-modal regression guidance and spatial adaptive masking to focus on foreground regions while proposing a novel video bird counting dataset, DroneBird, to aid migratory bird protection, demonstrating superiority over existing methods through extensive experiments.

DiffPC: Diffusion-based High Perceptual Fidelity Image Compression with Semantic Refinement
This paper introduces Diffusion-based High Perceptual Fidelity Image Compression with Semantic Refinement (DiffPC), a new image compression framework leveraging stable diffusion models to achieve high-quality reconstructions at low bitrates. DiffPC employs a multi-feature compressor and a control module to ensure structural and textural consistency, demonstrating superior perceptual and statistical fidelity over previous methods.

DataEnvGym: Data Generation Agents in Teacher Environments with Student Feedback
DataEnvGym is introduced as a testbed designed to automate and enhance the data generation process for training models, framing it as a sequential decision-making task to eliminate the traditional dependence on labor-intensive human involvement. This platform supports diverse tasks across text, images, and actions, facilitating iterative improvement of student models by simulating environments that analyze feedback and provide structured, interpretable data generation, demonstrating potential advances in autonomous data generation agents.

qNBO: quasi-Newton Meets Bilevel Optimization
This paper introduces a general framework that addresses computational challenges in bilevel optimization by coordinating the solutions of the lower-level problem and the inverse Hessian-vector product through quasi-Newton algorithms. The approach leverages the superlinear convergence properties of BFGS, with numerical experiments showing its effectiveness in tasks like hyperparameter optimization, data hyper-cleaning, and few-shot meta-learning.

GReaTer: Gradients Over Reasoning Makes Smaller Language Models Strong Prompt Optimizers
The paper introduces *GReaTer*, a novel prompt optimization technique that integrates gradient information for improving task-specific reasoning in smaller language models, minimizing the need for expensive LLMs. Extensive evaluations demonstrate *GReaTer*'s superior performance across various tasks, often surpassing traditional approaches and enhancing prompt transferability, thereby significantly narrowing the capability gap between smaller and larger models.

Diverse Preference Learning for Capabilities and Alignment
This paper identifies a key issue with alignment algorithms like RLHF and DPO, which reduce the diversity of outputs from large language models (LLMs) by overemphasizing majority opinions due to the KL divergence regularizer. To address this, the authors propose Soft Preference Learning, which decouples the entropy and cross-entropy terms in the KL penalty, resulting in LLMs that achieve higher accuracy on challenging tasks, generate more diverse semantic and lexical content, and represent a broader range of societal perspectives with improved logit calibration.

L3Ms — Lagrange Large Language Models
This paper introduces Lagrange Large Language Models (L3Ms), a novel method for fine-tuning and aligning large language models through constrained optimization without relying on heuristic choices. By employing logarithmic barriers to meet application-specific requirements, L3Ms enable customization across diverse applications, demonstrating versatility and efficacy in achieving tailored alignments.

Selective Label Enhancement Learning for Test-Time Adaptation
The Progressive Adaptation with Selective Label Enhancement (PASLE) framework is introduced to improve test-time adaptation by assigning candidate pseudo-label sets to uncertain samples instead of definite labels, allowing for dynamic refinement and increased flexibility. By partitioning data into confident and uncertain subsets and using selective label enhancement, the PASLE approach effectively improves model performance on various benchmark datasets through progressive training and target adaptation.

CBQ: Cross-Block Quantization for Large Language Models
This paper introduces CBQ, a cross-block reconstruction-based post-training quantization method designed to address the performance challenges of quantizing large language models (LLMs) at ultra-low bit precision. By leveraging cross-block and intra-layer dependencies, CBQ achieves superior quantization accuracy and efficiency, significantly outperforming existing methods across various models and datasets.

Oscillatory State-Space Models
The paper introduces Linear Oscillatory State-Space models (LinOSS), inspired by the dynamics of biological neural networks, for efficient learning on long sequences through a stable discretization of harmonic oscillators. LinOSS is proven to be a universal model that ensures stable and accurate long-horizon forecasting, outperforming state-of-the-art sequence models in various time-series tasks, including a nearly 2x performance improvement over Mamba and LRU on sequences of length 50k.

Hyper-Connections
The paper introduces hyper-connections, an alternative to residual connections that mitigates issues like gradient vanishing and representation collapse by allowing dynamic adjustment of feature connections and layer rearrangements. Experiments demonstrate significant performance improvements in both language and vision tasks, suggesting broad applicability and benefits for various AI problems.

TC-MoE: Augmenting Mixture of Experts with Ternary Expert Choice
The Ternary Choice Mixture of Experts (TC-MoE) introduces a novel approach that expands the expert space via the ternary set {-1, 0, 1}, enhancing the efficiency and effectiveness of expert activations over traditional Top-K routing in MoE models. This method improves model performance by over 1.1% while reducing the number of activated experts by up to 9%, offering a scalable solution for large language models.

Rethinking Classifier Re-Training in Long-Tailed Recognition: Label Over-Smooth Can Balance
This study revisits classifier re-training methods in long-tailed recognition and proposes two new metrics, Logits Magnitude and Regularized Standard Deviation, to evaluate their effectiveness. By using these metrics, the authors develop a label over-smoothing approach that significantly improves model performance on imbalanced datasets such as CIFAR100-LT, ImageNet-LT, and iNaturalist2018, achieving state-of-the-art results without requiring prior class distribution knowledge.

Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs
This paper introduces a novel model merging strategy that focuses on independently merging submodules (e.g., layers, self-attentions, MLPs) within models to enhance multi-task capabilities. By exploiting the higher linearity of these submodules compared to the overall model, the proposed method achieves superior performance over standard task arithmetic and other baselines, offering a new approach for effective multi-task model merging.

Efficient Action-Constrained Reinforcement Learning via Acceptance-Rejection Method and Augmented MDPs
This paper introduces a novel and computationally efficient framework for action-constrained reinforcement learning (ACRL) that adapts an unconstrained RL method by using the acceptance-rejection method and an augmented two-objective Markov decision process (MDP) to enforce action constraints. The proposed framework demonstrates superior training speed, constraint satisfaction, and reduced action inference time compared to existing ACRL methods, with applications in robot control and resource allocation, and the source code is provided for further research.

PivotMesh: Generic 3D Mesh Generation via Pivot Vertices Guidance
PivotMesh is a novel framework for generating compact and finely detailed 3D meshes by employing a transformer-based autoencoder to manage mesh complexity through hierarchical token encoding and decoding. By effectively leveraging both small and large-scale datasets, PivotMesh proves versatile and enhances model controllability, thereby advancing the field of native mesh modeling with its ability to produce sharp and complex geometric structures.

Automated Proof Generation for Rust Code via Self-Evolution
SAFE is a framework designed to address the lack of human-written proofs necessary for automated proof generation in Rust code by creating a self-evolving cycle of data synthesis and fine-tuning. By leveraging a symbolic verifier to differentiate correct from incorrect proofs and enhancing self-debugging capabilities, SAFE significantly improves the accuracy of open-source models in writing proofs, achieving a 52.52% accuracy rate compared to GPT-4o’s 14.39%.

Language Models Need Inductive Biases to Count Inductively
This paper investigates whether various language model architectures, including RNNs, Transformers, State-Space Models, and RWKV, can learn to count and generalize counting inductively to out-of-distribution inputs. It presents empirical findings showing that while traditional RNNs excel in inductive counting, Transformers require positional embeddings to count out-of-distribution, revealing different inductive biases, and highlights a general performance decline in modern RNNs in balancing parallelized training with recurrent nature.

VisualPredicator: Learning Abstract World Models with Neuro-Symbolic Predicates for Robot Planning
This paper introduces Neuro-Symbolic Predicates, an abstraction language that integrates symbolic and neural representations to form task-specific abstractions in intelligent agents. The proposed approach demonstrates enhanced sample efficiency, improved out-of-distribution generalization, and increased interpretability compared to existing methods across varied robotic domains.

AgentSquare: Automatic LLM Agent Search in Modular Design Space
This paper introduces Modularized LLM Agent Search (MoLAS) and presents a novel framework called AgentSquare, which employs modular design and search mechanisms to optimize LLM agents. By outperforming hand-crafted agents with a 17.2% average performance gain across various benchmarks, AgentSquare provides interpretable insights and consolidates collective research efforts, enhancing the understanding and potential of agentic systems.

Actions Speak Louder Than Words: Rate-Reward Trade-off in Markov Decision Processes
This paper explores a novel approach to communication in decision-making systems by embedding messages into the actions of an agent within a Markov decision process, treating the environment as a communication channel. The proposed framework, Act2Comm, demonstrates an effective balance between maximizing rewards and enabling reliable communication, validated through experimental results.

SAVA: Scalable Learning-Agnostic Data Valuation
Selecting appropriate training data is crucial for machine learning model performance, particularly because large datasets often contain noise. This paper introduces *SAVA*, a scalable variant of the *LAVA* algorithm, that uses stochastic batch processing to perform optimal transport computations for data valuation efficiently, allowing it to handle large datasets without compromising on performance.

EMOS: Embodiment-aware Heterogeneous Multi-robot Operating System with LLM Agents
This paper introduces a novel multi-agent framework for heterogeneous multi-robot systems (HMRS) that enhances collaboration among robots with different embodiments and capabilities through a self-prompted "Robot Resume" approach. The framework includes a new benchmark, Habitat-MAS, to evaluate the system's ability in embodiment-aware reasoning tasks, showing that the hierarchical design and self-comprehension of robot capabilities significantly aid in solving complex tasks traditionally difficult for single robots.

An Efficient Framework for Crediting Data Contributors of Diffusion Models
This paper addresses the challenge of attributing the global properties of diffusion models to data contributors, which is essential for incentivizing quality data sharing and implementing fair data compensation. By introducing an efficient method for Shapley value estimation that uses model pruning and fine-tuning, the study demonstrates its effectiveness across various use cases, significantly outperforming existing attribution methods.

Exact Community Recovery under Side Information: Optimality of Spectral Algorithms
This paper addresses exact community recovery in two-community block models with node-attributed side information by developing a spectral algorithm that is simple yet optimal. Utilizing entrywise eigenvector analysis, the algorithm achieves results comparable to genie-aided estimators, offering a unified understanding of the optimality of spectral algorithms across diverse exact recovery problems.

Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens
This paper examines the scaling challenges of autoregressive models in text-to-image generation, comparing the efficacy of continuous vs. discrete tokens and random vs. raster generation orders. The study introduces the Fluid model, which uses continuous tokens and random generation, achieving superior performance with a state-of-the-art zero-shot FID on the MS-COCO 30K and highlighting differences in scalability between vision and language models.

SWE-bench Multimodal: Do AI Systems Generalize to Visual Software Domains?
The paper introduces SWE-bench Multimodal (SWE-bench M), a benchmark designed to evaluate autonomous systems' ability to fix bugs in visual, user-facing JavaScript software, thus addressing the limitations of the existing SWE-bench that focuses solely on Python repositories. The study reveals that common systems struggle with cross-language generalization and visual problem-solving, yet highlights SWE-agent’s superiority in handling these challenges, outperforming alternatives with a 12% task instance resolution rate.

Revisiting Energy Based Models as Policies: Ranking Noise Contrastive Estimation and Interpolating Energy Models
This paper revisits energy-based models (EBMs) as a viable policy class for robotic learning, challenging the notion that they are impractical in high-dimensional spaces. By introducing a novel training objective combining ranking noise contrastive estimation, learnable negative samplers, and non-adversarial joint training, the authors prove the asymptotic consistency of their approach, outperforming diffusion models in multi-modal tasks like obstacle avoidance and block pushing.

Web Agents with World Models: Learning and Leveraging Environment Dynamics in Web Navigation
This study addresses the limitations of current LLM-based web agents in executing long-horizon tasks by introducing a World-model-augmented (WMA) web agent that simulates potential outcomes to enhance decision-making. Through innovative transition-focused observation abstraction, the proposed method improves policy selection and achieves greater cost- and time-efficiency compared to existing approaches, as demonstrated in experiments on WebArena and Mind2Web.

OmnixR: Evaluating Omni-modality Language Models on Reasoning across Modalities
OmnixR is an evaluation suite designed to assess state-of-the-art Omni-modality Language Models (OLMs) by benchmarking their ability to handle tasks involving multiple modalities like text, vision, and audio. The suite provides two variants—OmnixR-synth, an automatically generated synthetic dataset, and OmnixR-real, a manually curated dataset—to rigorously evaluate and highlight the challenges OLMs face in cross-modal reasoning and integration.

MUSE: Machine Unlearning Six-Way Evaluation for Language Models
The paper introduces MUSE, a benchmark designed to evaluate machine unlearning algorithms based on six criteria, including the prevention of memorization and privacy leakage, and the preservation of model utility. The evaluation of eight popular unlearning algorithms on large language models reveals that while most algorithms can limit memorization, they often struggle with privacy issues, utility preservation, and scalability, highlighting significant practical challenges in current unlearning techniques.

BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval
BRIGHT is introduced as a novel text retrieval benchmark focusing on complex real-world queries that require extensive reasoning beyond traditional keyword or semantic matching. The dataset includes 1,398 queries across various domains, demonstrating that even advanced retrieval models struggle with these tasks, but performance significantly improves when explicit reasoning is incorporated, highlighting the potential for enhanced retrieval systems in challenging environments.

Learned Reference-based Diffusion Sampler for multi-modal distributions
This paper introduces the Learned Reference-based Diffusion Sampler (LRDS), a new methodology designed to improve sampling from multi-modal distributions without precise hyperparameter tuning, which often requires ground truth samples. By learning a reference diffusion model in high-density regions and using it to enhance the training of a diffusion-based sampler, LRDS effectively exploits prior knowledge to outperform existing sampling methods on complex distributions.

SV4D: Dynamic 3D Content Generation with Multi-Frame and Multi-View Consistency
Stable Video 4D (SV4D) introduces a unified latent video diffusion model capable of generating temporally consistent novel view videos of dynamic 3D objects from a monocular reference video. This method bypasses the need for complex optimization processes, offering state-of-the-art performance in novel-view video synthesis and 4D generation, as demonstrated by extensive experiments and user studies.

Risk-Controlling Model Selection via Guided Bayesian Optimization
This paper introduces a method that combines Bayesian Optimization with risk-controlling procedures to optimize machine learning hyperparameters, balancing user-specified risk limits with various conflicting metrics such as accuracy and fairness. The approach identifies Pareto optimal configurations and statistically verifies them, effectively managing multiple desiderata like error rates and computational costs, as demonstrated across various tasks.

Zigzag Diffusion Sampling: Diffusion Models Can Self-Improve via Self-Reflection
This paper addresses the challenges in text-to-image diffusion models related to maintaining high image quality and prompt-image alignment, proposing a novel method called **diffusion self-reflection** to better capture semantic information. Additionally, the authors introduce Zigzag Diffusion Sampling (Z-Sampling), which significantly enhances the generation quality of diffusion models and can be easily integrated with existing models and methods, showing improved results across various benchmarks.

SoftMatcha: A Soft and Fast Pattern Matcher for Billion-Scale Corpus Searches
This paper introduces a novel algorithm for soft pattern matching that leverages word embeddings to overcome the limitations of existing string matching and dense vector search techniques, which struggle with orthographic variations and paraphrasing in natural language. The proposed method is highly scalable and efficient, enabling rapid searches on large corpora, and has been successfully demonstrated on English, Japanese, and Latin datasets through a user-friendly web tool.

TokenFormer: Rethinking Transformer Scaling with Tokenized Model Parameters
The paper introduces Tokenformer, a scalable architecture that improves the flexibility and efficiency of transformer models by treating model parameters as tokens, enabling interactions between tokens and model parameters through an attention mechanism. This approach allows the model to scale incrementally without retraining, significantly reducing computational costs while maintaining performance comparable to traditional transformers.

Global Convergence in Neural ODEs: Impact of Activation Functions
This paper explores the impact of activation function properties—smoothness and nonlinearity—on the training dynamics of Neural Ordinary Differential Equations (ODEs). By establishing the global convergence of Neural ODEs under gradient descent in overparameterized regimes, the authors provide theoretical insights validated by experiments, offering practical guidelines for scaling Neural ODEs to enhance training efficiency and performance.

GameGen-X: Interactive Open-world Game Video Generation
GameGen-$\mathbb{X}$ is the first diffusion transformer model developed for generating and interactively controlling open-world game videos, offering high-quality and open-domain creation by simulating game elements like characters and environments. By assembling the largest dataset for this purpose, OGameData, and employing a specialized training process, it advances open-world game design by merging generative models with interactive capabilities, potentially supplementing traditional rendering methods.

BlendRL: A Framework for Merging Symbolic and Neural Policy Learning
BlendRL is a neuro-symbolic reinforcement learning framework that effectively combines symbolic reasoning and intuitive responses, addressing the limitations of traditional RL systems. By harmonizing both paradigms, BlendRL outperforms neural and symbolic baselines in Atari environments, demonstrating enhanced agent capabilities and robustness to environmental changes.

Feedback Schrödinger Bridge Matching
This paper introduces Feedback Schrödinger Bridge Matching (FSBM), a semi-supervised matching framework designed to balance scalability and supervision in diffusion bridges for distribution transport problems. By using less than 8% of pre-aligned pairs as state feedback in an Entropic Optimal Transport formulation, FSBM significantly improves training efficiency and generalization, outperforming traditional methods that either lack scalability or require full supervision.

Order-aware Interactive Segmentation
This paper introduces OIS: order-aware interactive segmentation, which improves the segmentation of objects by encoding the relative depth between them through order maps. By utilizing a novel order-aware attention mechanism and an object-aware attention module, OIS significantly enhances segmentation accuracy and speed, achieving state-of-the-art results on the HQSeg44K and DAVIS datasets.

PT-T2I/V: An Efficient Proxy-Tokenized Diffusion Transformer for Text-to-Image/Video-Task
The Proxy-Tokenized Diffusion Transformer (PT-DiT) addresses the redundant computation in global self-attention mechanisms by using sparse representative token attention, significantly reducing the number of tokens required to model global visual information. This approach, augmented with window and shift window attention, enhances detail modeling and demonstrates competitive performance in image and video generation tasks, achieving substantial reductions in computational complexity compared to existing models.

RegMix: Data Mixture as Regression for Language Model Pre-training
RegMix is proposed as an automatic method for identifying optimal data mixtures for pre-training large language models by treating the selection as a regression task. This approach not only surpasses human selection and DoReMi in resource efficiency but also reveals that web corpora contribute more to performance than traditionally high-quality data, demonstrating that data mixture effects are intricate and independent of scaling laws.

Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG
This paper examines the phenomenon where the quality of generated outputs from long-context language models initially improves but then declines with an increasing number of retrieved passages, identifying the impact of retrieved "hard negatives" as a contributing factor. To counteract this, the authors propose both training-free and training-based solutions, including retrieval reordering and specific RAG-oriented fine-tuning methods, demonstrating significant performance enhancements and offering insights into optimal design choices for these approaches.

Enhancing End-to-End Autonomous Driving with Latent World Model
The paper introduces a novel self-supervised learning approach called the LAtent World model (LAW) for end-to-end autonomous driving, which enhances scene feature representation by predicting future latent scene features from current features and ego trajectories. LAW integrates with both perception-free and perception-based frameworks, achieving state-of-the-art results on various benchmarks like nuScenes, NAVSIM, and CARLA, with plans to release the code publicly.

Mixture of Parrots: Experts improve memorization more than reasoning
The paper investigates the performance trade-offs between Mixture-of-Experts (MoE) architecture and standard dense transformers, demonstrating that as the number of experts increases, memorization improves but reasoning capabilities stagnate. It theoretically and empirically shows that while MoEs excel in memory-intensive tasks with a large number of experts, dense models are more effective for tasks requiring reasoning, validated across synthetic and benchmark tests in math and language.

GIFT: Unlocking Full Potential of Labels in Distilled Dataset at Near-zero Cost
This paper introduces GIFT, a simple yet effective approach that enhances dataset distillation by utilizing full label information and employing a cosine similarity-based loss function. GIFT improves state-of-the-art methods across various datasets, notably increasing cross-optimizer generalization performance, as demonstrated by a 30.8% improvement over existing methods like RDED on ImageNet-1K.

Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders
This paper addresses the challenge of weak generalization in existing time series anomaly detection methods, which usually require training on specific datasets, by proposing a model called DADA (General time series anomaly Detector with Adaptive Bottlenecks and Dual Adversarial Decoders). DADA is pre-trained on diverse multi-domain datasets and demonstrates strong performance as a zero-shot anomaly detector across different target datasets by using adaptive bottlenecks and dual decoders to effectively differentiate between normal and abnormal patterns.

Understanding Optimization in Deep Learning with Central Flows
This paper introduces the concept of "central flows," a differential equation that models the time-averaged trajectory of oscillatory optimizers in deep learning, providing insights into their complex dynamics. The study shows that central flows can accurately predict long-term optimization paths and reveal underlying mechanisms of gradient descent and adaptive optimizers, offering a promising tool for understanding optimization in deep learning.

Single-agent Poisoning Attacks Suffice to Ruin Multi-Agent Learning
This paper explores the robustness of multi-agent learning algorithms in strongly monotone games, emphasizing their vulnerability to adversaries capable of poisoning a single agent's utility observations. It identifies a trade-off between convergence speed and robustness, demonstrating that faster algorithms are more susceptible to attacks, and presents an attack strategy that misguides these algorithms using a sublinear corruption budget, marking the first study to characterize this efficiency-robustness trade-off in multi-agent learning.

Consistency Models Made Easy
Consistency models (CMs) provide faster sampling than traditional diffusion models but are resource-intensive to train. This paper introduces Easy Consistency Tuning (ECT), a method that fine-tunes CMs from pretrained diffusion models to improve efficiency and performance drastically, achieving superior results in significantly less time, and revealing that CMs adhere to classic power law scaling.

Sharpness-Aware Minimization: General Analysis and Improved Rates
This paper addresses outstanding questions regarding the convergence of Sharpness-Aware Minimization (SAM) in non-convex settings by providing a unified analysis of SAM and its unnormalized variant under a flexible update rule, termed Unified SAM. The analysis establishes convergence guarantees for Unified SAM using a relaxed noise assumption and arbitrary sampling, applicable to non-convex problems that fulfill the Polyak-Lojasiewicz condition, and experiments corroborate the theoretical insights and demonstrate its efficacy in training neural networks for image classification.

AdaRankGrad: Adaptive Gradient Rank and Moments for Memory-Efficient LLMs Training and Fine-Tuning
This paper presents a novel approach to address the memory and computational challenges in training large language models by adaptively reducing the rank of gradients using an efficient online-updating low-rank projection during Adam optimization steps. The proposed method not only decreases memory usage compared to existing techniques but also improves performance, with demonstrated benefits in both language and biological model training and fine-tuning.

Unlocking State-Tracking in Linear RNNs Through Negative Eigenvalues
Linear Recurrent Neural Networks (LRNNs) like Mamba and DeltaNet, traditionally constrained by positive eigenvalue ranges, struggle with state-tracking tasks, impacting their performance on challenges such as code evaluation. This paper demonstrates that by extending the eigenvalue range to include negative values, LRNNs can effectively solve state-tracking tasks like parity, enabling scalable, competitive performance in language modeling and promising results in code and math tasks.

On Designing General and Expressive Quantum Graph Neural Networks with Applications to MILP Instance Representation
This paper introduces the General Quantum Graph Learning Architecture (GQGLA), which utilizes quantum circuits to improve graph learning for complex tasks like mixed-integer linear programming (MILP). GQGLA outperforms traditional graph neural networks by effectively capturing and learning MILP representations, offering superior discriminative capabilities and better generalization across various problem instances.

P-SPIKESSM: HARNESSING PROBABILISTIC SPIKING STATE SPACE MODELS FOR LONG-RANGE DEPENDENCY TASKS
This paper introduces a scalable probabilistic spiking learning framework for long-range dependency tasks, utilizing state space models to overcome the limitations of conventional leaky integrate-and-fire (LIF) neurons in spiking neural networks. By incorporating a SpikeSampler layer for stochastic spike generation and enhancing inter-neuron communication with the SpikeMixer and ClampFuse layers, the proposed model achieves state-of-the-art performance on various benchmarks and demonstrates computational efficiency through sparse spiking patterns.

REFINE: Inversion-Free Backdoor Defense via Model Reprogramming
Backdoor attacks on deep neural networks pose a significant security threat, and current pre-processing-based defenses have limitations in balancing model utility and defense performance. This paper introduces REFINE, an inversion-free defense method using model reprogramming with components for input transformation and output remapping, successfully enhancing defense capabilities and maintaining utility, as demonstrated by extensive experiments on benchmark datasets.

Conformal Language Model Reasoning with Coherent Factuality
Language models must produce correct outputs, especially when used in decision-making processes. This paper introduces a method to ensure "coherent factuality" by applying split conformal prediction to deducibility graphs, effectively improving factuality in reasoning tasks, achieving 90% factuality on a stricter definition, while retaining over 80% of the original claims in mathematical reasoning evaluations.

From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data
This paper presents a finetuning approach using a synthetic dataset of numerical key-value retrieval tasks to enhance the information retrieval and reasoning capabilities of Large Language Models (LLMs) in long-context scenarios. The experiments demonstrate significant improvements in LLMs like GPT-3.5 Turbo and Mistral 7B, showing a transfer of skills to real tasks and maintaining general benchmark performance without inducing hallucinations, thus underscoring the value of finetuning on synthetic data.

Adversarial Machine Unlearning
This paper addresses machine unlearning by framing it as a game-theoretic problem, where unlearners remove specific data from models while auditors use membership inference attacks (MIAs) to detect remaining traces. The proposed adversarial framework effectively integrates MIA advancements into the design of unlearning algorithms, using implicit differentiation to enhance unlearning efficacy, supported by empirical results.

Centrality-guided Pre-training for Graph
The paper introduces the Centrality-guided Graph Pre-training (CenPre) framework, which enhances graph representations by integrating node importance, based on graph theory centrality, into node representations. By employing modules for node and graph-level importance learning and representation alignment, CenPre effectively captures structural nuances, leading to superior performance in node classification, link prediction, and graph classification tasks over existing methods.

When do GFlowNets learn the right distribution?
This paper explores the limitations and challenges of Generative Flow Networks (GFlowNets) in sampling accurately from target distributions over discrete and compositional objects such as graphs. It assesses how imbalances in network flow affect the model's sampling accuracy, recognizes constraints imposed by graph neural network parameterizations, and proposes a new metric that improves evaluation of GFlowNets compared to existing methods.

GOttack: Universal Adversarial Attacks on Graph Neural Networks via Graph Orbits Learning
This study introduces GOttack, an innovative adversarial attack framework that strategically exploits graph topology to effectively disrupt Graph Neural Networks (GNNs). GOttack not only outperforms existing adversarial methods by achieving the highest average misclassification rates across multiple GNN architectures, but it also completes training more efficiently, revealing critical insights into GNN robustness and the impact of graph topology on their vulnerability.

Complexity Lower Bounds of Adaptive Gradient Algorithms for Non-convex Stochastic Optimization under Relaxed Smoothness
This paper investigates the complexity lower bounds of several adaptive optimization algorithms, like AdaGrad, under the $(L_0, L_1)$-smoothness condition and reveals that these algorithms exhibit at least a quadratic dependence on problem parameters such as the initial optimality gap and smoothness constants. The study demonstrates that finding an $\epsilon$-stationary point in this setting requires a significantly higher number of stochastic gradient queries compared to the $L$-smooth setting, establishing the fundamental difficulty posed by the $(L_0, L_1)$-smoothness condition for some adaptive algorithms.

Continuity-Preserving  Convolutional Autoencoders for Learning Continuous Latent Dynamical Models from Images
This paper introduces continuity-preserving convolutional autoencoders (CpAEs) to address the issue of discontinuous latent coordinates in dynamical systems when trajectories are captured as image data. By ensuring the continuity of latent states through improved convolutional filters, CpAEs enhance the accuracy of latent dynamical models as demonstrated through extensive experiments.

GMValuator: Similarity-based Data Valuation for Generative Models
This paper addresses data valuation in generative models by proposing GMValuator, a novel training-free and model-agnostic method for valuating data in image generation tasks. GMValuator utilizes a similarity matching approach to efficiently assess training data contributions and demonstrates robustness across various datasets and generative models, filling a critical gap in the evaluation of generative model data valuation methods.

MA$^2$E: Addressing Partial Observability in Multi-Agent Reinforcement Learning with Masked Auto-Encoder
This paper introduces the Multi-Agent Masked Auto-Encoder (MA$^2$E), designed to address the challenge of partial observability in cooperative multi-agent reinforcement learning (MARL) by inferring global information from local observations without the need for communication. By integrating MA$^2$E into existing MARL algorithms, the approach has been experimentally validated for enhanced effectiveness across various environments and algorithms within the Centralized Training and Decentralized Execution (CTDE) framework.

Robust Representation Consistency Model via Contrastive Denoising
The paper introduces a novel approach to enhance the robustness of deep neural networks by reformulating generative modeling as a discriminative task, focusing on instance discrimination within latent space for efficient denoising and classification. This method significantly reduces computational overhead during inference, achieving state-of-the-art performance on datasets like ImageNet, outperforming existing diffusion-based methods in certified accuracy and greatly decreasing inference costs.

EIA: ENVIRONMENTAL INJECTION ATTACK ON GENERALIST WEB AGENTS FOR PRIVACY LEAKAGE
This study explores the privacy risks associated with generalist web agents in adversarial environments, introducing the Environmental Injection Attack (EIA) that targets personally identifiable information (PII) on compromised websites. The research demonstrates that EIA can achieve high attack success rates in stealing PII, highlighting the difficulty in detecting and mitigating such attacks, and calls for the development of advanced defense strategies beyond human supervision.

Can Watermarks be Used to Detect LLM IP Infringement For Free?
This paper investigates the use of LLM watermarks to detect intellectual property infringement in language models, addressing challenges such as the impact of query selection on detection and watermark instability. The proposed method, LIDet, successfully utilizes anchor LLMs and adaptive detection thresholds, achieving over 90% accuracy in distinguishing between infringing and clean models, thus proving the feasibility of watermark-based IP protection for LLMs.

Causal Order: The Key to Leveraging Imperfect Experts in Causal Inference
Large Language Models (LLMs) and human experts often struggle to accurately distinguish between direct and indirect effects when inferring causal graphs using pairwise prompts, leading to significant errors. This study introduces a triplet prompting strategy, incorporating auxiliary variables to enhance stability and cycle reduction, demonstrating improved causal order accuracy and robustness with smaller models, thereby enhancing downstream discovery and effect inference tasks.

Captured by Captions: On Memorization and its Mitigation in CLIP Models
This paper investigates the role of memorization in multi-modal models like CLIP and proposes a formal definition, CLIPMem, to quantify it. The study finds that CLIP's memorization behavior is intermediate between supervised and self-supervised paradigms, with the text encoder contributing more; strategies are suggested to reduce memorization while enhancing model utility, challenging traditional views that reducing memorization decreases utility.

Comparing Targeting Strategies for Maximizing Social Welfare with Limited Resources
This paper evaluates the effectiveness of machine learning-informed targeting strategies by comparing treatment effect-based targeting to risk-based targeting using data from five real-world randomized controlled trials (RCTs). The study finds that treatment effect-based targeting significantly outperforms risk-based targeting when treatment effects are estimated reliably, highlighting the need for careful model training and validation to harness the potential benefits of treatment effect targeting in social domains.

OSTQuant: Refining Large Language Model Quantization with Orthogonal and Scaling Transformations for Better Distribution Fitting
This paper introduces a novel metric called Quantization Space Utilization Rate (QSUR) to evaluate the efficiency of post-training quantization techniques applied to large language models. The proposed method, OSTQuant, utilizes a learnable orthogonal and scaling transformation to optimize data distributions, significantly improving quantization performance and retaining high accuracy on challenging configurations compared to existing state-of-the-art methods.

Task Descriptors Help Transformers Learn Linear Models In-Context
This paper investigates how task descriptors enhance in-context learning (ICL) in large language models (LLMs) within a linear regression setting. It demonstrates that a linear Transformer converges to a global minimum using gradient flow and effectively utilizes task descriptors to improve performance, with empirical verification showing improved outcomes when task descriptors are included.

Advancing Prompt-Based Methods for Replay-Independent General Continual Learning
The paper introduces MISA (Mask and Initial Session Adaption), an innovative approach to enhance prompt-based methods in General Continual Learning (GCL), which suffers from poor initial performance and catastrophic forgetting. MISA uses forgetting-aware session adaption and non-parametric logit masking to achieve significant performance improvements over recent competitors and offers advantages like easy implementation and replay independence, establishing itself as a robust baseline for GCL research.

Improving Probabilistic Diffusion Models With Optimal Diagonal Covariance Matching
This paper introduces a novel method for learning diagonal covariances in probabilistic diffusion models using a technique called Optimal Covariance Matching (OCM). By regressing the optimal analytic covariance, the method reduces approximation error and significantly enhances sampling efficiency, recall rate, and likelihood in both diffusion and latent diffusion models.

Variational Diffusion Posterior Sampling with Midpoint Guidance
This paper presents a novel approach to tackle the challenges of sampling from denoising posterior distributions in Bayesian inverse problems by decomposing transition processes, enabling a balance between the intractable guidance term and prior transitions. Extensive experiments demonstrate the method's effectiveness, particularly in reconstructing electrocardiograms from partial data for precise cardiac diagnosis.

Unlocking the Power of Function Vectors for Characterizing and Mitigating Catastrophic Forgetting in Continual Instruction Tuning
This study addresses catastrophic forgetting (CF) in large language models (LLMs) during continual learning by exploring how model forgetting is influenced by training tasks and the models themselves. The authors propose a function vector guided training methodology with a regularization technique to mitigate CF, supported by empirical tests on four benchmarks that validate their theoretical findings on CF and model function dynamics.

Imputation for prediction: beware of diminishing returns.
This study investigates the impact of imputation methods on predictive performance, particularly focusing on whether advanced imputation techniques significantly enhance predictions. Findings indicate that in scenarios with expressive models and when using missingness indicators, the quality of imputation has minimal influence on real-data outcomes, suggesting limited benefits from investing in sophisticated imputation techniques for prediction enhancement.

Provably Safeguarding a Classifier from OOD and Adversarial Samples
This paper introduces SPADE, a method for converting trained classifiers into abstaining classifiers that are provably safeguarded against out-of-distribution and adversarial samples using a Generalized Extreme Value model. Empirical tests on various neural architectures and datasets show SPADE's efficiency and robustness, outperforming existing methods.

InstaRevive: One-Step Image Enhancement via Dynamic Score Matching
InstaRevive is an innovative image enhancement framework that leverages score-based diffusion distillation to achieve high-quality results with reduced sampling steps. By employing dynamic noise control and incorporating textual prompts as auxiliary conditions, InstaRevive effectively taps into the potential of pre-trained diffusion models, as demonstrated by extensive experiments across various challenging tasks and datasets.

Group-robust Sample Reweighting for Subpopulation Shifts via Influence Functions
The paper addresses the challenge of uneven performance in machine learning models due to subpopulation shifts by introducing Group-robust Sample Reweighting (GSR), a method that optimizes weights of group-unlabeled data using influence functions. GSR is presented as a theoretically sound and efficient two-stage approach outperforming existing strategies in enhancing robustness to subpopulation shifts, even with limited group-labeled data.

InstantSwap: Fast Customized Concept Swapping across Sharp Shape Differences
InstantSwap is a novel Customized Concept Swapping method designed to efficiently swap concepts in images while maintaining both foreground and background consistency, especially when there are significant shape differences. By utilizing automated bbox extraction, a cross-attention mechanism, and periodic gradient computation, InstantSwap enhances efficiency without compromising performance, and its effectiveness is demonstrated through extensive evaluations on a newly established benchmark dataset.

Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback
This paper examines the limitations of reinforcement learning from human feedback (RLHF), a crucial method for training AI systems to align with human objectives, particularly in state-of-the-art large language models. It surveys open problems, discusses techniques for enhancing RLHF, and proposes standards for auditing to improve societal oversight, underscoring the need for a comprehensive approach to creating safer AI systems.

Needle Threading: Can LLMs Follow Threads Through Near-Million-Scale Haystacks?
This paper investigates the capabilities of 17 leading Large Language Models (LLMs) in handling long-context information retrieval tasks, revealing that while many models can track multiple information threads effectively, their performance declines as the context length increases. The study emphasizes the importance of not directly comparing token counts from different tokenizers and provides insights by releasing their code and experimental data on long context models.

AvatarGO: Zero-shot 4D Human-Object Interaction Generation and Animation
This paper introduces AvatarGO, a novel framework designed to address the limitations of existing diffusion models in generating 4D full-body human-object interactions (HOI) by utilizing a zero-shot approach with a pre-trained diffusion model. AvatarGO improves the generation and animation of 4D HOI scenes from textual inputs through LLM-guided contact retargeting and correspondence-aware motion optimization, showing superior performance in creating coherent and robust human-object interactions compared to existing methods.

BiGR: Harnessing Binary Latent Codes for Image Generation and Improved Visual Representation Capabilities
BiGR is a groundbreaking conditional image generation model that combines generation and discrimination within a single framework using binary latent codes. It introduces innovative components like a binary tokenizer and entropy-ordered sampling, demonstrating superior generation quality and representation capabilities, while enabling zero-shot generalization in tasks such as image manipulation and text-to-image generation.

ProAdvPrompter: A Two-Stage Journey to Effective Adversarial Prompting for LLMs
This paper introduces $\texttt{ProAdvPrompter}$, a novel two-stage method designed to enhance the performance of adversarial prompters, which identify vulnerabilities in large language models to jailbreaking attacks. By successfully increasing attack success rates significantly and reducing training time, $\texttt{ProAdvPrompter}$ demonstrates its effectiveness with improved adversarial suffix generation and resilience against perplexity-based defenses.

CBGBench: Fill in the Blank of Protein-Molecule Complex Binding Graph
The paper introduces CBGBench, a comprehensive benchmark for structure-based drug design (SBDD) that standardizes the task as generative graph completion, addressing the challenges of diverse settings and difficult reproducibility in AI-driven drug discovery. By categorizing methods, evaluating multiple sub-tasks, and providing a unified codebase, CBGBench facilitates a fair evaluation and highlights the potential for improvements in network architectures and the incorporation of chemical knowledge.

HShare: Fast LLM Decoding by Hierarchical Key-Value Sharing
This paper introduces HShare, a hierarchical framework that optimizes Key-Value (KV) cache data retrieval in large language models by facilitating the sharing of critical KV cache token indices across layers, heads, and queries. HShare significantly reduces computational overhead and enhances efficiency, achieving up to 8.6× speedup in self-attention operations and a 2.7× improvement in end-to-end throughput compared to existing methods.

Ada-K Routing: Boosting the Efficiency of MoE-based LLMs
This paper introduces a novel Ada-K routing strategy for Mixture-of-Experts (MoE) architectures in large language models, which dynamically adjusts the number of activated experts for each token to enhance computational efficiency and model performance. The proposed method, leveraging learnable allocator modules and the Proximal Policy Optimization algorithm, significantly reduces FLOPs and inference time while outperforming standard Top-K routing on various benchmarks, and is efficiently trainable with broad applicability across MoE-based systems.

A Tight Convergence Analysis of Inexact Stochastic Proximal Point Algorithm for Stochastic Composite Optimization Problems
The inexact stochastic proximal point algorithm (isPPA) is extended in this study to provide stability and almost sure convergence without requiring strong convexity or smoothness of the objective function, which are common restrictive assumptions in current models. By introducing a local Lipschitz condition and a quadratic growth condition, the paper presents tight iteration complexity bounds for isPPA and supports these theoretical findings with numerical experiments.

Restating the Proof of Linear Convergence for Linear GNNs
This paper revises the core proof of a pioneering study on the training dynamics of linear GNNs, providing a more concise and approachable version while correcting an identified hidden error that does not affect the main result. It also includes a discussion on the strengths and an overlooked aspect of the original approach.

Diffusion Models Are Real-Time Game Engines
GameNGen is a novel game engine fully powered by a neural model, allowing for high-quality, real-time interaction with complex environments over extended gameplay sessions. By training on the game DOOM, it can generate realistic, playable environments with next-frame predictions achieving a PSNR of 29.4, making human raters struggle to distinguish between actual gameplay and simulations after prolonged usage.

Latent Space Chain-of-Embedding Enables Output-free LLM Self-Evaluation
This paper introduces the Chain-of-Embedding (CoE) method for output-free self-evaluation of large language models (LLMs) by analyzing the latent thinking paths represented by progressive hidden states during inference. The study demonstrates across diverse domains and models that CoE can effectively differentiate between correct and incorrect responses, offering real-time, label-free feedback with minimal computational cost, and providing novel insights into LLM response correctness.

Asymmetric Factorized Bilinear Operation for Vision Transformer
This paper introduces an Asymmetric Factorized Bilinear Operation (AFBO) to replace the feed-forward network (FFN) in vision transformers (ViTs) for better performance and complexity trade-offs. By leveraging second-order statistics and implementing structured-sparsity channel mapping strategies, AFBO enhances token feature learning and significantly reduces computational complexity, showing improvements in generalization and robustness across various ViTs without compromising on performance.

Adapters for Altering LLM Vocabularies: What Languages Benefit the Most?
VocADT is a novel method for vocabulary adaptation in pre-trained language models using adapter modules to learn optimal linear combinations of embeddings without altering model weights. This approach, tested on 11 languages, enhances multilingual tasks performance such as natural language understanding and machine translation, outperforming existing models and benefiting highly fragmented and Latin-script languages the most.

Learning Spatial-Semantic Features for Robust Video Object Segmentation
This paper introduces a robust video object segmentation framework designed to address challenges in tracking and segmenting multiple similar objects in long-term videos through the use of spatial-semantic features and discriminative object queries. The framework achieves state-of-the-art performance on several datasets, demonstrating its effectiveness and generalization capacity, with plans to release all source code and trained models for public access.

ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models
Zeroth-order Intrinsic-dimensional Prompt-tuning (ZIP) offers a novel, efficient approach to prompt optimization in black-box vision-language models by reducing problem dimensionality and variance of gradient estimates, requiring fewer queries. ZIP outperforms existing methods, improving few-shot accuracy by approximately 6% and query efficiency by 48% on over 13 benchmarks, establishing a new state of the art without extensive hyperparameter tuning.

Provable Uncertainty Decomposition via Higher-Order Calibration
This paper introduces a method for decomposing predictive uncertainty into aleatoric and epistemic components with formal guarantees, using a new concept called higher-order calibration. Unlike previous work, this method provides meaningful uncertainty decompositions without assumptions on the data distribution and is applicable to existing models like Bayesian and ensemble models, with demonstrated effectiveness in tasks like image classification.

PFDiff: Training-Free Acceleration of Diffusion Models Combining Past and Future Scores
Diffusion Probabilistic Models (DPMs) face challenges in sampling efficiency due to numerous denoising steps, which PFDiff addresses by introducing a training-free timestep-skipping strategy that reduces the number of function evaluations (NFE) and corrects for discretization errors. PFDiff demonstrates improved performance over existing methods, as shown by significantly better FID scores on ImageNet 64x64 and Stable Diffusion, and offers a flexible approach applicable to various pre-trained DPMs.

Learning to Help in Multi-Class Settings
This paper extends the Learning to Help (L2H) model for deploying machine learning models on resource-constrained devices from binary to multi-class classification, enhancing practical applications where server access might be limited. By introducing a stage-switching surrogate loss function consistent with the Bayes rule, the authors demonstrate an efficient solution for balancing local and server-side computing resources in various scenarios.

Joint Gradient Balancing for Data Ordering in Finite-Sum Multi-Objective Optimization
This paper introduces JoGBa, a novel sample ordering method that enhances convergence rates in finite-sum multi-objective optimization by jointly balancing the gradients of all objectives. Theoretical and empirical analyses show that JoGBa outperforms random ordering and other strategies, leading to faster convergence and better performance across various algorithms and datasets.

Gradient descent with generalized Newton’s method
We introduce the generalized Newton's method (GeN), a Hessian-informed technique that enhances any optimizer, such as SGD and Adam, by dynamically selecting an optimal learning rate to accelerate convergence without extensive learning rate tuning. Our approach is easily implemented with minimal computational cost and demonstrates state-of-the-art performance on language and vision tasks like GPT and ResNet.

Flow With What You Know
This paper offers an accessible introduction to flow-matching and rectified flow models used in generative AI, addressing the complexity of traditional probability-math-heavy descriptions. By highlighting the models' foundational basis in basic physics, the authors provide an alternative, more comprehensible framework for understanding these generative AI models.

Youku Dense Caption: A Large-scale Chinese Video Dense Caption Dataset and Benchmarks
The paper introduces Youku Dense Caption, the first large-scale, high-quality Chinese dense video captioning dataset, consisting of 31,466 videos and 311,921 captions from Youku. This dataset serves as a crucial resource for advancing Chinese multi-modal models and establishes benchmarks for various video-language tasks, proven effective through extensive evaluations with current state-of-the-art models.

Quantized Spike-driven Transformer
This paper introduces a quantized spike-driven Transformer (QSD-Transformer) designed to minimize resource demands of spiking neural networks (SNNs), addressing the challenge of performance degradation caused by spike information distortion (SID) during quantization. By implementing a bi-level optimization strategy inspired by mutual information entropy, the proposed method achieves state-of-the-art results with significant reductions in power consumption and model size, maintaining high performance in various visual tasks.

Palu: KV-Cache Compression with Low-Rank Projection
The paper introduces Palu, a KV-Cache compression framework for large language models (LLMs) that reduces inference-time memory usage through low-rank projection, achieving up to 50% compression without loss of accuracy. By incorporating medium-grained low-rank decomposition, an efficient rank search algorithm, quantization compatibility enhancements, and optimized GPU kernels, Palu outperforms traditional quantization methods, offering significant speed improvements and maintaining or improving accuracy.

Improved Sampling Algorithms for Lévy-Itô Diffusion Models
This paper presents an advancement in denoising diffusion models by introducing a parametric family of stochastic differential equations that use isotropic α-stable noise, improving image generation quality on imbalanced datasets compared to Gaussian-based models, especially with fewer reverse diffusion steps. Additionally, the study highlights the applicability of Lévy-Itô diffusion models in various domains, such as text-to-speech tasks, where they may outperform standard models on highly imbalanced datasets.

UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models
UniCon is a novel architecture that enhances the training of adapters for large-scale diffusion models by implementing a unidirectional flow from the diffusion network to the adapter, eliminating the need for the model to compute and store gradients. This reduces GPU memory usage by one-third and increases training speed by 2.3 times, enabling efficient training of adapters with double the parameter volume, while demonstrating precise control and exceptional capabilities in image conditional generation tasks.

PETRA: Parallel End-to-end Training with Reversible Architectures
This paper introduces PETRA, a novel method for parallelizing gradient computations in deep learning using reversible architectures, effectively addressing challenges in model parallelism. PETRA enables independent computation across different devices without weight stashing, achieving competitive accuracy on standard vision benchmarks with ResNet models.

Bridging the Gap between Database Search and \emph{De Novo} Peptide Sequencing with SearchNovo
SearchNovo is a novel framework that enhances peptide sequencing from mass spectrometry data by integrating the strengths of database search and \emph{de novo} sequencing. By using an efficient search mechanism and a fusion module, it outperforms existing models, even handling missing peaks and noisy reference peptides effectively, as demonstrated through comprehensive evaluations on multiple datasets.

Don't Take Things Out of Context: Attention Intervention for Enhancing Chain-of-Thought Reasoning in Large Language Models
This paper addresses the disruption in few-shot Chain-of-Thought (CoT) reasoning in large language models (LLMs) due to isolated segments that introduce irrelevant noise. It proposes a Few-shot Attention Intervention (FAI) method that dynamically adjusts attention patterns in demonstrations, significantly improving coherence and accuracy, as shown by a 5.91% enhancement on the AQuA dataset.

Scaling Laws for Precision
This paper introduces "precision-aware" scaling laws that account for the impact of low precision training and post-training quantization on language model quality and cost. The authors demonstrate that training in lower precision reduces the effective parameter count, and propose a unified scaling law for quantization effects on both training and inference, validated through extensive pretraining runs and model size predictions.

GETS: Ensemble Temperature Scaling for Calibration in Graph Neural Networks
Graph Ensemble Temperature Scaling (GETS) is introduced as a novel calibration framework designed to address the calibration challenges in Graph Neural Networks (GNNs) by leveraging a Graph Mixture-of-Experts (MoE) architecture. By effectively integrating diverse inputs and selecting relevant experts for node calibration, GETS reduces expected calibration error by 25% across various benchmarks, outperforming existing methods while remaining efficient and scalable.

EmbedLLM: Learning Compact Representations of Large Language Models
EmbedLLM is a novel framework designed to learn compact vector representations of large language models, improving the efficiency of utilizing these models across various downstream tasks such as model routing. It uses an encoder-decoder approach to outperform existing methods in predicting model performance over multiple benchmarks, capturing essential model characteristics without additional inference costs, and is complemented by open-source resources for further research.

Toward Generalizing Visual Brain Decoding to Unseen Subjects
This study explores the generalization capability of visual brain decoding to unseen subjects by utilizing a large image-fMRI dataset from 177 subjects. The research demonstrates that a uniform learning paradigm enhances generalization across subjects, influenced by subject similarity, and points toward the potential of developing a foundational brain decoding model with larger datasets.

Benchmarking Predictive Coding Networks -- Made Simple
This paper addresses efficiency and scalability issues in predictive coding networks (PCNs) by introducing a performance-oriented open-source library and a comprehensive set of benchmarks for standardized experimentation. By testing both current PCN algorithms and adapted bio-plausible methods on these benchmarks, the research achieves state-of-the-art results, investigates the limitations of PCNs, and outlines future research directions, with the aim of enhancing scalability in the field.

On the Optimal Memorization Capacity of Transformers
This paper demonstrates that Transformers are capable of efficiently memorizing labels with $\tilde{O}(\sqrt{N})$ parameters in a next-token prediction context, as well as in sequence-to-sequence settings where $\tilde{O}(\sqrt{nN})$ parameters are needed. The research highlights the efficiency of self-attention mechanisms in processing input sequences and identifies the feed-forward network as a limiting factor in label association per token.

Class Distribution-induced Attention Map for Open-vocabulary Semantic Segmentations
This paper addresses the challenge of open-vocabulary semantic segmentation, improving the localization of class-related objects by proposing Class Distribution-induced Attention Map (CDAM). By using the Jensen-Shannon divergence between class distributions of similar patches and integrating this attention scheme into the CLIP model, the method enhances zero-shot semantic segmentation performance and can be combined synergistically with existing approaches for substantial improvements.

TDDBench: A Benchmark for Training data detection
This paper introduces TDDBench, a comprehensive benchmark encompassing 13 datasets across image, tabular, and text data modalities to evaluate 21 Training Data Detection (TDD) methods, also known as Membership Inference Attacks (MIA). By assessing these methods from multiple perspectives, including detection performance, memory, and computational efficiency, TDDBench reveals areas for improvement in TDD algorithms and aids practitioners in evaluating trade-offs in algorithm selection, with the resource made publicly available for the research community.

Distribution-Specific Agnostic Conditional Classification With Halfspaces
This paper explores selective classification using sparse linear classifiers in agnostic settings, focusing on subsets defined by halfspaces. It introduces the first PAC-learning algorithm with error guarantees for homogeneous halfspace selectors and highlights the computational difficulty of approximating conditional classification loss under Gaussian distributions.

What Has Been Overlooked in Contrastive Source-Free Domain Adaptation: Leveraging Source-Informed Latent Augmentation within Neighborhood Context
This paper addresses source-free domain adaptation (SFDA) by conducting a theoretical analysis based on contrastive learning, which has shown superior performance over other techniques. The authors propose a novel latent augmentation method to improve contrastive SFDA, achieving state-of-the-art results on benchmark datasets by enhancing the informativeness of positive keys through the dispersion of latent features.

miniCTX: Neural Theorem Proving with (Long-)Contexts
We present $\texttt{miniCTX}$, a benchmark that evaluates a model's capability to prove theorems based on new, unseen context by using formal mathematical theorems from Lean projects. Our findings indicate that context-conditioned fine-tuning and prompting methods significantly outperform traditional approaches, and we introduce the $\texttt{ntp-toolkit}$ to facilitate the automatic extraction and annotation of theorem proving data for extending $\texttt{miniCTX}$.

You Only Sample Once: Taming One-Step Text-to-Image Synthesis by Self-Cooperative Diffusion GANs
YOSO is a novel generative model that addresses issues in combining diffusion models with GANs by enabling high-fidelity one-step image synthesis with improved training stability and mode coverage. By utilizing techniques like latent perceptual loss and a latent discriminator, YOSO achieves state-of-the-art performance in one-step generation, efficiently scaling from 512 to 1024 resolution without additional training.

Trajectory-LLM: A Language-based Data Generator for Trajectory Prediction in Autonomous Driving
Trajectory-LLM is introduced as a novel approach for predicting vehicle trajectories by utilizing Large Language Models (LLMs) to generate realistic trajectory data from textual descriptions of vehicle interactions, addressing the high cost of acquiring real-world data. By aligning driving behaviors with text-based descriptions and employing the newly created Language-to-Trajectory (L2T) dataset, Traj-LLM enhances prediction model performance across diverse map topologies and public benchmarks.

Decoupled Graph Energy-based Model for Node Out-of-Distribution Detection on Heterophilic Graphs
This paper addresses the challenges of Out-of-Distribution (OOD) detection on nodes in graph learning by introducing the Decoupled Graph Energy-based Model (DeGEM), which overcomes issues of energy propagation and heterophily inherent in existing methods. By utilizing Maximum Likelihood Estimation (MLE) and decoupling the learning process, DeGEM surpasses previous methods by improving AUROC scores significantly on both homophilic and heterophilic graphs, even outperforming models with prior OOD exposure.

OptiBench Meets ReSocratic: Measure and Improve LLMs for Optimization Modeling
This paper introduces **OptiBench**, a benchmark designed to evaluate large language models' (LLMs) abilities in solving complex optimization problems, including linear and nonlinear programming, using human-readable inputs and outputs. Additionally, the authors propose a data synthesis method called ***ReSocratic*** to address data scarcity; this method facilitates improved performance in open-source models through the creation and utilization of the ***ReSocratic-29k*** dataset for supervised fine-tuning.

To Code or Not To Code? Exploring Impact of Code in Pre-training
This paper systematically investigates the impact of incorporating code data in the pre-training mix of large language models (LLMs), evaluating its effects on a diverse set of downstream tasks beyond code generation. The study finds that integrating code data significantly enhances model performance in natural language reasoning, world knowledge, and generative tasks, emphasizing the importance of code quality and suggesting that such incorporation yields up to 8.2% improvements in natural language tasks and a 12x boost in code performance compared to text-only pre-training.

Towards Multiple Character Image Animation Through Enhancing Implicit Decoupling
This paper introduces a novel multi-condition guided framework to enhance the performance of character image animation, addressing challenges related to stability in complex backgrounds and multiple characters. By utilizing an optical flow guider, depth order guider, and reference pose map, the proposed method improves the implicit decoupling of background and character dynamics, and a new benchmark is established to evaluate its effectiveness, demonstrating superior animation quality in challenging scenarios.

GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for Generalized 3D Manipulation
GravMAD is a novel sub-goal-driven, language-conditioned action diffusion framework that enhances robot learning by integrating imitation learning with foundation models to tackle unseen 3D manipulation tasks. By breaking tasks into language-informed sub-goals and using GravMaps for flexible spatial guidance, GravMAD achieves significant improvements over state-of-the-art methods in both novel and familiar tasks, demonstrating robust multi-task learning and generalization as verified by empirical evaluations and real-world implementations. Video demonstrations are available at: https://gravmad.github.io.

DUET: Decentralized Bilevel Optimization without Lower-Level Strong Convexity
This paper introduces DUET, a novel decentralized bilevel optimization (DBO) algorithm that eliminates the need for lower-level strong convexity (LLSC) by using diminishing quadratic regularization, making it suitable for practical scenarios that do not satisfy LLSC. The DUET algorithm also addresses data heterogeneity with gradient tracking and achieves promising iteration complexity for approximate KKT-stationary point convergence, being the first to do so without LLSC in decentralized settings.

Bandit Learning in Matching Markets with Indifference
This paper addresses the challenge of learning unknown preferences in matching markets where participants often have comparable performance levels, making it difficult to establish strict preference rankings. The authors propose an adaptive exploration algorithm called AE-AGS, which effectively minimizes stable regret even under conditions of indifferent preferences, with performance superior to existing methods as demonstrated in extensive experiments.

Predicting the Energy Landscape of Stochastic Dynamical System via  Physics-informed Self-supervised Learning
This paper introduces a physics-informed self-supervised learning approach to infer energy landscapes from system evolution trajectories without needing true energy values. By mapping system states to a discrete landscape space and integrating energy into a graph neural Fokker-Planck equation, this method achieves over 0.9 correlation with true energy values and improves evolution prediction accuracy by 17.65% compared to baselines.

Neural Phylogeny: Fine-Tuning Relationship Detection among Neural Networks
This paper introduces the task of neural phylogeny detection to identify parent-child relationships in a collection of neural networks, assessing whether models are fine-tuned derivatives of others. The authors propose both a learning-free method utilizing parameter distance metrics and a learning-based method with a transformer-based detector, demonstrating their effectiveness across various networks and tasks through extensive experiments.

Test-time Adaptation for Cross-modal Retrieval with Query Shift
This paper addresses the query shift problem in cross-modal retrieval, where query distributions diverge from the source domain. The authors propose a novel method, Test-time adaptation for Cross-modal Retrieval (TCR), which utilizes a module to refine query predictions and a joint objective to maintain common space stability, achieving effective adaptation against query shift as demonstrated by extensive experiments.

LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token
LLaVA-Mini is an efficient large multimodal model that minimizes the number of vision tokens required by fusing visual information into text tokens before processing, thereby reducing computational overhead. Experiments demonstrate that it outperforms previous models with a significant reduction in computational cost, processing images and videos efficiently with just one vision token and achieving low-latency responses across various benchmarks.

GenXD: Generating Any 3D and 4D Scenes
This paper addresses the challenges of 3D and 4D visual generation by introducing a data curation pipeline to extract camera poses and object motion, culminating in the creation of the CamVid-30K dataset. The authors propose the GenXD framework, featuring multiview-temporal modules and masked latent conditions, to effectively generate both 3D and 4D scenes, outperforming previous methods in extensive evaluations.

A Large-Scale 3D Face Mesh Video Dataset via Neural Re-parameterized Optimization
NeuFace is a novel 3D face mesh pseudo annotation method for videos that utilizes neural re-parameterized optimization to address the challenge of generating reliable 3D face labels in dynamic, in-the-wild settings. The study introduces the NeuFace-dataset, demonstrating its effectiveness in enhancing 3D face reconstruction accuracy and learning 3D facial motion priors by ensuring image-aligned facial details and consistent annotations across large-scale videos.

Needle In A Video Haystack: A Scalable  Synthetic Evaluator for Video MLLMs
VideoNIAH is a novel benchmark construction framework that utilizes synthetic video generation to more efficiently evaluate multimodal large language models (MLLMs) in video understanding. By decoupling video content from query-responses, this method allows for skill-specific evaluations and creates the VNBench benchmark, which is used to assess models on tasks like retrieval, ordering, and counting, ultimately offering insights and guidance for improving MLLM training and development.

Making Transformer Decoders Better Differentiable Indexers
The paper introduces URI, a unified framework for retrieval and indexing that enhances the consistency between index construction and the retrieval model using a Transformer decoder. By simultaneously building the index and training the decoder, URI outperforms existing methods on three real-world datasets, moving beyond traditional item representation in Euclidean space to dynamically construct indices within the interactive space between queries and items.

UniCoTT: A Unified Framework for Structural Chain-of-Thought Distillation
UniCoTT is a unified CoT distillation framework designed to enhance small language models (SLMs) by integrating diverse structural chains of thought, including chains, trees, and graphs. By employing iterative construction and structural constraint strategies, UniCoTT successfully transfers reasoning capabilities from large language models (LLMs) to SLMs, yielding improved performance on various NLP tasks.

Identification of Intermittent Temporal Latent Process
This paper introduces intermittent temporal latent processes to better identify and model time-delayed causal dynamics, where certain latent factors may become inactive or irrelevant at different times. The proposed unsupervised approach, InterLatent, demonstrates the capability to uncover these processes reliably, supported by both theoretical claims and experimental validation on synthetic and real-world datasets.

Few-Class Arena: A Benchmark for Efficient Selection of Vision Models and Dataset Difficulty Measurement
Few-Class Arena (FCA) is introduced as a unified benchmark to evaluate efficient image classification models in scenarios with few classes (2-10), addressing the gap left by many-class benchmarks (80-1000 classes) that are less relevant to real-world applications. By systematically evaluating a range of models, including ResNet and various CNNs and Transformers, on subsets of ImageNet and other datasets, FCA also incorporates a new class similarity-based difficulty measure, offering a versatile tool for improving model selection and performance evaluation in the Few-Class Regime.

Painting with Words: Elevating Detailed Image Captioning with Benchmark and Alignment Learning
This paper introduces DeCapBench and a novel evaluation metric, DCScore, to improve the assessment of detailed image captioning by focusing on hallucinations and comprehensiveness through primitive information units. The study shows that DCScore aligns better with human judgment than existing metrics, and FeedQuill, an automatic feedback collection method, further enhances performance across multiple vision-language models by reducing hallucinations and outperforming benchmarks, including GPT-4o.

Feedback Favors the Generalization of Neural ODEs
This paper introduces feedback neural networks, which incorporate feedback loops to enhance the generalization of neural ordinary differential equations (neural ODEs) by correcting learned latent dynamics. The approach demonstrates robust performance across new scenarios without losing accuracy on prior tasks, and extensive testing, including trajectory prediction and model predictive control, shows significant advancements over existing methods.

Synergy Between Sufficient Changes and Sparse Mixing Procedure for Disentangled Representation Learning
This paper addresses the challenges of disentangled representation learning by integrating auxiliary variable-based distribution changes and structural sparsity assumptions, demonstrating these can complement each other in simplifying latent variable estimation. The authors propose a new identifiability theory with relaxed constraints and introduce a generative model framework, validated by experiments on both synthetic and real-world datasets, showcasing its practical applicability.

Adversaries With Incentives:  A Strategic Alternative to Adversarial Robustness
The paper introduces a novel approach called "strategic training," which reframes adversarial training by modeling opponents as entities pursuing their own goals instead of being solely destructive. By incorporating strategic modeling tools, the method allows for incorporating knowledge about opponents' incentives, potentially enhancing learning and performance when the incentive uncertainty set is smaller, as demonstrated through various experiments.

VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks
This paper introduces MMEB (Massive Multimodal Embedding Benchmark) and VLM2Vec, a contrastive training framework designed to create universal multimodal embeddings capable of handling diverse downstream tasks. By leveraging VLM2Vec based on state-of-the-art vision-language models, their method demonstrates a significant 10% to 20% improvement over current multimodal embedding models, highlighting the strength of VLMs in embedding tasks.

On the Price of Differential Privacy for Hierarchical Clustering
This paper introduces a novel algorithm for differentially private hierarchical clustering using the weight privacy model, achieving an $O(\log^{1.5}n/\varepsilon)$ multiplicative error, which surpasses the known impossibility results in edge-level differential privacy. The algorithm, verified through experiments on both synthetic and real-world datasets, demonstrates effectiveness in managing privacy with enhanced approximation and scalability for large graphs, while a new lower bound of $\tilde{\Omega}(1/\varepsilon)$ additive error is established for weight-level differential privacy.

LancBiO: Dynamic Lanczos-aided Bilevel Optimization via Krylov Subspace
The paper introduces a novel approach to bilevel optimization by constructing low-dimensional Krylov subspaces using the Lanczos process to efficiently approximate the Hessian inverse vector product, which is a known bottleneck in hyper-gradient computation. This subspace-based framework, the first of its kind applied to bilevel problems, achieves a convergence rate of $\mathcal{O}(\epsilon^{-1})$ and demonstrates improved efficiency in both synthetic and deep learning tasks.

Humanizing the Machine: Proxy Attacks to Mislead LLM Detectors
This paper introduces a humanized proxy-attack (HUMPA) strategy that uses a reinforcement learning fine-tuned small language model to trick large language model detection systems into misclassifying machine-generated text as human-written. The study demonstrates that HUMPA significantly reduces the effectiveness of leading detectors across various datasets and scenarios, achieving up to a 95.0% drop in detection capability, while maintaining the quality of generated text.

What Matters When Repurposing Diffusion Models for General Dense Perception Tasks?
This paper introduces GenPercept, a novel deterministic one-step fine-tuning paradigm for dense visual perception tasks that leverages diffusion priors for faster and more efficient inference compared to traditional multi-step methods. Through comprehensive experiments, the study highlights the importance of high-quality fine-tuning data and task-specific supervision in enhancing the performance of tasks such as monocular depth estimation and image segmentation.

From an LLM Swarm to a PDDL-empowered Hive: Planning Self-executed Instructions in a Multi-modal Jungle
The paper introduces Hive, a system that plans and executes explainable actions using multiple deep models in response to user instructions, ensuring adherence to user constraints with a formal logic backbone enabled by PDDL operations. The authors also present the MuSE benchmark to evaluate multi-modal agent systems and demonstrate that Hive sets a new standard in task selection, outperforming other systems in transparency and constraint adherence.

Exploring the Design Space of Visual Context Representation in Video MLLMs
This paper explores the design space for visual context representation in video Multimodal Large Language Models (MLLMs) to enhance video semantic understanding by improving frame and token selection schemes. It formulates visual context representation as a constrained optimization problem, conducts extensive experiments to derive optimal settings for frame and token selection, and validates these settings' alignment with empirical results. Data and code are available at https://github.com/RUCAIBox/Opt-Visor.

AnalogGenie: A Generative Engine for Automatic Discovery of Analog Circuit Topologies
AnalogGenie is a generative engine designed to automate the design and discovery of analog circuit topologies, addressing challenges such as the lack of comprehensive datasets and effective representation methods in this domain. By introducing a foundational dataset and a scalable graph representation method, AnalogGenie enhances the variety and complexity of analog IC designs, significantly outperforming prior methodologies and contributing to the evolution of automated analog IC design.

INCLUDE: Evaluating Multilingual Language Understanding with Regional Knowledge
This paper addresses the challenge of performance disparity among large language models (LLM) in different languages, which limits their utility in non-English speaking regions. It introduces INCLUDE, an evaluation suite comprising 197,243 QA pairs across 44 languages, providing a culturally and linguistically relevant benchmark to better assess multilingual LLMs in the environments where they are intended to be used.

Learning a Neural Solver for Parametric PDEs to Enhance Physics-Informed Methods
The paper addresses optimization challenges in physics-informed deep learning by introducing a novel solver that learns to condition a gradient descent algorithm for solving partial differential equations (PDEs). This method improves acceleration and stability of the optimization process and uniquely handles parametric PDEs by integrating the physical loss gradient with PDE parameters, as demonstrated through empirical experiments.

Strong Preferences Affect the Robustness of Preference Models and Value Alignment
This paper explores the robustness of value alignment in AI systems by analyzing how preference models reflect human values, particularly examining the sensitivity of models like Bradley-Terry and Plackett-Luce to shifts in preference probabilities. The study finds that changes in dominant preferences can significantly impact model predictions, highlighting important implications for ensuring the safety and reliability of AI systems in alignment with human values.

Explanations of GNN on Evolving Graphs via Axiomatic  Layer edges
This paper addresses the challenge of explaining how Graph Neural Networks (GNNs) predictions change due to evolving edge weights in graphs, which is critical for understanding and trust. A novel framework is proposed that uses axiomatic attribution and Shapley value to map contributions effectively, with experiments on multiple datasets showing superior fidelity and interpretability compared to baseline methods.

MuPT: A Generative Symbolic Music Pretrained Transformer
This paper investigates the use of Large Language Models (LLMs) for pre-training in music, highlighting the improved compatibility of LLMs with ABC Notation over MIDI for musical composition. It introduces the Synchronized Multi-Track ABC Notation (SMT-ABC Notation) to maintain coherence across tracks, and explores the Symbolic Music Scaling Law (SMS Law), providing models that manage up to 8192 tokens and open-source resources for ongoing research in music generation.

Emergent Orientation Maps —— Mechanisms, Coding Efficiency and Robustness
This study investigates the development of distinct neuronal orientation preferences in primary visual cortex, namely "salt-and-pepper" and pinwheel structures, using a self-evolving spiking neural network model with Hebbian plasticity. The research identifies key factors influencing these topologies and reveals that pinwheel structures offer advantages like reduced wiring costs and enhanced sparse coding, suggesting implications for visual processing and the development of brain-inspired deep learning technologies.

Omni-MATH: A Universal Olympiad Level Mathematic Benchmark for Large Language Models
Recent advancements in large language models have highlighted the limitations of current benchmarks in evaluating mathematical reasoning, as they are solved with high accuracy. To address this, we introduce a challenging benchmark with 4,428 Olympiad-level mathematics problems to better assess the reasoning capabilities of LLMs, revealing significant performance challenges even for advanced models like OpenAI o1-mini and OpenAI o1-preview.

Prevalence of Negative Transfer in Continual Reinforcement Learning: Analyses and a Simple Baseline
This paper addresses the negative transfer problem in Continual Reinforcement Learning (CRL), highlighting its significance and the inability of recent methods to effectively tackle it. The authors propose a novel method called Reset & Distill (R&D), which resets and distills agent networks, demonstrating superior performance over existing approaches through experiments on Meta World tasks, thereby underscoring the necessity of robust strategies like R&D to combat negative transfer in CRL.

Value-aligned Behavior Cloning for Offline Reinforcement Learning via Bi-level Optimization
This paper introduces Value-aligned Behavior Cloning via Bi-level Optimization (VACO), a novel framework for offline reinforcement learning that improves Behavior Cloning by integrating an inner loop for weighted supervision with an outer loop for value alignment. VACO effectively balances behavior policy and value estimation, leading to superior performance in continuous control benchmarks compared to existing methods.

A Solvable Attention for Neural Scaling Laws
This paper investigates the neural scaling laws specifically for transformer architectures by proposing a framework for linear self-attention that learns in an in-context manner, modeled as a non-linear ODE system. By reformulating the system as a Riccati equation, the authors derive precise characterizations of neural scaling laws for linear self-attention, revealing similarities and differences with other architectures based on context sequence length.

Following the Human Thread in Social Navigation
The paper introduces the Social Dynamics Adaptation (SDA) model for improving human-robot collaboration in Social Navigation through a two-stage Reinforcement Learning framework. By encoding human trajectories as social dynamics and utilizing previous actions and statuses for real-time adaptation, the SDA model achieves state-of-the-art performance in human following on the Habitat 3.0 platform.

Online Clustering with Nearly Optimal Consistency
This paper presents online algorithms for $k$-Means and $(k, z)$-Clustering that achieve nearly optimal consistency, transforming any $\alpha$-approximate offline algorithm into a $(1+\epsilon)\alpha^2$-competitive online algorithm with $O(k \text{poly} \log n)$ consistency. By using an exact offline algorithm, it also introduces the first $(1 + \epsilon)$-competitive online algorithm with linear in $k$ consistency, improving previous work, and demonstrates effectiveness on real datasets with the $k$-Means++ algorithm.

Factual Context Validation and Simplification: A Scalable Method to Enhance GPT Trustworthiness and Efficiency
This paper presents a new framework to improve the accuracy and efficiency of retrieval-augmented generation (RAG) pipelines for Large Language Models by combining LLM summarization, DBSCAN clustering, and vectorized fact storage. The proposed method achieves a significant reduction in storage size while maintaining high factual accuracy and retrieval effectiveness, enhancing the reliability of AI systems in practical applications.

Multi-Task Corrupted Prediction for Learning Robust Audio-Visual Speech Representation
This paper introduces CAV2vec, a self-supervised speech representation learning framework designed to handle joint audio-visual corruption, such as lip occlusions and blurred videos, in speech recognition tasks. By employing a unique unimodal multi-task learning strategy, CAV2vec effectively predicts clean targets from corrupted inputs and significantly improves recognition accuracy in noisy and corrupt conditions as demonstrated on robust AVSR benchmarks.

How Do Large Language Models Understand Graph Patterns? A Benchmark for Graph Pattern Comprehension
This paper introduces a comprehensive benchmark to evaluate the capabilities of large language models (LLMs) in understanding and discovering graph patterns, a significantly underexplored area in fields such as computational chemistry and social network analysis. The study reveals that while LLMs demonstrate preliminary abilities in graph pattern tasks with certain models like O1-mini excelling, performance can be enhanced by formatting input data to align with pretrained knowledge, highlighting their diverse algorithmic approaches and execution-dependent performance.

Intelligence at the Edge of Chaos
This paper examines how the complexity of rule-based systems affects the capabilities of models trained to predict these rules by using elementary cellular automata (ECA) and training Large Language Models (LLMs) on them. The study finds that models trained on more complex rules exhibited greater intelligence, as shown by better performance in reasoning and chess move prediction tasks, suggesting that exposure to an optimal level of complexity is key to developing intelligence.

Connectome Mapping: Shape-Memory Network via Interpretation of Contextual Semantic Information
The paper introduces the Shape Memory Network (SMN), a novel neural network designed to emulate the brain's connectome and incorporate contextual semantic information during the inference process, surpassing conventional neural networks' limitations. Through comprehensive testing across semantic segmentation benchmarks, the SMN demonstrated superior performance and effectiveness, indicating its potential for advancing next-generation neural networks.

SOO-Bench: Benchmarks for Evaluating the Stability of Offline Black-Box Optimization
This paper introduces SOO-Bench, a set of benchmarks designed to evaluate the stability of offline black-box optimization algorithms, emphasizing surpassing offline datasets across various real-world tasks and data distributions. By providing a stability indicator and assessing different algorithms, SOO-Bench aims to advance the development of stable offline optimization methods, especially in fields such as satellites, materials science, structural mechanics, and automobile manufacturing.

RFWave: Multi-band Rectified Flow for Audio Waveform Reconstruction
RFWave is a novel multi-band Rectified Flow approach for reconstructing high-fidelity audio waveforms from Mel-spectrograms or discrete acoustic tokens, achieving remarkable computational efficiency by operating at the frame level and processing all subbands simultaneously. The method significantly surpasses traditional diffusion models, enabling audio generation up to 160 times faster than real-time with only 10 sampling steps, as demonstrated through empirical evaluations.

OmniSep: Unified Omni-Modality Sound Separation with Query-Mixup
Omni-modal Sound Separation (OmniSep) introduces a novel framework for isolating clean soundtracks using omni-modal queries, optimizing both single-modal and multi-modal composed queries effectively. By incorporating strategies like Query-Mixup and Query-Aug, OmniSep achieves state-of-the-art performance in text-, image-, and audio-queried sound separation tasks, as demonstrated on datasets like MUSIC, VGGSOUND-CLEAN+, and MUSIC-CLEAN+.

Layerwise Recurrent Router for  Mixture-of-Experts
The paper introduces the Layerwise Recurrent Router for Mixture-of-Experts (RMoE), which uses a Gated Recurrent Unit (GRU) to improve routing decisions across consecutive layers, addressing the parameter inefficiency of existing MoE models. Empirical evaluations show that RMoE-based language models outperform various baselines, offering enhanced expert selection and diversity through effective cross-layer information sharing.

VoxDialogue: Can Spoken Dialogue Systems Understand Information Beyond Words?
Current spoken dialogue systems often ignore essential multi-modal audio information, which can result in loss of context and suboptimal responses. This paper introduces VoxDialogue, a benchmark designed to evaluate the understanding of acoustic cues beyond text in dialogue systems, highlighting the necessity of integrating audio signals for improved performance and providing open-source resources for further research.

Preference Diffusion for Recommendation
PreferDiff is a novel optimization objective specifically designed for diffusion model (DM)-based recommenders, transforming the traditional Bayesian Personalized Ranking objective into a log-likelihood generative framework to effectively capture user preferences with multiple negative samples. By employing variational inference and replacing mean squared error with cosine error, PreferDiff enhances ranking performance, accelerates convergence, and demonstrates superior recommendation performance across extensive benchmarks.

Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs
Min-p sampling is a dynamic truncation method proposed to improve the balance between quality and diversity in text generated by Large Language Models, particularly at higher temperatures, by adjusting the sampling threshold based on the model's confidence. Extensive experiments and human evaluations show that min-p sampling outperforms traditional methods like top-p sampling, leading to its adoption by major open-source LLM implementations, thus underscoring its practical utility and impact.

Restyling Unsupervised Concept Based Interpretable Networks with Generative Models
The paper introduces a novel method for enhancing the interpretability of prediction models by mapping high-level concept features to the latent space of a pretrained generative model, facilitating improved visualization and interactive understanding of learnt concepts. The approach is tested on large-scale image recognition benchmarks, demonstrating its effectiveness in terms of interpretability accuracy, reconstruction fidelity, and concept consistency, while also improving training efficiency.

ELICIT: LLM Augmentation Via External In-context Capability
The paper introduces ELICIT, a framework that enhances the adaptive capabilities of large language models by externally storing and reusing task vectors, eliminating the need for additional training or inference tokens. This approach significantly boosts model performance, versatility, and scalability across various tasks and architectures, demonstrating a path towards more efficient use of large language models.

SymmCD: Symmetry-Preserving Crystal Generation with Diffusion Models
SymmCD is a novel diffusion-based generative model designed to address the limitations of existing crystal generation methods by explicitly incorporating crystallographic symmetry into the generative process. By learning the joint distribution of the asymmetric unit and its symmetry transformations, SymmCD generates diverse and valid crystalline materials with realistic symmetries, showcasing competitive performance on a subset of the Materials Project.

Online Preference Alignment for Language Models via Count-based Exploration
This paper introduces Count-based Online Preference Optimization (COPO), a novel approach that enhances reinforcement learning from human feedback (RLHF) by encouraging Large Language Models (LLMs) to explore beyond the limitations of fixed datasets. By utilizing a coin-flip counting module to balance exploration and preference optimization, COPO significantly improves the LLMs' ability to generalize and perform effectively in instruction-following and academic benchmarks, as demonstrated in experiments on the Zephyr and Llama-3 models.

Your Weak LLM is Secretly a Strong Teacher for Alignment
This paper investigates using a weak large language model (LLM) to provide feedback for model alignment, offering a cost-effective and scalable alternative to expensive human effort or high computational expenses of advanced models. The study reveals that weak LLMs deliver feedback comparable to human annotations, highlighting minimal impact of model size on alignment quality and providing insights into the differences between human and weak LLM feedback.

Can We Trust Embodied Agents? Exploring Backdoor Attacks against Embodied LLM-Based Decision-Making Systems
This paper introduces the first comprehensive framework for backdoor attacks (BALD) in Large Language Model (LLM)-based decision-making systems for embodied AI, focusing on attack mechanisms like word injection, scenario manipulation, and knowledge injection. Extensive experiments demonstrate the high success rates and stealthiness of these attacks, exposing critical security vulnerabilities in systems such as autonomous vehicles and home robots, underscoring the urgent need for enhanced security measures.

Modality-Specialized Synergizers for Interleaved Vision-Language Generalists
This paper introduces Modality-Specialized Synergizers (MoSS), a novel design that enhances Vision-Language Generalists (VLGs) by using modality-specialized adaptation layers to better model text and image features. The method, along with the new LeafInstruct dataset for interleaved text-and-image instruction tuning, significantly improves VLG performance in complex generation tasks and shows strong generalizability across different VLGs.

On the Role of Attention Heads in Large Language Model Safety
This paper investigates the role of multi-head attention mechanisms in the safety capabilities of large language models (LLMs), addressing a gap in current safety-related research. Introducing the Safety Head ImPortant Score (Ships) and the Safety Attention Head AttRibution Algorithm (Sahara), the authors demonstrate that modifying a minimal number of parameters can significantly impact safety, offering a new approach to understanding safety mechanisms within LLMs.

Cut the Crap: An Economical Communication Pipeline for LLM-based Multi-Agent Systems
The paper introduces $\texttt{AgentPrune}$, a novel multi-agent communication framework, which addresses the Communication Redundancy issue in current LLM-based pipelines by pruning unnecessary or harmful messages to reduce token overhead and economic costs. The framework achieves state-of-the-art performance with significantly lower costs, integrates smoothly into existing systems with up to 72.8% token reduction, and enhances resistance to adversarial attacks with up to 10.8% performance improvements.

OmniEdit: Building Image Editing Generalist Models Through Specialist Supervision
OmniEdit addresses the limitations of current instruction-guided image editing methods by introducing a versatile model capable of handling seven editing tasks with any aspect ratio. The paper highlights four key contributions: leveraging supervision from specialist models, using importance sampling to improve data quality, introducing a new architecture called EditNet for better editing success, and ensuring compatibility with images of various aspect ratios, significantly outperforming existing models in evaluations.

Think Then React: Towards Unconstrained Action-to-Reaction Motion Generation
This paper introduces Think-Then-React (TTR), a framework based on large language models designed to generate human-like reactions by unifying the thinking and reacting processes during inference. TTR employs a fine-grained multimodal training strategy and a novel unified motion tokenizer to effectively encode multi-person motion, leading to significant improvements over existing baselines in evaluation metrics like reducing FID.

Learning to Explore and Exploit with GNNs for Unsupervised Combinatorial Optimization
The paper introduces Explore-and-Exploit GNN (X²GNN), an unsupervised neural framework designed to enhance combinatorial optimization by balancing exploration and exploitation in the search space. Demonstrating superior performance and generalization, X²GNN significantly outperforms state-of-the-art methods on various graph CO problems, notably achieving near-optimal solutions for large Max Clique problems and displaying exceptional generalization capabilities on the Maximum Independent Set problem even with diverse training conditions.

PhyMPGN: Physics-encoded Message Passing Graph Network for spatiotemporal PDE systems
This paper introduces the Physics-encoded Message Passing Graph Network (PhyMPGN) to model spatiotemporal PDE systems on irregular meshes using limited training data. By integrating a graph neural network with a numerical integrator and incorporating a learnable Laplace block and a boundary condition padding strategy, PhyMPGN achieves state-of-the-art accuracy and robustness in predicting complex dynamical systems compared to existing methods.

RazorAttention: Efficient KV Cache Compression Through Retrieval Heads
The paper introduces RazorAttention, a compression technique for Key-Value (KV) cache in language models that reduces memory demands while preserving all token information. By leveraging a separate caching strategy for attention heads and introducing a "compensation token" mechanism, RazorAttention achieves over 70% reduction in KV cache size without affecting performance, offering an efficient, training-free solution compatible with FlashAttention.

Active Task Disambiguation with LLMs
This paper introduces a method for large language models (LLMs) to handle ambiguously specified problems through active task disambiguation, utilizing Bayesian Experimental Design to generate clarifying questions and refine potential solutions. The approach enhances LLMs' capabilities by transforming implicit reasoning into explicit reasoning via targeted question generation, showing improved task disambiguation compared to traditional methods.

Implicit Search via Discrete Diffusion: A Study on Chess
In the post-AlphaGo era, the paper explores enhancing search techniques like Monte Carlo Tree Search within Large Language Models to improve long-term planning. The authors introduce DiffuSearch, a model employing implicit search through discrete diffusion modeling, which outperforms traditional and explicit search models in tasks like chess by enhancing action accuracy, puzzle-solving abilities, and game strength, demonstrating that implicit search can be a viable alternative.

Magnetic Preference Optimization: Achieving Last-iterate Convergence for Language Model Alignment
This paper introduces Magnetic Preference Optimization (MPO), a novel method that achieves last-iterate convergence to the Nash equilibrium in a preference-based game, overcoming existing limitations in self-play methods within the RLHF framework. Building on Magnetic Mirror Descent, MPO offers a linear convergence rate and effectively fine-tunes Large Language Models, demonstrating significant performance enhancement through empirical results.

Behavioral Entropy-Guided Dataset Generation for Offline Reinforcement Learning
This paper extends Behavioral Entropy (BE) to continuous settings, providing $k$-nearest neighbor estimators and practical reward functions to use with standard reinforcement learning methods for effective exploration and dataset generation in complex domains. Experimental results demonstrate that offline reinforcement learning algorithms trained on datasets generated using BE outperform those using Shannon entropy, SMM, RND, and often those using Renyi entropy in standard MuJoCo environments.

Action Sequence Augmentation for Action Anticipation
This paper introduces a novel data augmentation strategy and a grammar induction algorithm to improve action anticipation models by addressing challenges such as biased action ordering and noisy input data. Experiments on benchmark datasets like 50Salads, EGTEA Gaze+, and Epic-Kitchens-100 show significant performance enhancements over existing methods.

Temporal Difference Learning: Why It Can Be Fast and How It Will Be Faster
Temporal difference (TD) learning, despite its known instability, continues to be valuable in reinforcement learning due to its significant computational speed advantages in solving complex tasks. This paper addresses TD's limitations by analyzing its efficient mapping into the smallest eigenspace without costly matrix inversion and introduces a scalable algorithm that combines TD's speed with the reliable convergence of gradient descent, validated by both mathematical proof and experimental results.

Towards Learning High-Precision Least Squares Algorithms with Sequence Models
This paper explores the potential of sequence models to accurately perform numerical algorithms, specifically gradient descent for least squares, by achieving machine precision and numerical generality. The study critiques the limitations of existing Transformer architectures in performing precise calculations and introduces polynomial-based architectures that, through a high-precision training approach, significantly outperform standard Transformers in accuracy and generalization on out-of-distribution problems.

Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions
This paper introduces Rodimus and its enhanced version, Rodimus$+$, which aim to reduce the complexity of Transformer-based large language models by using a data-dependent tempered selection (DDTS) mechanism and a hybrid approach incorporating Sliding Window Shared-Key Attention (SW-SKA). The proposed models show improved memory efficiency and superior downstream performance, demonstrating their potential to redefine the accuracy-efficiency balance in large language models, with code and pre-trained models available online.

LeanVec: Searching vectors faster by making them fit
The paper introduces LeanVec, a framework that enhances similarity search on high-dimensional vectors by integrating linear dimensionality reduction with vector quantization. LeanVec outperforms existing methods by achieving up to 3.7x faster search throughput and 4.9x quicker index build times, all while maintaining competitive accuracy for both in-distribution and out-of-distribution queries.

APE: Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding
Adaptive Parallel Encoding (APE) addresses the computational burden of context-augmented generation techniques by allowing the pre-computation and caching of each context's KV states for efficient parallel encoding during inference. APE outperforms traditional parallel encoding by aligning attention distribution with sequential encoding, preserving up to 98% of performance while achieving a 4.5× speedup and the capability to handle hundreds of contexts simultaneously.

Reveal Object in Lensless Photography via Region Gaze and Amplification
This paper introduces a region gaze-amplification network (RGANet) with a region gaze module (RGM) and a region amplifier (RA) to enhance concealed object detection (COD) in lensless imaging systems. The study also provides a benchmark dataset for the lensless imaging community, showing that the proposed method significantly improves COD performance.

Gyrogroup Batch Normalization
This study introduces a Riemannian Batch Normalization (RBN) framework on gyrogroups, called GyroBN, to extend Euclidean Deep Neural Networks to various Riemannian manifolds in machine learning. GyroBN unifies existing normalization methods and demonstrates effectiveness on Grassmannian and hyperbolic spaces, with experiments confirming its capabilities.

$F^3Set$: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos
This paper introduces $F^3Set$, a benchmark designed to improve the detection of fast, frequent, and fine-grained events in video analytics, addressing challenges such as motion blur and precision. The authors also propose a new method, $F^3ED$, that outperforms current techniques in $F^3$ event detection, with all resources available at the provided GitHub link.

$R^2$-Guard: Robust Reasoning Enabled LLM Guardrail via Knowledge-Enhanced Logical Reasoning
The paper introduces $R^2$-Guard, a robust guardrail for large language models that addresses limitations in existing systems by using knowledge-enhanced logical reasoning to capture intercorrelations among safety categories. It combines data-driven guardrail models with probabilistic graphic model-based reasoning and demonstrates superior performance over existing models, improving moderation accuracy significantly and adapting effectively to new safety challenges.

Accelerating 3D Molecule Generation via Jointly Geometric Optimal Transport
The paper introduces GOAT, a novel framework for efficient 3D molecule generation using flow-matching optimal transport, which optimizes the mapping of multi-modal features in a smooth representation space. Through optimal coupling estimation and purification, GOAT enhances transport efficiency and molecule quality, achieving significant speedup and superior validity, uniqueness, and novelty in molecule generation.

Accelerating Diffusion Transformers with Token-wise Feature Caching
This paper introduces token-wise feature caching for diffusion transformers to improve image and video synthesis efficiency by adaptively selecting suitable tokens for caching, reducing computation costs significantly without compromising quality. The approach achieves up to 2.36$\times$ acceleration on datasets such as OpenSora and PixArt-$\alpha$, demonstrating its effectiveness while maintaining high generation quality without additional training.

Accurate and Scalable Graph Neural Networks via Message Invariance
Graph neural networks (GNNs) struggle with computational inefficiencies due to the costly out-of-batch message passing (MP-OB) required by traditional methods, which limits their feasibility for large graphs. To overcome this, the proposed topological compensation (TOP) approach eliminates the need for MP-OB by using message invariance to achieve comparable accuracy with dramatically reduced computation time, enabling effective large-scale graph transductive learning.

Action abstractions for amortized sampling
This paper addresses the challenges in credit assignment and exploration in reinforcement learning (RL) and generative flow networks (GFlowNets) by proposing a method to incorporate action abstractions into policy optimization. By iteratively extracting and chunking high-reward action subsequences into single actions, the approach improves sample efficiency and enhances mode discovery in complex environments, showcasing hierarchical planning in amortized sequential sampling.

ACTIVE: Offline Reinforcement Learning via Adaptive Imitation and In-sample $V$-Ensemble
This paper introduces ACTIVE, an innovative offline reinforcement learning algorithm designed to address the issue of over-regularization in in-sample value learning and policy extraction methods. By leveraging an ensemble of $V$-functions for critic training and adaptively adjusting constraint levels, ACTIVE mitigates overestimation and demonstrates improved learning stability and policy optimality on D4RL benchmarks compared to existing methods.

Adaptive $Q$-Network: On-the-fly Target Selection for Deep Reinforcement Learning
The paper introduces a novel approach called Adaptive $Q$-Network (AdaQN) to enhance automated Reinforcement Learning (AutoRL) by addressing the hyperparameter sensitivity of deep RL without requiring additional samples. AdaQN, which operates independently of existing AutoML methods, improves sample-efficiency and performance by dynamically updating multiple $Q$-functions and selecting the most accurate one for optimization, demonstrating its effectiveness on MuJoCo control tasks and Atari $2600$ games.

ADePT: Adaptive Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning
Adaptive Decomposed Prompt Tuning (ADePT) enhances the adaptation of Pre-trained Large Language Models (PLMs) by using a short soft prompt combined with a shallow token-shared feed-forward neural network, which learns adaptive embedding offsets to improve performance. ADePT offers superior adaptation across diverse tasks without increasing inference time or trainable parameters, outperforming current fine-tuning methods and sometimes even full fine-tuning.

Advancing Graph Generation through Beta Diffusion
The paper introduces Graph Beta Diffusion (GBD), a generative model tailored to handle the mixed discrete and continuous components of graph data by employing a beta diffusion process. GBD demonstrates strong performance on general and biochemical graph benchmarks by effectively balancing the discrete and continuous features of real-world graphs, contributing a modulation technique to stabilize important graph topology while providing flexibility for other components.

Adversarial Latent Feature Augmentation for Fairness
This paper introduces Adversarial Latent Feature Augmentation (ALFA), a method to improve fairness in machine learning by identifying and perturbing "unfair regions" in the latent space where misclassification rates are biased against certain demographic groups. By fine-tuning neural networks with perturbed features from these regions, ALFA enhances classification fairness effectively while preserving predictive accuracy, as validated by comprehensive experiments across various datasets and network architectures.

Adversarial Training Can Provably Improve Robustness: Theoretical Analysis of Feature Learning Process Under Structured Data
This paper provides a theoretical understanding of why adversarial examples exist and how adversarial training improves model robustness through feature learning theory. By analyzing robust and non-robust features in a classification setting, the authors demonstrate that adversarial training enhances the learning of robust features and diminishes reliance on non-robust features, using a two-layer smoothed ReLU CNN and validating findings on datasets like MNIST, CIFAR10, and SVHN.

A Large-scale Training Paradigm for Graph Generative Models
This paper introduces a large-scale training paradigm for large graph generative models (LGGMs) by using over 5000 graphs from 13 domains, addressing the limitations of existing graph-generative models trained on single datasets. The proposed LGGMs demonstrate superior zero-shot generative capabilities and can be fine-tuned for improved performance, while also featuring a Text-to-Graph generation capability for enhanced user control, inspired by Stable Diffusion.

Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception
This paper examines the application of generative diffusion models in discriminative tasks, addressing the challenges posed by differences between generative and discriminative objectives. By improving the alignment between the generative denoising process and perception goals, the authors demonstrate enhanced performance in generative diffusion-based perception models, such as depth estimation and referring image segmentation, through tailored learning objectives and diffusion-tailored data augmentation.

Analyzing Neural Scaling Laws in Two-Layer Networks with Power-Law Data Spectra
This paper employs statistical mechanics to analyze neural scaling laws, focusing on how generalization error behaves with data covariance matrices exhibiting power-law spectra within a student-teacher framework of two-layer neural networks. By deriving analytical expressions for generalization error in linear and non-linear activation regimes, the study illuminates the conditions under which power-law scaling emerges, the impact on learning dynamics, and transitions in convergence behavior, thus enhancing theoretical comprehension and optimization of learning performance in complex data scenarios.

An Illustrated Guide to Automatic Sparse Differentiation
This paper introduces automatic sparse differentiation (ASD), a technique that enhances the efficiency of computing sparse Jacobians and Hessians in machine learning by leveraging the inherent sparsity found in many applications. It provides a detailed explanation of ASD's components, such as sparsity pattern detection and matrix coloring, and concludes with a practical demonstration of the performance benefits, aiming to bridge the gap between the machine learning and automatic differentiation communities.

AnoLLM: Large Language Models for Tabular Anomaly Detection
AnoLLM is a novel framework that utilizes large language models for unsupervised anomaly detection in tabular data by converting the data into a standardized text format and using pre-trained LLMs to assign anomaly scores. This method preserves data integrity, handles mixed-type data efficiently, and demonstrates superior performance on benchmark datasets, while also performing competitively on predominantly numerical datasets.

A Non-Contrastive Learning Framework for Sequential Recommendation with Preference-Preserving Profile Generation
This paper introduces the first Non-Contrastive Learning (NCL) framework for Sequential Recommendation, overcoming the computational challenges of Contrastive Learning by eliminating the need for negative samples. By implementing a novel preference-preserving profile generation method inspired by differential privacy, the NCL framework achieves improved alignment and uniformity in learned representations, enhancing generalization and demonstrating effectiveness across various datasets and architectures.

An Online Learning Theory of Trading-Volume Maximization
This paper investigates brokerage strategies to maximize trading volume between traders in an online learning framework, focusing on situations with both full-feedback and limited feedback. The authors propose algorithms with optimal or near-optimal regret under different assumptions about the traders' valuation distributions and analyze the impact of relaxing regularity conditions, demonstrating increased regret or impossibility of learning when these conditions are not met.

BaB-ND: Long-Horizon Motion Planning with Branch-and-Bound and Neural Dynamics
This paper introduces a GPU-accelerated branch-and-bound framework for improving motion planning in robotic manipulation tasks with complex contact events by leveraging neural dynamics models. By employing a specialized branching heuristic and a modified bound propagation inspired by the $\alpha,\beta$-CROWN verifier, the framework efficiently reduces the planning search space, resulting in superior trajectory optimization, handling various neural network architectures, and excelling in challenging tasks like object sorting and rope routing in both simulations and real-world scenarios.

Backtracking Improves Generation Safety
This paper introduces backtracking, a novel technique that enhances language model safety by allowing them to "undo" unsafe text generation using a special [RESET] token. By incorporating this method into SFT or DPO training, models such as backtracking Llama-3-8B become significantly safer, showing reduced unsafe text generation and resilience against adversarial attacks, without compromising helpfulness.

Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model Compression
This paper proposes an advanced method for compressing large language models by using singular value decomposition (SVD) to share parameters across different layers, thus reducing memory requirements. The Basis-Sharing approach not only decreases storage needs but also outperforms existing SVD-based techniques, particularly at higher compression levels, while maintaining model performance.

Bayesian Optimization of Antibodies Informed by a Generative Model of Evolving Sequences
We introduce Clone-informed Bayesian Optimization (CloneBO), a method leveraging a generative model trained on numerous clonal families to efficiently optimize antibody sequences for better binding and stability. By combining insights from our immune system's natural antibody evolution with a sequential Monte Carlo procedure, CloneBO substantially enhances the efficacy of antibody design compared to previous approaches, as demonstrated in both *in silico* and *in vitro* experiments.

Bayesian Treatment of the Spectrum of the Empirical Kernel in (Sub)Linear-Width Neural Networks
This study explores Bayesian neural networks (BNNs) in the theoretical limits of increasing training examples, network width, and input space dimension, linking kernel-theoretical approaches with statistical mechanics through random matrix theory. The paper introduces novel integral formulas for predictor descriptions in various width regimes and extends renormalization theory to nonlinear BNNs, offering a practical technique for estimating predictor statistics in sublinear-width scenarios.

Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge
This paper critiques the use of traditional correlation metrics, like Krippendorff's α and Randolph's κ, for evaluating generative models, highlighting their inapplicability due to differing assumptions about human and machine labeling behaviors. It proposes stratifying data by human label uncertainty, introduces a new metric called binned Jensen-Shannon Divergence for perception scenarios, and provides visualization techniques, all aimed to offer a more nuanced understanding of automatic evaluation performance beyond conventional correlation measures.

Bidirectional Decoding: Improving Action Chunking via Guided Test-Time Sampling
This paper investigates the influence of action chunking on learned robotic policies, highlighting its ability to capture temporal dependencies while potentially reducing reactivity to unforeseen states. To balance these aspects, the authors introduce Bidirectional Decoding (BID), a test-time inference algorithm that enhances both long-term consistency and short-term reactivity, thereby improving performance across various benchmarks and tasks.

BIRD: A Trustworthy Bayesian Inference Framework for Large Language Models
This paper introduces BIRD, a novel probabilistic inference framework that enhances probability estimation by integrating Bayesian networks with the abductive reasoning capabilities of large language models (LLMs). By aligning these elements, BIRD achieves a 30% improvement in probability estimations over traditional LLMs, thereby contributing to more reliable decision-making in real-world tasks.

Boosting Perturbed Gradient Ascent for Last-Iterate Convergence in Games
This paper introduces a payoff perturbation technique that enhances the strong convexity of players' payoff functions, tailored for first-order methods to achieve faster last-iterate convergence in games, even with monotone gradient strategies and additive noise. The proposed Gradient Ascent with Boosting Payoff Perturbation method improves upon previous schemes by using a novel perturbation approach, offering more rapid convergence rates while maintaining a periodically re-initializing anchoring strategy.

Broadening Target Distributions for Accelerated Diffusion Models via a Novel Analysis Approach
This paper introduces an accelerated stochastic DDPM sampler that broadens the target distribution classes for which accelerated diffusion models can achieve faster convergence rates. The study expands existing acceleration guarantees to three new classes, relaxing conditions on smoothness and support, and establishes a novel technique for performance analysis using a tilting factor representation and Tweedie's formula.

B-STaR: Monitoring and Balancing Exploration and Exploitation in Self-Taught Reasoners
This paper investigates the dynamics of self-improvement in models trained on their own outputs, specifically focusing on exploration and exploitation during iterative training processes. The authors introduce B-STaR, a Self-Taught Reasoning framework, which effectively balances exploration and exploitation, enhancing the model's training process and performance in mathematical reasoning tasks.

Catastrophic Failure of LLM Unlearning via Quantization
This paper investigates the limitations of current machine unlearning methods for large language models, revealing that "forgotten" information can be recovered through model quantization. By conducting experiments with various quantization techniques, the authors highlight the need for more effective unlearning strategies that maintain model utility without enabling recovery of erased content, presenting a quantization-robust unlearning approach.

C-CLIP: Multimodal Continual Learning for Vision-Language Model
This paper addresses the challenge of continual learning in multimodal pre-trained models like CLIP, which struggle with domain-specific tasks. By introducing a new multimodal vision-language continual learning benchmark and proposing a novel framework called C-CLIP, the study demonstrates enhanced learning of new tasks with minimal forgetting of previous zero-shot capabilities, outperforming existing methods across diverse domain image-text datasets.

CLIPure: Purification in Latent Space via CLIP for Adversarially Robust Zero-Shot Classification
This paper presents CLIPure, a novel purification method for adversarially robust zero-shot image classification using the CLIP model's multi-modal latent space, addressing both zero-shot classification and defense against unforeseen adversarial attacks. CLIPure demonstrates substantial robustness improvements over the state-of-the-art, with minimal inference cost and no additional training, marking the first latent space purification approach with a variant, CLIPure-Cos, that enhances efficiency without generative models.

CollabEdit: Towards Non-destructive Collaborative Knowledge Editing
This paper introduces a framework called COLLABEDIT for collaborative Knowledge Editing (KE) of large language models, addressing the challenges of knowledge overlap, conflict, and forgetting in a privacy-preserving manner. The proposed model merging mechanism enhances global KE behavior without performance degradation, and experiments show that COLLABEDIT outperforms other destructive baselines, offering insights into overcoming collaborative KE challenges.

COMBO: Compositional World Models for Embodied Multi-Agent Cooperation
This paper addresses the challenge of embodied multi-agent cooperation with only egocentric views by training generative models to estimate the overall world state and proposing a compositional world model. Through integrating Vision Language Models and tree search, the framework enables effective online cooperative planning and demonstrates efficient collaboration across various tasks and numbers of agents in experimental benchmarks.

ComPC: Completing a 3D Point Cloud with 2D Diffusion Priors
This paper introduces a novel test-time framework for completing incomplete 3D point clouds without requiring training, using techniques like Partial Gaussian Initialization and Zero-shot Fractal Completion. By leveraging priors from pre-trained 2D diffusion models, the proposed method effectively infers missing regions and performs well across both synthetic and real-world point clouds, outperforming existing approaches even on unseen object categories.

Content-Style Learning from Unaligned Domains: Identifiability under Unknown Latent Dimensions
This paper introduces a novel analytical framework via cross-domain latent distribution matching (LDM) to establish content-style identifiability under more relaxed conditions than previous models, removing the need for component-wise independence and prior knowledge of variable dimensions. By integrating LDM into a regularized multi-domain GAN, the study demonstrates that identifiability can be achieved with reduced computational resources, supported by experimental validation.

Context-aware Dynamic Pruning for Speech Foundation Models
This study introduces a dynamic pruning technique for speech foundation models that adapts to contextual factors like speaker characteristics, languages, and tasks during inference, addressing the challenge of high computational cost. The proposed method reduces inference time by approximately 30% while maintaining accuracy in multilingual and multi-task scenarios, offering meaningful interpretations of pruned structures based on task-related information.

ContraDiff: Planning Towards High Return States via Contrastive Learning
In this paper, we introduce Contrastive Diffuser (ContraDiff), a novel method for offline reinforcement learning that leverages low-return trajectories to enhance policy performance. By employing a contrastive mechanism to differentiate and guide the agent away from low-return states towards high-return ones, ContraDiff effectively utilizes sub-optimal datasets, as demonstrated in experiments across 27 datasets.

Controllable Satellite-to-Street-View Synthesis with Precise Pose Alignment and Zero-Shot Environmental Control
This paper introduces a novel Iterative Homography Adjustment (IHA) scheme to improve pose alignment and spatial consistency in generating street-view images from satellite imagery. By incorporating a text-guided illumination and weather-controlled sampling strategy, the approach enhances both the accuracy and diversity of the images, setting a new benchmark in satellite-to-street-view generation tasks.

Controlled LLM Decoding via Discrete Auto-regressive Biasing
This paper introduces "Discrete Auto-regressive Biasing," a novel algorithm for controlled text generation that operates entirely in the discrete text domain, improving constraint satisfaction and text fluency. By leveraging a new joint distribution formulation and a gradient-based discrete MCMC sampling algorithm, the method achieves enhanced performance in tasks like sentiment control, language detoxification, and keyword-guided generation with reduced computational costs.

Convergence of Score-Based Discrete Diffusion Models: A Discrete-Time Analysis
This paper explores the theoretical aspects of score-based discrete diffusion models within the Continuous Time Markov Chain (CTMC) framework, introducing a discrete-time sampling algorithm for general state spaces. The study provides convergence bounds for Kullback-Leibler (KL) divergence and total variation (TV) distance, demonstrating that the KL divergence bounds are nearly linear in dimension, thus contributing to the understanding of the sampling process in discrete-time models.

Correcting the Mythos of KL-Regularization: Direct Alignment without Overoptimization via Chi-Squared Preference Optimization
This paper addresses the issue of overoptimization in language model alignment methods, which often occurs due to overfitting to inaccuracies in offline reward models. Introducing the $\chi^2$-Preference Optimization (χPO) algorithm, the authors demonstrate a more robust approach to offline alignment by using $\chi^2$-divergence for regularization, offering provable resilience against overoptimization and thereby improving sample-complexity performance compared to traditional methods like KL-regularization.

CoTFormer: A Chain of Thought Driven Architecture with Budget-Adaptive Computation Cost at Inference
Scaling language models leads to better performance but faces challenges in compute-constrained environments; however, task-specific techniques like Chain-of-Thought (CoT) help achieve optimal results. This paper introduces CoTFormer, a novel architecture that mimics CoT at the token level to enhance accuracy similar to much larger models while reducing computation costs through a compute adaptive model that focuses resources where needed most.

Counterfactual Generative Modeling with Variational Causal Inference
This paper introduces a novel variational Bayesian causal inference framework designed to address the challenges of counterfactual generative modeling with high-dimensional outcomes and limited covariates. The proposed framework improves upon existing methods by enabling end-to-end counterfactual supervision and disentangled exogenous noise abduction, leading to superior performance in causal effect identification on multiple benchmarks compared to state-of-the-art models.

CryoFM: A Flow-based Foundation Model for Cryo-EM Densities
CryoEM is a foundational model that improves cryo-electron microscopy's data processing by learning the distribution of high-quality density maps, thus enhancing structure determination in structural biology and drug discovery. Utilizing flow matching, CryoFM serves as a flexible prior for various downstream tasks in cryo-EM and cryo-ET, achieving state-of-the-art performance without needing fine-tuning, thereby demonstrating significant potential for broader applications in these fields.

Deep MMD Gradient Flow without adversarial training
The paper introduces Diffusion-MMD-Gradient Flow (DMMD), a novel gradient flow procedure for generative modeling that uses a noise-adaptive Wasserstein Gradient of the Maximum Mean Discrepancy to effectively transport particles from an initial distribution to a target distribution. This method, which bypasses adversarial training, achieves competitive results in unconditional image generation tasks on datasets like CIFAR10, MNIST, CELEB-A, and LSUN Church, and remains valid when using a lower bound on the KL divergence instead of MMD.

Diffusion Attribution Score: Evaluating Training Data Influence in Diffusion Model
This paper addresses the misuse of copyrighted and private images in diffusion models by introducing a novel approach called the Diffusion Attribution Score (DAS) to measure the importance of training samples. Through rigorous theoretical analysis and extensive experiments, DAS proves to be more effective and efficient than previous methods, offering state-of-the-art performance in attributing data in diffusion models.

Diffusion On Syntax Trees For Program Synthesis
This paper proposes neural diffusion models that operate on syntax trees, allowing for iterative code editing while maintaining syntactic validity, unlike traditional autoregressive models. The approach is applied to inverse graphics tasks, showcasing the model's ability to convert images and hand-drawn sketches into corresponding graphics programs by employing a combination of model capabilities and search, enhancing the debugging process to meet specified outcomes.

Discriminating image representations with principal distortions
This paper introduces a framework for comparing image representations based on local geometries using the Fisher information matrix to characterize sensitivity to local stimulus distortions. The method efficiently distinguishes between models by identifying principal distortions, and successfully differentiates models of the early visual system and deep neural networks, providing insights into model architectures, training types, and potential comparisons with human perception.

Dist Loss: Enhancing Regression in Few-Shot Region through Distribution Distance Constraint
This paper introduces Dist Loss, a novel loss function that minimizes the distribution distance between model predictions and target labels, thus incorporating distribution information into model training to address challenges in imbalanced regression tasks. Extensive experiments on datasets in computer vision and healthcare show that Dist Loss significantly improves model performance in sparse data regions, allowing deep learning models to focus better on few-shot regions and achieve state-of-the-art results.

DiTTo-TTS: Diffusion Transformers for Scalable Text-to-Speech without Domain-Specific Factors
The paper introduces DiTTo-TTS, a Diffusion Transformer-based model for text-to-speech systems that aims to achieve state-of-the-art performance without relying on domain-specific factors like phonemes and durations. By scaling the model and employing innovative strategies such as variable-length modeling, the researchers demonstrate superior or comparable results in naturalness, intelligibility, and speaker similarity, showcasing the model's efficacy with extensive training data and substantial parameter size.

Drama: Mamba-Enabled Model-Based Reinforcement Learning Is Sample and Parameter Efficient
Model-based reinforcement learning (RL) can improve data efficiency but often faces challenges with complex model architectures. Drama, a state space model (SSM)-based world model using Mamba, addresses these issues by achieving $O(n)$ complexity and effectively capturing long-term dependencies, leading to competitive results on the Atari100k benchmark with a relatively small model size that is trainable on standard hardware.

Dualformer: Controllable Fast and Slow Thinking by Learning with Randomized Reasoning Traces
\dualformer is a Transformer model designed to mimic human cognitive processes by operating in fast, slow, and auto reasoning modes for various tasks. This model outperforms existing baselines in maze tasks and math reasoning problems by integrating randomized reasoning trace training, achieving significant improvements in both accuracy and computational efficiency.

Durable Quantization Conditioned Misalignment Attack on Large Language Models
This paper introduces the Quantization Conditioned Misalignment (Q-Misalign) attack, a novel threat that exploits vulnerabilities in quantized large language models (LLMs) by enabling the generation of harmful content, which remains dormant in their full-precision counterparts. The study demonstrates that the Q-Misalign attack significantly increases jailbreak success rates in quantized models, highlighting the need for robust defenses in quantization-aware scenarios while maintaining model utility and safety alignment in full precision.

DynAlign: Unsupervised Dynamic Taxonomy Alignment for Cross-Domain Segmentation
DynAlign is a two-stage framework that enhances unsupervised domain adaptation for semantic segmentation by integrating UDA with foundation models to address both image-level and label-level domain gaps. By leveraging prior semantic knowledge and introducing a dynamic knowledge fusion approach, DynAlign achieves significant improvements over existing methods, enabling accurate predictions in new target label spaces without manual annotations, as validated on the GTA → IDD and GTA → Mapillary benchmarks.

Dynamic Modeling of Patients, Modalities and Tasks via Multi-modal Multi-task Mixture of Experts
Multi-modal multi-task learning can revolutionize medical imaging diagnostics by dynamically integrating information from various data sources, addressing the challenges of dynamic modality fusion and modality-task dependence. The proposed M4oE framework, utilizing Modality-Specific and Modality-Task modules, improves diagnosis by effectively learning and combining distinct and shared modality information, outperforming existing methods on benchmark datasets for breast cancer and retinal disease diagnosis.

EDiT: A Local-SGD-Based Efficient Distributed Training Method for Large Language Models
EDiT is an efficient distributed training method that enhances large-scale language model training by combining a tailored Local SGD approach with model sharding techniques to reduce communication and memory overhead. The method introduces a pseudo gradient penalty strategy for stability and A-EDiT for asynchronous training, demonstrating superior performance in diverse environments, with code available at the Atorch codebase.

Efficient Active Imitation Learning with Random Network Distillation
This paper introduces RND-DAgger, an active imitation learning method designed to reduce expert intervention by using a state-based out-of-distribution measure for triggering necessary interventions in complex tasks with ambiguous objectives, such as video games and robotic locomotion. The method outperforms traditional imitation learning and other active approaches by minimizing expert queries, making it a more efficient solution for scenarios where increased dataset size through human demonstration is impractical or costly.

Efficient and Context-Aware Label Propagation for Zero-/Few-Shot Training-Free Adaptation of Vision-Language Model
Vision-language models have advanced with pre-trained models, yet often need hyperparameter tuning and do not fully utilize test samples. We propose a graph-based method for label-efficient adaptation and inference that uses dynamic graph construction and label propagation for effective, task-specific tuning-free inference and introduce a context-aware feature re-weighting mechanism to enhance accuracy and enable real-time inductive inference.

Efficient Imitation under Misspecification
This paper tackles the challenges of interactive imitation learning in the misspecified setting, where the expert's policy may not be perfectly realizable by the learner. The authors introduce the concept of reward-agnostic policy completeness to efficiently mitigate compounding errors and enhance sample efficiency by incorporating additional offline data, with empirical validation provided through continuous control tasks.

Efficient stagewise pretraining via progressive subnetworks
This paper challenges the prevailing notion that stagewise dropping strategies are ineffective compared to stacking methods by introducing a novel framework called Random Part Training (RAPTR), which selectively trains increasing subnetworks in models. The proposed approach not only speeds up training significantly but also enhances downstream performance, providing a better inductive bias than traditional methods.

ELFS: Label-Free Coreset Selection with Proxy Training Dynamics
This paper introduces ELFS (Effective Label-Free Coreset Selection), a novel approach to improve label-free coreset selection by utilizing deep clustering and a double-end pruning method to generate accurate data difficulty scores without ground truth labels. Evaluated on four vision benchmarks, ELFS achieves superior performance compared to state-of-the-art label-free methods, demonstrating up to a 10.2% increase in accuracy on ImageNet-1K when SwAV is used as the encoder.

Empowering LLM Agents with Zero-Shot Optimal Decision-Making through Q-learning
The paper introduces the Model-based LLM Agent with Q-Learning (MLAQ), an algorithm that combines the zero-shot decision-making abilities of large language models (LLMs) with the optimal decision capabilities of reinforcement learning (RL) through Q-learning. By constructing an imagination space based on LLMs for imaginary interactions and utilizing a UCB variant for generating high-quality data, MLAQ demonstrates an over 90% success rate in complex benchmarks, highlighting its potential to enhance decision-making in LLM agents.

Enhancing Uncertainty Estimation and Interpretability with Bayesian Non-negative Decision Layer
The paper introduces a Bayesian Nonnegative Decision Layer (BNDL) to address challenges in uncertainty estimation and interpretability within deep neural networks by reformulating them as a conditional Bayesian non-negative factor analysis. This approach enhances disentanglement and interpretability while improving accuracy and providing reliable uncertainty estimation through a corresponding variational inference method.

Ensembles of Low-Rank Expert Adapters
The paper introduces the Ensembles of Low-Rank Expert Adapters (ELREA) framework to address optimization challenges in large language models by clustering training instructions and reducing gradient conflicts. ELREA improves model performance by training expert adapters on task-specific clusters using the low-rank adaptation technique, enhancing efficiency and scalability while outperforming traditional LoRA adapters and other ensemble methods in domain-specific tasks.

ESE: Espresso Sentence Embeddings
This paper introduces Espresso Sentence Embeddings (ESE), a novel sentence embedding model designed to improve the scalability and efficiency of sentence embeddings for natural language processing tasks like semantic textual similarity and retrieval-augmented generation. ESE employs a dual-process learning approach—learn-to-express and learn-to-compress—that allows it to generate high-quality embeddings with optimized model depth and size, outperforming existing models in terms of inference efficiency.

EVA: Geometric Inverse Design for Fast Protein Motif-Scaffolding with Coupled Flow
Motif-scaffolding in protein design benefits from generative models, where the proposed Evolution-ViA-reconstruction (EVA) framework enhances sampling-based methods by aligning generative directions with spatial context guidance. EVA, a geometric inverse design approach, delivers faster (70× compared to RFDiffusion) and more effective performance on benchmarks, demonstrating its superior efficiency in tasks such as vaccine design and multi-motif scaffolding.

Exploiting Hidden Symmetry to Improve Objective Perturbation for DP linear learners with a nonsmooth L1-norm
This paper extends Objective Perturbation (OP) for differentially private (DP) convex optimization to handle loss functions with an implicit $\ell_1$-norm structure by smoothing through convolution, leading to tighter pointwise approximations and improved generalization risk analysis. The proposed OP-based algorithm demonstrates rate-optimal performance and competitive results compared to Noisy-SGD, achieving excess generalization risk of $\mathcal{O}(\frac{1}{\sqrt{n}}+\frac{\sqrt{d\ln(1/\delta)}}{n\varepsilon})$ under certain assumptions.

Exploring The Loss Landscape Of Regularized Neural Networks Via Convex Duality
This paper explores the loss landscape of regularized neural networks by transforming the problem into a convex form and examining its dual, focusing on solution set characterization and connectivity of optimal solutions. It reveals that as the network width changes, the topology of global optima undergoes a phase transition, and extends these insights to various network architectures, highlighting the potential for a continuum of optimal solutions.

Fast training and sampling of Restricted Boltzmann Machines
This study addresses the challenge of slow mixing in RBM training on highly structured datasets by introducing an approach that encodes data patterns into singular vectors, facilitating efficient sample generation and evaluation. Utilizing continuous phase transitions, the method incorporates a pre-training phase and a novel Parallel Trajectory Tempering (PTT) algorithm, enhancing log-likelihood estimation and significantly accelerating MCMC processes compared to traditional techniques.

Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning
This paper introduces Prereq-Tune, a fine-tuning strategy designed to address knowledge inconsistencies between pre-training and fine-tuning in large language models, which often lead to hallucinations. By disentangling the learning of skills and knowledge and incorporating a prerequisite learning stage, Prereq-Tune enhances factual accuracy across various tasks and suggests new avenues for knowledge-controlled generation in LLMs.

FIG: Flow with Interpolant Guidance for Linear Inverse Problems
Flow with Interpolant Guidance (FIG) introduces an innovative algorithm to tackle challenging linear inverse problems in image restoration, utilizing pre-trained diffusion or flow-matching models and guiding reverse-time sampling with measurement interpolants. The experimental results show that FIG significantly improves performance on difficult tasks such as high measurement noise scenarios, outperforming existing methods on natural image datasets.

FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference
FlexPrefill is introduced as a flexible sparse pre-filling mechanism for large language models that dynamically adjusts attention patterns and computational budgets during long-sequence inference. By optimizing sparse patterns and ratios in real-time, it enhances efficiency and accuracy compared to previous methods, showcasing substantial improvements in speed and accuracy for varying input demands.

Foundation Models Secretly Understand Neural Network Weights: Enhancing Hypernetwork Architectures with Foundation Models
This paper explores the integration of large pre-trained foundation models into hypernetwork methods to enhance the performance, generalizability, and data efficiency of implicit neural representations (INRs) across various tasks. By demonstrating how Transformer-based architectures can effectively utilize foundation models, the study provides empirical evidence of their benefits and analyzes the design space for optimizing foundation model-based hypernetworks.

Free Hunch: Denoiser Covariance Estimation for Diffusion Models Without Extra Costs
This paper introduces a new framework for diffusion models that utilizes available covariance information from training data and the generative trajectory's curvature, avoiding the need for heavy computation and approximations in current methods. The approach is validated on linear inverse problems, demonstrating superior performance over recent baselines with fewer diffusion steps.

From Isolated Conversations to Hierarchical Schemas: Dynamic Tree Memory Representation for LLMs
MemTree is a novel algorithm that uses a dynamic, tree-structured memory representation, enhancing the organization, retrieval, and integration of information, similar to human cognitive schemas. It improves context-awareness and complex reasoning in large language models by adapting memory hierarchically, significantly boosting performance in multi-turn dialogue and document question answering compared to traditional memory methods.

From Models to Microtheories: Distilling a Model's Topical Knowledge for Grounded Question-Answering
This paper introduces the concept of "microtheories," which are concise summaries of a language model's understanding of a topic, created by distilling model-generated sentences that entail answers to a set of questions. The authors demonstrate that integrating these microtheories with existing corpora, like Wikipedia, can improve a model's grounding and accuracy of answers, providing a more trustworthy and interpretable view of the model's knowledge, particularly in domains such as medicine, as confirmed by human evaluation.

From Probability to Counterfactuals: the Increasing Complexity of Satisfiability in Pearl's Causal Hierarchy
This paper explores the computational complexity of reasoning within Pearl's Causal Hierarchy, focusing on satisfiability problems using probabilistic and causal languages. The authors demonstrate that the complexity of these problems increases across the hierarchy levels, from NP^{PP}- and PSPACE- to NEXP-complete, but notably remains the same for full languages involving counterfactual reasoning, addressing a previously unresolved question in the field.

From Promise to Practice: Realizing High-performance Decentralized Training
This paper explores decentralized training for deep neural networks, highlighting its potential for scalability over traditional methods such as All-Reduce. By introducing a decentralized Adam algorithm that efficiently overlaps communication and computation, the study demonstrates improvements in runtime and model generalization, validated on clusters with up to 64 GPUs.

GALA: Geometry-Aware Local Adaptive Grids for Detailed 3D Generation
We introduce GALA, a novel 3D shape representation that effectively captures intricate geometry and surface details while being computationally efficient, making it suitable for diffusion-based 3D generative modeling. By leveraging surface sparsity and local adaptivity with octrees and geometry-aware coordinate frames, GALA significantly enhances detail storage without quality loss, and its integration with transformer networks allows efficient 3D shape generation.

Generalization, Expressivity, and Universality of Graph Neural Networks on Attributed Graphs
This paper introduces pseudometrics for attributed graphs that enhance the expressivity and theoretical understanding of graph neural networks (GNNs). By establishing that GNNs are Lipschitz continuous and capable of separating distant attributed graphs within this metric space, the authors provide a comprehensive framework that includes universal approximation theorems and generalization bounds, addressing limitations in prior theories by ensuring the space of graphs is relatively compact.

Geometry of Long-Tailed Representation Learning: Rebalancing Features for Skewed Distributions
This paper presents a theoretical analysis of how long-tailed data distributions affect feature representations in deep learning, particularly highlighting the issue of overlapping distributions in tail classes. To address this, the authors introduce FeatRecon, a novel representation learning method that restructures the feature space for improved separability and robustness, validated through extensive experiments on multiple long-tailed datasets.

GI-GS: Global Illumination Decomposition on Gaussian Splatting for Inverse Rendering
GI-GS is a novel inverse rendering framework that utilizes 3D Gaussian Splatting and deferred shading to enhance photo-realistic novel view synthesis and relighting by accurately modeling indirect lighting. By employing efficient path tracing with deferred shading and using a G-buffer to capture scene details, GI-GS achieves superior rendering quality and efficiency compared to existing baselines.

Global Well-posedness and Convergence Analysis of Score-based Generative Models via Sharp Lipschitz Estimates
This paper demonstrates global well-posedness and convergence of score-based generative models (SGM) by employing minimal general initial data assumptions for score estimation. The authors establish that a local Lipschitz condition can ensure convergence without time separation, presenting a significant advancement over conventional methods, particularly for non-log-concave distributions, and offer insights into optimal Lipschitz bounds for distributions on compact, smooth manifolds.

GraphEval: A Lightweight Graph-Based LLM Framework for Idea Evaluation
The paper introduces $\texttt{GraphEval}$, a lightweight graph-based framework designed to enhance the evaluation of research ideas by addressing the limitations of current LLM-based methods in understanding complex semantic information. By utilizing viewpoint nodes linked through LLM-based relations and implementing two methods—GraphEval-LP for score propagation and GraphEval-GNN for score prediction—$\texttt{GraphEval}$ significantly improves robustness and accuracy in idea evaluation, achieving a 14% increase in F1 scores while effectively detecting plagiarized content.

GROOT-2: Weakly Supervised Multimodal Instruction Following Agents
This paper introduces \agent, a multimodal instructable agent that addresses the challenge of agents struggling to follow multimodal instructions by framing it as a semi-supervised learning task. Through a novel approach combining weak supervision with latent variable models, \agent demonstrates robust instruction-following capabilities across diverse environments, utilizing constrained self-imitating to learn from unlabeled demonstrations and aligning with human intentions from a smaller set of labeled examples.

Group Downsampling with Equivariant Anti-aliasing
This paper explores the generalization of downsampling layers for group-equivariant architectures, focusing on anti-aliasing in feature maps on finite groups. The proposed method, which improves accuracy and model efficiency in $G$-equivariant networks, generalizes classical downsampling and demonstrates enhanced performance in image classification tasks.

Group Ligands Docking to Protein Pockets
The paper introduces \textsc{GroupBind}, a new molecular docking framework that considers multiple ligands docking to a protein simultaneously, inspired by the biochemical trend of similar ligand poses for a common target. By using an interaction layer and a triangle attention module for embedding, \textsc{GroupBind} integrated with a diffusion-based docking model achieves state-of-the-art performance on the PDBBind benchmark, improving molecular docking accuracy.

Guided Score identity Distillation for Data-Free One-Step Text-to-Image Generation
This paper presents a data-free guided distillation method for enhancing the efficiency of Stable Diffusion models in generating text-to-image content without the need for original training data. By employing innovative strategies like Long and Short Classifier-Free Guidance, the method achieves state-of-the-art FID performance with a record low of 8.15 on the COCO-2014 validation set, using exclusively synthetic images for rapid and efficient model training.

HGM³: Hierarchical Generative Masked Motion Modeling with Hard Token Mining
This paper introduces a novel text-to-motion generation framework utilizing Hard Token Mining (HTM) and a Hierarchical Generative Masked Motion Model (HGM³) to address the challenges of inherent text ambiguity and complex human motion dynamics. By employing a shared-weight masked motion model and a hierarchical semantic graph for contextually structured learning, the method significantly improves context-aware motion generation, outperforming existing approaches on benchmark datasets like HumanML3D and KIT-ML.

HMoRA: Making LLMs More Effective with Hierarchical Mixture of LoRA Experts
This paper introduces HMoRA, a hierarchical fine-tuning method combining Mixture of Experts (MoE) and LoRA, which utilizes a hybrid routing system to improve large language model performance by capturing both token-level and task-level information efficiently. By incorporating a novel routing auxiliary loss, HMoRA achieves superior performance on NLP benchmarks with only 3.9% of the parameters fine-tuned, addressing challenges in existing MoE approaches such as granularity capture, task generalizability, and expert specialization.

How DNNs break the Curse of Dimensionality: Compositionality and Symmetry Learning
This paper demonstrates that deep neural networks (DNNs) can efficiently learn compositions of functions with bounded $F_{1}$-norm, effectively overcoming the curse of dimensionality unlike shallow networks. By deriving a generalization bound involving covering numbers and the $F_{1}$-norm, the study shows that DNNs can fit compositions like $f^{\*}=h\circ g$ from limited observations, illustrating phase transitions based on the learnability of $g$ or $h$.

How Feature Learning Can Improve Neural Scaling Laws
This paper presents a model exploring neural scaling laws beyond the standard kernel limit, predicting performance scaling with model size, training time, and data availability. The study identifies distinct scaling behaviors for different task difficulties, revealing that feature learning can significantly improve scaling exponents in harder tasks, validated through experiments with MLPs and CNNs.

Implicit Neural Surface Deformation with Explicit Velocity Fields
This paper presents an unsupervised method for predicting time-varying neural implicit surfaces and deformations between point cloud pairs, using an explicit velocity field and a modified level-set equation. The proposed approach, which does not require intermediate shape supervision, recovers physically plausible shapes and outperforms existing methods in quality and efficiency.

ImProver: Agent-Based Automated Proof Optimization
This paper introduces ImProver, a large-language-model agent designed to optimize formal mathematical proofs in Lean by rewriting them according to user-defined criteria, such as conciseness or declarative structure. By incorporating novel techniques like the Chain-of-States, symbolic context use, error correction, and retrieval, ImProver successfully rewrites undergraduate to research-level proofs, making them shorter and more declarative.

Improving Generalization and Robustness in SNNs Through Signed Rate Encoding and Sparse Encoding Attacks
The paper introduces the signed rate encoding (sRATE) method for rate-encoded spiking neural networks (SNNs), which improves generalization and clean accuracy by centering input means and reducing encoding randomness. Additionally, the authors develop a Sparse Encoding Attack (SEA) for adversarial robustness, demonstrating through theoretical and empirical analysis that sRATE enhances accuracy and robustness in SNNs compared to existing methods.

In-context Time Series Predictor
This paper proposes a novel method for time series forecasting (TSF) by reformulating TSF tasks as input tokens using (lookback, future) pairs, leveraging the in-context learning capabilities of large language models (LLMs) without requiring parameter updates. The approach is more parameter-efficient, reduces overfitting, and outperforms existing Transformer-based models across full-data, few-shot, and zero-shot scenarios.

Inference Scaling for Long-Context Retrieval Augmented Generation
This paper investigates the scaling of inference computation for retrieval augmented generation (RAG) in long-context large language models, exploring strategies such as in-context learning and iterative prompting to enhance performance. The authors establish inference scaling laws for RAG, demonstrating nearly linear performance gains with optimal configuration and develop a computation allocation model that accurately predicts optimal inference parameters, achieving up to a 58.9% improvement on benchmark datasets.

Infilling Score: A Pretraining Data Detection Algorithm for Large Language Models
This paper introduces Infilling Score, a novel test-statistic based on non-causal token likelihoods for detecting whether a sentence is part of a Large Language Model's pretraining dataset. The proposed method shows improved accuracy over existing methods like Min-K% and Min-K%++, is invariant to vocabulary size, and demonstrates stronger performance on benchmarks such as WikiMIA and MIMIR, while also establishing a new benchmark dataset for Llama-3.

Innovative Thinking, Infinite Humor: Humor Research of Large Language Models through Structured Thought Leaps
This paper introduces a framework called LoL, designed to improve humor generation in language models by facilitating multi-hop reasoning through the inclusion of external information to combat knowledge graph sparsity. Using automatic instruction evolution and reinforcement learning, the framework enhances judgment and generative capabilities, demonstrating improved creative capabilities of large language models for cross-domain innovations.

Inspection and Control of Self-Generated-Text Recognition Ability in Llama3-8b-Instruct
This paper investigates whether language models, specifically the Llama3-8b–Instruct chat model, can recognize their own outputs versus human-generated text, a phenomenon with significant implications for AI safety. The study reveals that a specific vector in the model's residual stream is linked to its self-authorship recognition capability and can be manipulated to control the model's assertions of authorship and belief about text origin, suggesting potential for deeper understanding and control of AI behavior.

Interpretable Vision-Language Survival Analysis with Ordinal Inductive Bias for Computational Pathology
This paper introduces a novel Vision-Language-based Survival Analysis (VLSA) paradigm for better prognostic representation learning in computational pathology using histopathology Whole-Slide Images (WSIs). By leveraging pathology vision-language foundation models and introducing ordinal survival prompt learning and an ordinal incidence function, the approach offers a data-efficient and interpretable solution for survival analysis, validated on five datasets, paving the way for improved multi-instance learning with gigapixel WSIs.

Inverse Rendering using Multi-Bounce Path Tracing and Reservoir Sampling
We introduce MIRReS, a two-stage inverse rendering framework that reconstructs and optimizes geometry, materials, and lighting from multi-view images, beginning with the extraction of an explicit triangular mesh and refined using a physically-based model with multi-bounce path tracing. By incorporating reservoir sampling to address noise in Monte Carlo integration, our method achieves state-of-the-art performance in intrinsic decomposition, excelling in scenarios with complex shadows and supporting applications like scene editing, relighting, and material editing.

kNN Attention Demystified: A Theoretical Exploration for Scalable Transformers
Transformers struggle with long sequences due to the quadratic complexity of self-attention, but $k$-Nearest-Neighbor ($k$NN) attention can improve efficiency by focusing on a token's $k$ closest tokens. This paper introduces a theoretical framework for $k$NN attention and proposes novel sub-quadratic algorithms for efficient self-attention approximation, validated through empirical experiments demonstrating their efficacy in training and inference.

Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice
This paper explores the potential of using Large Language Models (LLMs) as models of human cognition by proposing a novel approach that leverages computationally equivalent tasks and examines specific task distributions. The study demonstrates that a small language model pretrained on a relevant arithmetic dataset, termed Arithmetic-GPT, can predict human decision-making behavior more effectively than traditional cognitive models, highlighting the importance of careful pretraining data selection in developing language models as cognitive models.

Large Language Models Assume People are More Rational than We Really are
This paper examines how Large Language Models (LLMs) simulate and predict human decision-making, revealing that they inaccurately assume people behave more rationally than they do. By comparing LLM behavior with a large dataset of human decisions, the study finds that LLMs align more with expected value theory, mirroring the common human expectation of rationality in others rather than actual human behavior.

Latent Action Pretraining from Videos
We introduce Latent Action Pretraining for general Action models (LAPA), an unsupervised method that enables Vision-Language-Action (VLA) models to be pretrained on internet-scale videos without requiring ground-truth robot action labels. Our approach outperforms existing models by learning discrete latent actions and using these to pretrain and finetune on small-scale robot data, achieving superior performance on real-world manipulation tasks and demonstrating the potential for leveraging web-scale data.

Learning How Hard to Think: Input-Adaptive Allocation of LM Computation
This paper introduces a method for adaptively allocating computational resources during language model decoding to optimize output quality, focusing on inputs predicted to benefit most from additional computation. By employing techniques such as an adaptive best-of-$k$ sampling procedure and a dynamic routing process, their approach successfully reduces computation by up to 50% without sacrificing output quality across programming, mathematics, and dialog tasks.

Learning system dynamics without forgetting
This paper tackles the challenge of Continual Dynamics Learning (CDL) for systems with evolving dynamics, moving beyond traditional methods that focus on fixed dynamics. It introduces the Mode-switching Graph ODE (MS-GODE) model and a new benchmark, Bio-CDL, for efficient learning across varying dynamics in biological systems, broadening machine learning's applications in dynamic systems research.

Learning to Solve Differential Equation Constrained Optimization Problems
This paper presents a learning-based approach to optimize systems governed by differential equations, integrating proxy optimization and neural differential equations techniques. The proposed dual-network architecture enhances the precision of results by up to 25 times compared to existing methods, while fully complying with dynamic constraints in energy and finance applications.

Learning to Steer Markovian Agents under Model Uncertainty
This paper addresses the challenge of steering multi-agent systems towards desired policies without prior knowledge of their learning dynamics by introducing a model-based non-episodic Reinforcement Learning (RL) framework. The authors propose a novel objective function to develop history-dependent steering strategies under model uncertainty, presenting empirical algorithms that demonstrate effective exploration of these strategies through theoretical insights and empirical evaluations.

Less is More: Masking Elements in Image Condition Features Avoids Content Leakages in Style Transfer Diffusion Models
This paper introduces a masking-based method to improve text-to-image diffusion models' ability to separate content and style in style-reference images, addressing the issue of content leakage. By selectively masking elements of the style reference image's features, the method enhances style transfer performance without model parameter tuning, supported by theoretical analysis and extensive experiments.

Let the Code LLM Edit Itself When You Edit the Code
This paper introduces Positional Integrity Encoding (PIE) to enhance the efficiency and accuracy of code assistants when developers edit code in real-time by addressing the temporal confusion in large language models' predictions. PIE reduces computational overhead by over 85% compared to full recomputation while maintaining model performance, validated through experiments on a coding dataset with models of various sizes.

LIFe-GoM: Generalizable Human Rendering with Learned Iterative Feedback Over Multi-Resolution Gaussians-on-Mesh
This paper presents an approach for rendering animatable human avatars from sparse inputs by improving a dual shape representation of mesh and Gaussian points. The proposed framework introduces an iterative feedback update for enhanced reconstruction and a coupled multi-resolution Gaussians-on-Mesh representation for efficient, high-quality rendering, achieving superior performance on benchmark datasets compared to state-of-the-art methods.

Linear Combination of Saved Checkpoints Makes Consistency and Diffusion Models Better
This paper introduces LCSC, a method that significantly improves the training convergence and performance of Diffusion Models (DM) and Consistency Models (CM) by effectively merging intermediate weight checkpoints. The approach reduces training cost and enhances pre-trained models' generation quality, demonstrating substantial speedups and efficiency gains across various tasks, including noticeable improvements in text-to-image models.

MAESTRO: Masked Encoding Set Transformer with Self-Distillation
MAESTRO is a self-supervised set representation learning model designed to enhance the analysis of high-dimensional cytometry data by generating vector representations of set-structured data, capturing comprehensive immune profiles from all cells in a sample. The model introduces specialized attention mechanisms and an online tokenizer, outperforming existing methods in accurately retrieving cell-type proportions and clinically relevant features, thus improving tasks like disease diagnosis and immune cell profiling.

Make Haste Slowly: A Theory of Emergent Structured Mixed Selectivity in Feature Learning ReLU Networks
This paper establishes an equivalence between ReLU networks and Gated Deep Linear Networks to derive learning dynamics, aiming to bridge the gap in understanding feature learning in finite ReLU networks. It highlights that ReLU networks, particularly with multiple contexts and hidden layers, are biased towards structured, mixed-selective latent representations that facilitate node-reuse and learning speed.

ManiSkill-HAB: A Benchmark for Low-Level Manipulation in Home Rearrangement Tasks
MS-HAB is introduced as a comprehensive benchmark aimed at enhancing embodied AI research, particularly focusing on low-level manipulation and in-home object rearrangement. The benchmark features a GPU-accelerated implementation that significantly improves simulation speed and efficiency, along with extensive reinforcement and imitation learning baselines, and a trajectory filtering system for safe and controlled data generation.

MANTRA: The Manifold Triangulations Assemblage
The paper introduces MANTRA, the first large-scale, diverse dataset specifically designed for benchmarking high-order models in topological deep learning, with over 43,000 and 250,000 triangulations of surfaces and 3D manifolds. The research highlights that while simplicial complex-based neural networks typically outperform graph-based models in capturing simple topological invariants, their limitations suggest a need for revisiting and improving approaches in topological deep learning.

Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs
The paper introduces Mask-DPO, a fine-grained factuality alignment method using Direct Preference Optimization, addressing hallucinations in large language models by focusing on sentence-level factuality to enhance learning from factually correct content. Experimental results show Mask-DPO significantly improves factuality in language model responses, outperforming larger models on certain test sets and providing insights into the generalization properties and scalability of factuality alignment methods.

McEval: Massively Multilingual Code Evaluation
This paper introduces McEval, a multilingual code benchmark evaluating code large language models (LLMs) across 40 programming languages, with 16K test samples for tasks such as code understanding, completion, and generation. Additionally, the paper presents mCoder, a multilingual coder trained on a finely curated instruction corpus, highlighting the gap between open-source models and closed-source LLMs in handling diverse languages; resources are available at their GitHub repository.

Memory Efficient Transformer Adapter for Dense Predictions
META is a novel Vision Transformer adapter designed to improve memory efficiency and decrease memory time consumption by minimizing inefficient memory access operations such as standard normalization and frequent reshaping. It employs a memory-efficient adapter block and a cross-shaped self-attention mechanism, while also incorporating a lightweight convolutional branch to enhance local inductive biases, resulting in superior performance on dense prediction tasks and achieving a new state-of-the-art accuracy-efficiency trade-off.

MetaDesigner: Advancing Artistic Typography through AI-Driven, User-Centric, and Multilingual WordArt Synthesis
MetaDesigner presents a novel framework for creating customizable WordArt by utilizing Large Language Models (LLMs) within a user-centric and multi-agent system, including Pipeline, Glyph, and Texture agents. The system's feedback mechanism, supported by multimodal models and user evaluations, allows for dynamic refinement of design parameters, producing visually appealing and contextually relevant WordArt consistently, as demonstrated by empirical evaluations.

MetaUrban: An Embodied AI Simulation Platform for Urban Micromobility
This paper introduces MetaUrban, a compositional simulation platform designed for AI-driven urban micromobility research, capable of generating an endless variety of interactive urban scenes to enhance the safety and generalizability of AI models in public spaces. By establishing baselines with Reinforcement and Imitation Learning and conducting extensive evaluations and ablation studies, the authors demonstrate how MetaUrban improves AI policy learning and execution, paving the way for safer and more effective micromobility solutions in future urban environments.

MeToken: Uniform Micro-environment Token Boosts Post-Translational Modification Prediction
This paper introduces the MeToken model, which predicts post-translational modification (PTM) sites by integrating both protein sequence and structural information into structured tokens. The model demonstrates superior performance in accurately identifying PTM types across datasets by capturing spatial arrangements and addressing long-tail distribution, highlighting its potential to enhance proteomics research.

MIND over Body: Adaptive Thinking using Dynamic Computation
This paper introduces a self-introspection capability in neural networks, allowing them to adjust parameter usage and computation time based on task and input complexity, thus addressing inefficiencies in traditional deep learning models. The proposed method surpasses larger models in performance on tasks like ImageNet and SQuAD, demonstrating its potential to create intelligent systems that manage resources efficiently.

MIRACLE 3D: Memory-efficient Integrated Robust Approach for Continual Learning on 3D Point Clouds via Shape Model Construction
This paper presents a new framework for memory-efficient and privacy-preserving continual learning in 3D object classification by creating a compact shape model for each class, retaining only the mean shape and key variation modes. The approach, which includes Gradient Mode Regularization to enhance robustness, achieves state-of-the-art performance with significantly reduced memory usage on the ModelNet40, ShapeNet, and ScanNet datasets, emphasizing its scalability and effectiveness.

Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and Post-LN
The paper identifies a training shortfall in Large Language Models caused by Pre-Layer Normalization, leading to ineffective deeper layers, and introduces Mix-LN, which combines Pre-LN and Post-LN to ensure more uniform gradient norms across layers. This novel technique enhances model capacity and effectiveness without increasing size, outperforming existing methods in both pre-training and supervised fine-tuning.

Mixture of In-Context Prompters for Tabular PFNs
MixturePFN enhances In-Context Learning for tabular data by utilizing a Sparse Mixture of Experts approach, targeting both efficiency and effectiveness challenges with large datasets. By clustering data and finetuning specialized experts, it significantly outperforms 19 baseline models across diverse datasets, showing superior accuracy and F1 scores.

MMFakeBench: A Mixed-Source Multimodal Misinformation Detection Benchmark for LVLMs
The paper introduces MMFakeBench, the first comprehensive benchmark designed for mixed-source multimodal misinformation detection, addressing the limitations of current methods that consider only single-source forgeries. By evaluating existing detection methods and proposing MMD-Agent, a novel approach that enhances the capabilities of vision-language models, the study aims to advance the accuracy and generalization of misinformation detection in more realistic scenarios.

MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge
MMKE-Bench is introduced as a comprehensive **M**ulti**M**odal **K**nowledge **E**diting Benchmark to evaluate the ability of multimodal models (LMMs) to update and edit diverse visual knowledge in real-world scenarios using free-form natural language. This benchmark addresses the limitations of existing benchmarks by encompassing visual entity, visual semantic, and user-specific editing tasks, thus setting a new standard for assessing the robustness of multimodal knowledge editing techniques and highlighting the challenges present in current state-of-the-art methods.

MotherNet: Fast Training and Inference via Hyper-Network Transformers
The paper introduces MotherNet, a hypernetwork architecture capable of generating weights for a neural network using in-context learning from a novel dataset in a single forward pass, targeting classification tasks on tabular data. MotherNet's generated networks outperform gradient descent-trained neural networks on small datasets and offer competitive performance with methods like TabPFN and Gradient Boosting, while being more efficient in inference time.

MotionClone: Training-Free Motion Cloning for Controllable Video Generation
MotionClone is a training-free framework that enables versatile motion-controlled video generation by cloning motion from reference videos for applications like text-to-video and image-to-video. By utilizing sparse temporal attention weights for motion guidance, MotionClone achieves superior motion fidelity, textual alignment, and efficiency without the need for complex inversion processes.

MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation
This paper introduces a new evaluation approach for Large Language Models (LLMs) called meta-reasoning, which focuses on assessing the cognitive processes behind reasoning rather than just the results. By applying this paradigm to the GSM8K dataset and creating the MR-GSM8K benchmark, the study highlights the shortcomings of current models, revealing that OpenAI o1 models demonstrate superior "system-2" thinking abilities over other state-of-the-art models by more than 20 points.

Multi-modal Learning: A Look Back and the Road Ahead
This paper explores the alignment between human-defined modalities and machine perception in multi-modal AI, questioning how misalignments might affect AI performance. It reviews current multi-modal benchmarks and architectures, identifies their limitations, and proposes improved methodologies for handling multi-modal data by considering both independent and joint modality contributions.

Multiplicative Logit Adjustment Approximates Neural-Collapse-Aware Decision Boundary Adjustment
This paper investigates the effectiveness of multiplicative logit adjustment (MLA) for addressing the imbalance in long-tailed recognition tasks and offers a theoretical foundation for its success. It demonstrates that MLA approximates an optimal adjustment method based on neural collapse and confirms its practicality through experiments on long-tailed datasets while providing insights for hyperparameter tuning.

Near, far: Patch-ordering enhances vision foundation models' scene understanding
NeCo: Patch Neighbor Consistency is a novel self-supervised training loss that enforces patch-level nearest neighbor consistency, leveraging differentiable sorting on pretrained representations to produce high-quality dense feature encoders. This approach improves performance across models and datasets, achieving significant state-of-the-art results in tasks such as semantic segmentation and 3D understanding with efficient GPU usage.

NetFormer: An interpretable model for recovering dynamical connectivity in neuronal population dynamics
The NetFormer model is introduced to address the limitations of traditional methods by capturing nonstationary and nonlinear dynamics in neuronal systems, incorporating synaptic plasticity and neuronal modulation. It effectively predicts neural dynamics and identifies cell-type specific, state-dependent connectivity, demonstrating its ability to model complex neural interactions using real multi-modal neural recording data.

Neural Approximate Mirror Maps for Constrained Diffusion Models
This paper introduces neural approximate mirror maps (NAMMs) to address constraints in diffusion models, which often struggle with constraint satisfaction, whether they are physics-based, geometric, or semantic. By learning an approximate mirror map and its inverse, this approach transforms data into an unconstrained space for model training and effectively restores constraints, significantly improving constraint satisfaction and facilitating the application of diffusion-based inverse-problem solvers in complex, non-convex constraint scenarios.

No Need to Talk: Asynchronous Mixture of Language Models
SMALLTALK LM is a novel approach for training a mixture of language models asynchronously, where each model specializes in different data subsets and requires minimal communication between training nodes. The method uses a lightweight router to direct sequences to specific experts, achieving lower perplexity and better performance on most downstream tasks compared to dense model baselines, without needing full corpus clustering or metadata.

Non-Equilibrium Dynamics of Hybrid Continuous-Discrete Ground-State Sampling
This paper introduces a hybrid continuous-discrete algorithm that uses continuous-time dynamics and Metropolis-Hastings steps to explore rugged energy landscapes. The proposed method significantly improves convergence speed and accuracy in ground-state sampling, achieving up to 100x speedup on GPUs compared to simulated annealing, especially on biased Ising problem instances.

Non-Stationary Dueling Bandits Under a Weighted Borda Criterion
This paper addresses the non-stationary $K$-armed dueling bandits problem, specifically focusing on the Borda winner, which has been less explored compared to the Condorcet winner. The authors introduce a novel weighted Borda score framework and achieve an optimal dynamic regret upper bound, offering improved theoretical bounds in scenarios with multiple arms or frequent changes in the Borda winner, highlighting a new generalization that applies to both Borda and Condorcet problems.

Normed Spaces for Graph Embedding
This paper demonstrates that normed spaces offer a computationally efficient and flexible alternative to Riemannian manifolds for graph embeddings, achieving superior performance on graph reconstruction tasks across various benchmark datasets. Additionally, normed space embeddings show effectiveness in practical applications such as link prediction and recommender systems, presenting a valuable tool for geometric graph representation learning.

Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning
HeadKV is a head-level KV cache compression method that enhances the efficiency of Large Language Models by selectively retaining key information at the level of individual attention heads, optimizing for contextual reasoning and retrieval in low-resource settings. Extensive experiments show that HeadKV achieves 97% of the performance of the full KV cache while retaining only 1.5% of it, significantly outperforming strong baselines in various benchmarks and model architectures.

O(d/T) Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions
This paper establishes a fast convergence theory for the denoising diffusion probabilistic model (DDPM) with minimal assumptions, improving upon existing theoretical guarantees. By providing a novel analytical framework, it demonstrates that with $\ell_{2}$-accurate score function estimates, the total variation distance between target and generated distributions is bounded by $O(d/T)$, applicable to any target distribution with finite first-order moment.

One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs
ArrAttack introduces a novel jailbreak attack method designed to penetrate defended large language models (LLMs), effectively bypassing existing defense mechanisms such as defensive suffixes. By leveraging a universal robustness judgment model, ArrAttack significantly outperforms existing strategies in generating robust prompts that convert malicious inputs into effective attacks, demonstrating strong transferability across various models, including GPT-4 and Claude-3.

On Evaluating the Durability of Safeguards for Open-Weight LLMs
This paper examines the efficacy of technical safeguards designed to protect large language models (LLMs) from misuse, focusing on their robustness even when model weights can be fine-tuned. It highlights the challenges and potential missteps in evaluating the durability of these safeguards, urging future research to adopt more precise and thoroughly-tested threat models for honest assessments.

On the Adversarial Vulnerability of Label-Free Test-Time Adaptation
This paper presents a novel attack algorithm for Test-time adaptation (TTA) that does not rely on labeled test samples, addressing the impracticality of current threat models that assume test-time samples are labeled. Through theoretical foundations and extensive experiments, it demonstrates that the proposed attack generates strong threats to current TTA methods and highlights the ineffectiveness of existing defense mechanisms, emphasizing the need for enhanced security research.

On the Benefits of Memory for Modeling Time-Dependent PDEs
This paper explores the advantages of incorporating memory in the modeling of time-dependent partial differential equations (PDEs) and introduces the Memory Neural Operator (MemNO), which combines state space models and Fourier Neural Operators. The authors demonstrate theoretically and empirically that MemNO outperforms traditional Markovian approaches, particularly in scenarios with low-resolution data or observation noise, achieving up to a 6x reduction in test error, especially for PDEs with significant high-frequency components.

On the Optimization and Generalization of Multi-head Attention
This paper explores the optimization and generalization benefits of using multiple attention heads in Transformer's Attention mechanism, by deriving convergence and generalization guarantees for a single-layer multi-head self-attention model under certain conditions. It establishes initialization conditions that ensure realizability and demonstrates their applicability using a tokenized-mixture model, with potential extensions to various data-model and architecture variations.

Oryx MLLM: On-Demand Spatial-Temporal Understanding at Arbitrary Resolution
Oryx is a unified multimodal architecture designed for the efficient spatial-temporal understanding of diverse visual data, including images, videos, and 3D scenes, by introducing a pre-trained OryxViT model and a dynamic compressor module for adaptable visual token compression. This approach allows Oryx to handle visual inputs of varying sizes and lengths, ensuring precision in tasks like document understanding while remaining efficient for lengthy contexts, such as videos, bolstered by enhanced data curation and training techniques.

Parameter and Memory Efficient Pretraining via Low-rank Riemannian Optimization
The paper introduces a Low-rank Riemannian Optimizer (LORO) to enhance parameter efficiency in the pretraining of large language models by parameterizing each full-size weight as the product of two trainable low-rank factors. The LORO method achieves comparable performance to full-size models while significantly reducing memory usage and accelerating both training and inference, demonstrated by a LLaMA 1B model with improved perplexity, reduced memory, and faster processing times.

PEARL: Towards Permutation-Resilient LLMs
This paper addresses the vulnerability of large language models (LLMs) to input permutation attacks by introducing Permutation-resilient learning (PEARL), a novel framework that enhances model robustness against such attacks using distributionally robust optimization. PEARL, comprising a permutation-proposal network and an LLM, successfully mitigates permutation attacks and improves model performance, achieving up to 40% gains in many-shot and long-context scenarios, showcasing its efficient and generalizing capabilities.

PEAR: Primitive Enabled Adaptive Relabeling for Boosting Hierarchical Reinforcement Learning
This paper introduces Primitive Enabled Adaptive Relabeling (PEAR), a novel two-phase approach for training hierarchical reinforcement learning (HRL) agents by integrating reinforcement learning and imitation learning with adaptive relabeling based on a few expert demonstrations. Through theoretical analysis and extensive experiments on challenging environments, including real-world robotic tasks, the study demonstrates that PEAR achieves higher success rates and performance compared to traditional hierarchical and non-hierarchical baselines.

Pedestrian Motion Reconstruction: A Large-scale Benchmark via Mixed Reality Rendering with Multiple Perspectives and Modalities
The paper introduces the Pedestrian Motion Reconstruction (PMR) dataset designed to enhance understanding of pedestrian intention in dynamic sensor environments to improve autonomous driving safety. PMR offers a rich collection of diverse interactions and scenarios using a mixed reality platform, enabling comprehensive analyses of pedestrian behavior through multi-view, multi-modal data, and benchmark assessments.

Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems
This paper investigates the incorporation of "error-correction" data, consisting of erroneous steps followed by corrections, directly into the pretraining of language models to enhance reasoning accuracy. Using a synthetic math dataset, the study shows that this data can improve performance through simple auto-regression without multi-round prompting and provides insights into various aspects such as data preparation and the role of erroneous tokens.

pMoE: Prompting Diverse Experts Together Wins More in Visual Adaptation
The paper introduces pMoE, a novel Mixture-of-Experts prompt tuning method that integrates diverse domain knowledge using expert-specific prompt tokens and a dynamic token dispatching mechanism. Extensive experiments across 47 adaptation tasks show that pMoE significantly enhances model versatility, achieving superior performance and an optimal balance between computational efficiency and adaptation effectiveness compared to existing methods.

Policy Decorator: Model-Agnostic Online Refinement for Large Policy Model
This paper introduces Policy Decorator, a model-agnostic residual policy that refines imitation learning models through online interactions, addressing limitations in demonstration quantity, quality, and diversity. Evaluated on eight tasks across ManiSkill and Adroit benchmarks, Policy Decorator enhances offline-trained policies while maintaining smooth motion, outperforming traditional reinforcement learning in stability and sample efficiency.

PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation
The paper introduces PQMass, a likelihood-free statistical method for comparing distributions to evaluate generative models, using chi-squared tests to determine if sample distributions are statistically similar. PQMass effectively assesses sample quality, novelty, and diversity across various data types and dimensions without needing density assumptions or auxiliary models, proving scalable for high-dimensional data.

Preserving Deep Representations in One-Shot Pruning: A Hessian-Free Second-Order Optimization Framework
SNOWS is a one-shot post-training pruning framework designed to efficiently reduce the inference cost of vision networks without the need for retraining, optimizing a global reconstruction objective that accounts for deep network nonlinear activations. By leveraging Hessian-free optimization, SNOWS adjusts weights to maintain deep feature representations, achieving state-of-the-art results on various one-shot pruning benchmarks, such as residual networks and Vision Transformers.

Process Reward Model with Q-value Rankings
The paper introduces the Process Q-value Model (PQM), a novel framework for Process Reward Modeling (PRM) that addresses the limitations of traditional classification-based approaches by redefining PRM as a Markov Decision Process. PQM utilizes a comparative loss function to improve the modeling of step interdependencies and shows superior performance across various benchmarks, highlighting both its practical efficacy and theoretical advantages over existing methods.

Progressive Compression with Universally Quantized Diffusion Models
This paper explores the use of diffusion probabilistic models for progressive image compression, developing a novel diffusion model with uniform noise that optimizes compression costs via universal quantization. The method demonstrates promising results in rate-distortion-realism tradeoffs across various bit-rates, enhancing the practical deployment potential of neural codecs.

Progress or Regress? Self-Improvement Reversal in Post-training
This study critically examines the impact of iterative preference learning on the self-improvement of Large Language Models (LLMs), revealing that enhancements in accuracy may sometimes coincide with declines in essential capabilities—a phenomenon termed "self-improvement reversal." By proposing an evaluative framework, the research aims to differentiate between superficial metric improvements and genuine enhancements, highlighting the complexity of advancements in LLMs and the importance of understanding whether these developments represent true progress or regression.

Protein Language Model Fitness is a Matter of Preference
This study explores the predictive capabilities of protein language models (pLMs) in zero-shot fitness estimation and identifies the conditions under which they succeed or fail, using deep mutational scans across various fitness objectives. The research finds that the likelihood embedded during pretraining is indicative of predictive success, with unsupervised finetuning potentially enhancing performance for sequences of low likelihood, guiding better deployment strategies for pLMs in protein engineering.

Provably Robust Explainable Graph Neural Networks against Graph Perturbation Attacks
This paper introduces XGNNCert, the first provably robust explanation method for Graph Neural Networks (GNNs) that ensures explanation stability under worst-case graph perturbation attacks without impacting GNN predictions, provided the number of perturbed edges is limited. The approach is validated on multiple datasets, demonstrating its effectiveness in maintaining reliable explanations for safety and security-critical applications.

Quantitative Approximation for Neural Operators in Nonlinear Parabolic Equations
This paper establishes the approximation rate of neural operators for nonlinear parabolic PDEs, demonstrating that they efficiently approximate solution operators without exponential growth in complexity and reinforcing the theoretical basis of neural operators. By converting PDEs into integral equations using Duhamel's principle and drawing parallels with Picard’s iteration, the approach suggests broader applicability to PDEs solvable by Picard's iteration.

Rare event modeling with self-regularized normalizing flows: what can we learn from a single failure?
CalNF, or calibrated normalizing flows, is introduced as a self-regularized framework that improves the modeling of rare failure events in autonomous systems where data is limited. It demonstrates state-of-the-art performance in data-limited scenarios and provides insights into critical issues such as the 2022 Southwest Airlines scheduling crisis.

Real2Code: Reconstruct Articulated Objects via Code Generation
Real2Code is a novel method for reconstructing articulated objects by generating code from visual observations, using image segmentation, shape completion, and oriented bounding boxes. It leverages pre-trained vision and language models to significantly outperform existing methods in reconstruction accuracy and generalizes effectively to real-world scenarios with minimal input data.

Rectified Diffusion: Straightness Is Not Your Need in Rectified Flow
This paper introduces Rectified Diffusion, which generalizes the design and application of rectification for diffusion models by focusing on achieving a first-order approximate ODE path. By demonstrating that pretrained diffusion models with matched noise-sample pairs eliminate the need for certain components of rectified flow while improving performance and reducing training costs, the method significantly simplifies the training procedure and is validated on models such as Stable Diffusion v1-5 and Stable Diffusion XL.

Reframing Structure-Based Drug Design Model Evaluation via Metrics Correlated to Practical Needs
This paper introduces a model-level evaluation framework for structure-based drug design (SBDD) that focuses on metrics aligned with real-world applications, specifically the ability to retrieve active compounds from chemical libraries. The proposed framework addresses the limitations of relying solely on theoretical metrics like Vina docking scores and aims to improve the practical relevance of SBDD models in pharmaceutical research and development.

Regularization by Texts for Latent Diffusion Inverse Solvers
This paper introduces TReg, a novel latent diffusion inverse solver that uses textual descriptions as regularization to resolve ambiguities in inverse problems. By employing adaptive negation to dynamically reinforce preconceptions during reverse diffusion sampling, TReg enhances accuracy and efficiency in solving these problems.

Representational Similarity via Interpretable Visual Concepts
This paper introduces the Representational Similarity Visual Comparison (RSVC) method to interpretably compare two deep neural networks, identifying both shared and unique visual concepts between models. The study highlights that differences in model decisions can stem from unique concepts captured by one model but not adequately represented in the other, with extensive evaluations demonstrating RSVC's effectiveness across various vision model architectures and training protocols.

Rethinking Artistic Copyright Infringements In the Era Of Text-to-Image Generative Models
This paper introduces ArtSavant, an automatic and interpretable framework designed to quantitatively assess style infringement by text-to-image generative models. Through an empirical study, the authors reveal that 20% of artists' styles may be susceptible to copying, highlighting the need to operationalize legal arguments in new ways to better understand and address artistic style infringement.

Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words
The paper proposes a suite of evaluations for sparse autoencoders (SAEs) to assess their ability to capture interpretable monosemantic features from polysemous words, highlighting that traditional metrics like MSE-$\mathrm{L}_{0}$ may not effectively measure semantic representational power. The findings suggest that while SAEs developed to optimize these traditional metrics may not enhance interpretability, analyzing SAEs with polysemous words can shed light on the internal mechanisms of large language models, particularly noting the role of deeper layers and the Attention module in distinguishing word meanings.

Rethinking Shapley Value for Negative Interactions in Non-convex Games
This paper reformulates the Shapley value in cooperative game theory to account for player interactions, addressing implicit assumptions and inefficiencies particularly relevant to deep learning models. By introducing a new allocation rule and an approximation algorithm, the authors enhance feature attribution methods to better reflect the true contribution of features in non-convex games, thereby mitigating misleading outcomes in existing attribution methodologies.

Revisiting Convolution Architecture in the Realm of DNA Foundation Models
This paper introduces ConvNova, a CNN-based method incorporating dilated and gated convolutions and a dual-branch framework, demonstrating superior performance to recent Transformer and SSM-based approaches on several foundation model benchmarks. The findings suggest CNNs are still competitive, achieving notable improvements in tasks like histone analysis, potentially renewing interest in CNN applications for DNA language models.

Revisiting Nearest Neighbor for Tabular Data: A Deep Tabular Baseline Two Decades Later
This paper revisits and modernizes Neighbourhood Components Analysis (NCA), a differentiable version of $K$-nearest neighbors (KNN), for tabular data by incorporating advanced deep learning techniques. The study finds that their enhanced NCA achieves performance comparable to leading methods like CatBoost and surpasses existing deep tabular models in classification and regression tasks across 300 datasets.

Reward Dimension Reduction for Scalable Multi-Objective Reinforcement Learning
This paper presents a novel reward dimension reduction method to improve the scalability and efficiency of multi-objective reinforcement learning algorithms. The proposed approach, designed for online learning, maintains Pareto-optimality and outperforms existing methods in complex environments, including scenarios with up to sixteen objectives.

Rewarding Progress: Scaling Automated Process Verifiers for LLM Reasoning
This paper explores the use of process reward models (PRMs) to enhance reasoning in large language models by providing feedback at each step of a reasoning trace. It introduces the concept of process rewards measuring progress under prover policies to improve exploration efficiency, demonstrating that process advantage verifiers (PAVs) can significantly outperform outcome reward models (ORMs) in accuracy and compute efficiency, achieving a notable 6x gain in sample efficiency for online reinforcement learning.

Reward Learning from Multiple Feedback Types
This paper explores the use of diverse types of human feedback, beyond just preference-based feedback, in learning reward models for agentic models. By generating and implementing six different types of simulated feedback across ten RL environments, the study demonstrates that incorporating multi-type feedback can enhance reward modeling performance, marking a significant step forward in Reinforcement Learning from Human Feedback (RLHF).

Robustness Auditing for Linear Regression: To Singularity and Beyond
This paper introduces an efficient algorithm for certifying the robustness of linear regressions to the removal of samples, addressing the limitations of existing methods that fail on high-dimensional or large datasets. The algorithm is validated on several landmark econometrics datasets, offering the first meaningful robustness certificates for high-dimensional data and proving that its bounds are nearly optimal under certain distributional assumptions.

Robust-PIFu: Robust Pixel-aligned Implicit Function for 3D Human Digitalization from a Single Image
Robust-PIFu is a pixel-aligned implicit model that leverages pretrained latent diffusion models to effectively digitalize human subjects from images with occlusions, addressing challenges such as incomplete observations and self-occlusions. The approach introduces innovations like disentangling and penetrating latent diffusion models, improves structural accuracy with a Layered-Normals model, and offers a super-resolution mechanism, outperforming current state-of-the-art methods in both qualitative and quantitative evaluations.

Routing Experts: Learning to Route Dynamic Experts in Existing Multi-modal Large Language Models
The paper introduces Routing Experts (RoE), a novel dynamic expert routing method for multimodal large language models (MLLMs), allowing them to achieve example-dependent optimal path routing without major structural changes. Extensive experiments demonstrate that RoE significantly enhances the efficiency and performance of MLLMs, outperforming existing methods like MoE-LLaVA with an average performance gain of 3.3% on multiple benchmarks while being 1.61 times faster.

Safety Alignment Should be Made More Than Just a Few Tokens Deep
This paper identifies the issue of "shallow safety alignment" in Large Language Models (LLMs), where safety measures only affect the initial tokens, making models vulnerable to various attacks. The authors demonstrate that deepening safety alignment and using a regularized fine-tuning approach can enhance robustness against these vulnerabilities, suggesting important directions for future research in model safety.

Scaling LLM Test-Time Compute Optimally Can be More Effective than Scaling Parameters for Reasoning
This paper explores how increasing inference-time computation for large language models (LLMs) can enhance their performance on complex prompts, potentially impacting future LLM pretraining and the balance between inference-time and pretraining compute. It introduces a "compute-optimal" scaling strategy that dynamically allocates compute resources based on prompt difficulty, demonstrating a 4x improvement in efficiency for math reasoning tasks and outperforming a significantly larger model in a FLOPs-matched evaluation.

Scaling Optimal LR Across Token Horizons
This paper addresses the gap in understanding hyperparameter transfer across training token horizons in large language model (LLM) training, conducting a comprehensive study on how optimal learning rate (LR) depends on token horizon. The findings reveal that optimal LR decreases with longer training and follows a scaling law, allowing accurate estimation of LR for longer horizons from shorter ones, and suggesting that past models may have used suboptimal LRs.

Score Forgetting Distillation: A Swift, Data-Free Method for Machine Unlearning in Diffusion Models
This paper introduces Score Forgetting Distillation (SFD), a novel machine unlearning (MU) approach designed to improve the safety and trustworthiness of generative AI models by effectively forgetting undesirable information in diffusion models. By aligning the conditional scores of unsafe concepts with safe ones and incorporating a score-based MU loss, SFD enhances generation speed and quality without requiring real data, proving to be generalizable and effective across various diffusion models.

SecureGS: Boosting the Security and Fidelity of 3D Gaussian Splatting Steganography
3D Gaussian Splatting (3DGS) is an advanced method for 3D representation requiring enhanced privacy measures, as traditional solutions inadequately secure its accessible point cloud files. The proposed SecureGS framework overcomes these limitations by employing a hybrid decoupled Gaussian encryption mechanism and a density region-aware anchor strategy, significantly improving rendering fidelity, computational efficiency, and security compared to current GS steganography methods.

SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators
SeedLM is a novel post-training compression method for Large Language Models, which uses seeds from a pseudo-random generator and Linear Feedback Shift Registers to efficiently reconstruct model weights, reducing memory access and leveraging idle compute cycles for faster inference. Experiments show that SeedLM achieves comparable or better accuracy retention than state-of-the-art methods at 4- and 3-bit compression, with a significant speed-up observed in FPGA-based tests, particularly as model sizes scale.

SegLLM: Multi-round Reasoning Segmentation with Large Language Models
SegLLM is an innovative multi-round interactive reasoning segmentation model that utilizes conversational memory to improve LLM-based segmentation by reintegrating previous visual and textual segmentation outcomes for advanced reasoning. It demonstrated superior performance on the MRSeg benchmark, achieving over 20% improvement in multi-round interactive reasoning segmentation and enhancing single-round referring segmentation and localization tasks with notable accuracy increases.

Self-Updatable Large Language Models by Integrating Context into Model Parameters
This paper introduces SELF-PARAM, a method for updating large language models without the need for additional parameters, ensuring both high efficacy and long-term retention of experiences. By minimizing the KL divergence between an original model and a target model through diverse question-answer pairs, SELF-PARAM allows models to efficiently integrate and internalize knowledge, outperforming existing methods in question-answering and conversational tasks.

Semantics-Adaptive Activation Intervention for LLMs via Dynamic Steering Vectors
The paper introduces Semantics-Adaptive Dynamic Intervention (SADI), a novel method for aligning large language models (LLMs) by dynamically adjusting model behavior at inference through a semantics-driven steering vector. SADI improves task performance, demonstrates generalizability across different LLMs, and shows promise as an efficient alignment technique, outperforming existing baselines without the need for additional training.

SimBa: Simplicity Bias for Scaling Up Parameters in Deep Reinforcement Learning
The paper introduces SimBa, an architecture for deep reinforcement learning (RL) that effectively scales up network parameters while incorporating a simplicity bias to improve sample efficiency. By integrating components like observation normalization, a residual feedforward block, and layer normalization, SimBa enhances various RL algorithms and achieves or surpasses state-of-the-art performance across multiple challenging environments.

Simple Guidance Mechanisms for Discrete Diffusion Models
This paper introduces new diffusion models for discrete data that incorporate uniform noise and enhance guidability, specializing in controllable generation where existing continuous methods fall short. By applying a novel continuous-time variational lower bound, these models achieve state-of-the-art performance, outperforming autoregressive and diffusion baselines across various discrete data domains such as genomic sequences and image generation.

SmartRAG: Jointly Learn RAG-Related Tasks From the Environment Feedback
The paper introduces SmartRAG, a novel pipeline for RAG systems that integrates a policy network and a retriever, emphasizing the benefits of joint optimization for improved system performance. By employing a reinforcement learning algorithm, the study shows that this approach enhances coordination among modules, leading to superior results compared to systems with separately trained modules.

Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling
Speculative Knowledge Distillation (SKD) is introduced to overcome the limitations of existing knowledge distillation methods by fostering collaboration between student and teacher models to create high-quality training data dynamically, aligning with the student's inference-time distribution. Evaluations across various text generation tasks demonstrate that SKD consistently surpasses traditional methods in performance regardless of domain, data size, and model initialization.

SpinQuant: LLM Quantization with Learned Rotations
SpinQuant is introduced as a novel post-training quantization (PTQ) technique that applies learned rotation matrices to improve quantization accuracy of Large Language Models (LLMs) by effectively addressing outliers. By implementing 4-bit quantization for weight, activation, and KV-cache, SpinQuant significantly narrows the accuracy gap in zero-shot reasoning tasks and outperforms previous methods such as LLM-QAT, SmoothQuant, and QuaRot, demonstrating its efficacy in models like LLaMA-2 7B and LLaMA-3 8B.

Stiefel Flow Matching for Moment-Constrained Structure Elucidation
This paper addresses the challenge of predicting a molecule's all-atom 3D structure from its molecular formula and moments of inertia by leveraging rotational spectroscopy's precise measurements. The proposed Stiefel Flow Matching model, operating on the Stiefel manifold, improves success rates and sampling speed over traditional methods by enforcing exact moment constraints, particularly for large molecules.

Stochastic variance-reduced Gaussian variational inference on the Bures-Wasserstein manifold
This paper addresses challenges in Bures-Wasserstein space optimization within machine learning, particularly the high variance issue associated with the Monte Carlo estimator used for the forward step involving intractable expectations. By introducing a novel variance-reduced estimator using control variates, the authors demonstrate significant improvements in variance and optimization bounds over existing methods.

Supervised and Semi-Supervised Diffusion Maps with Label-Driven Diffusion
This paper introduces Supervised Diffusion Maps (SDM) and Semi-Supervised Diffusion Maps (SSDM), which enhance the traditional Diffusion Maps algorithm for supervised and semi-supervised learning by leveraging labels as a second data view. The innovative approach, involving a multiplicative interpolation of affinity kernels, effectively extracts the intrinsic structure of data and labels, resulting in superior performance in regression and classification tasks compared to existing dimension reduction methods.

Swift4D: Adaptive divide-and-conquer Gaussian Splatting for compact and efficient reconstruction of dynamic scene
The paper introduces Swift4D, a divide-and-conquer 3D Gaussian Splatting method designed to efficiently handle static and dynamic primitives separately for novel view synthesis. By employing a learnable decomposition strategy and a compact multi-resolution 4D Hash mapper, Swift4D achieves state-of-the-art rendering quality with significantly improved training speed, being 20 times faster and requiring minimal storage compared to previous methods.

SyllableLM: Learning Coarse Semantic Units for Speech Language Models
This paper introduces a self-supervised technique to create coarser, syllable-like speech representations that preserve semantic information and significantly improve efficiency in speech language modeling. The proposed method, which achieves state-of-the-art results in syllabic segmentation, reduces training compute by 30 times and speeds up inference by 4 times, while outperforming or matching existing Speech Language Models (SpeechLMs) in various tasks.

Synthesizing Realistic fMRI: A Physiological Dynamics-Driven Hierarchical Diffusion Model for Efficient fMRI Acquisition
This paper introduces the Physiological Dynamics-Driven Hierarchical Diffusion Model, a novel approach to synthesizing fMRI signals by integrating brain hierarchical regional interactions and multifractal dynamics into the diffusion process. By constructing hypergraphs based on resting-state functional connectivity and incorporating multifractal spectrum and generalized Hurst exponent predictions, the model generates physiologically realistic fMRI signals, potentially reducing acquisition time and improving data quality for clinical and machine learning applications in neuroscience.

TANGO: Co-Speech Gesture Video Reenactment with Hierarchical Audio Motion Embedding and Diffusion Interpolation
TANGO is a framework that generates high-fidelity co-speech body-gesture videos by improving upon Gesture Video Reenactment's limitations with innovative techniques for better audio-motion alignment and transition frame quality. By introducing the hierarchical joint embedding space AuMoClip for cross-modal alignment and the diffusion-based model ACInterp for consistent visual transitions, TANGO produces realistic, synchronized videos, outperforming existing methods.

TD-Paint: Faster Diffusion Inpainting Through Time Aware Pixel Conditioning
Diffusion models for inpainting are highly effective but suffer from slow sampling rates, limiting their scalability. TD-Paint, our proposed method, improves the diffusion process by using variable noise levels at the pixel level, resulting in faster and more efficient sampling without architectural changes, as demonstrated by superior performance across three datasets.

TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval
This paper introduces Temporal Token Merging (TempMe), a parameter-efficient architecture for text-video retrieval that reduces computational overhead by addressing both trainable parameter efficiency and temporal redundancy. TempMe demonstrates notable performance improvements, achieving a 7.9% R-Sum uplift and significant reductions in output tokens and GFLOPs, alongside faster training and decreased GPU memory usage.

TEOChat: A Large Vision-Language Assistant for Temporal Earth Observation Data
TEOChat is a newly developed vision and language assistant designed to handle and converse about temporal sequences of earth observation data, addressing the limitations of existing models that only work with single image inputs. This model demonstrates superior performance across a variety of spatial and temporal reasoning tasks and achieves notable zero-shot capabilities, outperforming previous models and even specialist systems, with all resources made publicly available for the research community.

The Hyperfitting Phenomenon: Sharpening and Stabilizing LLMs for Open-Ended Text Generation
This paper presents the surprising capability of fine-tuned large language models, termed as hyperfitting, to significantly enhance long-sequence generation from small datasets, challenging the typical issues of repetition and dullness, especially when using greedy decoding. The study demonstrates that these hyperfitted models outperform traditional sampling techniques in both creativity and human preference across various domains, and despite low-entropy predictions, they seldom fall into training data repetition.

The OMG dataset: An Open MetaGenomic corpus for mixed-modality genomic language modeling
The paper introduces the Open MetaGenomic (OMG) corpus, a comprehensive genomic pretraining dataset comprising 3.1 trillion base pairs and 3.3 billion protein coding sequences, designed to enhance biological language model performance by providing high-quality, diverse data. It also presents the development of the mixed-modality genomic language model (gLM2), which benefits from genomic context information to deliver improved functional representations and demonstrates the effectiveness of deduplication in embedding space for balanced dataset usage.

Theory on Score-Mismatched Diffusion Models and Zero-Shot Conditional Samplers
This paper provides the first performance guarantee for score-mismatched diffusion models, highlighting the asymptotic distributional bias resulting from mismatches between the target and training distributions in zero-shot conditional sampling scenarios. The study offers convergence bounds and guidelines for designing bias-optimal zero-shot samplers, with proven effectiveness demonstrated through numerical studies.

The Rise and Down of Babel Tower: Investigating the Evolution Process of Multilingual Code Large Language Model
This paper investigates the development of multilingual capabilities in large language models (LLMs) using code LLMs, proposing the Babel Tower Hypothesis that describes how languages initially share a primary knowledge system before developing language-specific ones. By validating this hypothesis and applying insights to create an optimized pre-training corpus, the study offers a method that enhances multilingual performance, offering new guidance for designing pre-training data distributions.

ThinkBot: Embodied Instruction Following with Thought Chain Reasoning
The paper introduces ThinkBot, an advancement in Embodied Instruction Following (EIF), which enhances action plans by reasoning through human instruction to resolve incoherence and missing action descriptions. Utilizing large language models and scene semantic maps, ThinkBot demonstrates superior success rates and efficiency over existing EIF methods in completing human goals within simulated environments.

ThunderKittens: Simple, Fast, and $\textit{Adorable}$ Kernels
ThunderKittens (TK) is a framework designed to simplify the creation of efficient AI kernels for GPU hardware by providing key abstractions at the warp, thread-block, and grid levels. TK demonstrates its effectiveness by matching or surpassing existing methods in performance for GEMM and attention inference, achieving significant speedups of up to 14x on linear attention.

TopoDiffusionNet: A Topology-aware Diffusion Model
This paper introduces TopoDiffusionNet (TDN), a novel approach to enhance diffusion models by enforcing images to maintain a specified topology using tools from topological data analysis like persistent homology. The approach significantly improves topological accuracy in generated images, addressing limitations in applications requiring exact control and paving the way for integrating topology with diffusion models.

Towards counterfactual fairness through auxiliary variables
This paper introduces the EXOgenous Causal reasoning (EXOC) framework, which addresses the challenge of balancing fairness and predictive accuracy by leveraging auxiliary variables to enhance counterfactual fairness in machine learning models. Through evaluations on synthetic and real-world datasets, EXOC demonstrates superior performance over existing methods, achieving counterfactual fairness without compromising accuracy.

Towards Fast, Specialized Machine Learning Force Fields: Distilling Foundation Models via Energy Hessians
This paper introduces a method to create faster, specialized Machine Learning Force Fields (MLFFs) by transferring knowledge from foundation models using an architecture-agnostic knowledge distillation process. The approach allows the specialized MLFFs to match the performance of larger models while being significantly faster, maintaining energy conservation during simulations, and suggesting a new paradigm in MLFF development with specialized engines for different chemical subsets.

Towards Understanding Why Label Smoothing Degrades Selective Classification and How to Fix It
This paper investigates the impact of label smoothing (LS) on selective classification (SC), demonstrating that LS consistently degrades SC performance across various tasks by disrupting the uncertainty rank ordering of predictions. The authors propose and elucidate the effectiveness of post-hoc logit normalization as a solution to recover SC performance loss caused by LS, providing an explanation based on logit-level gradient analysis.

Towards Universality: Studying Mechanistic Similarity Across Language Model Architectures
This paper examines the hypothesis of Universality in interpretability by comparing Transformers and Mambas, two mainstream architectures for language modeling, to explore their mechanistic similarities. Utilizing Sparse Autoencoders (SAEs), the study finds that most interpretable features are similar in both models, and reveals structural analogies and distinct differences at the circuit level, such as the Off-by-One motif in Mambas, contrasting with Transformers.

TraceVLA: Visual Trace Prompting Enhances Spatial-Temporal Awareness for Generalist Robotic Policies
This paper introduces visual trace prompting to improve the spatial-temporal awareness of vision-language-action (VLA) models in robotic manipulation by encoding state-action trajectories visually. The new TraceVLA model, developed by finetuning OpenVLA on 150K robot manipulation trajectories, achieves state-of-the-art performance across various tasks, significantly surpassing existing models in both simulated and real-world environments, and demonstrates enhanced efficiency with a compact VLA model rivaling larger baselines.

Tracing Representation Progression: Analyzing and Enhancing Layer-Wise Similarity
This paper investigates the similarity of internal representations in transformer models using a simple cosine similarity metric, revealing that representations across layers are positively correlated and that similarity increases as layers become closer. The study proposes an aligned training method, enhancing shallow layer effectiveness, resulting in early saturation events and improved layer-wise accuracies, demonstrating that a single classifier can suffice for multi-exit models, a claim supported by experiments across vision and NLP tasks.

Training Free Guided Flow-Matching with Optimal Control
Controlled generation with pre-trained Diffusion and Flow Matching models is enhanced by OC-Flow, a training-free framework that utilizes optimal control for guided flow matching across both Euclidean and complex geometries like SO(3), with significant applications in fields such as protein design. OC-Flow builds on optimal control theory to improve algorithm performance in ODE-based generation tasks, demonstrating superior results in experiments on text-guided image manipulation, conditional molecule generation, and all-atom peptide design, while providing a systematic theoretical analysis of convergence guarantees.

Trajectory attention for fine-grained video motion control
This paper presents trajectory attention, a novel method for improving camera motion control in video diffusion models by incorporating trajectory information alongside traditional temporal attention. The approach enhances precision and consistency in video generation, and its applicability extends to various video motion control tasks, such as first-frame-guided video editing, demonstrating significant improvements in consistency and quality over spatial and temporal ranges.

Understanding Factual Recall in Transformers via Associative Memories
This paper demonstrates that shallow transformers can achieve near-optimal storage capacity for factual recall tasks by using a combination of associative memories, with their storage capacities scaling linearly with parameter count. By introducing a synthetic factual recall task, the authors prove that a transformer with a single layer of self-attention and an MLP can achieve 100% accuracy, and they analyze the gradient flow of a simplified model to show sequential learning behavior.

Understanding Warmup-Stable-Decay Learning Rates: A River Valley Loss Landscape View
The paper introduces the Warmup-Stable-Decay (WSD) learning rate schedule for training language models, which allows training without a predefined compute budget by maintaining a constant learning rate and introducing a rapid decay phase. This method, alongside its variant WSD-S, not only explains the optimization process through a "river valley" landscape analogy but also demonstrates empirical superiority over traditional methods by efficiently producing multiple strong model checkpoints across different compute budgets.

U-Nets as Belief Propagation: Efficient Classification, Denoising, and Diffusion in Generative Hierarchical Models
This paper provides a novel interpretation of the U-Net architecture by analyzing its function within generative hierarchical models, showing how it effectively implements belief propagation for denoising tasks. The study highlights the efficacy of U-Nets in these models, presents an efficient sample complexity bound for learning denoising functions, and unifies the understanding of ConvNets and U-Nets in modeling complex data distributions.

Unified Convergence Analysis for Score-Based Diffusion Models with Deterministic Samplers
This paper introduces a unified convergence analysis framework for deterministic samplers in score-based diffusion models, addressing the challenges posed by their analysis and the lack of a generalized approach. The framework is demonstrated by analyzing the variance-preserving forward process with the exponential integrator scheme and Denoising Diffusion Implicit Models (DDIM)-type samplers, achieving significant computational efficiency.

uniINF: Best-of-Both-Worlds Algorithm for Parameter-Free Heavy-Tailed MABs
This paper introduces `uniINF`, a novel algorithm designed for the Heavy-Tailed Multi-Armed Bandits problem, which effectively balances robustness and adaptability in both stochastic and adversarial environments without prior knowledge of the environment type or heavy-tailed parameters. `uniINF` is the first parameter-free algorithm to achieve nearly-optimal regret, possessing the Best-of-Both-Worlds property, thanks to innovative techniques such as refined log-barrier dynamics, auto-balancing learning rate scheduling, adaptive loss tuning, and a sophisticated stopping-time analysis.

UniRestore3D: A Scalable Framework For General Shape Restoration
This paper introduces a unified model for general shape restoration, capable of recovering 3D shapes with various types of defects, improving applicability and scalability in real-world scenarios. The approach employs high-resolution TSDF grids, a large-scale diverse defect dataset, a hierarchical shape generation model, and a noise-robust encoder, demonstrating effectiveness across multiple restoration tasks on datasets like Objaverse, ShapeNet, GSO, and ABO.

Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign Relearning
This paper reveals that existing methods for machine unlearning in large language models (LLMs) are vulnerable to relearning attacks, where simple, unrelated data can reverse the effects of unlearning. By demonstrating this flaw across several benchmarks, the authors argue that current unlearning approaches merely suppress outputs without genuinely eliminating memorized content, and they propose directions for improving deletion robustness in LLMs.

Unsupervised Zero-Shot Reinforcement Learning via Dual-Value Forward-Backward Representation
The paper introduces the Dual-Value Forward-Backward (DVFB) framework to address challenges in online unsupervised reinforcement learning, particularly focusing on zero-shot generalization and fine-tuning adaptation. By employing a contrastive entropy intrinsic reward and a dual-value scheme, DVFB enhances exploration and improves successor measures, which results in superior performance in both zero-shot generalization and task-specific fine-tuning across multiple tasks, outperforming current state-of-the-art methods.

Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs
This paper aims to democratize the fine-tuning of small-sized large language models (LLMs) by exploring and documenting various training configurations and strategies, particularly focusing on their cost-efficiency and accessibility. Key contributions include insights that larger batch sizes with lower learning rates improve model performance, early-stage training dynamics can predict final performance, and that stacked training is more efficient than phased training, thus providing practical guidance for practitioners and promoting inclusivity in LLM development.

Verifying Properties of Binary Neural Networks Using Sparse Polynomial Optimization
This paper introduces a novel approach to verify the robustness of Binary Neural Networks (BNNs) against adversarial attacks using Semidefinite Programming relaxations from sparse Polynomial Optimization. This method addresses scalability issues experienced in traditional approaches and proves effective for ensuring robustness against $\||.|\|_\infty$ and $\||.|\|_2$-based adversarial attacks.

WeatherGFM: Learning a Weather Generalist Foundation Model via In-context Learning
This paper introduces WeatherGFM, the first generalist foundation model designed to address a variety of weather understanding tasks in a unified manner, unlike existing models that focus on single tasks. WeatherGFM employs a visual prompting question-answering paradigm and demonstrates effective handling of 12 tasks, highlighting its capability to generalize to unseen tasks, with its source code available online for further exploration.

What Matters in Learning from Large-Scale Datasets for Robot Manipulation
This study investigates the optimal composition of large-scale robotics datasets for improved policy learning, utilizing a data generation framework to simulate diversity in dataset features like sensor placements and object arrangements. Key findings highlight the importance of camera poses and spatial arrangements for data collection and retrieval, with new strategies outperforming existing ones by up to 70% in real-world settings.

When does compositional structure yield compositional generalization? A kernel theory.
This paper presents a general theory of compositional generalization in kernel models with fixed, compositionally structured representations, highlighting how dataset statistics influence generalization capabilities. The authors identify constraints on learning tasks in these models and uncover new failure modes that impede compositional generalization, demonstrating their findings empirically on deep neural networks, and offering insights into improving deep learning models.

Wide Neural Networks Trained with Weight Decay Provably Exhibit Neural Collapse
This paper investigates the phenomenon of neural collapse in deep neural networks (DNNs) and provides a theoretical framework that proves its emergence without assuming unconstrained features. The study demonstrates that neural collapse can occur during end-to-end training using gradient descent with weight decay when certain conditions of low training error, balancedness of linear layers, and bounded conditioning of features are met.

WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct
WizardMath is introduced as a model that enhances mathematical reasoning in large language models (LLMs) using a novel method called Reinforcement Learning from Evol-Instruct Feedback (RLEIF). Through experiments on benchmarks like GSM8k and MATH, WizardMath demonstrates superior performance over existing open-source LLMs, even surpassing models like ChatGPT-3.5, highlighting the importance of instruction evolution and process supervision.

You Only Prune Once: Designing Calibration-Free Model Compression With Policy Learning
The paper introduces PruneNet, a novel model compression technique that reformulates the pruning process as a policy learning problem, eliminating the dependency on external calibration datasets and enhancing flexibility across compression ratios. PruneNet demonstrates significant improvements by compressing the LLaMA-2-7B model while retaining over 80% of its zero-shot performance with a 30% compression ratio, outperforming existing methods, and showcases robustness in maintaining model performance on complex multitask language understanding tasks.

Zero-shot forecasting of chaotic systems
Foundation models pre-trained on diverse time-series data demonstrate the potential to perform zero-shot forecasting for chaotic systems without re-training, producing competitive results against specialized models. These models not only deliver accurate point forecasts but also maintain the geometric and statistical properties of chaotic attractors, showcasing their utility in understanding complex systems through in-context learning and context parroting.

### Miscellaneous Aspects of Machine Learning->Causality
Deriving Causal Order from Single-Variable Interventions: Guarantees & Algorithm
This paper introduces a novel approach to extracting causal information from interventional datasets by proposing a variant of interventional faithfulness, focusing on comparisons between marginal distributions across observational and interventional settings. The introduced algorithm, Intersort, demonstrates superior performance over established baselines in inferring causal order from large datasets with single-variable interventions, offering promising advancements in causal inference methods.

Causal Identification for Complex Functional Longitudinal Studies
This paper introduces a nonparametric causal identification framework for functional longitudinal data in medical research, using stochastic process theory and net convergence to handle complex treatment-confounder feedbacks that traditional methodologies cannot address. By generalizing classical causal inference techniques and accommodating time-varying outcomes, the framework advances current methodologies and lays the groundwork for future estimation studies in the field.

### Miscellaneous Aspects of Machine Learning->General Machine Learning Techniques
KAN: Kolmogorov–Arnold Networks
Kolmogorov-Arnold Networks (KANs) are introduced as innovative alternatives to Multi-Layer Perceptrons (MLPs), featuring learnable activation functions on edges instead of fixed functions on nodes. KANs demonstrate improved accuracy and interpretability over MLPs, potentially aiding in scientific discoveries, although future work is needed to enhance their training efficiency.

Symbolic regression via MDLformer-guided search: from minimizing prediction error to minimizing description length
This paper proposes a novel symbolic regression method, SR4MDL, which utilizes a neural network called MDLformer to estimate the minimum description length as a new search objective, enhancing the recovery rate of correct mathematical formulas. The authors demonstrate that SR4MDL significantly outperforms existing methods by 43.92% in recovering formulas and showcases strong generalization performance on unseen problems.

### Miscellaneous Aspects of Machine Learning->Representation Learning
Artificial Kuramoto Oscillatory Neurons
The paper introduces Artificial Kuramoto Oscillatory Neurons (AKOrN) as a dynamic alternative to traditional threshold units, enhancing network performance through synchronization dynamics among neurons. This approach improves performance in various tasks, suggesting the significance of dynamic representations in neural architectures.

Multiview Equivariance Improves 3D Correspondence Understanding with Minimal Feature Finetuning
This paper evaluates and enhances the 3D spatial awareness of Vision Transformer (ViT)-based models, which have primarily excelled in 2D image comprehension. By introducing a finetuning strategy based on 3D correspondences, the study significantly improves the models' performance in pose estimation, tracking, and semantic transfer, demonstrating gains even after just one iteration on a single object.

### Miscellaneous Aspects of Machine Learning->Transfer, Multitask and Meta-learning
A Second-Order Perspective on Model Compositionality and Incremental Learning
This paper explores the conditions that promote compositionality in non-linear neural networks using a theoretical approach based on the second-order Taylor approximation of the loss function, emphasizing the maintenance of the pre-training basin for effective composability. It presents two incremental training algorithms that facilitate the creation of multi-task models and support specialized unlearning and task specialization, with practical applications demonstrated in incremental classification tasks.

Swiss Army Knife: Synergizing Biases in Knowledge from Vision Foundation Models for Multi-Task Learning
This paper introduces the "Swiss Army Knife" (SAK) approach, which enhances multi-task learning by adaptively distilling knowledge from multiple Vision Foundation Models (VFMs) while maintaining their unique representation biases. The proposed framework outperforms previous state-of-the-art methods in multi-task learning on the NYUD-v2 benchmark by 10%, offering a flexible and robust system that integrates the strengths of various VFMs.

Towards Robust Multimodal Open-set Test-time Adaptation via Adaptive Entropy-aware Optimization
The paper introduces Adaptive Entropy-aware Optimization (AEO), a framework for tackling Multimodal Open-set Test-time Adaptation (MM-OSTTA), which adapts models to an unknown target domain involving multiple data modalities. Through the development of Unknown-aware Adaptive Entropy Optimization (UAE) and Adaptive Modality Prediction Discrepancy Optimization (AMP), the method enhances online adaptation performance by improving the model’s ability to differentiate between known and unknown classes, as demonstrated by robust results across various domain shifts and tasks like action recognition and 3D semantic segmentation.

Black Sheep in the Herd: Playing with Spuriously Correlated Attributes for Vision-Language Recognition
This study identifies that Vision-Language Models (VLMs) poorly generalize due to their reliance on spuriously correlated attributes, and proposes two novel solutions: Spurious Attribute Probing (SAP) to filter out these attributes and enhance generalization, and Spurious Attribute Shielding (SAS), a module that reduces the influence of these attributes across various Parameter-Efficient Fine-Tuning methods. Experiments demonstrate that SAP and SAS significantly improve performance on distribution shifts in multiple datasets and generalization tasks, setting a new benchmark without sacrificing downstream accuracy.

Unsupervised Meta-Learning via In-Context Learning
This paper introduces a novel unsupervised meta-learning approach that utilizes transformer architectures to reframe meta-learning as a sequence modeling problem, allowing for the transfer of feature representations to downstream tasks with limited labeled data. By employing diverse task generation through data augmentation and a mixing strategy, the method demonstrates superior performance on benchmark datasets, surpassing existing baselines and achieving competitive results compared to supervised and self-supervised methods.

Efficient Model Editing with Task-Localized Sparse Fine-tuning
Task arithmetic is promising for model editing, but existing methods have computational bottlenecks and don't ensure weight disentanglement. We present TaLoS, which creates sparse task vectors without explicit linearization, enhancing efficiency and performance in task addition and negation, promoting adaptable foundation model deployment.

### Miscellaneous Aspects of Machine Learning->Unsupervised and Semi-supervised Learning
Pursuing Feature Separation based on Neural Collapse for Out-of-Distribution Detection
This paper introduces a novel approach for out-of-distribution (OOD) detection by focusing on enlarging feature disparity between in-distribution (ID) and OOD data using a feature separation loss, leveraging the Neural Collapse (NC) property. The proposed method achieves state-of-the-art performance on CIFAR10, CIFAR100, and ImageNet benchmarks by optimizing feature separation without additional data augmentation, highlighting the effectiveness of feature-level discrimination in improving OOD detection.

Tree-Wasserstein Distance for High Dimensional Data with a Latent Feature Hierarchy
This paper introduces a novel tree-Wasserstein distance (TWD) tailored for high-dimensional data with hierarchical latent features, using a method that embeds features into a multi-scale hyperbolic space via diffusion geometry. The proposed TWD efficiently recovers the latent feature hierarchy and demonstrates superior performance in applications to word-document and single-cell RNA-sequencing datasets compared to existing TWDs and methods reliant on pre-trained models.

Semi-Supervised CLIP Adaptation by Enforcing Semantic and Trapezoidal Consistency
Vision-language pre-training models like CLIP face challenges when adapting to tasks with limited image-text paired data. To overcome this, we introduce SemiCLIP, a semi-supervised training method that enhances cross-modal alignment by using a mix of paired and unpaired data, improving CLIP's performance with gains in zero-shot classification accuracy and image-text retrieval.

Towards Understanding Why FixMatch Generalizes Better Than Supervised Learning
This paper provides the first theoretical justification for the superior generalization performance of FixMatch-like semi-supervised learning (SSL) algorithms over supervised learning (SL) in deep neural networks by analyzing the differing semantic feature learning processes involved. The authors also introduce an improved variant, Semantic-Aware FixMatch (SA-FixMatch), which demonstrates enhanced generalization capabilities, supported by experimental results.

### Optimization->Global Optimization
From Decoupling to Adaptive Transformation: a Wider Optimization Space for PTQ
This paper introduces an Adaptive Quantization Transformation (AdaQTransform) technique for Post-Training Quantization (PTQ), optimizing the quantization process by expanding the optimization space and achieving a better fit to FP32 outputs with negligible extra costs. The proposed AdaQTransform demonstrates significant improvements in various models, including CNNs and LLMs, notably enhancing performance over existing methods, such as a 5.7% improvement on ImageNet for W2A2-MobileNet-v2.

### Optimization->Non-Convex
Linear Mode Connectivity in Differentiable Tree Ensembles
This paper introduces a method to achieve Linear Mode Connectivity (LMC) for soft tree ensembles, a type of tree-based differentiable model, by incorporating subtree flip invariance and splitting order invariance. The study highlights the importance of architecture-specific invariances in achieving LMC and shows that decision list-based tree architectures can maintain LMC even without these additional invariances.

AdaFisher: Adaptive Second Order Optimization via Fisher Information
AdaFisher is an adaptive second-order optimizer that addresses the computational challenges of training deep neural networks by using a diagonal block-Kronecker approximation of the Fisher information matrix for gradient preconditioning. It shows superior performance to state-of-the-art optimizers in terms of accuracy and convergence speed, enhancing practicality for image classification and language modeling tasks.

### Optimization->Sampling and Optimization
Group Distributionally Robust Dataset Distillation with Risk Minimization
This paper addresses the limitations of dataset distillation (DD) by focusing on its generalization, particularly when encountering samples from low population density regions. By introducing an algorithm that combines clustering with risk measure minimization, the authors provide a theoretically supported method that improves the representativeness and robustness of synthetic datasets, demonstrating its effectiveness through numerical experiments.

Improved Approximation Algorithms for $k$-Submodular Maximization via Multilinear Extension
This paper explores $k$-submodular maximization and introduces a multilinear extension approach along with a Frank-Wolfe-type framework to handle both monotone and non-monotone functions under various constraints. The proposed method achieves an improved $1/2$-approximation for monotone cases and a $1/3$-approximation for non-monotone cases, advancing the current benchmarks in these optimization problems.

SFESS: Score Function Estimators for $k$-Subset Sampling
This paper revisits score function estimators for $k$-subset sampling, a fundamental operation challenging for gradient-based optimization due to its non-differentiability. By employing a discrete Fourier transform and control variates, the authors efficiently compute the score function, reducing variance and enabling unbiased gradient estimates applicable to non-differentiable models, with results comparable to state-of-the-art methods.

### Optimization->Zero-order and Black-box Optimization
Revisiting Zeroth-Order Optimization:  Minimum-Variance Two-Point Estimators and  Directionally Aligned Perturbations
This paper introduces the concept of directionally aligned perturbation (DAP) schemes for two-point zeroth-order gradient estimators, aiming to minimize asymptotic variance by aligning perturbations with the true gradient direction rather than maintaining a fixed length. By providing both theoretical insights and empirical evidence, the study shows that DAPs can enhance the accuracy and efficiency of stochastic gradient descent, outperforming traditional fixed-length perturbation methods under certain conditions.

### Probabilistic Methods->Bayesian Models and Methods
KLay: Accelerating Arithmetic Circuits for Neurosymbolic AI
This paper introduces knowledge layers (KLay), a novel data structure designed to efficiently parallelize arithmetic circuits on GPUs, addressing the challenge of sparsity in executing such circuits on modern tensor accelerators. The authors demonstrate that KLay provides significant speedups over existing methods, enabling the scaling of neurosymbolic AI to more complex, real-world applications.

Dimension Agnostic Neural Processes
Meta-learning seeks models that swiftly adapt to new tasks using minimal labeled data, with Neural Process (NP) being a key method in this area, though it struggles with varied input dimensions. The paper introduces Dimension Agnostic Neural Process (DANP), incorporating Dimension Aggregator Block (DAB) and leveraging Transformer architecture to surpass traditional NP models in handling diverse regression tasks, as shown through extensive experimentation.

### Probabilistic Methods->Monte Carlo and Sampling Methods
Bootstrapped Energy Based Models: What are they good for?
This paper investigates the effectiveness of generative models trained with energy or unnormalized density functions for sampling from the Boltzmann distribution, highlighting the lack of sufficient benchmarking against traditional Markov Chain Monte Carlo (MCMC) methods. It reveals that MCMC outperforms two recent models (IDEM and IEFM) in terms of energy evaluations and wall clock time and suggests revisiting benchmarking strategies to accurately assess the utility of generative models in these scenarios.

Provable Benefit of Annealed Langevin Monte Carlo for Non-log-concave Sampling
This paper addresses the problem of sampling from an unnormalized density that is potentially non-log-concave and multimodal by examining the performance of annealed Markov chain Monte Carlo (MCMC) methods. The authors present a pioneering non-asymptotic analysis and provide an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\cal A}^2}{\varepsilon^6}\right)$ for the annealed Langevin Monte Carlo algorithm, offering new theoretical guarantees for achieving desired accuracy in Kullback-Leibler divergence to the target distribution.

Amortized Control of Continuous State Space Feynman-Kac Model for Irregular Time Series
The paper introduces the Amortized Control of continuous State Space Model (ACSSM), designed to model irregular time series by employing a continuous dynamical system using multi-marginal Doob's $h$-transform. ACSSM integrates variational inference and stochastic optimal control for efficient training and inference, showing superior performance and scalability across various real-world datasets in tasks like classification and regression.

### Probabilistic Methods->Variational Inference
Injective flows for star-like manifolds
The paper introduces injective flows for star-like manifolds to enable efficient and exact computation of the Jacobian determinant, an improvement over existing methods that require approximations. This advancement enhances variational inference, particularly benefiting applications like Objective Bayesian models with penalized likelihoods and probabilistic mixing models with variational inference on the probability simplex.

### Reinforcement Learning->Batch/Offline
Learning on One Mode: Addressing Multi-modality in Offline Reinforcement Learning
Offline reinforcement learning faces challenges with multi-modal action distributions in static datasets. The proposed Weighted Imitation Learning on One Mode (LOM) approach improves performance by focusing on a single promising mode, using a Gaussian mixture model for mode identification, and outperforms existing methods in multi-modal scenarios.

Scrutinize What We Ignore: Reining In Task Representation Shift Of Context-Based Offline Meta Reinforcement Learning
This paper addresses the theoretical underpinnings of offline meta reinforcement learning (OMRL), particularly questioning the reliance on mutual information maximization between task variables and their latent representations for performance improvement. By identifying and addressing the issue of "task representation shift," the study theoretically proves that proper updates to the context encoder can ensure monotonic performance improvements, and empirical results support the benefits of managing this shift for enhanced OMRL outcomes.

Model Risk-sensitive Offline Reinforcement Learning
This paper introduces a model risk-sensitive offline reinforcement learning framework aimed at minimizing the worst-case risks across various plausible scenarios, rather than focusing solely on estimated risks, to address inaccuracies in risk assessments and distribution shifts. The proposed approach, utilizing a critic-ensemble method and incorporating learned Fourier features and the IQN framework, significantly reduces risk by 11.2% to 18.5% in finance and self-driving scenarios compared to existing baselines, particularly in uncertain environments.

### Reinforcement Learning->Deep RL
VICtoR: Learning Hierarchical Vision-Instruction Correlation Rewards for Long-horizon Manipulation
We present VICtoR, a novel hierarchical reward model for long-horizon manipulation tasks that learns from action-free videos and language instructions. By using a stage detector and motion progress evaluator, it effectively addresses challenges like sub-stage awareness and object state estimation, outperforming existing methods with a 43% improvement in success rates.

### Reinforcement Learning->Everything Else
VVC-Gym: A Fixed-Wing UAV Reinforcement Learning Environment for Multi-Goal Long-Horizon Problems
The paper introduces VVC-Gym, a multi-goal long-horizon Reinforcement Learning (RL) environment based on the velocity vector control of realistic fixed-wing UAVs, and provides various demonstration sets to assist in training. Through experiments, the study evaluates how different environment designs, demonstration qualities, and RL algorithms influence the exploration challenges in multi-goal long-horizon problems, establishing VVC-Gym as an effective platform for such analyses.

HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents
The paper introduces HASARD, a novel benchmark suite designed to advance safe reinforcement learning by utilizing complex tasks in a Doom environment that demand strategic decision-making and egocentric vision-based learning. By providing varying difficulty levels and action spaces, along with open-sourced environments and baseline implementations, HASARD uniquely focuses on evaluating and enhancing the capabilities of RL agents in safely navigating and interacting with complex environments through visual perception.

Asynchronous Federated Reinforcement Learning with Policy Gradient Updates: Algorithm Design and Convergence Analysis
This paper introduces AFedPG, a novel asynchronous federated reinforcement learning framework that leverages policy gradient updates across multiple agents to construct a global model while addressing lagged policies through a delay-adaptive lookahead technique. AFedPG demonstrates significant improvements in sample and time complexity over traditional methods, achieving linear speedup relative to the number of agents and showing robust performance in heterogeneous computing environments, as confirmed through empirical tests in MuJoCo environments.

Execution-guided within-prompt search for programming-by-example
This paper presents a novel approach where large language models (LLMs) are enhanced with search capabilities by generating lines of code, executing them, and using the execution results to guide subsequent iterations. The study demonstrates that this within-prompt search method effectively improves Python code generation across various benchmarks, even under low token budgets, and highlights the model's ability to parallelize and backtrack during the search process.

### Reinforcement Learning->Online
DOPL: Direct Online Preference Learning for Restless Bandits with Preference Feedback
Restless multi-armed bandits (RMAB) models often rely on scalar reward signals, but practical constraints can make specifying these rewards difficult. This paper introduces Pref-RMAB, which utilizes only preference feedback rather than scalar rewards and presents the direct online preference learning (DOPL) algorithm that ensures sublinear regret, marking the first such approach to demonstrate $\tilde{\mathcal{O}}(\sqrt{T\ln T})$ regret for RMAB with preference feedback, with experimental results confirming its effectiveness.

### Social Aspects->Accountability, Transparency and Interpretability
Beyond Surface Structure: A Causal Assessment of LLMs' Comprehension ability
This paper investigates the comprehension abilities of large language models (LLMs) by using causal mediation analysis to separately evaluate deep and surface structure understanding, introducing quantifiable surrogates like approximated direct and indirect causal effects (ADCE and AICE). The analysis reveals that closed-source LLMs (e.g., GPT) focus more on deep structure compared to open-source models (e.g., Llama), providing new insights into their comprehension capacities and offering a more comprehensive evaluation method beyond accuracy metrics.

Controllable Context Sensitivity and the Knob Behind It
This paper investigates a mechanism for controlling the sensitivity of language models to context versus prior knowledge, essential for tasks like retrieval-augmented generation and question-answering. By designing a task for controllable context sensitivity and fine-tuning models like Llama-3.1, Mistral-v0.3, and Gemma-2, the authors pinpoint a 1-D subspace in a single layer that determines the use of contextual or prior knowledge, demonstrating its efficacy across both fine-tuned and base models and correlating with improved performance.

Explaining Modern Gated-Linear RNNs via a Unified Implicit Attention Formulation
This paper presents a unified framework for understanding attention-free sequence modeling layers as implicit causal self-attention layers, encompassing models like Mamba, RWKV, and gated RNNs. The proposed framework enhances explainability by outperforming existing methods for various models and provides a competitive alternative to state-of-the-art Transformer explainability, with publicly available code.

Efficient Automated Circuit Discovery in Transformers using Contextual Decomposition
This paper introduces Contextual Decomposition for Transformers (CD-T), a method for efficiently producing interpretable circuits in large language models, capable of isolating contributions of model features down to attention heads at specific sequence positions. CD-T outperforms existing techniques by drastically reducing circuit discovery runtime and improving faithfulness and recovery of manual circuits, demonstrating its potential for scalable mechanistic interpretability of neural networks.

Influence Functions for Scalable Data Attribution in Diffusion Models
This paper addresses challenges in diffusion models related to data attribution and interpretability by extending influence functions to predict changes in the probability of generating particular examples. By utilizing a K-FAC approximation for scalable Hessian computations, the proposed method outperforms existing data attribution techniques in common evaluations without requiring method-specific hyperparameter tuning.

NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals
NNsight and NDIF are introduced as complementary technologies designed to facilitate the investigation of representations and computations in large neural networks by extending PyTorch for deferred remote execution and scalable inference services, respectively. By leveraging the Intervention Graph architecture, this framework allows efficient access to internal workings of massive models like LLMs, addressing the increasing gap in studying large-scale AI internals without the need for individual model hosting, with comprehensive documentation and tutorials available online.

### Social Aspects->Fairness, Equity, Justice and Safety
Bayesian WeakS-to-Strong from Text Classification to Generation
This paper introduces WeakS-to-Strong, an extension of the Weak-to-Strong framework, which uses an ensemble of weak models to simulate human opinion variability and guide model supervision with a Bayesian approach. The study applies this method from text classification to text generation tasks, demonstrating effective preference optimization and improving strong student model reliability for potential superalignment.

Model Editing as a Robust and Denoised variant of DPO: A Case Study on Toxicity
The paper introduces ProFS (Projection Filter for Subspaces), a tuning-free alignment method that mitigates model toxicity by identifying and projecting away toxic subspaces in large language models. ProFS is shown to be more sample-efficient and robust to noisy data compared to existing methods like direct preference optimization (DPO), offering a new approach to enhancing model safety and transparency without extensive preference data.

Rethinking Fair Representation Learning for Performance-Sensitive Tasks
This paper examines fair representation learning methods for bias mitigation by utilizing causal reasoning to identify assumptions and limitations these methods have when dataset biases are present. Through experiments across various medical modalities, the study highlights the impact of distribution shifts and dataset biases on these methods, challenging current evaluation practices and stressing the need for detailed analysis in bias assessment.

AgentHarm: A Benchmark for Measuring Harmfulness of LLM Agents
This paper introduces AgentHarm, a new benchmark designed to evaluate the robustness of LLM agents against misuse through jailbreak attacks, focusing on their ability to handle 110 explicitly malicious tasks spanning 11 harm categories, such as fraud and cybercrime. The study reveals that leading LLMs can comply with malicious requests without jailbreaking and that simple jailbreak methods can convert these agents to perform harmful multi-step tasks, with the AgentHarm benchmark made publicly available to aid in ongoing research on safeguarding LLM-based agents.

Tracking the Copyright of Large Vision-Language Models through Parameter Learning Adversarial Images
This paper introduces a novel method called Parameter Learning Attack (PLA) to track the copyright of large vision-language models (LVLMs) by creating adversarial images that trigger specific outputs without altering the original model. The method, which does not impact the model's performance, effectively identifies the copyright of fine-tuned models and detects unauthorized usage, offering a robust tool for copyright verification.

FairDen: Fair Density-Based Clustering
This paper introduces FairDen, the first fair, density-based clustering algorithm that manipulates a similarity matrix to encourage balanced clustering. FairDen effectively handles categorical attributes, noise, and multiple sensitive attributes, demonstrating its capability to find fair and meaningful clusters through extensive experiments.

A Causal Lens for Learning Long-term Fair Policies
This paper proposes a framework for evaluating long-term fairness in reinforcement learning by assessing differences in average expected qualification gains among different groups. By decomposing fairness into direct, delayed, and spurious effects, the study connects these components to benefit fairness and presents an approach to balance various fairness notions in dynamic decision-making systems.

### Social Aspects->Privacy-preserving Statistics and Machine Learning
Measuring Non-Adversarial Reproduction of Training Data in Large Language Models
This work investigates non-adversarial memorization in large language models by examining the overlap between model responses and their training data in natural, benign scenarios. The study finds that language models can produce up to 15% verbatim text from internet sources and sometimes entirely replicate content, whereas human-written texts exhibit significantly lower overlap, suggesting a need for stronger defenses to reduce such memorization in non-adversarial contexts.

Dataset Ownership Verification in Contrastive Pre-trained Models
This paper introduces a novel dataset ownership verification method specifically designed for self-supervised pre-trained models using contrastive learning, addressing the limitations of existing approaches limited to supervised models. The method effectively determines if a suspicious black-box backbone has been pre-trained on a specific unlabeled dataset by analyzing variations in unary and binary instance relationships within the embedding space, with results outperforming previous methods and validated across various models like SimCLR, BYOL, SimSiam, MOCO v3, and DINO.

Optimality of Matrix Mechanism on $\ell_p^p$-metric
This paper introduces the $\ell_p^p$-error metric for $p \geq 2$ in the context of answering linear queries under $(\epsilon,\delta)$-differential privacy in the add/remove model, providing a tight characterization of such errors. The study extends known results by offering tight bounds for answering prefix sum and parity queries under differential privacy for any constant $p$, thereby generalizing previous bounds established for $p=2$.

### Social Aspects->Trustworthy Machine Learning
Predictive Uncertainty Quantification for Bird's Eye View Segmentation: A Benchmark and Novel Loss Function
This paper presents a comprehensive benchmark for predictive uncertainty quantification in Bird's Eye View (BEV) semantic segmentation, evaluating various methods across multiple datasets and network architectures. By introducing a novel loss function, Uncertainty-Focal-Cross-Entropy (UFCE), and a simple uncertainty-scaling regularization term, the study enhances the detection of misclassified and out-of-distribution pixels and improves model calibration, addressing challenges in existing uncertainty quantification methods.


Start Smart: Leveraging Gradients For Enhancing Mask-based XAI Methods
This paper introduces StartGrad, a novel gradient-based initialization technique for mask-based explanation methods used in interpreting deep learning model predictions. StartGrad is shown to be superior in enhancing the optimization process, achieving target metrics more quickly, and sometimes improving overall performance compared to traditional initialization strategies.

Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation
The paper addresses the problem of harmful fine-tuning attacks on large language models by identifying harmful perturbations of model weights as a potential root cause of alignment issues. It introduces a solution called Booster, which incorporates a loss regularizer during the alignment stage to reduce the harmful impact of perturbations, effectively lowering the harmful score of fine-tuned models while preserving task performance.

Convergent Privacy Loss of Noisy-SGD without Convexity and Smoothness
This paper addresses the Differential Privacy (DP) guarantees of hidden-state Noisy-SGD algorithms, providing a convergent R\'enyi DP bound for non-convex non-smooth losses with H\"older continuous gradients, thus extending beyond the limitations of prior work on smooth convex losses. The authors introduce an improved privacy bound for smooth strongly convex losses by enhancing shifted divergence analysis, highlighting the utility of hidden-state analysis in DP research.

### Theory->Deep Learning
The impact of allocation strategies in subset learning on the expressive power of neural networks
This paper explores how the strategic allocation of a fixed number of learnable weights impacts the expressivity of neural networks. Through a teacher-student model, it benchmarks the expressivity of different weight allocations, demonstrating that widespread distribution of learnable weights generally enhances expressive power, with theoretical findings supported by empirical experiments.

### Theory->Learning Theory
The Computational Complexity of Positive Non-Clashing Teaching in Graphs
This paper investigates the classical and parameterized complexity of determining the positive non-clashing teaching dimension for concepts within a model transformed to graphs. The authors demonstrate the NP-hardness of the problem with specific conditions, establish time complexity bounds, prove fixed-parameter tractability based on vertex integrity, and provide a lower bound for tractability concerning feedback vertex number and pathwidth, improving the understanding of this computational problem and addressing existing open questions.

ONLINE EPSILON NET & PIERCING SET FOR GEOMETRIC CONCEPTS
This paper addresses the online $\varepsilon$-net problem for geometric concepts with bounded VC-dimension, a problem that has not seen theoretical results until now. It presents the first deterministic online algorithm with an optimal competitive ratio for intervals in $\mathbb{R}$, and a randomized algorithm for axis-aligned boxes, alongside introducing novel techniques for analyzing similar-sized objects in $\mathbb{R}^d$, contributing significantly to online learning in various geometric contexts.

### Theory->Online Learning and Bandits
Stochastic Bandits Robust to Adversarial Attacks
This paper explores stochastic multi-armed bandit algorithms resilient to adversarial attacks, focusing on scenarios where an attacker can alter reward observations after observing the learner's actions. The study presents algorithms with regret bounds for cases with and without a known attack budget, achieving optimal performance and highlighting a distinct separation between models with attacks and corruption.



## Oral Session 2D (15:30-16:42)
15:30-15:42: Prioritized Generative Replay
This paper proposes a prioritized, parametric version of an agent's memory using generative models to enhance sample-efficient online reinforcement learning by densifying past experiences and guiding them towards more useful parts of the agent's history. By utilizing conditional diffusion models and relevance functions, the approach improves performance and sample efficiency, promotes diversity in generated transitions, reduces overfitting, and supports higher update-to-data ratios, offering scalability advantages for online RL agents.
15:42-15:54: Representation Alignment for Generation: Training Diffusion Transformers Is Easier Than You Think
This paper introduces REPresentation Alignment (REPA), a straightforward regularization technique to improve generative diffusion models by aligning noisy input hidden states with clean image representations from external, pretrained visual encoders. The method significantly enhances training efficiency and generation quality, demonstrated by a 17.5x speedup in SiT training and achieving state-of-the-art FID=1.42 in final generation quality.
15:54-16:06: Simplifying, Stabilizing and Scaling Continuous-time Consistency Models
This paper presents a simplified theoretical framework to address training instability in continuous-time consistency models (CMs), a class of diffusion-based generative models. By enhancing diffusion process parameterization, network architecture, and training objectives, the authors successfully train large-scale CMs, achieving competitive FID scores with only two sampling steps across several datasets, significantly narrowing the performance gap with leading diffusion models.
16:06-16:18: One Step Diffusion via Shortcut Models
Shortcut Models present a novel approach to generative modeling that accelerates image generation by conditioning a single network on both the current noise level and desired step size, allowing for flexible and efficient sampling. This method simplifies the generation process compared to existing techniques, producing superior quality samples with reduced complexity and adaptability in sampling steps.
16:18-16:30: Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models
This paper introduces block diffusion language models, which blend discrete denoising diffusion with autoregressive models to enable flexible-length generation and faster inference. The proposed models set a new performance standard among diffusion models, overcoming limitations like fixed-length generation, and the authors provide their code and model weights online for further exploration.
16:30-16:42: Scaling In-the-Wild Training for Diffusion-based Illumination Harmonization and Editing by Imposing Consistent Light Transport
Diffusion-based image generators face challenges in preserving image details and intrinsic properties during illumination editing. The proposed IC-Light method introduces consistent light transport during training, enabling scalable and precise illumination manipulation across diverse data types while maintaining the integrity of intrinsic image properties, thus reducing uncertainties and artifacts.


## Oral Session 2F (15:30-16:42)
15:30-15:42: NeuralPlane: Structured 3D Reconstruction in Planar Primitives with Neural Fields
This paper introduces **NeuralPlane**, a novel approach for multi-view 3D plane reconstruction using neural fields to create a unified 3D representation from inconsistent 2D plane observations. The method, requiring no prior plane annotations, offers high-fidelity reconstructions with crisply aligned planar primitives and demonstrates superior performance in both geometry and semantics on datasets such as ScanNetv2 and ScanNet++.
15:42-15:54: TetSphere Splatting: Representing High-Quality Geometry with Lagrangian Volumetric Meshes
TetSphere Splatting is a novel geometry representation method using volumetric tetrahedral meshes for high-quality 3D shape modeling, addressing issues like irregular triangles and non-manifoldness. The method shows superior mesh quality and competitive accuracy in reconstruction tasks while also integrating effectively into generative modeling for applications like image-to-3D and text-to-3D generation.
15:54-16:06: High-Dynamic Radar Sequence Prediction for Weather Nowcasting Using Spatiotemporal Coherent Gaussian Representation
The paper introduces a novel framework for 3D radar sequence prediction in weather nowcasting, utilizing SpatioTemporal Coherent Gaussian Splatting (STC-GS) for dynamic radar representation and GauMamba for efficient forecasting. This approach maintains high spatial resolution and outperforms existing methods by effectively handling dynamic meteorological signals, thereby enhancing prediction efficiency and accuracy.
16:06-16:18: Residual Deep Gaussian Processes on Manifolds
The paper introduces deep Gaussian process models on Riemannian manifolds, designed to handle data with complex, nonstationary patterns that simpler models struggle with, such as low-altitude wind patterns. These models enhance prediction quality and uncertainty calibration while being robust to overfitting, and they demonstrate significant improvements in Bayesian optimization problems on manifolds, with potential applications for faster inference on non-manifold data.
16:18-16:30: No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images
NoPoSplat is a feed-forward model that reconstructs 3D scenes using 3D Gaussians from unposed, sparse multi-view images, achieving real-time performance with photometric loss training. By anchoring local camera coordinates into a canonical space and employing novel intrinsic embedding methods, the model circumvents errors in pose estimation, excels in novel view synthesis, and significantly surpasses state-of-the-art methods in pose estimation without the need for ground truth depth, proving its effectiveness in real-world applications.
16:30-16:42: On Scaling Up 3D Gaussian Splatting Training
Grendel is a distributed system that enhances 3D Gaussian Splatting (3DGS) for high-resolution and large-scale 3D reconstruction by partitioning parameters and parallelizing computation across multiple GPUs, overcoming the memory limitations of single GPU setups. It improves rendering quality through batched training with multiple views and dynamic load balancing, achieving superior results demonstrated by a PSNR of 27.28 on the 4K "Rubble" dataset by distributing parameters across 16 GPUs.


## Oral Session 2B (15:30-16:42)
15:30-15:42: MLE-bench: Evaluating Machine Learning Agents on Machine Learning Engineering
MLE-bench is a benchmark designed to evaluate AI agents' performance in machine learning engineering, comprising 75 diverse competitions from Kaggle that test skills like model training and dataset preparation. The study finds that the best AI setup reaches at least a Kaggle bronze medal level in 16.9% of tasks, and offers insights into resource-scaling and pre-training contamination, with all codes available on GitHub for further research.
15:42-15:54: MMQA: Evaluating LLMs with Multi-Table Multi-Hop Complex Questions
The paper introduces the MMQA dataset to assess large language models' capabilities in multi-table understanding and reasoning, going beyond single-table benchmarks. It also presents a state-of-the-art multi-table retrieval method, highlighting significant room for improvement in LLMs' performance compared to human capabilities.
15:54-16:06: MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models
The paper introduces MMIE, a comprehensive benchmark designed to evaluate interleaved multimodal comprehension and generation in Large Vision-Language Models using a diverse set of 20,000 queries across various fields. MMIE addresses the limitations of existing benchmarks by providing scalable, reliable, and less biased evaluation through diverse question formats and an automated scoring model, ultimately demonstrating significant room for improvement in current models and aiming to catalyze advancements in the field.
16:06-16:18: Dynamic Multimodal Evaluation with Flexible Complexity by Vision-Language Bootstrapping
Large Vision-Language Models (LVLMs) excel in multimodal tasks but face challenges with static benchmarks and data contamination. To address this, the Vision-Language Bootstrapping (VLB) protocol dynamically generates and evaluates new visual question-answering samples, offering a robust assessment of LVLMs' evolving capabilities with reduced data contamination and varied complexity.
16:18-16:30: PathGen-1.6M: 1.6 Million Pathology Image-text Pairs Generation through Multi-agent Collaboration
Vision Language Models (VLMs), such as CLIP, are enhanced for pathology by training on high-quality image-caption pairs derived from large-scale Whole Slide Image (WSI) datasets like TCGA, resulting in the creation of the PathGen-1.6M dataset. This work demonstrates significant improvements in pathology-specific image analysis tasks and enhances multimodal model capabilities using the new PathGen-CLIP model, offering scalable data generation and open-access resources for advancing pathology research.
16:30-16:42: Two Effects, One Trigger: On the Modality Gap, Object Bias, and Information Imbalance in Contrastive Vision-Language Models
Contrastive vision-language models (VLMs), like CLIP, excel in tasks such as zero-shot object recognition but struggle with attribute recognition due to issues like the modality gap and object bias. This paper investigates these phenomena, finding that an information imbalance between images and captions drives both the modality gap and object bias, and suggests that addressing the modality gap can enhance performance.


## Oral Session 2E (15:30-16:42)
15:30-15:42: A Theoretically-Principled Sparse, Connected, and Rigid Graph Representation of Molecules
This paper introduces a novel hyperparameter-free method for constructing molecular graphs that guarantees sparsity, connectivity, and rigidity, addressing limitations of existing methods. The proposed approach consistently produces connected and sparse graphs with an edge-to-node ratio capped at 3, enhancing the performance of graph neural networks in molecular modeling benchmarks.
15:42-15:54: GeSubNet: Gene Interaction Inference for Disease Subtype Network Generation
GeSubNet addresses the challenge of retrieving gene functional networks by learning a unified representation that predicts gene interactions and distinguishes between disease subtypes. This novel multi-step framework improves the integration of gene interaction knowledge and excels in graph evaluation metrics, demonstrating its potential to identify subtype-specific genes and impact patient distribution shifts with high accuracy.
15:54-16:06: Towards a Complete Logical Framework for GNN Expressiveness
This paper introduces a framework for identifying the equivalent logical formulas for arbitrary Graph Neural Networks (GNNs), aiming to unify and enhance understanding within the field by relating GNN expressiveness to logic. It provides case studies of prominent GNN architectures and offers a method for determining their homomorphism expressivity, addressing open challenges and inspiring future research.
16:06-16:18: Homomorphism Expressivity of Spectral Invariant Graph Neural Networks
This paper investigates the theoretical understanding of spectral invariants in Graph Neural Networks (GNNs) by analyzing their expressive power through homomorphism expressivity. The study proves that spectral invariant GNNs can accurately count homomorphisms in specific tree-like graphs called "parallel trees," establishing a hierarchy of expressiveness among different GNN architectures and extending previous work by resolving open questions in the field.
16:18-16:30: Robustness Inspired Graph Backdoor Defense
Graph Neural Networks (GNNs) are susceptible to backdoor attacks, endangering their deployment in real-world applications. This study proposes using prediction variance under random edge dropping to identify poisoned nodes and introduces a robust training strategy that successfully mitigates the effects of diverse backdoor attacks while maintaining high accuracy, with extensive experiments validating its effectiveness.
16:30-16:42: Joint Graph Rewiring and Feature Denoising via Spectral Resonance
This paper introduces the Joint Denoising and Rewiring (JDR) algorithm to enhance node classification in graph neural networks by simultaneously denoising features and optimizing graph structure. JDR effectively aligns spectral spaces and tackles varying class and homophily conditions, demonstrating superior performance over existing methods in diverse tasks.


## Oral Session 2C (15:30-16:42)
15:30-15:42: ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids
ProtComposer is a tool developed to generate protein structures by conditioning on spatial layouts specified through 3D ellipsoids, representing substructure shapes and semantics. This approach allows for enhanced control and diversity in protein design, offering new capabilities in redesigning substructures and generating novel proteins with expanded traits and designability, while overcoming limitations of existing models that typically oversample simple helix bundles.
15:42-15:54: ShEPhERD: Diffusing shape, electrostatics, and pharmacophores for bioisosteric drug design
ShEPhERD is an SE(3)-equivariant diffusion model designed to facilitate 3D interaction-aware chemical design by learning the joint distribution over 3D molecular structures and their interaction profiles. The model demonstrates potential in drug design tasks by conditionally generating novel molecules with desired interaction profiles through 3D similarity scoring functions, aiding in natural product ligand hopping, bioactive hit diversification, and bioisosteric fragment merging.
15:54-16:06: ECD: A Machine Learning Benchmark for Predicting Enhanced-Precision Electronic Charge Density in Crystalline Inorganic Materials
This paper introduces the ECD dataset, containing electronic charge density data for 140,646 crystal geometries, to enhance machine learning predictions for electronic structures beyond traditional DFT methods. By combining medium-precision PBE data with high-precision HSE data, the study provides benchmarks for improving machine learning models, ultimately supporting advancements in materials design and fostering community-driven research through open-source resources.
16:06-16:18: Rethinking the generalization of drug target affinity prediction algorithms via similarity aware evaluation
This paper highlights the challenge of drug-target binding affinity prediction models overfitting to high-similarity samples in conventional test set splits, resulting in unreliable performance on diverse datasets. To overcome this, a novel framework for similarity-aware evaluation is introduced, employing a new split methodology through optimization problems, and demonstrates improved model development across multiple datasets and methods.
16:18-16:30: PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding
This paper introduces PhysBench, a comprehensive benchmark designed to evaluate Vision-Language Models' (VLMs) understanding of physical phenomena across diverse tasks, revealing that while VLMs excel in common-sense reasoning, they fall short in physical understanding due to lack of embedded physical knowledge. To address this, the authors propose PhysAgent, a framework that enhances VLMs' physical understanding by integrating vision models, achieving significant improvements and contributing to the advancement of embodied agents' capabilities.


## Oral Session 2A (15:30-16:42)
15:30-15:42: Flat Reward in Policy Parameter Space Implies Robust Reinforcement Learning
This paper explores the significance of flat reward landscapes in reinforcement learning (RL) and their impact on model robustness, extending the concept of flat minima from supervised learning to RL. The study demonstrates, through extensive simulations, that flatter reward landscapes improve the generalization and robustness of RL models across variations in action selection, transition dynamics, and reward functions, with code available for further exploration.
15:42-15:54: DeepLTL: Learning to Efficiently Satisfy Complex LTL Specifications for Multi-Task RL
This paper proposes a novel learning approach leveraging Büchi automata to address challenges in efficiently satisfying complex LTL specifications in multi-task RL, including arbitrary specifications not observed during training. The method demonstrates superior performance in both finite- and infinite-horizon domains, achieving higher satisfaction probability and efficiency than existing approaches.
15:54-16:06: Geometry of Neural Reinforcement Learning in Continuous State and Action Spaces
This paper advances theoretical understanding in reinforcement learning (RL) by employing a geometric approach to explore continuous state and action spaces, revealing that the training dynamics of a two-layer neural policy create a low-dimensional manifold of attainable states. The authors demonstrate, through both theoretical proof and empirical evidence, that the dimensionality of this manifold corresponds to the action space's dimensionality, significantly enhancing RL performance in high-degree-of-freedom control environments through a novel manifold learning layer.
16:06-16:18: Interpreting Emergent Planning in Model-Free Reinforcement Learning
This paper provides the first mechanistic evidence that model-free reinforcement learning agents can learn to plan using a methodology grounded in concept-based interpretability, showcased in the Sokoban benchmark. The study demonstrates that the DRC agent employs learned concept representations to form plans influencing action selection, revealing a resemblance to parallelized bidirectional search, thereby enhancing comprehension of planning behaviors in artificial agents.
16:18-16:30: Learning to Search from Demonstration Sequences
The paper introduces Differentiable Tree Search Network (D-TSN), an innovative neural network architecture that constructs search trees from demonstration sequences using gradient descent on a best-first search tree algorithm. D-TSN enhances planning by jointly learning essential submodules and demonstrates effectiveness, particularly in scenarios where the world model with a latent state space is also learned; the authors validate their approach through experiments on various problem scenarios like Game of 24, 2D grid navigation, and Procgen games.
16:30-16:42: Open-World Reinforcement Learning over Long Short-Term Imagination
The paper introduces LS-Imagine, a method to enhance exploration efficiency in visual reinforcement learning by extending the imagination horizon, thereby allowing agents to consider long-term payoffs in high-dimensional, open-world environments. By employing goal-conditioned jumpy state transitions and affordance maps, LS-Imagine improves over state-of-the-art techniques, particularly demonstrated in the MineDojo environment.


# 4/25
## Poster Session 3 (10:00-12:30)
### Applications->Chemistry and Drug Discovery
GlycanML: A Multi-Task and Multi-Structure Benchmark for Glycan Machine Learning
This paper introduces GlycanML, a comprehensive machine learning benchmark designed to evaluate models on various glycan-related tasks, such as taxonomy and immunogenicity prediction. It demonstrates the effectiveness of using multi-relational graph neural networks and multi-task learning to enhance model performance, providing datasets and tools for future research in glycan understanding.

SAGEPhos: Sage Bio-Coupled and Augmented Fusion for Phosphorylation Site Detection
SAGEPhos is a novel framework for phosphorylation site prediction that incorporates structural information, addressing limitations of existing individual kinase-targeted methods that rely mostly on sequence data. By introducing a new dataset with structural elements and employing Bio-Coupled and Bio-Augmented fusion techniques, SAGEPhos significantly improves prediction accuracy and AUC-ROC, advancing the field's state-of-the-art.

3D-MolT5: Leveraging Discrete Structural Information for Molecule-Text Modeling
3D-MolT5 is a novel framework that integrates molecular sequences and 3D structures into a unified tokenized format, bridging the gap between these modalities and improving their interaction through a shared representation space. By enhancing cross-modal alignment and utilizing a specialized 3D token vocabulary, 3D-MolT5 significantly outperforms existing methods in downstream tasks, demonstrating strong generalization abilities, with notable improvements in molecular property prediction.

ProtComposer: Compositional Protein Structure Generation with 3D Ellipsoids
ProtComposer is a tool developed to generate protein structures by conditioning on spatial layouts specified through 3D ellipsoids, representing substructure shapes and semantics. This approach allows for enhanced control and diversity in protein design, offering new capabilities in redesigning substructures and generating novel proteins with expanded traits and designability, while overcoming limitations of existing models that typically oversample simple helix bundles.

Efficient Biological Data Acquisition through Inference Set Design
This paper presents an innovative approach to reduce the costs of drug discovery by employing a sequential subset selection model, which aims to select the smallest necessary set of compounds to achieve a desired accuracy. By using a confidence-based active learning solution, the proposed methodology effectively prunes challenging examples, leading to significant experimental cost reduction while maintaining high performance, as demonstrated on image, molecular, and large-scale biological assay datasets.

Boltzmann priors for Implicit Transfer Operators
This paper presents Boltzmann Priors for ITO (BoPITO), a method that enhances Implicit Transfer Operator Learning to improve the efficiency and reliability of predicting thermodynamic properties via molecular dynamics simulations. BoPITO achieves this by facilitating more efficient data generation and embedding inductive biases, thus improving sample efficiency by an order of magnitude and ensuring asymptotically unbiased equilibrium statistics, with code made available for further research use.

### Applications->Computer Vision
Cross-Attention Head Position Patterns Can Align with Human Visual Concepts in Text-to-Image Generative Models
This paper introduces Head Relevance Vectors (HRVs) to enhance the interpretability of cross-attention layers in text-to-image diffusion models, aligning them with human-specified visual concepts. By using HRVs, the study demonstrates improved fine control in visual generative tasks, leading to reduced misinterpretations in image generation and successful modification of challenging attributes, thus advancing our understanding and manipulation of cross-attention mechanisms.

SplineGS: Learning Smooth Trajectories in Gaussian Splatting for Dynamic Scene Reconstruction
This paper introduces SplineGS, a novel approach for reconstructing complex deforming scenes for novel view synthesis using 3D Gaussian Splatting along with non-uniform rational B-splines (NURBS) for temporally smooth deformation. SplineGS achieves competitive performance with existing methods while significantly reducing training time, thanks to its efficient trajectory representation and training design.

MonST3R: A Simple Approach for Estimating Geometry in the Presence of Motion
Motion DUSt3R (MonST3R) is introduced as a novel geometry-first method that estimates per-timestep geometry from dynamic scenes, addressing the complexity and error-prone nature of multi-stage pipelines in current computer vision approaches. By adapting a static scene representation to dynamic ones and fine-tuning on limited but strategic datasets, MonST3R achieves robust video depth and camera pose estimation, outperforming previous methods while also showing potential in feed-forward 4D reconstruction.

AugKD: Ingenious Augmentations Empower Knowledge Distillation for Image Super-Resolution
This paper enhances knowledge distillation (KD) for image super-resolution (SR) by introducing AugKD, which uses unpaired data augmentations to create auxiliary distillation samples and enforce label consistency regularization. The proposed method significantly outperforms existing KD techniques in SR tasks, demonstrating the underestimated potential of vanilla KD when combined with innovative data augmentation strategies.

Cocoon: Robust Multi-Modal Perception with Uncertainty-Aware Sensor Fusion
Cocoon is a fusion framework that introduces object- and feature-level uncertainty awareness to enhance 3D object detection accuracy across various conditions. By quantifying uncertainties and aligning heterogeneous representations, Cocoon outperforms existing methods and proves the validity of its approach on diverse datasets.

CAT-3DGS: A Context-Adaptive Triplane Approach to Rate-Distortion-Optimized 3DGS Compression
This paper introduces CAT-3DGS, a context-adaptive triplane approach for rate-distortion-optimized compression of 3D Gaussian Splatting (3DGS), addressing the overlooked need for compressing and transmitting 3DGS representations efficiently. By utilizing multi-scale triplanes and a view frequency-aware masking mechanism, CAT-3DGS achieves state-of-the-art compression performance by effectively capturing spatial and intra correlation for improved coding efficiency.

Once-for-All: Controllable Generative Image Compression with Dynamic Granularity Adaptation
This paper introduces Control-GIC, a novel generative image compression framework that enables fine-grained bitrate adaptation while maintaining high-fidelity and generality. By using a VQGAN framework and probabilistic conditional decoding, Control-GIC achieves flexible bitrate control and outperforms current state-of-the-art methods in dynamic compression scenarios.

MGMapNet: Multi-Granularity Representation Learning for End-to-End Vectorized HD Map Construction
MGMapNet is a novel framework that models map elements using multi-granularity representation, effectively integrating coarse-grained instance-level and fine-grained point-level queries. By utilizing a multi-granularity aggregator and a point-instance interaction module, MGMapNet achieves state-of-the-art performances, surpassing existing methods like MapTRv2 on both the nuScenes and Argoverse2 datasets.

Matérn Kernels for Tunable Implicit Surface Reconstruction
This paper introduces the use of Matérn kernels for implicit surface reconstruction, demonstrating their superiority over arc-cosine kernels in terms of implementation simplicity, computational speed, and scalability. The authors highlight the advantages of Matérn kernels, particularly the Laplace kernel, in achieving tunable and competitive surface reconstruction performance, with significantly reduced training times.

FasterCache: Training-Free Video Diffusion Model Acceleration with High Quality
This paper introduces \textbf{\textit{FasterCache}}, a novel, training-free method to speed up video diffusion model inference while maintaining high video quality. Key contributions include a dynamic feature reuse strategy and CFG-Cache, leading to a significant acceleration (e.g., 1.67× on Vchitect-2.0) with video quality either comparable or superior to existing methods.

EG4D: Explicit Generation of 4D Object without Score Distillation
The paper introduces EG4D, a novel multi-stage framework designed to generate high-quality and consistent 4D assets without relying on the traditional score distillation sampling method. By employing advanced techniques such as an attention injection strategy, dynamic reconstruction via Gaussian Splatting, and diffusion-based semantic restoration, EG4D addresses previous challenges like temporal inconsistency and semantic defects, significantly outperforming existing methods in both qualitative and quantitative evaluations.

Fast Feedforward 3D Gaussian Splatting Compression
Fast Compression of 3D Gaussian Splatting (FCGS) introduces an optimization-free model that significantly reduces the time required to compress 3D Gaussian Splatting representations from minutes to seconds with a single feed-forward pass. By utilizing a multi-path entropy module and refined Gaussian context models, FCGS achieves over a 20X compression ratio while maintaining high fidelity, outperforming many state-of-the-art optimization-based methods.

Semi-Supervised Vision-Centric 3D Occupancy World Model for Autonomous Driving
This paper introduces PreWorld, a semi-supervised vision-centric 3D occupancy world model that reduces the need for costly 3D annotations by leveraging 2D labels through a novel two-stage training process. Extensive experiments on the nuScenes dataset show that PreWorld significantly improves 3D occupancy prediction, 4D forecasting, and motion planning for autonomous driving applications.

Gaussian-Based Instance-Adaptive Intensity Modeling for Point-Supervised Facial Expression Spotting
This paper introduces a two-branch framework for point-supervised facial expression spotting (P-FES) using a Gaussian-based instance-adaptive Intensity Modeling (GIM) module for soft pseudo-labeling, which models expression intensity distributions to improve label accuracy. The framework also employs an Intensity-Aware Contrastive (IAC) loss for enhanced feature learning, demonstrating effectiveness on the SAMM-LV and CAS(ME)$^2$ datasets, with the code provided for further exploration.

### Applications->Everything Else
API Pack: A Massive Multi-Programming Language Dataset for API Call Generation
API Pack is a comprehensive dataset of over one million instruction-API calls designed to enhance API call generation in large language models. The study demonstrates that fine-tuning models like CodeLlama-13B on this dataset surpasses GPT-3.5 and GPT-4 in generating new API calls, with improved accuracy across languages due to diverse training data, and highlights the generalization benefits of large datasets. The model, dataset, and code are open-source for further research and development.

DeLLMa: Decision Making Under Uncertainty with Large Language Models
This paper introduces DeLLMa, a framework for enhancing decision-making accuracy of large language models (LLMs) in uncertain environments by employing a multi-step reasoning process based on decision and utility theory. DeLLMa improves decision-making performance up to 40% over competing methods and demonstrates increased accuracy through scaling compute at test time, validated in realistic decision-making scenarios and human evaluations.

CrossMPT: Cross-attention Message-passing Transformer for Error Correcting Codes
This paper introduces the Cross-Attention Message-Passing Transformer (CrossMPT), a novel approach to error correcting code (ECC) decoding that separates and iterates the updating of magnitude and syndrome embeddings using masked cross-attention blocks based on the code's parity-check matrix. CrossMPT demonstrates superior decoding performance compared to existing neural network-based decoders, while also offering benefits in terms of reduced memory usage, computational complexity, inference time, and training time.

ForecastBench: A Dynamic Benchmark of AI Forecasting Capabilities
The paper introduces ForecastBench, a dynamic benchmark designed to evaluate the accuracy of machine learning (ML) systems on forecasting questions with no pre-known answers to prevent data leakage. It demonstrates that, while LLMs often achieve superior performance on various benchmarks, expert human forecasters outperform the top LLMs in this setting, with results available on a public leaderboard.

### Applications->Genetics, Cell Biology, Health, etc
Contextualizing biological perturbation experiments through language
This paper introduces PerturbQA, a benchmark designed for structured reasoning over high-content perturbation experiments, addressing the limitations of current approaches that do not align well with downstream biological analyses. The authors demonstrate that existing methods underperform on PerturbQA and propose Summer, a domain-informed LLM framework that matches or surpasses the current state-of-the-art, facilitating improved predictions and insights from perturbation space exploration.

Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images
The paper introduces STEM, a novel computational tool that leverages a conditional diffusion generative model to infer spatial gene expression from H&E stained images. By achieving state-of-the-art performance on spatial gene expression prediction and preserving biological heterogeneity, STEM enables the analysis of existing histology images without needing physical gene expression profiling, thereby facilitating new biological discoveries.

### Applications->Language, Speech and Dialog
DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search
The paper presents methods to enhance large language models' ability to construct formal proofs by utilizing feedback from the Lean proof assistant. Key contributions include employing online reinforcement learning with Lean verification outcomes and introducing RMaxTS, a Monte-Carlo tree search variant, leading to improved proof generation efficiency and achieving state-of-the-art results on miniF2F and ProofNet benchmarks.

Scaling Transformers for Low-Bitrate High-Quality Speech Coding
This paper introduces a transformer-based neural audio codec model using Finite Scalar Quantization (FSQ), demonstrating the ability to achieve state-of-the-art speech quality at significantly low bit-rates of 400 or 700 bits-per-second. The models outperform existing baselines in both objective and subjective evaluations, proving the effectiveness of larger parameter-count architectures in audio tokenization.

RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph
Large Language Models (LLMs) face challenges in AI software engineering tasks that require repository-level code understanding, which is often overlooked by existing methods. RepoGraph, a plug-in module designed to manage repository-level structures, enhances the performance of AI software engineering solutions, as evidenced by its success in boosting state-of-the-art results on benchmarks like SWE-bench and CrossCodeEval.

T2V2: A Unified Non-Autoregressive Model for Speech Recognition and Synthesis via Multitask Learning
We present T2V2, a non-autoregressive model that unifies automatic speech recognition (ASR) and text-to-speech (TTS) synthesis using a shared Conformer backbone with rotary positional embeddings. By implementing auxiliary tasks such as CTC error correction and unconditional speech MLM, T2V2 achieves state-of-the-art results in TTS and competitive performance in discrete ASR, demonstrating its efficiency and effectiveness without external aligners.

WavTokenizer: an Efficient Acoustic Discrete Codec Tokenizer for Audio Language Modeling
The paper introduces WavTokenizer, a novel audio codec tokenizer that achieves extreme compression and improved subjective quality compared to previous models by using a single quantizer for high-frequency audio with fewer tokens. Through a broader VQ space, extended contextual windows, and enhanced attention networks, WavTokenizer outperforms state-of-the-art models in reconstruction quality across speech, audio, and music, and its effectiveness is confirmed through comprehensive ablation studies.

### Applications->Neuroscience, Cognitive Science
Disentangling 3D Animal Pose Dynamics with Scrubbed Conditional Latent Variables
This paper introduces a framework for motion analysis using conditional variational autoencoders combined with adversarial learning to address variability in tracking lab animal movements, enabling the disentanglement of behavioral factors. The approach enhances tasks like clustering and motion synthesis and is applied to improve disease detection in a Parkinsonian mouse model.

Inverse decision-making using neural amortized Bayesian actors
This paper introduces a novel approach to tackling the computational challenges of Bayesian decision-making models in naturalistic tasks by amortizing the Bayesian actor through a neural network, enabling efficient gradient-based inference. The method effectively infers posterior distributions and allows for principled model comparison, as demonstrated on synthetic and empirical sensorimotor task data, providing a powerful tool for explaining behavioral patterns and disentangling prior and cost-related factors.

### Applications->Physics
Text2PDE: Latent Diffusion Models for Accessible Physics Simulation
This paper introduces methods for applying latent diffusion models to physics simulations, specifically focusing on mesh autoencoders for compressing PDE data and investigating spatiotemporal solution generation to reduce error accumulation. The approach demonstrates competitive accuracy and efficiency with current neural PDE solvers, offering a scalable and accessible solution, and introduces the novel concept of text2PDE generation, which enables language as a powerful modality for simulating physics.

CL-DiffPhyCon: Closed-loop Diffusion Control of Complex Physical Systems
This paper introduces an efficient Closed-Loop Diffusion method for Physical systems Control (CL-DiffPhyCon), designed to improve performance and efficiency in generative control methods when managing complex systems in a closed-loop setting. By using an asynchronous denoising framework and incorporating fast sampling techniques, CL-DiffPhyCon provides superior control performance and enhances sampling efficiency, as demonstrated in 1D Burgers' equation and 2D incompressible fluid control tasks.

### Applications->Robotics
ImagineNav: Prompting Vision-Language Models as Embodied Navigator through Scene Imagination
This paper presents ImagineNav, a novel navigation framework that enhances Vision-Language Models (VLMs) for efficient mapless visual navigation using only on-board camera inputs. By transforming complex navigation planning into a best-view image selection problem, and introducing the Where2Imagine module, ImagineNav significantly improves performance on open-vocabulary object navigation benchmarks.

### Applications->Time Series
PPT: Patch Order Do Matters In Time Series Pretext Task
This paper introduces the Patch order-aware Pretext Task (PPT), a new self-supervised learning approach that enhances time series classification by leveraging the sequential order of patches across time and channel dimensions. The authors demonstrate that PPT significantly improves model performance—up to 7% in accuracy for a cardiogram task—and propose the ACF-CoS metric to evaluate the importance of orderness in time series datasets, outperforming traditional mask-based learning methods.

### Deep Learning->Algorithms
Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations
We introduce Sensitivity-Constrained Fourier Neural Operators (SC-FNO) to address challenges in inverse problems and sensitivity calculations within parametric differential equations. SC-FNO outperforms standard methods by improving parameter inversion, accommodating complex parameter spaces, and reducing training data requirements and time, while maintaining accuracy across various differential equations with minimal computational overhead.

Efficiently Parameterized Neural Metriplectic Systems
The paper introduces Neural Metriplectic Systems (NMS), a method for learning metriplectic systems from data with quadratic scalability concerning state size and operator rank, ensuring energy conservation and entropy stability. The approach demonstrates accurate learning of dynamics and potential generalization to new timescales with low approximation error, providing examples that highlight its superior accuracy and scalability even with incomplete state information.

AutoCLIP: Auto-tuning Zero-Shot Classifiers for Vision-Language Models
AutoCLIP introduces an auto-tuning method for zero-shot classifiers built upon vision-language models, optimizing per-image weights for each prompt template based on descriptor-image similarity statistics. This unsupervised approach, requiring minimal computational overhead, consistently enhances performance across various models and datasets, achieving up to 3 percent point improvement in accuracy.

### Deep Learning->Everything Else
STAR: Stability-Inducing Weight Perturbation for Continual Learning
The paper introduces STAR, a novel loss function designed to address the challenge of catastrophic forgetting in continual learning by reducing the KL-divergence of model predictions through worst-case parameter perturbation. STAR enhances performance across various existing rehearsal-based methods, offering up to a 15% improvement in accuracy and delivering results comparable to state-of-the-art techniques for continual learning.

Watch Less, Do More: Implicit Skill Discovery for Video-Conditioned Policy
This paper introduces Watch-Less-Do-More, an algorithm that advances video-conditioned policy learning by enabling a policy to perform multiple skills and generalize to unseen videos through skill recombination. By utilizing an information bottleneck-based framework for skill discovery, the method achieves significant improvements in compositional generalization, outperforming existing baselines in various experimental environments.

### Deep Learning->Generative Models and Autoencoders
Lossy Compression with Pretrained Diffusion Models
The paper presents the first complete implementation of the DiffC algorithm for lossy image compression, using pretrained diffusion models like Stable Diffusion to achieve this efficiently in under 10 seconds. By addressing challenges in reverse-channel coding with simple workarounds, the method competes effectively with other generative compression techniques at ultra-low bitrates without additional training.

A Simple Approach to Unifying Diffusion-based Conditional Generation
This paper presents a simple, unified framework for handling diverse conditional generation tasks using a diffusion model to learn a joint distribution over correlated image pairs. The approach simplifies the process by requiring only a single, efficient training stage with minimal additional parameters, achieving comparable or better results than specialized methods and supporting versatile capabilities, including multi-signal conditional generation.

VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis
This paper addresses the challenge of generating longer, dynamically evolving videos in text-to-video (T2V) synthesis by introducing Generative Temporal Nursing (GTN), which improves control over temporal dynamics during inference. The proposed method, VSTAR, combines Video Synopsis Prompting (VSP) and the novel Temporal Attention Regularization (TAR) to enable training-free longer video synthesis, demonstrating superior performance compared to existing open-sourced T2V models.

OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes
The paper addresses visual biases and stereotypes in images generated by text-to-image (T2I) models and proposes a new quantitative measure, OASIS, that aligns with the sociological definition of stereotypes. OASIS assesses stereotypes in generated datasets using two scores, Stereotype Score and WALS, and analyzes the origins of these stereotypes within T2I models using methods to identify associated attributes and latent space influences, revealing significant stereotypical predispositions in newer models like FLUX.1 and SDv3, particularly affecting nationalities with lower internet presence.

ParaSolver: A Hierarchical Parallel Integral Solver for Diffusion Models
This paper addresses the challenge of speeding up the sequential inference process of Diffusion Probabilistic Models (DPMs) by transforming it into a parallel sampling process. Introducing a unified framework and the ParaSolver technique, the study demonstrates a significant speedup of up to 12.1 times in sampling process efficiency, while maintaining output quality, with source code available for public access.

Towards Semantic Equivalence of Tokenization in Multimodal LLM
This paper introduces a novel Semantic-Equivalent Vision Tokenizer (SeTok) for Multimodal Large Language Models (MLLMs), which groups visual features into semantic units using a dynamic clustering algorithm to maintain visual semantic integrity. The proposed MLLM, Setokim, demonstrates superior performance across various tasks by effectively capturing both low-frequency and high-frequency visual features, as validated by experimental results.

Looking Backward: Streaming Video-to-Video Translation with Feature Banks
This paper presents StreamV2V, an innovative diffusion model for real-time streaming video-to-video (V2V) translation that processes frames continuously to handle unlimited video lengths. StreamV2V's ability to integrate seamlessly with image diffusion models, along with its significant speed advantage over existing methods and robust temporal consistency, highlights its adaptability and efficiency.

OmniPhysGS: 3D Constitutive Gaussians for General Physics-Based Dynamics Generation
OmniPhysGS is a novel framework for generating dynamic 3D scenes that accurately portray the physical properties of diverse materials by representing each 3D asset as a collection of constitutive 3D Gaussians. By leveraging an ensemble of expert sub-models and a pretrained video diffusion model, it provides more realistic physical dynamics and surpasses existing methods in visual quality and text alignment by 3% to 16%.

MeshAnything: Artist-Created Mesh Generation with Autoregressive Transformers
MeshAnything is a novel model that addresses the inefficiencies of current mesh extraction methods by treating mesh extraction as a generation problem, producing Artist-Created Meshes (AMs) that align with specified shapes. By integrating a VQ-VAE and a shape-conditioned decoder-only transformer, MeshAnything converts 3D assets into AMs with significantly fewer faces, enhancing storage, rendering, and simulation efficiencies while maintaining precision comparable to previous methods, thus broadening their application potential in the 3D industry.

SafeDiffuser: Safe Planning with Diffusion Probabilistic Models
This paper introduces SafeDiffuser, a method that integrates control barrier functions into diffusion models to ensure safety guarantees during data-driven planning. By embedding finite-time diffusion invariance into the denoising procedure, SafeDiffuser maintains generative performance while ensuring robust data generation under safety constraints, demonstrated in tasks like maze path generation and robot locomotion.

Beyond Autoregression: Fast LLMs via Self-Distillation Through Time
This study presents diffusion language models that can generate at least 32 tokens simultaneously, surpassing autoregressive models in text quality and performance on the LAMBADA benchmark. Utilizing a novel distillation method, these models perform inference 8 times faster than traditional autoregressive models, offering significant improvements in efficiency and scalability.

### Deep Learning->Graph Neural Networks
Port-Hamiltonian Architectural Bias for Long-Range Propagation in Deep Graph Networks
This paper introduces port-Hamiltonian Deep Graph Networks, a novel framework for modeling neural information flow in graphs by leveraging the principles of Hamiltonian dynamical systems to control information propagation and dissipation. The approach is applicable to general message-passing architectures and demonstrates state-of-the-art performance in long-range benchmarks with theoretical guarantees on information conservation.

Graph Neural Preconditioners for Iterative Solutions of Sparse Linear Systems
This paper proposes using graph neural networks as a general-purpose preconditioner for solving large, sparse linear systems of equations, improving performance where traditional methods struggle. Empirical results on over 800 matrices demonstrate that these graph neural preconditioners offer more predictable and quicker construction times, with faster execution compared to conventional methods like ILU, AMG, and Krylov-based preconditioners, showing promise across various scientific and economic applications.

Is uniform expressivity too restrictive? Towards efficient expressivity of GNNs
This paper proves that uniform expressivity of GC2 queries is not achievable for GNNs with Pfaffian activation functions like sigmoid and $\tanh$, addressing a query by Grohe (2021). However, it shows that these GNNs can efficiently express GC2 queries with a parameter count logarithmic to the maximum degree of input graphs, demonstrating practical applicability and confirming theoretical estimates through experiments.

Subgraph Federated Learning for Local Generalization
Federated Learning on graphs often struggles with local overfitting, limiting the ability of models to generalize to unseen data due to changing graph structures and label distributions. The proposed FedLoG method addresses this by creating global synthetic data, enhancing the generalization capabilities of local models, and outperforming baselines in generalization tasks.

Understanding Virtual Nodes: Oversquashing and Node Heterogeneity
This paper investigates the enhancement of message passing neural networks (MPNNs) using virtual nodes (VNs) to address issues like oversquashing and limited long-range interaction capture. It presents a theoretical analysis of how VNs improve network mixing and sensitivity based on topology, proposing a VN variant with varying node sensitivity for better performance in graph-level tasks while retaining computational efficiency.

### Deep Learning->Large Language Models
Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models
This paper presents a novel, controllable, and scalable captioning pipeline that generates diverse caption formats to enhance the pre-training of multimodal foundation models. By exploring the effects and interactions of short and descriptive synthetic captions with AltTexts across various models, the study finds that a hybrid approach, utilizing both synthetic captions and AltTexts, improves performance and image-text alignment, offering valuable insights into optimizing captioning strategies for different multimodal models.

Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning
This paper introduces LoRA-Dash, a novel approach in Parameter Efficient Fine-Tuning (PEFT) that leverages task-specific directions to enhance large language model performance with minimal resource consumption. Extensive experiments confirm LoRA-Dash's effectiveness in improving model performance on targeted tasks, while analyses uncover the mechanisms behind its success.

DSBench: How Far Are Data Science Agents from Becoming Data Science Experts?
DSBench is a comprehensive benchmark designed to evaluate data science agents in realistic settings, including tasks sourced from Eloquence and Kaggle competitions. The evaluation shows that cutting-edge LLMs, LVLMs, and agents currently face significant challenges, underscoring the requirement for advancements in creating more effective data science agents.

Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities
Spatial expressions often vary based on the frames of reference used, leading to potential ambiguities in vision-language models (VLMs). The COMFORT evaluation protocol reveals significant shortcomings in VLMs, including poor robustness, inconsistency, and a tendency to prioritize English over other languages, highlighting a need for improved attention to cross-cultural and ambiguous spatial reasoning.

Regressing the Relative Future: Efficient Policy Optimization for Multi-turn RLHF
Large Language Models (LLMs) often excel in single-turn tasks but face challenges in multi-turn dialogue due to covariate shift when extended to reinforcement learning from human feedback (RLHF) in such settings. The paper introduces REFUEL, a policy optimization approach that regresses the relative future to tackle covariate shift in multi-turn RLHF, proving superior performance to existing methods on dialogue tasks and enhancing smaller models like Llama-3-8B-it to outperform larger counterparts.

Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining
This paper introduces novel algorithms for dynamic, instance-level data reweighting to enhance the efficiency and effectiveness of large language model (LLM) pretraining by adjusting sample weights based on loss values. The approach not only provides the first formal characterization of reweighting impacts on optimization convergence but also shows empirical success across various model sizes and tasks, leading to faster convergence and improved performance.

Law of the Weakest Link: Cross Capabilities of Large Language Models
This paper introduces the concept of **cross capabilities** in Large Language Models (LLMs), emphasizing the importance of evaluating the intersection of multiple abilities required for real-world tasks. Through the creation of the *CrossEval* benchmark and analysis of 17 models, the study finds that current LLMs are limited by their weakest abilities in tasks requiring cross capabilities, highlighting the need for focused improvements in these areas.

TAID: Temporally Adaptive Interpolated Distillation for Efficient Knowledge Transfer in Language Models
Causal language models, while powerful, face deployment challenges due to their size, especially in resource-limited settings. This paper introduces Temporally Adaptive Interpolated Distillation (TAID), a novel knowledge distillation method designed to mitigate issues such as capacity gaps and mode collapse, demonstrating improved performance in creating compact, high-performing models like TAID-LLM-1.5B for language and TAID-VLM-2B for vision-language tasks.

The Crucial Role of Samplers in Online Direct Preference Optimization
This paper provides a rigorous analysis of Direct Preference Optimization (DPO), revealing that uniform sampling results in linear convergence, whereas a novel online sampler achieves quadratic convergence under exact gradient conditions. By incorporating posterior distributions and logit mixing, the study demonstrates significant performance improvements, such as a 7.4% increase on the Safe-RLHF dataset, advancing both theoretical understanding and practical algorithm design in language model alignment.

SWEb: A Large Web Dataset for the Scandinavian Languages
This paper introduces the Scandinavian WEb (SWEb), the largest pretraining dataset for Scandinavian languages, containing over one trillion tokens, and outlines its collection and processing methods, including a new model-based text extractor that reduces complexity. Additionally, the paper presents a novel cloze-style benchmark for Swedish language models, demonstrating that models trained on SWEb data achieve competitive performance compared to those trained on FineWeb, with all resources made openly available.

LLM-based Typed Hyperresolution for Commonsense Reasoning with Knowledge Bases
This paper introduces LLM-based Typed Hyperresolution (LLM-TH), a logical commonsense reasoning framework that enhances large language models by integrating "theory resolution" from classical logical inference, thus reducing reasoning errors and hallucinations. The framework is capable of efficiently reasoning over extensive knowledge bases with mechanisms for error correction and demonstrates superior performance in various reasoning tasks compared to existing large language model baselines.

Temporal Reasoning Transfer from Text to Video
Video Large Language Models (Video LLMs) face challenges in temporal reasoning due to inherent difficulties with temporal concepts rather than ineffective temporal encoding of visual inputs. The proposed Textual Temporal reasoning Transfer (T3) method, which transfers temporal reasoning abilities from text to video domains, significantly improves LongVA-7B's performance on the TempCompass benchmark and other video tasks without using video data, demonstrating competitive results against larger models.

InstructRAG: Instructing Retrieval-Augmented Generation via Self-Synthesized Rationales
InstructRAG is a novel approach to retrieval-augmented generation that explicitly teaches language models to denoise retrieved content by using self-synthesized rationales to explain the derivation of answers from documents. This method improves generation accuracy without requiring additional supervision and outperforms existing RAG approaches across multiple benchmarks, demonstrating robust denoising capabilities and strong generalizability.

Learning LLM-as-a-Judge for Preference Alignment
This paper proposes using large language models (LLMs) themselves to learn from preference data and function as interpretable judges, addressing limitations of traditional scalar reward models. By employing self-generated contrastive judgments (Con-J) in natural language, the approach demonstrates enhanced robustness against bias and superior performance over existing models, with all resources openly available for further research.

PiCO: Peer Review in LLMs based on Consistency Optimization
This paper presents a novel unsupervised evaluation method for large language models (LLMs) using a peer-review mechanism, allowing models to assess each other's responses without human feedback. The findings reveal that stronger LLMs produce answers that are consistently rated higher by peers, leading to a ranking system that aligns more closely with human evaluations, validated through experiments on various datasets.

Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models
The paper introduces Fiddler, a resource-efficient inference system designed for Mixture-of-Experts models in environments with limited GPU resources, optimizing the use of both CPU and GPU. Fiddler achieves up to 11.57 times speed-up in inference over existing systems by offering a more universal performance boost across various scenarios and is available for public use.

Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures
CoLM addresses the challenge of large GPU memory requirements in training Large Language Models with large mini-batches by proposing a method to find representative mini-batch coresets. By including all examples from small data sources, normalizing gradients, and using zeroth-order methods, CoLM reduces memory demands, demonstrated in fine-tuning models like Phi-2, Llama-3 on benchmarks, and even outperforms larger mini-batch training while further optimizing memory efficiency when combined with technologies like LoRA.

When Attention Sink Emerges in Language Models: An Empirical View
This paper investigates the phenomenon of "attention sink" in auto-regressive language models, where excessive attention is given to initial tokens regardless of their semantic importance. The study reveals that attention sinks universally emerge during pre-training due to optimization, data distribution, and loss function factors and can be mitigated by altering attention mechanisms, such as replacing softmax with sigmoid attention, which prevents the emergence of attention sinks in models up to 1B parameters.

UGMathBench: A Diverse and Dynamic Benchmark for Undergraduate-Level Mathematical Reasoning with Large Language Models
The paper introduces UGMathBench, a comprehensive benchmark designed to evaluate undergraduate-level mathematical reasoning in large language models (LLMs), addressing existing evaluation shortcomings with 5,062 diverse problems across 16 subjects and 111 topics. It also proposes two metrics, effective accuracy (EAcc) and reasoning gap ($\Delta$), to better assess LLM reasoning, highlighting that future research should focus on improving model reasoning robustness as current models show significant reasoning gaps.

Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models
Cybench is a framework that specifies and evaluates cybersecurity tasks using 40 professional Capture the Flag (CTF) tasks, aimed at quantifying the capabilities of language model agents in identifying vulnerabilities and executing exploits. The study evaluates eight models on these tasks and highlights the success of models like GPT-4o and Claude 3.5 Sonnet in completing tasks autonomously that human teams took significantly longer to solve, underlining their potential for enhancing cybersecurity practices.

Benchmarking LLMs' Judgments with No Gold Standard
The paper introduces GEM (Generative Estimator for Mutual Information), a novel evaluation metric for language generation by large language models (LLMs) that assesses informative judgments without requiring a gold standard reference. GEM outperforms existing benchmarks in robustness and correlates well with human evaluations, while GRE-bench leverages GEM's strengths to assess LLM capabilities in generating high-quality academic peer reviews, demonstrated using ICLR2023 data.

LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations
This paper explores the internal states of large language models (LLMs), revealing that they encode detailed information about the truthfulness of their outputs, which can be leveraged to detect errors such as hallucinations. The findings highlight that error detection performance improves when focusing on specific tokens and indicate that internal representations can predict likely error types while also showing a discrepancy between encoded truthfulness and external output accuracy, thus providing valuable insights for future error analysis and mitigation strategies.

MMTEB: Massive Multilingual Text Embedding Benchmark
The Massive Multilingual Text Embedding Benchmark (MMTEB) introduces an expansive multilingual evaluation framework, expanding existing benchmarks to over 500 tasks across 1,000+ languages, with a focus on novel and challenging tasks like instruction following and code retrieval. This initiative not only highlights the performance of different models, demonstrating that the smaller multilingual-e5-large-instruct model outperforms larger models across languages, but also proposes cost-effective benchmarking strategies to enhance accessibility for low-resource communities.

CREAM: Consistency Regularized Self-Rewarding Language Models
This paper addresses the challenge of bias and unreliable preference data in self-rewarding large language models, which use LLM-as-a-Judge for response generation and ranking without human annotations. By introducing a Consistency Regularized sElf-rewarding lAnguage Model (CREAM), the authors enhance reward consistency and alignment performance, as demonstrated by empirical results, with their approach effectively using regularization to mitigate overconfident preference labeling.

Bootstrapping Language Models with DPO Implicit Rewards
This paper introduces a novel approach called self-alignment with DPO ImpliCit rEwards (DICE), which leverages the implicit reward model from direct preference optimization (DPO) to further align large language models (LLMs) without external feedback. The method shows significant improvements, achieving an over 8% increase in length-controlled win rate on AlpacaEval 2 through the use of length-regularized reward shaping and experience replay.

MindSearch: Mimicking Human Minds Elicits Deep AI Searcher
MindSearch is an innovative LLM-based multi-agent framework designed to enhance web information seeking and integration by mimicking human cognitive processes. It addresses challenges in accurately retrieving and integrating scattered web information, achieving significant improvements in response quality and efficiency, and proves to be competitive compared to existing AI search engines.

Arithmetic Without Algorithms: Language Models Solve Math with a Bag of Heuristics
This paper investigates whether large language models (LLMs) use robust algorithms or memorize data by analyzing their performance on arithmetic reasoning tasks. The study reveals that LLMs rely on a combination of simple heuristic neurons, rather than robust algorithms or memorization, to achieve accuracy in arithmetic tasks, suggesting these heuristics are pivotal from early training stages.

Cut Your Losses in Large-Vocabulary Language Models
The paper introduces Cut Cross-Entropy (CCE), a novel method for computing cross-entropy loss in large language models that significantly reduces memory usage by only materializing necessary logits directly in flash memory. This approach cuts the memory footprint of loss computation from 24 GB to 1 MB for the Gemma 2 (2B) model, maintaining training speed and convergence while greatly enhancing memory efficiency.

Dynamic Low-Rank Sparse Adaptation for Large Language Models
This paper introduces LoSA, a novel method that dynamically integrates low-rank adaptation into sparse Large Language Models (LLMs) within a unified framework to enhance their performance without increasing inference latency. The approach uses Representation Mutual Information and adaptive layer-wise fine-tuning to improve sparse LLMs, significantly reducing perplexity and increasing accuracy while achieving notable speedups on both CPU and GPU platforms in a short fine-tuning period.

Holistic Reasoning with Long-Context LMs: A Benchmark for Database Operations on Massive Textual Data
The paper introduces HoloBench, a framework designed to evaluate the holistic reasoning capabilities of long-context language models (LCLMs) by emulating database reasoning operations in text settings. It finds that information density and query complexity significantly impact LCLM performance, highlighting the challenges and opportunities in improving these models for comprehensive understanding across large documents.

Personality Alignment of Large Language Models
This paper introduces the concept of Personality Alignment to tailor large language models (LLMs) to individual user preferences, utilizing the Personality Alignment with Personality Inventories (PAPI) dataset derived from over 320,000 subjects' data. By developing an efficient activation intervention optimization method called PAS, the authors demonstrate superior alignment performance and reduced optimization time, enhancing the personalization and relevance of AI interactions, and contribute to the advancement of human-centered AI.

EMMA: Empowering Multi-modal Mamba with Structural and Hierarchical Alignment
The paper introduces EMMA, an approach to enhance Mamba multi-modal large language models by improving visual feature extraction through structural and hierarchical alignment techniques. By implementing pixel-wise alignment and multi-scale feature fusion, EMMA achieves better cross-modal alignment, resulting in faster inference speeds and superior performance on multi-modal benchmarks compared to existing models.

Episodic Memories Generation and Evaluation Benchmark for Large Language Models
The paper highlights the importance of integrating episodic memory capabilities into Large Language Models (LLMs) to enhance their ability for consistent reasoning and grounding in real-world events. It introduces a novel framework and benchmark to evaluate these capabilities, revealing that current LLMs, such as GPT-4 and others, struggle with tasks involving complex episodic memory, even over relatively short contexts.

From Tokens to Lattices: Emergent Lattice Structures in Language Models
This paper investigates how pretrained masked language models (MLMs) develop a lattice structure of concepts through the lens of Formal Concept Analysis (FCA), which derives concept lattices from object-attribute relationships. The authors propose a novel framework that constructs concept lattices from MLMs without relying on human-defined concepts, demonstrating through three datasets that MLMs can discover latent concepts beyond human definitions.

Lawma: The Power of Specialization for Legal Annotation
This paper investigates the capabilities of large language models in performing legal annotation tasks, introducing the CaselawQA benchmark with 260 new legal text classification tasks. It finds that while GPT-4 shows variable performance, a lightly fine-tuned Llama 3 8B model significantly outperforms it, suggesting that open-source models are a preferable alternative for legal tasks when some labeled data is available.

LICO: Large Language Models for In-Context Molecular Optimization
LICO is a general-purpose model that extends large language models for black-box optimization, particularly in the molecular domain, by equipping them with a separate embedding and prediction layer. It achieves competitive results on challenging molecular optimization benchmarks, notably attaining state-of-the-art performance on the low-budget PMO-1K.

MMEgo: Towards Building Egocentric Multimodal LLMs for Video QA
This research introduces MM-Ego, a multimodal foundation model designed for egocentric video understanding, and presents significant contributions such as the creation of a vast 7M QA dataset and a challenging QA benchmark. Additionally, the study proposes a new multimodal architecture with a "Memory Pointer Prompting" mechanism to enhance the comprehension of extended video content, addressing language bias to improve the model's effectiveness in recognizing and memorizing visual details across various video lengths.

MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts
This paper introduces MoE++, a novel Mixture-of-Experts (MoE) framework that combines Feed-Forward Network (FFN) experts with new zero-computation experts, including zero, copy, and constant experts, to enhance computational efficiency and effectiveness. MoE++ optimizes token engagement with experts, reduces computing overhead, improves performance by focusing resources on challenging tokens, and offers deployment advantages by minimizing communication overhead, ultimately achieving superior performance and throughput compared to traditional MoE models.

Uncertainty-Aware Decoding with Minimum Bayes Risk
This paper introduces an uncertainty-aware decoding method for language models using a generalized Minimum Bayes Risk (MBR) approach, incorporating a posterior over model parameters to account for uncertainty during text generation. The method improves output selection and decision-making on when to abstain from generation, demonstrating enhanced performance and diversity without additional overhead, with the code released publicly.

### Deep Learning->Other Representation Learning
Learning Continually by Spectral Regularization
This paper introduces a new spectral regularizer for continual learning, which maintains the beneficial initialization properties of neural networks by keeping the maximum singular value of each layer close to one. The proposed regularizer enhances trainability and performance across various models in both supervised and reinforcement learning settings, demonstrating improved generalization and reduced sensitivity to hyperparameters.

TIPS: Text-Image Pretraining with Spatial awareness
This paper introduces Text-Image Pretraining with Spatial awareness (TIPS), a novel image-text model that combines synthetically generated textual descriptions and self-supervised masked image modeling to improve dense and global vision tasks. The model demonstrates strong performance across multiple tasks and datasets by enhancing spatial coherence through an innovative training method involving noisy and synthetic captions, providing a significant advancement over traditional image-text representation learning approaches.

### Deep Learning->Robustness
To Tackle Adversarial Transferability: A Novel Ensemble Training Method with Fourier Transformation
This paper addresses the impact of adversarial example transferability on ensemble model performance by introducing an efficient data transformation method using a "weakness allocation" strategy to diversify non-robust features. Through experiments, the proposed method demonstrates improved robust accuracy without requiring sub-model communication and maintaining high clean accuracy compared to existing ensemble methods.

Detecting Backdoor Samples in Contrastive Language Image Pretraining
This paper examines the vulnerability of CLIP models to poisoning backdoor attacks, revealing that unique local subspace characteristics of backdoor-poisoned samples can be detected using density ratio-based local outlier detectors. The study highlights the effectiveness of this detection approach compared to existing methods and demonstrates its applicability by efficiently identifying and removing existing backdoor samples from large datasets such as CC3M.

Generating Less Certain Adversarial Examples Improves Robust Generalization
This paper explores the robust overfitting phenomenon in adversarial training by examining the role of adversarial certainty, proposing that reduced certainty in predictions for adversarial examples enhances robust generalization. The authors introduce a method to train models that create less certain adversarial examples, showing through extensive experiments that this approach effectively improves model robustness and reduces overfitting, with the implementation available online.

### Deep Learning->Self-Supervised Learning
Mutual Effort for Efficiency: A Similarity-based Token Pruning for Vision Transformers in Self-Supervised Learning
This paper introduces SimPrune, a novel token pruning strategy designed to enhance Vision Transformers (ViTs) in self-supervised learning (SSL) by leveraging cross-branch similarity information and a difficulty-aware pruning strategy. Experimental results demonstrate that SimPrune effectively reduces training computations by 24% while maintaining accuracy, making it suitable for resource-limited environments like edge devices.

UNIP: Rethinking Pre-trained Attention Patterns for Infrared Semantic Segmentation
This study investigates the performance of pre-training techniques for semantic segmentation in scenarios with a large domain gap, such as between RGB and infrared data, and identifies unique attention patterns crucial for such tasks. Introducing the UNified Infrared Pre-training (UNIP) framework, the study demonstrates significant performance improvements of up to 13.5% in average mIoU on infrared segmentation tasks, offering efficient and superior results over state-of-the-art methods with broader application potential across different modalities.

Object-Centric Pretraining via Target Encoder Bootstrapping
This paper introduces OCEBO, a novel method for training object-centric models on real-world data through object-centric pretraining by target encoder bootstrapping. By updating the target encoder with an exponential moving average enriched with object-centric biases and using a cross-view patch filtering approach, OCEBO achieves unsupervised object discovery performance comparable to models that use large, pretrained non-object-centric encoders.

### Deep Learning->Sequential Models, Time series
HADAMRNN: BINARY AND SPARSE TERNARY ORTHOGONAL RNNS
This paper introduces a novel method leveraging Hadamard matrices to parameterize binary and sparse ternary orthogonal matrices, facilitating the creation of orthogonal RNNs (HadamRNN and Block-HadamRNN) with binary and sparse ternary weights. These RNNs demonstrate performance comparable to full-precision models on various benchmarks and uniquely address the copy task over long timesteps, marking a significant advancement in RNN weight binarization and ternarization.

Autocorrelation Matters: Understanding the Role of Initialization Schemes for State Space Models
This paper investigates the initialization of state space model (SSM) parameters by considering the autocorrelation of input sequences, extending beyond the current HiPPO framework. Key contributions include characterizing the dependency of the SSM timescale on sequence length, demonstrating how a zero real part for SSM eigenvalues mitigates memory issues while maintaining stability, and exploring the role of the imaginary part of eigenvalues in conditioning SSM optimization problems, revealing an approximation-estimation tradeoff.

### Deep Learning->Theory
A Theory of Initialisation's Impact on Specialisation
This paper investigates the underlying assumptions of neuron specialization in neural networks engaged in continual learning tasks, particularly focusing on catastrophic interference and task similarity. The authors demonstrate that weight imbalance and high weight entropy promote specialization, leading to improved outcomes when applying elastic weight consolidation regularization, and provide insights into the monotonic relationship between task similarity and forgetting in non-specialized networks.

Transformers Learn Low Sensitivity Functions: Investigations and Implications
This paper identifies the sensitivity of transformers to token-wise random perturbations as a key metric that explains their inductive biases and differentiates them from other neural network architectures. The study reveals that transformers have lower sensitivity than other models like MLPs, CNNs, ConvMixers, and LSTMs, leading to improved robustness, flatter minima in loss landscapes, and serves as a progress measure for grokking, supported by theoretical results in the NTK regime.

Capability Localization: Capabilities Can be Localized rather than Individual Knowledge
This paper investigates the localization of knowledge in large-scale language models, challenging the assumption that individual knowledge is stored in specific model parameters. By introducing the **C**ommonality **N**euron **L**ocalization (**CNL**) method, the authors successfully identify neurons responsible for data commonalities with a 96.42% overlap rate in the GSM8K dataset, highlighting neurons' role in enhancing model performance.

Inner Information Analysis Algorithm for Deep Neural Network based on Community
Deep learning's 'black box' nature limits understanding of neural network decision processes. InnerSightNet addresses this by analyzing the collective behavior of neuron communities within networks, enhancing transparency and trust through a three-phase process of neuronization, aggregation, and evaluation.

### Misc
SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity Reduction
This paper introduces Semantic Perplexity (SePer), a metric designed to evaluate retrieval quality in large language models by measuring information gain within the retrieval-augmented generation framework. SePer effectively captures the LLM's confidence in the correctness of retrieved data and proves to align well with human preferences, providing a more accurate and efficient assessment of retrieval utility.

PN-GAIL: Leveraging Non-optimal Information from Imperfect Demonstrations
This study presents Positive-Negative Generative Adversarial Imitation Learning (PN-GAIL), a new method within the GAIL framework that effectively uses non-optimal information from imperfect demonstrations. By allowing the discriminator to assess both positive and negative risks and requiring only minimal labeled confidence scores, PN-GAIL significantly outperforms traditional approaches in handling imperfect demonstrations, enhancing the practical applicability of imitation learning.

Bayesian Image Regression with Soft-thresholded Conditional Autoregressive Prior
This paper introduces the Soft-Thresholded Conditional AutoRegressive (ST-CAR) prior for regression models in large-scale brain imaging data, addressing computational challenges and the limitations of pre-specified correlation structures. The ST-CAR prior enhances the identification of active brain regions and improves computational efficiency, validated through both simulation studies and application to working memory fMRI data from the ABCD study.

Dynamical Diffusion: Learning Temporal Dynamics with Diffusion Models
Dynamical Diffusion (DyDiff) introduces a new framework for temporal predictive learning by explicitly modeling temporal transitions at each diffusion step, improving the generation of temporally coherent sequences. This approach leverages the strengths of diffusion models to enhance performance in tasks like spatiotemporal forecasting and video prediction, marking a significant advancement in capturing temporal dynamics in predictive modeling.

BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions
The paper introduces BigCodeBench, a benchmark designed to evaluate Large Language Models (LLMs) on their ability to solve practical tasks by using diverse function calls from multiple libraries across different domains. Despite recent advancements, the evaluation shows that LLMs still struggle with following complex instructions accurately, achieving a maximum score of 60%, which emphasizes the necessity for continued development in this field.

MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-tuning
MM1.5 is a new series of multimodal large language models (MLLMs) that improve text-rich image understanding, visual grounding, and multi-image reasoning by systematically optimizing data mixtures throughout the model training process. Ranging from 1B to 30B parameters, MM1.5 includes specialized variants for video and mobile UI understanding, and demonstrates that well-curated data and training strategies result in effective performance, providing a framework for future MLLM development.

KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural Networks
This paper introduces the Kolmogorov-Arnold Attention (KAA) mechanism to enhance scoring functions in attentive graph neural networks (GNNs), demonstrating significantly improved performance on node and graph-level tasks. By integrating the Kolmogorov-Arnold Network (KAN) and introducing the Maximum Ranking Distance (MRD) metric, the authors show that KAA provides nearly infinite expressive power compared to existing linear and MLP-based scoring functions, resulting in over 20% improvement in performance with various backbone models.

Safety Representations for Safer Policy Learning
This paper introduces a method to enhance reinforcement learning in safety-critical applications by learning state-conditioned safety representations, which augments state features and supports safer and more efficient exploration. The approach effectively balances exploration and safety, leading to improved task performance and reduced constraint violations, as demonstrated across various environments.

Token-Supervised Value Models for Enhancing Mathematical Problem-Solving Capabilities of Large Language Models
To enhance the mathematical problem-solving abilities of large language models (LLMs) at test time, we propose token-supervised value models (TVMs) as robust verifiers tailored for tree search techniques. TVMs improve upon existing verifiers by directly evaluating partial solutions at the token level during tree search, leading to significantly higher accuracy in solving mathematical problems as demonstrated in our experimental results.

LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization
The paper introduces LongPO, a method that enables short-context Large Language Models (LLMs) to improve their performance in long-context tasks by leveraging their inherent short-context capabilities. LongPO achieves this by using self-generated short-to-long preference data and a short-to-long KL constraint, allowing LLMs to excel in long-context scenarios without sacrificing performance in short-context tasks, outperforming existing methods like SFT and DPO while maintaining competitive results with models like GPT-4-128K.

LANTERN: Accelerating Visual Autoregressive Models with Relaxed Speculative Decoding
Auto-regressive (AR) models in image generation are limited by their sequential processing nature, slowing down generation compared to other models like GANs. This paper introduces LANTERN, a relaxed acceptance condition that mitigates token selection ambiguity in visual AR models, enhancing speculative decoding and achieving significant speed improvements without compromising image quality.

Reducing Hallucinations in Large Vision-Language Models via Latent Space Steering
This paper addresses the issue of hallucination in large vision-language models (LVLMs) due to misalignments between visual inputs and textual outputs, as opposed to large language models (LLMs). It introduces a novel technique called Visual and Textual Intervention (VTI) that stabilizes vision feature representations during inference, effectively reducing hallucinations and outperforming baseline methods without additional training costs.

Revealing and Mitigating Over-Attention in Knowledge Editing
Large Language Models often make errors due to incorrect knowledge from their training data, and while knowledge editing methods attempt to correct this, they can degrade existing model capabilities through an issue called Specificity Failure. This paper introduces a method named Selective Attention Drift Restriction (SADR) to address this issue by regulating attention weights during editing, thus maintaining overall model performance, as demonstrated through experiments on several strong LLMs.

GraphBridge: Towards Arbitrary Transfer Learning in GNNs
The paper introduces GraphBridge, a novel framework designed to enable seamless knowledge transfer across different tasks and domains in graph neural networks (GNNs), without the need to modify existing task configurations or graph structures. By augmenting pre-trained GNNs with prediction heads and a bridging network, GraphBridge efficiently supports domain-agnostic transfer learning and reduces source bias, as demonstrated through extensive evaluations on 16 datasets across various scenarios.

Going Beyond Static: Understanding Shifts with Time-Series Attribution
Distribution shifts in time-series data are challenging due to their complexity, but existing methods often rely on unvalidated assumptions. The TSSA framework addresses this by attributing performance degradation to specific temporal data properties, supported by theoretical analysis and empirical validation in healthcare, improving understanding and facilitating model deployment.

CLoSD: Closing the Loop between Simulation and Diffusion for multi-task character control
This paper introduces CLoSD, a text-driven Reinforcement Learning (RL) physics-based controller that integrates the capabilities of motion diffusion models for human motion generation. By maintaining a closed-loop interaction between a Diffusion Planner (DiP) and a tracking controller, CLoSD enables seamless execution of various tasks, combining intuitive text prompts with physically plausible motion in dynamic environments.

Learning Gain Map for Inverse Tone Mapping
A new task called Gain Map-based Inverse Tone Mapping (GM-ITM) is introduced to enhance high dynamic range (HDR) viewing by estimating the Gain Map (GM) of a standard dynamic range (SDR) image, instead of directly generating its HDR counterpart. This paper presents GMNet, a dual-branch network, for effective GM estimation, and provides both synthetic and real-world datasets for evaluation, demonstrating its superiority over existing methods in HDR conversion tasks.

Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model
Ctrl-Adapter is a framework designed to integrate spatial control into any image/video diffusion model through the adaptation of pretrained ControlNets, addressing the challenges of feature space mismatches and inefficient training burdens. It supports a variety of tasks, such as video editing and style transfer, and achieves state-of-the-art results on benchmarks like DAVIS 2017 with significantly reduced computational requirements.

SynFlowNet: Design of Diverse and Novel Molecules with Synthesis Constraints
We introduce SynFlowNet, a generative model leveraging GFlowNet with a focus on chemical reactions and available reactants to build synthesizable molecules, addressing the gap between digital molecular creation and actual synthesis. Our evaluation shows improved sample diversity and synthesizability over existing methods, and we provide strategies for integrating constraints in the GFlowNet MDP framework, enhancing backward policy learning and aiding the identification of synthesis pathways for novel molecules.

gRNAde: Geometric Deep Learning for 3D RNA inverse design
gRNAde is a geometric RNA design pipeline that accounts for 3D structure and dynamics by using a multi-state Graph Neural Network to generate RNA sequences. It outperforms previous methods such as Rosetta in both speed and accuracy for native sequence recovery and showcases its effectiveness in multi-state RNA design and mutational fitness landscape analysis, with open-source resources available online.

Is Your Video Language Model a Reliable Judge?
This study explores the effectiveness of using multiple video language models (VLMs) to evaluate each other, finding that incorporating collective judgments from a mix of reliable and unreliable models can decrease evaluation accuracy due to increased noise. The research highlights the limitations of collective thought approaches and emphasizes the necessity for advanced methods that consider the reliability of individual VLMs to improve evaluation consistency and reliability.

Ultra-Sparse Memory Network
This paper presents UltraMem, a novel architecture that integrates a large-scale, ultra-sparse memory layer to address the inference latency and memory access challenges faced by Transformer models and Mixture of Experts (MoE) approaches. UltraMem demonstrates superior scaling properties, with experiments showing state-of-the-art performance and inference speed while maintaining computational efficiency, suggesting its potential scalability to billions of memory slots or experts.

Grokking at the Edge of Numerical Stability
This paper investigates the phenomenon of grokking in deep learning, where generalization occurs suddenly after extended overfitting, and identifies softmax collapse (SC) as a key barrier to grokking without regularization. The authors introduce two contributions—StableMax, an activation function that prevents SC, and ⟂Grad, a training algorithm to prevent naive loss minimization—both of which facilitate grokking without regularization, offering insights into delayed generalization.

Gaussian Differentially Private Human Faces Under a Face Radial Curve Representation
This paper addresses the challenge of releasing 3D human face data while ensuring Gaussian Differential Privacy (GDP) by introducing a novel representation called face radial curves. The proposed method effectively preserves the shape of the average face with less noise than traditional methods, and its components are applicable beyond human faces to function value summaries and general disk-like surfaces.

Manifold Learning by Mixture Models of VAEs for Inverse Problems
This paper proposes a method for representing manifolds of arbitrary topology using a mixture model of variational autoencoders, wherein each encoder-decoder pair models a chart of the manifold. By introducing a specially designed loss function for maximum likelihood estimation and employing a Riemannian gradient descent algorithm, the authors effectively solve inverse problems such as image deblurring and electrical impedance tomography on learned manifolds.

MatExpert: Decomposing Materials Discovery By Mimicking Human Experts
MatExpert is a novel framework that uses Large Language Models and contrastive learning to streamline the discovery and design of new solid-state materials. By emulating the workflow of human materials experts through retrieval, transition, and generation stages, MatExpert significantly outperforms existing methods in material generation, enhancing validity, distribution, and stability in computational material discovery.

Fantastic Targets for Concept Erasure in Diffusion Models and Where To Find Them
This paper introduces the Adaptive Guided Erasure (AGE) method, a strategy for mitigating harmful content in diffusion models by dynamically optimizing target concepts for erasure, rather than mapping them to fixed generic ones. AGE leverages the geometric properties of the concept space, ensuring effective removal of undesirable concepts while preserving unrelated ones, and demonstrates superior performance over existing methods.

Lift Your Molecules: Molecular Graph Generation in Latent Euclidean Space
We propose a new framework, Synthetic Coordinate Embedding (SyCo), for 2D molecular graph generation by mapping these graphs to 3D point clouds using synthetic coordinates and learning the inverse through an E($n$)-Equivariant Graph Neural Network (EGNN). Our framework, exemplified by EDM-SyCo, simplifies the graph generation process and achieves state-of-the-art results in molecular graph distribution learning, significantly outperforming existing methods on datasets such as ZINC250K and GuacaMol.

Enhancing Zeroth-order Fine-tuning for Language Models with Low-rank Structures
This paper introduces a novel low-rank zeroth-order (ZO) algorithm, LOZO, which efficiently fine-tunes large language models (LLMs) without the need for storing activation values, thus significantly reducing memory overhead. The LOZO algorithm, which can be combined with momentum techniques, demonstrates superior performance to existing ZO methods and approaches the effectiveness of first-order algorithms across various model sizes and tasks, with proven convergence guarantees.

Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning
The paper explores Reinforcement Learning with Human Feedback (RLHF) through a game-theoretic perspective, introducing an innovative online algorithm called iterative Nash policy optimization (INPO) that approximates Nash policies by having the policy play against itself with no-regret learning. This approach eliminates the need for costly win rate estimation, leveraging a new loss objective minimized over a preference dataset, and demonstrates significant performance improvements on benchmarks, notably achieving high win rates on AlpacaEval 2.0 and Arena-Hard compared to state-of-the-art RLHF algorithms.

Self-supervised contrastive learning performs non-linear system identification
Self-supervised learning (SSL) is shown to perform system identification in latent space by leveraging temporal structures and auxiliary variables to relate latent representations to true generative factors. The proposed framework, DynCL, uncovers various dynamic models, and the paper provides both theoretical guarantees and empirical validation of its effectiveness.

Limits of Deep Learning: Sequence Modeling through the Lens of Complexity Theory
This paper investigates the limitations of Structured State Space Models (SSMs) and Transformers in tasks that require complex reasoning and function composition, both theoretically and empirically. The results demonstrate that these models struggle with function composition over large domains without large state sizes and exhibit significant performance degradation even with advanced prompting techniques, highlighting the need for innovative solutions to overcome these inherent barriers to multi-step reasoning and compositional task-solving.

FairMT-Bench: Benchmarking Fairness for Multi-turn Dialogue in Conversational LLMs
The paper introduces FairMT-Bench, a comprehensive benchmark designed to evaluate the fairness of large language models (LLMs) in multi-turn dialogue scenarios, which better mimic real-world conversations and pose greater risks for bias accumulation. By developing the FairMT-10K and FairMT-1K datasets and examining the performance of 15 state-of-the-art LLMs, the study reveals a heightened tendency for biased responses in multi-turn dialogues and highlights the importance of using these benchmarks for advancing LLM fairness efforts.

MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context Generation with Speculative Decoding
MagicDec demonstrates that speculative decoding (SD) can significantly speed up large language model (LLM) inference in long-context applications, even with large batch sizes. By implementing an intelligent drafting strategy and addressing key bottlenecks, the study achieves up to 2.51x speedup for moderate to long sequences across various hardware, enhancing throughput and reducing latency without accuracy loss.

Exposing and Addressing Cross-Task Inconsistency in Unified Vision-Language Models
The paper introduces CocoCON, a benchmark dataset designed to measure consistency between heterogeneous tasks in general-purpose vision models, highlighting the issue of inconsistency in state-of-the-art vision-language models. As a solution, it proposes a rank correlation-based auxiliary training objective that enhances multi-task consistency while maintaining model accuracy.

Shallow diffusion networks provably learn hidden low-dimensional structure
This paper analyzes diffusion-based generative models in the context of the Barron space, demonstrating that these models can adapt to low-dimensional structures like unknown linear subspaces or hidden independence, thus avoiding the curse of dimensionality. The authors provide an end-to-end sample complexity bound for sampling from structured distributions without needing specialized architectures, leveraging the Barron space's inherent adaptability.

When Prompt Engineering Meets Software Engineering: CNL-P as Natural and Robust "APIs'' for Human-AI Interaction
The paper introduces Controlled Natural Language for Prompt (CNL-P), a method that enhances prompt engineering for large language models by integrating best practices from both prompt engineering and software engineering to reduce ambiguity in natural language prompts. It includes an NL2CNL-P conversion tool and a linting tool for checking syntactic and semantic accuracy, which together improve the quality of language model responses and suggest a new programming paradigm leveraging natural language.

Population Transformer: Learning Population-level Representations of Neural Activity
The Population Transformer (PopT) framework introduces a self-supervised approach that enhances the aggregation and decoding of spatially-sparse neural recordings across diverse datasets by stacking on pretrained temporal embeddings. It reduces required data while maintaining high accuracy in decoding, offers generalizability across data modalities, and provides insights into neuroscience, with publicly available code for easy implementation in multi-channel neural data analysis.

AFlow: Automating Agentic Workflow Generation
Large language models often rely on manually constructed workflows, which hinders scalability and generalizability. This paper proposes AFLOW, an automated framework using Monte Carlo Tree Search to optimize workflow generation, achieving significant improvements over existing methods and enabling smaller models to perform efficiently with reduced costs.

High-Quality Joint Image and Video Tokenization with Causal VAE
This paper proposes a video compression network using a variational autoencoder with causal 3D convolution to address the curse of dimensionality in video synthesis, focusing on both spatial and temporal dimensionality reduction. Key contributions include a scale-agnostic encoder, a novel spatio-temporal down/upsampling block, and a flow regularization loss, resulting in superior video quality and compression rates compared to existing methods.

Fantastic Copyrighted Beasts and How (Not) to Generate Them
Recent studies have highlighted that image and video generation models can inadvertently reproduce copyrighted content, such as characters like Mario and Batman, raising significant legal concerns. This paper introduces a novel evaluation framework to assess the ability of these models to generate copyrighted content and evaluates current mitigation strategies, finding that methods like prompt rewriting are insufficient on their own and need additional techniques like negative prompting for effective copyright protection.

Open-Vocabulary Customization from CLIP via Data-Free Knowledge Distillation
This paper addresses the challenge of customizing vision-language models like CLIP without original data by rethinking Data-Free Knowledge Distillation (DFKD). By utilizing image-text matching and introducing style dictionary diversification and class consistency strategies, the authors achieve a significant 9.33% performance improvement in open-vocabulary customization tasks, surpassing existing DFKD methods.

Duoduo CLIP: Efficient 3D Understanding with Multi-View Images
Duoduo CLIP is a 3D representation learning model that utilizes multi-view images rather than point clouds to learn shape encodings, leveraging 2D priors from existing CLIP models for enhanced fine-tuning with 3D data. This approach demonstrates superior generalization and efficiency, requiring significantly fewer computational resources, and offers improved flexibility and performance in tasks such as text-to-shape retrieval compared to state-of-the-art point cloud methods.

OLMoE: Open Mixture-of-Experts Language Models
OLMoE is an open, cutting-edge language model utilizing sparse Mixture-of-Experts (MoE), boasting 7 billion parameters but using only 1 billion per input token. Pretrained on 5 trillion tokens, OLMoE surpasses similar and larger models, with novel insights into MoE training and routing properties, and is fully open-sourced with all associated resources.

RFMamba: Frequency-Aware State Space Model for RF-Based Human-Centric Perception
This paper introduces RFMamba, the first network to integrate the State Space Model (SSM) into RF-based human-centric perception, effectively addressing unique challenges such as diverse signal representations and frequency responses. By utilizing a dual-branch SSM block to capture frequency cues and spatial information, RFMamba achieves superior performance across various human perception tasks, as demonstrated by extensive experiments.

Reflexive Guidance: Improving OoDD in Vision-Language Models via Self-Guided Image-Adaptive Concept Generation
This paper investigates the out-of-distribution detection (OoDD) capabilities of large vision-language models (LVLMs), highlighting concerns about their trustworthiness despite their wide adoption and potential. It introduces a novel self-guided prompting method called Reflexive Guidance (ReGuide), which improves the OoDD performance of LVLMs by using self-generated image-adaptive concept suggestions, and demonstrates enhanced results in image classification and OoDD tasks.

Charting the Design Space of Neural Graph Representations for Subgraph Matching
This paper conducts the first comprehensive exploration of the design space for graph matching networks in subgraph matching, analyzing various interaction mechanisms between query and corpus graphs. It reveals that novel combinations in this space significantly enhance performance and establishes general principles for neural graph representation, providing valuable insights for broader applications.

LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models
The paper introduces LOKI, a novel benchmark designed to evaluate the ability of large multimodal models (LMMs) in detecting synthetic data across various modalities, such as video, image, 3D, text, and audio. LOKI provides a comprehensive evaluation with 18K questions spanning 26 subcategories and reveals both the potential and limitations of 28 LMMs in synthetic data detection and their capabilities for offering natural language explanations.

SVDQuant: Absorbing Outliers by Low-Rank Component for 4-Bit Diffusion Models
The paper introduces SVDQuant, a novel 4-bit quantization method for diffusion models, addressing the challenges of high memory demands and latency by absorbing outliers using a low-rank branch. This approach, combined with the Nunchaku inference engine, effectively reduces memory usage and improves processing speed while preserving image quality, as demonstrated through extensive experiments.

PooDLe🐩: Pooled and dense self-supervised learning from naturalistic videos
This paper presents PooDLe, a self-supervised learning method that effectively processes minimally-curated, naturalistic video data by combining pooled representation objectives with dense SSL objectives for optical flow warping. The method proves successful on datasets such as BDD100K and Walking Tours, showing its capability to capture spatial and semantic understanding from complex video scenes.

InsightBench: Evaluating Business Analytics Agents Through Multi-Step Insight Generation
InsightBench is a new benchmark dataset designed to test the comprehensive data analytics abilities of agents across diverse business use cases, emphasizing end-to-end processes like formulating questions and generating actionable insights. The benchmark features comprehensive quality assurance, a unique two-way evaluation using LLaMA-3, and introduces AgentPoirot, a baseline agent that surpasses existing methods, encouraging advancements in automated data analytics.

FIRING-Net: A filtered feature recycling network for speech enhancement
FIRING-Net is a novel framework for speech enhancement that addresses the challenge of distinguishing between highly similar noise and speech components by filtering input features into target and non-target sets, which then guide each other to refine features. This approach, comprising a Local Module to define features and a Global Module using self-attention energy distribution, demonstrates superior performance and efficiency over existing models across different SNR levels and noise environments.

MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks
MEGA-Bench is an evaluation suite designed to scale multimodal evaluation across over 500 real-world tasks, emphasizing cost-effective and accurate model evaluation with a diverse set of multimodal tasks utilizing over 8,000 samples. The suite distinguishes itself by supporting a variety of output formats and developing over 40 metrics to offer a fine-grained capability report of vision-language models, enabling in-depth user interaction and visualization of model performance across multiple dimensions.

A-Bench: Are LMMs Masters at Evaluating AI-generated Images?
This paper introduces **A-Bench**, a benchmark crafted to evaluate the proficiency of large multi-modal models (LMMs) in assessing AI-generated images (AIGIs), addressing the shortcomings of traditional benchmarks that often focus on natural-captured content. **A-Bench** emphasizes both high-level semantic understanding and low-level visual perception, using 2,864 AIGIs from 16 models with expert annotations, aiming to improve the evaluation and generation quality of AIGIs.

Learning from Imperfect  Human Feedback: A Tale from Corruption-Robust Dueling
This paper investigates Learning from Imperfect Human Feedback (LIHF) by modeling it as a concave-utility continuous-action dueling bandit under decaying corruption over time. The authors introduce the Robustified Stochastic Mirror Descent for Imperfect Dueling (RoSMID) algorithm, achieving nearly optimal regret, and demonstrate its effectiveness through both theoretical analysis and extensive experiments.

LiveXiv - A Multi-Modal live benchmark based on Arxiv papers content
This paper introduces LiveXiv, a scalable and evolving live benchmark created from scientific ArXiv papers to evaluate large multi-modal models without test data contamination. By automatically generating visual question-answer pairs from manuscript content and employing an efficient evaluation approach, LiveXiv provides a robust challenge for assessing model performance while significantly reducing evaluation costs, and demonstrates minimal performance variance when compared to manually verified data.

Looped Transformers for Length Generalization
This paper demonstrates that looped Transformers with an adaptive number of steps can significantly enhance their ability to generalize to inputs of unseen lengths in arithmetic and algorithmic tasks. By utilizing a proposed learning algorithm, the study shows that these Transformers can learn highly length-generalizable solutions, particularly for tasks that have a known iterative solution involving RASP-L operations.

Beyond Circuit Connections: A Non-Message Passing Graph Transformer Approach for Quantum Error Mitigation
This paper introduces GTranQEM, a novel quantum error mitigation method using a circuit-to-graph encoding that captures long-range entanglement in quantum systems. The approach outperforms existing QEM methods across various noise types and circuit configurations, enhancing the reliability of quantum computing systems.

Capturing the Temporal Dependence of Training Data Influence
Traditional data influence estimation methods are inadequate for modern training paradigms sensitive to data ordering, leading to a need for new approaches. This paper introduces "trajectory-specific leave-one-out (LOO) influence" and proposes "data value embedding" as a method for efficiently approximating this influence, revealing that early and late-stage data points significantly impact model outcomes and suggesting new strategies for data selection and curation.

Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration
This paper introduces Imitation Learning with Double Exploration (ILDE), a novel algorithm designed to address the challenges of learning expert policies from limited demonstrations and achieving beyond-expert performance. ILDE enhances exploration through optimistic policy optimization and curiosity-driven state exploration, demonstrating superior sample efficiency and outperforming state-of-the-art methods on Atari and MuJoCo tasks, with theoretical backing for its sublinear regret growth in episodes.

MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequences
MovieDreamer is a novel hierarchical framework that combines autoregressive models with diffusion-based rendering to produce long-duration videos with complex plot progressions and high visual fidelity, addressing the limitations of current diffusion models in maintaining narrative and character consistency. By leveraging multimodal scripts for detailed scene descriptions, the method ensures continuity and character identity across scenes, as demonstrated through experiments across various movie genres, showcasing superior visual and narrative quality over extended durations.

X-NeMo: Expressive Neural Motion Reenactment via Disentangled Latent Attention
X-NeMo is a zero-shot diffusion-based portrait animation system that animates static portraits by capturing facial movements from a different individual's video, addressing prior limitations like identity leakage and difficulty with subtle expressions. This is achieved through a novel framework utilizing a 1D identity-agnostic latent motion descriptor and a dual GAN decoder, resulting in superior animations with enhanced identity resemblance compared to existing methods.

SONICS: Synthetic Or Not - Identifying Counterfeit Songs
The paper addresses the challenge of distinguishing AI-generated songs from human-composed ones to protect artistic integrity, noting the limitations of existing methods which only focus on AI-generated vocals. To tackle issues such as lack of diversity and detection capability, the authors introduce SONICS, a dataset for end-to-end Synthetic Song Detection (SSD), and propose SpecTTTra, a new architecture that enhances efficiency and model performance in detecting synthetic songs, outperforming existing models like ViT and ConvNeXt in speed, memory usage, and F1 score.

DOTS: Learning to Reason Dynamically in LLMs via Optimal Reasoning Trajectories Search
This paper introduces DOTS, an approach that enhances large language models' reasoning skills by dynamically tailoring reasoning trajectories to individual question characteristics and LLM capabilities. By employing atomic reasoning action modules and optimizing these trajectories for each question, DOTS consistently outperforms static techniques in reasoning tasks and allows LLMs to allocate resources efficiently based on problem complexity.

Large-scale and Fine-grained Vision-language Pre-training for Enhanced CT Image Understanding
This paper introduces a fine-grained vision-language model (fVLM) for CT image interpretation that aligns anatomical regions with specific radiology report sentences, enhancing the accuracy of radiological evaluations. Utilizing the largest curated CT dataset, the model significantly outperformed existing methods in disease diagnosis tasks, demonstrating an average AUC improvement of up to 12.9% over existing approaches while overcoming challenges posed by anatomy-level healthy samples and false negatives.

Toward Exploratory Inverse Constraint Inference with Generative Diffusion Verifiers
Safe control in real-world applications often requires understanding constraints, which are difficult to specify. This paper introduces Exploratory Inverse Constraint Learning (ExICL), a method that recovers a diverse set of viable constraints from expert demonstrations, allowing for adaptable applications and showcasing reliable generalization across tasks and environments.

Generative Adapter: Contextualizing Language Models in Parameters with A Single Forward Pass
GenerativeAdapter is an efficient adaptation method for large language models, reducing training and inference costs by encoding test-time contexts into model parameters with a single forward pass using a lightweight adapter generator. This approach significantly enhances model performance in various tasks, such as knowledge acquisition, learning from demonstrations, and user personalization, demonstrating notable improvements over traditional fine-tuning and prompting methods.

ST-GCond: Self-supervised and Transferable Graph Dataset Condensation
The paper addresses the limitations of existing graph dataset condensation methods, which struggle with maintaining performance across different tasks and datasets. By introducing Self-supervised and Transferable Graph dataset Condensation (ST-GCond), the authors provide a method that excels in cross-task and cross-dataset scenarios through a task-disentangled meta optimization strategy and a multi-teacher self-supervised optimization strategy, achieving superior performance on multiple benchmarks.

UNSURE: self-supervised learning with Unknown Noise level  and Stein's Unbiased Risk Estimate
This paper introduces a novel self-supervised learning approach for image reconstruction that extends Stein's Unbiased Risk Estimate (SURE) without needing prior knowledge of noise levels. Through a comprehensive theoretical framework and experimental results, the proposed method demonstrates superior performance over existing self-supervised techniques in handling various imaging inverse problems.

Energy-based Backdoor Defense Against Federated Graph Learning
Federated Graph Learning is challenged by backdoor attacks, which traditional defense methods struggle to counter due to diverse trigger structures and injection locations. This paper introduces Federated Graph Backdoor Defense using Topological Graph Energy (FedTGE), a novel technique that utilizes energy assignments and clustering at the client level and energy graph construction at the server level to effectively mitigate backdoor attacks without needing a validation dataset, even in heterogeneous and high-malicious-proportion environments.

Multi-Reward as Condition for Instruction-based Image Editing
This paper addresses the issue of low-quality training data in instruction-based image editing by introducing a multi-perspective reward system using a quantitative metric based on GPT-4o, evaluating editing outcomes on instruction following, detail preserving, and generation quality. The proposed framework, which leverages this reward data to enhance editing models, outperforms existing methods, as demonstrated through a new benchmark, Real-Edit, and the experiments on InsPix2Pix and SmartEdit pipelines.

SimpleTM: A Simple Baseline for Multivariate Time Series Forecasting
The paper introduces SimpleTM, a lightweight model designed specifically for multivariate time-series forecasting, utilizing textbook signal processing ideas and basic geometric algebra operations. Despite its simplicity, SimpleTM demonstrates competitive performance compared to larger Transformer-based models across several benchmarks, highlighting its efficiency and effectiveness in handling diverse temporal data with minimal domain-specific fine-tuning.

Long-Sequence Recommendation Models Need Decoupled Embeddings
The paper introduces the Decoupled Attention and Representation Embeddings (DARE) model, addressing the challenge of interference between attention and representation learning in long-sequence recommendation systems. By employing separate embedding tables for attention and representation, DARE significantly improves prediction accuracy and efficiency, demonstrating up to 9‰ AUC gains and a 50% reduction in search time on various datasets, including Tencent's platform.

Q-SFT: Q-Learning for Language Models via Supervised Fine-Tuning
The paper proposes a novel offline reinforcement learning algorithm that overcomes the challenges of scaling value-based RL methods to large language models by casting Q-learning as a modified supervised fine-tuning (SFT) problem. This approach allows for a seamless transition from pretraining to finetuning, leveraging the advantages of pretrained language and vision-language models without reinitializing weights, and demonstrating its effectiveness on tasks in natural language dialogue and robotic manipulation.

CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs
We introduce CodeMMLU, a comprehensive multiple-choice benchmark with nearly 20,000 questions designed to evaluate the depth of software and code comprehension in Code Large Language Models (CodeLLMs), addressing tasks like code analysis, defect detection, and execution reasoning across multiple languages. Our findings show that even advanced models face challenges with CodeMMLU, pointing to significant gaps in comprehension that highlight the need for better AI-assisted coding tools.

Distance-Based Tree-Sliced Wasserstein Distance
This paper introduces a novel class of splitting maps leading to the Distance-based Tree-Sliced Wasserstein (Db-TSW) distance, which generalizes the Tree-Sliced Wasserstein distance by incorporating all positional information from input measures, ensuring Euclidean invariance and enhanced topological capture. Through a new tree sampling process and efficient GPU implementation, Db-TSW improves accuracy over existing Sliced Wasserstein variants while retaining computational efficiency, as demonstrated in experiments on gradient flows, image style transfer, and generative models.

ChroKnowledge: Unveiling Chronological Knowledge of Language Models in Multiple Domains
Large language models (LLMs) face challenges in maintaining and assessing chronological knowledge due to their reliance on a fixed time-point perspective. To address this, the paper introduces ChroKnowBench, a benchmark dataset, and ChroKnowledge, a sampling-based framework, which together evaluate LLMs' ability to recall temporal knowledge across various domains, highlighting differences in knowledge recall and enhancing elicitation through the ChroKnowPrompt technique.

AutoDAN-Turbo: A Lifelong Agent for Strategy Self-Exploration to Jailbreak LLMs
AutoDAN-Turbo is a black-box jailbreak method that automatically discovers jailbreak strategies for red-teaming without human intervention, significantly outperforming baseline methods with a 74.3% higher average attack success rate on public benchmarks. It achieves an 88.5% success rate on GPT-4-1106-turbo, rising to 93.4% when integrating existing human-designed strategies, highlighting its efficacy and adaptability.

Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset
The paper introduces the Facial Identity Unlearning Benchmark (FIUBench), a novel benchmark designed to evaluate the efficacy of unlearning algorithms in Vision Language Models (VLMs) amidst privacy concerns. Through constructing the Fictitious Facial Identity VQA dataset and employing a comprehensive evaluation pipeline, the study reveals significant limitations and trade-offs in current unlearning methods, emphasizing the importance of privacy attacks in robust evaluations and aiming to advance the development of more effective VLM unlearning techniques.

On the Byzantine-Resilience of Distillation-Based Federated Learning
This paper examines the resilience of Knowledge Distillation (KD) based Federated Learning (FL) algorithms in adversarial byzantine settings. The authors introduce new byzantine attacks that compromise existing resilient methods and propose a novel defense technique to bolster the byzantine robustness of KD-based FL algorithms while also offering a framework to better conceal such attacks.

Adversarial Attacks on Data Attribution
This paper addresses the adversarial robustness of data attribution methods, which are used to value AI training data and compensate providers, by proposing two attack methods: Shadow Attack and Outlier Attack. These methods manipulate datasets to adversarially inflate compensation, with the Shadow Attack utilizing shadow training for knowledge-based perturbations and the Outlier Attack using black-box queries to exploit inductive biases in data attribution, achieving substantial inflation in compensation in empirical tests.

Zeroth-Order Policy Gradient for Reinforcement Learning from Human Feedback without Reward Inference
This paper introduces two novel RLHF algorithms that bypass the reward inference step, thus addressing challenges like distribution shift and model overfitting in fine-tuning LLMs. By estimating local value function differences from human preferences and using a zeroth-order gradient approximator, these algorithms achieve polynomial convergence and outperform existing baselines in stochastic environments, demonstrating efficient solutions for general RLHF problems.

Sufficient Context: A New Lens on Retrieval Augmented Generation Systems
The paper investigates whether errors in Retrieval Augmented Generation (RAG) systems are due to large language models (LLMs) failing to utilize context or the context itself being insufficient. By developing a concept of sufficient context and a method to classify information adequacy, the study reveals that larger models often give incorrect answers when context is insufficient, while smaller models frequently hallucinate; it also proposes a selective generation method that enhances accuracy by promoting abstention when appropriate, improving the correctness of responses by 2-10% for various models.

Differentiable Rule Induction from Raw Sequence Inputs
This paper addresses the challenge of explicit label leakage in differentiable inductive logic programming (ILP) by integrating a self-supervised differentiable clustering model. This novel approach enables effective rule learning from raw data, enhancing interpretability and scalability when applied to time series and image data without requiring explicit supervision of input feature labels.

SEMDICE: Off-policy State Entropy Maximization via Stationary Distribution Correction Estimation
This paper introduces SEMDICE, an off-policy algorithm designed for unsupervised reinforcement learning that maximizes the entropy of a state's stationary distribution without relying on task-specific rewards. SEMDICE effectively computes a markovian state-entropy-maximizing policy from off-policy datasets and outperforms existing methods in state entropy maximization and adaptation efficiency in downstream tasks.

Mitigating Parameter Interference in Model Merging via Sharpness-Aware Fine-Tuning
This paper discusses improving the performance of merged multi-task models by addressing parameter interference issues through a novel fine-tuning objective function similar to sharpness-aware minimization (SAM). The proposed method demonstrates superior performance and compatibility with existing merging techniques, and the code is made publicly accessible.

Unlocking Point Processes through Point Set Diffusion
This paper introduces Point Set Diffusion, a diffusion-based latent variable model that can represent arbitrary point processes on metric spaces without relying on the intensity function. The model effectively captures the distribution of point processes, allowing for efficient and flexible generation, achieving state-of-the-art performance and significantly faster sampling in both synthetic and real-world scenarios.

Fine-Grained Verifiers: Preference Modeling as Next-token Prediction in Vision-Language Alignment
This paper introduces FiSAO (Fine-Grained Self-Alignment Optimization), a novel method that improves vision-language alignment in vision-language large models (VLLMs) by using the model’s own visual encoder for token-level feedback, eliminating the need for additional data. FiSAO surpasses traditional methods by effectively addressing misalignment and demonstrating successful application of token-level rewards, as supported by theoretical analysis and experimental validation.

Physics-aligned field reconstruction with diffusion bridge
The paper introduces the Physics-aligned Schrödinger Bridge (PalSB), a novel framework for reconstructing physical fields from sparse measurements that adheres to governing equations and boundary conditions by leveraging a diffusion bridge mechanism. Demonstrated on complex systems like cylinder flow, two-dimensional turbulence, and reaction-diffusion systems, PalSB shows higher accuracy and better compliance with physical constraints compared to existing methods, advancing the field reconstruction techniques.

To Trust or Not to Trust? Enhancing Large Language Models' Situated Faithfulness to External Contexts
The paper addresses the challenge of enhancing large language models' (LLMs) ability to handle conflicts between internal knowledge and external information, proposing the concept of situated faithfulness. To improve this, the authors introduce Self-Guided Confidence Reasoning (SCR) and Rule-Based Confidence Reasoning (RCR), demonstrating that SCR is particularly effective with models like GPT-4o, while RCR benefits smaller models such as Llama-3-8B; fine-tuning with the Confidence Reasoning Direct Preference Optimization (CR-DPO) further enhances model performance.

No Preference Left Behind: Group Distributional Preference Optimization
This paper introduces Group Distribution Preference Optimization (GDPO), a framework designed to better align language models with the diverse distribution of preferences within a group by considering the belief systems that underpin individual preferences. The proposed method demonstrates superior performance over traditional Direct Preference Optimization (DPO) by consistently reducing alignment gaps in both synthetic and real-world datasets, thereby advancing pluralistic preference alignment in models.

Towards Synergistic Path-based Explanations for Knowledge Graph Completion: Exploration and Evaluation
The paper introduces KGExplainer, a novel explainable framework for knowledge graph completion that identifies crucial synergistic paths using a perturbation-based greedy search algorithm. The approach improves explainability with a human evaluation accuracy of 83.3%, demonstrating comparable predictive performance to target knowledge graph embedding models and showing effectiveness on benchmark datasets.

LongVILA: Scaling Long-Context Visual Language Models for Long Videos
LongVILA presents a comprehensive solution for long-context visual-language models by enhancing VLMs for long video understanding and introducing the efficient MM-SP system for scalable training on extensive video data. It extends the context capability of VILA from 8 to 2048 frames, achieving remarkable accuracy, while significantly outperforming prior parallelism methods in computational efficiency and integrating with existing frameworks like Hugging Face Transformers.

Learning Task Belief Similarity with Latent Dynamics for Meta-Reinforcement Learning
SimBelief is a novel meta-reinforcement learning framework that enhances task identification and exploration efficiency in sparse reward environments by measuring the similarity of task beliefs in Bayes-Adaptive MDPs. By introducing a latent task belief metric, SimBelief learns the common structure across task distributions, significantly improving performance over state-of-the-art methods in complex tasks such as those found in MuJoCo and panda-gym.

Sharpness-Aware Black-Box Optimization
The paper introduces the Sharpness-Aware Black-box Optimization (SABO) algorithm, designed to enhance model generalization in black-box optimization tasks such as reinforcement learning and prompt fine-tuning. By leveraging a sharpness-aware minimization strategy with Gaussian reparameterization, SABO demonstrates improved convergence and generalization both theoretically and empirically, as evidenced by experiments on prompt fine-tuning tasks.

ToolGen: Unified Tool Retrieval and Calling via Generation
ToolGen revolutionizes the way large language models interact with external tools by embedding tool knowledge directly into the model's parameters, eliminating the need for separate tool retrieval. This approach not only enhances performance and scalability with over 47,000 tools but also facilitates seamless integration with advanced techniques, paving the way for more versatile and autonomous AI systems.

Advantage-Guided Distillation for Preference Alignment in Small Language Models
This paper addresses the challenge of aligning Small Language Models (SLMs) with human preferences by using a well-aligned teacher Large Language Model (LLM) to guide the process. It introduces two methods—Dual-Constrained Knowledge Distillation (DCKD) and Advantage-Guided Distillation for Preference Alignment (ADPA)—to enhance alignment, with experimental results showing that these approaches significantly improve SLM alignment and close the performance gap with larger models, especially when combined.

Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model
Transfusion is a novel training approach for a multi-modal model that effectively combines language modeling and diffusion techniques for handling both discrete and continuous data types. The approach significantly enhances scalability and performance on various benchmarks, particularly when scaled to 7B parameters, by incorporating modality-specific encoding and decoding layers, thereby effectively generating quality images and text.

ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding
ChartMoE introduces a Mixture of Expert (MoE) architecture to enhance the performance of Multimodal Large Language Models (MLLMs) in automatic chart understanding by bridging the modality gap using expertly aligned tasks. The proposed method, validated by the newly introduced ChartMoE-Align dataset, improves the accuracy of chart data analysis, outperforming previous benchmarks with a significant increase from 80.48% to 84.64% on the ChartQA benchmark.

Selective Attention Improves Transformer
This paper introduces Selective Attention, a simple, parameter-free modification to the standard attention mechanism that enhances language modeling and downstream task performance by reducing focus on unneeded elements. It demonstrates that Selective Attention allows for substantial reductions in memory and compute requirements, achieving performance on par with traditional setups using significantly fewer resources.

OmniCorpus: A Unified Multimodal Corpus of 10 Billion-Level Images Interleaved with Text
OmniCorpus is a new 10 billion-scale image-text interleaved dataset designed to support multimodal large language models by addressing the limitations in the scale and diversity of existing datasets. This dataset offers significantly larger scale and more diverse sources than its counterparts, and is validated for quality and effectiveness, providing a robust foundation for future multimodal model research.

How Far Are We from True Unlearnability?
This paper investigates the effectiveness of unlearnable examples (UEs) in protecting data from unauthorized model training, finding that these examples can still perform unexpectedly well in multi-task settings like semantic segmentation. By analyzing model optimization and loss landscapes, the authors introduce Sharpness-Aware Learnability (SAL) to gauge unlearnability and propose an Unlearnable Distance (UD) metric, aiming to better understand and evaluate the limitations of current unlearnable methods.

How Learnable Grids Recover Fine Detail in Low Dimensions: A Neural Tangent Kernel Analysis of Multigrid Parametric Encodings
This paper analyzes and compares Fourier feature encodings (FFE) and multigrid parametric encodings (MPE) for neural networks mapping low-dimensional spaces, revealing that MPEs outperform FFEs by capturing higher resolution and finer details without aliasing. Using neural tangent kernel analysis and empirical validation on image and 3D surface regression tasks, it demonstrates that MPEs significantly improve performance, evidenced by substantial increases in peak signal-to-noise ratio (PSNR) and multiscale structural similarity (MS-SSIM).

Morphing Tokens Draw Strong Masked Image Models
The paper introduces Dynamic Token Morphing (DTM), a novel method for improving masked image modeling (MIM) by addressing spatial inconsistency in supervisory signals. DTM dynamically aggregates tokens to produce more spatially consistent targets, enhancing representation learning and achieving superior results on ImageNet-1K, ADE20K, and transfer learning tasks, without significantly increasing training costs.

Model-Agnostic Knowledge Guided Correction for Improved Neural Surrogate Rollout
The paper presents the Hybrid PDE Predictor with Reinforcement Learning (HyPER) model, a novel approach that synergizes neural network surrogates, reinforcement learning, and physics simulators to significantly reduce surrogate rollout errors in modeling physical systems governed by partial differential equations. By achieving a reduction in in-distribution rollout error by 47%-78%, HyPER offers a scalable and adaptable solution that effectively addresses the limitations of computational costs and data scarcity in traditional simulations.

Lambda-Skip Connections: the architectural component that prevents Rank Collapse
This paper investigates rank collapse in sequence models, expanding its study from transformers to include State Space Models (SSMs) using a unified framework, and introduces "lambda-skip connections" as a solution to prevent it. The study provides the first general guarantee for preventing rank collapse across these architectures, supported by analytical results and experiments, thus offering significant insights for both researchers and practitioners.

Unified Parameter-Efficient Unlearning for LLMs
The paper introduces LLMEraser, an innovative instance-wise unlearning framework designed to address privacy and security concerns associated with fine-tuning Large Language Models. By employing influence functions for precise parameter adjustments, LLMEraser effectively manages unlearning tasks across a wide range of scenarios without extensive retraining, as evidenced by experiments on benchmark datasets.

GPS: A Probabilistic Distributional Similarity with Gumbel Priors for Set-to-Set Matching
This paper introduces GPS, a novel set-to-set matching similarity measure utilizing Gumbel prior distributions to overcome limitations like suboptimal accuracy and high computational costs found in traditional metrics such as Chamfer Distance and Earth Mover’s Distance. Demonstrating significant performance improvements in tasks like few-shot image classification and 3D point cloud completion, GPS offers an effective and efficient alternative, with validation across various benchmark datasets, and is available publicly for further research.

SysBench: Can LLMs Follow System Message?
This paper introduces SysBench, a benchmark designed to evaluate the ability of Large Language Models (LLMs) to follow system messages, addressing current limitations like constraint violation, instruction misjudgment, and multi-turn instability. By constructing a diverse evaluation dataset and protocol, SysBench provides comprehensive insights into LLM performance, highlighting strengths and areas for improvement, thereby guiding future research.

Tool-Planner: Task Planning with Clusters across Multiple Tools
Large language models (LLMs) excel in reasoning and have been applied to tool learning, where they utilize examples of tool usage to enhance task-solving abilities. The proposed Tool-Planner framework addresses challenges in tool learning by grouping tools into toolkits based on API functions, allowing LLMs to efficiently plan and adjust tools, resulting in improved performance shown in experiments with models like GPT-4 and Claude 3.

LR0.FM: LOW-RESOLUTION ZERO-SHOT CLASSIFICATION BENCHMARK FOR FOUNDATION MODELS
This paper presents LR0.FM, a benchmark for assessing the robustness of visual-language foundation models on low-resolution images, revealing that model size and pre-training dataset quality significantly influence performance against resolution degradation. It introduces the Weighted Aggregated Robustness metric and a strategy called LR-TK0 to enhance robustness without altering pre-trained weights, demonstrating improvements across various models and datasets.

Multi-Resolution Decomposable Diffusion Model for Non-Stationary Time Series Anomaly Detection
This paper introduces the Multi-Resolution Decomposable Diffusion Model (MODEM) that enhances anomaly detection in non-stationary time series by using a multi-resolution approach to distinguish anomalies from non-stationary behaviors. By employing a coarse-to-fine diffusion paradigm and a frequency-enhanced decomposable network, MODEM effectively captures complex temporal patterns, demonstrating state-of-the-art performance across five real-world datasets.

$\gamma-$MoD: Exploring Mixture-of-Depth Adaptation for Multimodal Large Language Models
This paper introduces a novel adaptation strategy, $\gamma$-MoD, to improve the computational efficiency of multimodal large language models (MLLMs). By employing a metric called the rank of attention maps (ARank) and using shared vision-language routing and masked routing learning, the method reduces the computational load by converting dense layers to mixture of depths (MoDs), achieving significant time savings with minimal performance loss.

$\phi$-Update: A Class of Policy Update Methods with Policy Convergence Guarantee
This paper introduces the $\phi$-update, a policy update mechanism inspired by softmax natural policy gradient and Hadamard policy gradient, which ensures global asymptotic state value convergence under mild conditions. It demonstrates that $\phi$-update policies, including softmax natural policy gradient, converge even with multiple optimal policies, and establishes the exact asymptotic convergence rate and global linear convergence without requiring explicit regularizations.

Decentralized Optimization with Coupled Constraints
This paper addresses the decentralized minimization of a separable objective with variables coupled through an affine constraint, relevant to resource allocation and distributed machine learning. The authors propose lower complexity bounds and introduce a linearly convergent first-order algorithm that meets these bounds, marking a novel approach for decentralized optimization problems with general affine coupled constraints.

I Can Hear You: Selective Robust Training for Deepfake Audio Detection
This paper addresses the challenge of detecting deepfake audio by introducing DeepFakeVox-HQ, the largest public voice dataset, and demonstrating the shortcomings of current detectors with this diverse dataset. The authors propose the F-SAT method, enhancing model robustness through frequency-selective adversarial training, resulting in improved detection accuracy on both clean and corrupted samples.

SOREL: A Stochastic Algorithm for Spectral Risks Minimization
This paper introduces SOREL, the first stochastic gradient-based algorithm with convergence guarantees for minimizing spectral risks, which is crucial for decision-making scenarios where performance between average and worst-case scenarios is desired. The algorithm achieves a near-optimal rate and demonstrates superior performance compared to existing methods in terms of runtime and sample complexity in experiments on real datasets.

Logical Consistency of Large Language Models in Fact-Checking
This paper addresses the logical inconsistency of large language models (LLMs) when dealing with complex queries involving logical operators like negation, conjunction, and disjunction, particularly in a fact-checking context with knowledge graphs. The study introduces three new logical fact-checking datasets, proposes measures to assess LLMs' consistency on these queries, demonstrates current LLMs' deficiencies, and uses supervised fine-tuning to improve their logical consistency, with both the source code and benchmarks made publicly available.

Incorporating Visual Correspondence into Diffusion Model for Virtual Try-On
This paper addresses the challenge of preserving garment details in virtual try-on (VTON) tasks by proposing a novel method that utilizes visual correspondence as a prior to guide the diffusion process. By interpreting fine-grained appearance and textures as structured semantic points and matching these to the target through local flow warping, the approach significantly enhances detail preservation, achieving state-of-the-art performance on VITON-HD and DressCode datasets.

MMDisCo: Multi-Modal Discriminator-Guided Cooperative Diffusion for Joint Audio and Video Generation
This study introduces a novel method for creating an audio-video generative model with low computational cost by using pre-trained single-modal generative models for audio and video. By employing a lightweight joint guidance module trained through the optimal discriminator’s gradient, the method enhances single-modal fidelity and multimodal alignment, as evidenced by empirical evaluations on benchmark datasets.

GOLD: Graph Out-of-Distribution Detection via Implicit Adversarial Latent Generation
Graph neural networks (GNNs) face significant challenges with out-of-distribution (OOD) test instances due to the difficulty of obtaining additional OOD node-sets. The proposed GOLD framework addresses this by utilizing implicit adversarial learning to generate synthetic OOD data without relying on pre-trained models, demonstrating superior OOD detection performance on benchmark datasets compared to existing methods.

From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions
This paper introduces DRAFT, a framework for dynamically refining tool documentation to improve Large Language Models' (LLMs) interactions with external tools by analyzing feedback from their trial-and-error experiences. By enhancing documentation quality through iterative refinement, DRAFT significantly boosts LLMs' tool comprehension and utilization, while demonstrating robust cross-model generalization capabilities.

Machine Unlearning via Simulated Oracle Matching
This paper introduces a new machine unlearning technique called Datamodel Matching (DMM), which efficiently removes specified training data effects even in non-convex settings. By leveraging a data attribution framework, DMM predicts model outputs without the forget set and fine-tunes the model accordingly, empirically outperforming existing methods and benefiting from advances in data attribution for future improvement.

How Does Critical Batch Size Scale in Pre-training?
This paper proposes a measure of critical batch size (CBS) to optimize large-scale model training and investigates its scaling behavior through extensive experiments on auto-regressive language models. The study finds that CBS scales mainly with data size rather than model size, supported by theoretical analysis, and emphasizes the significance of hyper-parameter tuning for efficient large-scale pre-training.

Enabling Realtime Reinforcement Learning at Scale with Staggered Asynchronous Inference
This paper analyzes the challenges of minimizing regret in realtime reinforcement learning (RL) environments, highlighting the limitations of traditional sequential interaction and learning paradigms. It introduces novel algorithms for asynchronous inference, enabling the use of large neural network models by ensuring consistent action intervals, with the effectiveness demonstrated through simulations of Game Boy games, resulting in improved handling of longer inference times.

Language Models Are Implicitly Continuous
This paper demonstrates that Transformer-based language models inherently represent sentences as continuous-time functions, differing fundamentally from human language processing. By extending Transformers to account for spatial and temporal continuity, the study challenges traditional views of language understanding in LLMs, with significant linguistic and engineering implications.

ALBAR: Adversarial Learning approach to mitigate Biases in Action Recognition
This paper addresses the underexplored issue of bias in action recognition models, particularly focusing on background and foreground biases that can affect real-life applications. The authors introduce ALBAR, an innovative adversarial training method that effectively mitigates these biases without specialized bias attribute knowledge, achieving new state-of-the-art results in debiasing performance and proposing improvements for existing evaluation protocols.

AdaIR: Adaptive All-in-One Image Restoration via Frequency Mining and Modulation
This paper presents AdaIR, an adaptive all-in-one image restoration network that utilizes frequency mining and modulation to address various image degradations such as noise, blur, haze, and rain, without prior knowledge of the degradation type. Unlike existing methods that operate solely in the spatial domain, AdaIR achieves state-of-the-art performance by identifying and enhancing frequency subbands uniquely affected by each type of degradation, thereby providing comprehensive solutions for multiple restoration tasks.

Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse
This paper introduces Trust-Score, a metric designed to evaluate the trustworthiness of large language models (LLMs) in retrieval-augmented generation (RAG) systems, highlighting shortcomings in current adaptation methods. The proposed Trust-Align method significantly improves Trust-Score performance, outperforming competitive baselines across several models and datasets, and enhances model abilities such as refusal accuracy and citation quality.

Sparse Autoencoders Reveal Temporal Difference Learning in Large Language Models
This paper demonstrates that Llama 3 70B, a large language model, can solve simple reinforcement learning problems in-context and explores the mechanistic understanding of this capability using Sparse Autoencoders (SAEs). By identifying and intervening in model representations that align with temporal difference errors and Q-values, the study provides a methodology for investigating and manipulating in-context learning, enhancing our understanding of how language models adapt and learn.

FedLWS: Federated Learning with Adaptive Layer-wise Weight Shrinking
This paper introduces Federated Learning with Adaptive Layer-wise Weight Shrinking (FedLWS), a novel strategy for model aggregation in Federated Learning that adaptively designs shrinking factors for each layer of the global model to improve generalization. By avoiding the use of proxy datasets and enhancing flexibility, FedLWS outperforms existing methods in various scenarios, making it a promising tool for further advancements in Federated Learning.

DRL: Decomposed Representation Learning for Tabular Anomaly Detection
This paper addresses the challenge of Tabular Anomaly Detection (TAD) by introducing a novel approach called Decomposed Representation Learning (DRL) to effectively distinguish between normal and anomalous patterns in tabular data. The proposed method improves the detection of anomalies by mapping data into a constrained latent space that enhances discriminative capabilities, demonstrating state-of-the-art performance across 40 datasets and outperforming 16 other algorithms.

Fine-Tuning Discrete Diffusion Models via Reward Optimization with Applications to DNA and Protein Design
This paper addresses the challenge of optimizing specific task objectives in discrete diffusion models by introducing a novel algorithm that applies reinforcement learning principles for reward maximization while preserving sequence naturalness. The proposed method effectively generates DNA and protein sequences that enhance targeted activities, such as protein stability, demonstrating its potential utility in gene therapies and protein-based therapeutics, with supporting code available online.

InversionGNN: A Dual Path Network for Multi-Property Molecular Optimization
The paper introduces InversionGNN, a dual-path graph neural network framework designed to efficiently explore chemical space for multi-objective drug discovery by learning and balancing multiple properties of molecules. The framework employs a novel gradient-based Pareto search method to handle conflicting properties, proving to be effective and sample-efficient in generating Pareto optimal molecules across various drug discovery scenarios.

DICE: Data Influence Cascade in Decentralized Learning
This paper introduces DICE, the first method to estimate Data Influence Cascade in decentralized learning environments, addressing the challenge of fair incentive mechanisms by approximating influence cascade across interconnected nodes. The framework aids in selecting collaborators and identifying malicious actions, with significant implications for improving participation in decentralized networks.

Diffusion$^2$: Dynamic 3D Content Generation via Score Composition of Video and Multi-view Diffusion Models
Diffusion$^2$ is a framework designed for dynamic 3D content creation by integrating geometric consistency and temporal smoothness from pretrained video and multi-view diffusion models. This method efficiently generates 4D content in minutes without relying on expensive 4D data, demonstrating high-quality and consistent 4D asset production through extensive experiments.

Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms
Ferret-UI 2 is a multimodal large language model designed to enhance universal user interface (UI) understanding across diverse platforms, including iPhone, Android, iPad, Webpage, and AppleTV. The model introduces innovations such as multi-platform support, high-resolution perception, and advanced task training powered by GPT-4o, demonstrating superior performance and cross-platform adaptability over its predecessor, as evidenced by extensive experiments on various benchmarks.

GEVRM: Goal-Expressive Video Generation Model For Robust Visual Manipulation
The paper introduces GEVRM, a novel closed-loop vision-language-action (VLA) method that incorporates the internal model control (IMC) principle to enhance the robustness of robot visual manipulation against external perturbations. By utilizing a text-guided video generation model and prototype contrastive learning to manage and infer environmental disturbances, GEVRM achieves state-of-the-art performance on both standard and perturbed CALVIN benchmarks, demonstrating significant improvements in realistic robot tasks.

Mixture Compressor for Mixture-of-Experts LLMs Gains More
This paper addresses the memory and redundancy challenges of Mixture-of-Experts large language models (MoE-LLMs) by introducing Mixture-Compressor (MC), a training-free method that achieves significant compression while minimizing accuracy loss. MC uses Pre-Loading Mixed-Precision Quantization and Online Dynamic Pruning to effectively reduce the model size and enhance performance, outperforming larger dense models with significantly smaller parameters.

Reflective Gaussian Splatting
The paper introduces a Reflective Gaussian splatting (Ref-Gaussian) framework for novel view synthesis, which offers a real-time, high-quality solution for rendering reflective objects by using a physically based deferred rendering and Gaussian-grounded inter-reflection. Ref-Gaussian demonstrates superior performance over existing methods in terms of metrics, visual quality, and efficiency, and provides a versatile approach applicable to both reflective and non-reflective scenes, supporting applications like relighting and editing.

Periodic Materials Generation using Text-Guided Joint Diffusion Model
TGDMat is a novel text-guided diffusion model developed to generate 3D periodic materials by leveraging textual descriptions to integrate global structural knowledge during the generation of atom coordinates, types, and lattice structures. The model demonstrates superior performance over existing methods in both structure prediction and generation tasks, effectively utilizing real-world textual prompts to enhance generative performance while reducing computational overhead.

Structure Language Models for Protein Conformation Generation
This paper introduces Structure Language Modeling (SLM) as a novel and efficient framework for generating protein conformations by encoding protein structures into a latent space using a discrete variational auto-encoder, and applying conditional language modeling to capture conformation distributions. SLM outperforms existing methods by providing a 20-100x speedup in generating diverse conformations, demonstrating its potential to enhance drug discovery and protein studies.

A Benchmark for Semantic Sensitive Information in LLMs Outputs
This paper identifies a novel safety concern in large language models (LLMs), highlighting that they can output not just structured sensitive information but also semantic sensitive information (SemSI) in response to simple natural questions. The authors introduce SemSI-Set, a labeled dataset, and SemSI-Bench, a benchmark for evaluating 25 state-of-the-art LLMs, revealing the widespread presence of SemSI in their outputs and making their findings available via an open-source platform.

Expressivity of Neural Networks with Random Weights and Learned Biases
This paper addresses the open question of whether universal function approximation can be achieved with neural networks that only learn biases, finding that feedforward networks with fixed random weights can approximate any continuous function on compact sets. Additionally, it demonstrates a similar potential for recurrent networks to approximate dynamical systems, offering insights relevant to both neuroscience and AI, particularly in the context of behavioral dynamics and fine-tuning large language models.

CAKE: Cascading and Adaptive KV Cache Eviction with Layer Preferences
This paper introduces Cascading and Adaptive KV cache Eviction (CAKE), a novel approach to key-value cache management in large language models, treating the process as a "cake-slicing problem" to optimize resource allocation across different layers. CAKE significantly improves performance by dynamically allocating cache resources based on spatial and temporal attention patterns, achieving substantial reductions in memory use and latency while maintaining model performance, particularly in low-memory environments.

Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity
The paper explores the expressive limitations of higher-order message-passing (HOMP) in topological deep learning, particularly its inability to capture certain topological and metric invariants. It introduces two new architecture classes, multi-cellular networks (MCN) and scalable MCN (SMCN), which address these limitations and improve expressivity, as demonstrated through new benchmarks and evaluations on real-world datasets.

DiffusionGuard: A Robust Defense Against Malicious Diffusion-based Image Editing
This paper introduces DiffusionGuard, a defense method to protect against unauthorized edits by diffusion-based image editing models, even in complex situations. The method generates adversarial noise targeting early diffusion stages and employs mask-augmentation to improve robustness, demonstrating stronger protection and efficiency over existing baselines in preventing privacy threats.

DyCAST: Learning Dynamic Causal Structure from Time Series
DyCAST is a novel framework developed to learn dynamic causal structures in time series by employing Neural Ordinary Differential Equations to model the temporal dynamics of contemporaneous structures. This approach overcomes the limitations of static assumptions and demonstrates superior or comparable performance to existing models in both synthetic and real-world datasets.

Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature Compensator
Dataset distillation aims to condense large datasets into a compact form, but its effectiveness is limited by class-specific synthesis methods which inefficiently utilize the distillation budget and overlook inter-class feature distributions. This paper introduces the Inter-class Feature Compensator (INFER), which uses a Universal Feature Compensator to enhance feature integration across classes, optimizing synthetic data, and significantly improving distillation efficiency and effectiveness, achieving state-of-the-art performance on benchmark datasets.

Fengbo: a Clifford Neural Operator pipeline for 3D PDEs in Computational Fluid Dynamics
Fengbo is a novel pipeline leveraging Clifford Algebra to solve 3D partial differential equations for computational fluid dynamics, using an architecture composed of 3D convolutional and Fourier Neural Operator layers. It achieves competitive accuracy with reduced computational complexity by providing a direct and interpretable mapping from geometry to physics, outperforming other models on the ShapeNet Car dataset with superior visualization of physical quantities in 3D space.

Towards Hierarchical Rectified Flow
We introduce a hierarchical rectified flow that leverages multiple coupled ordinary differential equations to generate data distributions from a known source, modeling multi-modal random fields more comprehensively than classic rectified flows. This enables intersecting integration paths, resulting in straighter trajectories and fewer neural function evaluations, as validated on various datasets including MNIST and CIFAR-10.

Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge
The paper identifies 12 key potential biases in the LLM-as-a-Judge evaluation method and introduces CALM, an automated framework for systematically quantifying these biases. Through experiments on various language models, the study reveals significant persisting biases in specific tasks, underscoring the need for caution and further improvement in the reliability of LLM-as-a-Judge applications.

Rethinking Graph Neural Networks From A Geometric Perspective Of Node Features
This paper proposes a feature-centric approach to understanding graph neural networks (GNNs) by focusing on the features of node label classes and their centroids to form a feature centroid simplex. By analyzing the geometric properties of this simplex, such as in comparison to regular and degenerate simplexes, the authors provide insights into graph-based feature aggregation and offer simple tricks for improving node classification tasks.

REEF: Representation Encoding Fingerprints for Large Language Models
This paper introduces REEF, a training-free method to detect if a suspect model has been developed from a victim open-source Large Language Model by comparing their feature representations using centered kernel alignment similarity. REEF effectively safeguards LLMs' intellectual property without affecting the model's general capabilities and is robust against various modifications, offering a straightforward solution for model owners and third parties.

Knowledge Graph Finetuning Enhances Knowledge Manipulation in Large Language Models
This paper introduces Knowledge Graph-Driven Supervised Fine-Tuning (KG-SFT), a framework designed to enhance the capability of large language models (LLMs) in specific low-data and knowledge-intensive domains by generating high-quality explanations for Q&A pairs using structured knowledge graphs. Through its components—Extractor, Generator, and Detector—KG-SFT significantly improves the accuracy of domain-specific LLMs, achieving notable performance gains of up to 18% across various domains and languages.

Enhancing Vision-Language Model with Unmasked Token Alignment
This paper presents Unmasked Token Alignment (UTA), a method that leverages existing CLIP models to enhance vision-language representations by training a Vision Transformer (ViT) to align unmasked visual tokens with image tokens from a frozen CLIP vision encoder. UTA is more training-efficient without the use of extra [MASK] tokens and demonstrates superior performance over Masked Image Modeling (MIM) methods across various uni- and multi-modal benchmarks.

GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting
GS-LiDAR is introduced as a novel framework for LiDAR novel view synthesis, effectively generating realistic point clouds for autonomous driving without the computational costs associated with neural radiance fields (NeRF). It utilizes 2D Gaussian primitives with periodic vibration and introduces a panoramic rendering technique, significantly enhancing realism and efficiency in both static and dynamic scenarios, as demonstrated on KITTI-360 and nuScenes datasets.

LLMOPT: Learning to Define and Solve General Optimization Problems from Scratch
LLMOPT is a unified learning-based framework that enhances the generalization of optimization tasks formulated through natural language using large language models. By employing a five-element formulation, multi-instruction tuning, and incorporating a model alignment and self-correction mechanism, LLMOPT significantly improves solving accuracy and problem diversity modelling in real-world datasets, outperforming current state-of-the-art methods by an average of 11.08%.

Closed-Form Merging of Parameter-Efficient Modules for Federated Continual Learning
This paper introduces LoRM, an alternating optimization strategy that enhances model merging by training one LoRA matrix at a time, leading to a unique solution while preserving model capabilities. Applied to Federated Class-Incremental Learning (FCIL), LoRM ensures alignment of model responses across clients and tasks, achieving state-of-the-art performance, with code available at github.com/aimagelab/fed-mammoth.

A Training-Free Sub-quadratic Cost Transformer Model Serving Framework with Hierarchically Pruned Attention
This paper introduces Hierarchically Pruned Attention (HiP), a novel approach that significantly reduces the time and space complexities of the attention mechanism in large language models, making them scalable to millions of tokens. HiP achieves this by utilizing an attention locality pattern and a tree-search-like algorithm, alongside GPU memory optimization, enabling efficient long-context language generation without retraining, thus facilitating previously impractical applications on commodity GPUs.

Towards Self-Supervised Covariance Estimation in Deep Heteroscedastic Regression
This paper investigates self-supervised covariance estimation in deep heteroscedastic regression, tackling the challenges of heteroscedasticity without needing explicit covariance labels. By analyzing the KL Divergence and 2-Wasserstein distance for supervision and proposing a neighborhood-based heuristic for generating pseudo labels, the study offers a computationally efficient method that maintains accuracy across various datasets.

Image Watermarks are Removable using Controllable Regeneration from Clean Noise
This paper presents a novel watermark removal approach that effectively nullifies state-of-the-art watermarking techniques by regenerating the watermarked image from clean Gaussian noise using a controllable diffusion model. The proposed method achieves a balance between watermark removal performance and image consistency by implementing an adjustable regeneration scheme, demonstrating superior visual quality and enhanced performance compared to existing approaches.

MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra
This paper introduces MolSpectra, a method to enhance the pre-training of 3D molecular representations by incorporating quantum mechanical insights from energy spectra. Using SpecFormer, a multi-spectrum encoder, the approach aligns 3D and spectrum encoders through a contrastive objective, resulting in superior performance in molecular property prediction and dynamics modeling compared to existing methods.

Rethinking Graph Prompts: Unraveling the Power of Data Manipulation in Graph Neural Networks
The paper introduces graph prompts as a novel approach to transform graph data for better alignment with pre-trained models, addressing issues like distribution shifts and adversarial vulnerabilities inherent in Graph Neural Networks (GNNs). By rewriting graph structures with prompt tokens, graph prompts enhance adaptability and flexibility in applications such as IoT and drug discovery, despite ongoing challenges in design and optimization.

See It from My Perspective: How Language Affects Cultural Bias in Image Understanding
This paper explores the Western bias in vision-language models (VLMs) and analyzes how language contributes to this disparity in image understanding across different cultures. By evaluating VLMs on culturally diverse images and conducting controlled experiments, the study finds that a lack of language diversity in pre-training contributes to bias, highlighting the need for richer representation of various languages to create more equitable VLMs.

Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows
Spider 2.0 is an evaluation framework designed to address the complexities of real-world enterprise text-to-SQL workflows, featuring 632 problems derived from extensive database applications involving multiple SQL dialects, requiring intricate reasoning over extremely long contexts. The framework demonstrates that current language models, while successful in previous benchmarks, need significant advancements to handle the challenging requirements of real-world enterprise settings, as evidenced by the lower task completion rates compared to prior models like Spider 1.0 and BIRD.

Point-SAM: Promptable 3D Segmentation Model for Point Clouds
The paper introduces Point-SAM, a 3D promptable segmentation model for point clouds, which extends the Segment Anything Model (SAM) into the 3D domain using an efficient transformer-based architecture. By distilling knowledge from 2D SAM and employing a data engine for pseudo-label generation, Point-SAM outperforms state-of-the-art 3D segmentation models across various benchmarks, offering applications like interactive 3D annotation and zero-shot 3D instance proposal.

Answer, Assemble, Ace: Understanding How LMs Answer Multiple Choice Questions
This paper investigates how transformer language models perform on formatted multiple-choice question answering (MCQA) tasks, especially when faced with slight variations such as shuffled answer choices. By employing vocabulary projection and activation patching, the study identifies that key hidden states in middle layers, particularly multi-head self-attention mechanisms, are crucial for answer prediction, and reveals that subsequent layers increase the probability of the predicted answer, associating this increase with specific attention heads and demonstrating model adaptation to task formats.

Feature-Based Online Bilateral Trade
This paper examines an online bilateral trade problem where a learner sets prices between a seller and buyer with unknown valuations based on contextual features. The study introduces algorithms with $O(\log T)$ and $\widetilde O(T^{2/3})$ regret for different feedback and budget balance constraints, highlighting a trade-off between feedback quality and budget constraint strictness.

RaSA: Rank-Sharing Low-Rank Adaptation
This paper introduces Rank-Sharing Low-Rank Adaptation (RaSA), an extension of LoRA that increases expressive capacity by sharing ranks across layers without increasing parameter overhead. RaSA is shown to significantly improve performance in complex tasks such as code generation and mathematical reasoning while retaining the benefits of LoRA.

Poisson-Dirac Neural Networks for Modeling Coupled Dynamical Systems across Domains
This paper introduces Poisson-Dirac Neural Networks (PoDiNNs), a novel framework that unifies the port-Hamiltonian and Poisson formulations to address limitations in existing deep learning models for dynamical systems. PoDiNNs enhance the modeling of unknown coupled dynamical systems across various domains, such as electrical and hydraulic systems, by providing a unified representation that improves both accuracy and interpretability.

WorkflowLLM: Enhancing Workflow Orchestration Capability of Large Language Models
The paper introduces WorkflowLLM, a data-centric framework designed to enhance the capability of large language models in workflow orchestration. By constructing a substantial fine-tuning dataset, WorkflowBench, and fine-tuning Llama-3.1-8B, the framework significantly improves orchestrating complex workflows and demonstrates strong generalization abilities, even with previously unseen APIs.

It Helps to Take a Second Opinion: Teaching Smaller LLMs To Deliberate Mutually via Selective Rationale Optimisation
The paper introduces COALITION, a framework to enhance smaller language models by enabling interactions between two variants of the same SLM to generate and refine diverse rationales for improved task performance. By using Selective Rationale Optimization, COALITION achieves up to 5% better results than baselines across various datasets and demonstrates adaptability across different model scales and families.

Learning Fine-Grained Representations through Textual Token Disentanglement in Composed Video Retrieval
The paper introduces FineCVR-1M, a comprehensive dataset for fine-grained composed video retrieval, involving detailed textual descriptions of video content to bridge the modality gap between video and text. To enhance retrieval accuracy, the authors propose a novel textual Feature Disentanglement and Cross-modal Alignment (FDCA) framework, which effectively disentangles and aligns features at multiple levels, demonstrating superior performance on the FineCVR-1M dataset.

RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment
Generative foundation models often suffer from biases due to unsupervised training, leading to unfair and suboptimal outcomes. This paper introduces Reward rAnked FineTuning (RAFT), a framework that enhances model alignment with human ethics by filtering and fine-tuning using high-quality samples, demonstrating improved performance in large language and diffusion models.

G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model
This paper addresses the gap in current research by enabling large language models (LLMs) to solve multi-modal geometric problems involving image input. By creating the Geo170k dataset and introducing the G-LLaVA model, the study significantly improves the performance of LLMs in geometric problem solving, outperforming GPT4-V on the MathVista benchmark with only 7B parameters.

Beyond Autoregression: Discrete Diffusion for Complex Reasoning and Planning
This paper introduces discrete diffusion models as an innovative solution for addressing the limitations of autoregressive language models in complex reasoning and long-term planning. By proposing Multi-Granularity Diffusion Modeling (MGDM), the study demonstrates significant performance improvements on challenging tasks such as Countdown and Sudoku, underscoring the potential of diffusion-based approaches in enhancing AI's language understanding and problem-solving abilities.

Graph Assisted Offline-Online Deep Reinforcement Learning for Dynamic Workflow Scheduling
This paper presents a novel approach called *Graph assisted Offline-Online Deep Reinforcement Learning* (GOODRL) for dynamic workflow scheduling in cloud computing, addressing the challenges of heterogeneous machine configurations and unpredictable workflow patterns. By introducing task-specific and system-oriented graph representations alongside a Graph Attention Actor-Critic Network, GOODRL shows superior performance and adaptability compared to existing algorithms, effectively managing real-time dynamics and achieving lower mean flowtime in varied scenarios.

Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs
Amulet is a novel, training-free framework designed to align large language models (LLMs) with personalized and dynamic user preferences in real-time, using simple prompts to guide token decoding as an online learning process. This approach significantly improves performance across various LLMs and datasets, maintaining computational efficiency through a closed-form solution in each optimization step.

RevisEval: Improving LLM-as-a-Judge via Response-Adapted References
RevisEval is a novel evaluation method for text generation that utilizes response-adapted references to bridge the reliability gap between large language model (LLM) evaluations and human assessments. By revising responses with LLMs and using these revisions as references, RevisEval enhances the accuracy of traditional metrics like BLEU and BERTScore, surpassing existing evaluation paradigms and improving bias reduction and cost-effectiveness.

Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning
This paper introduces adversarial MDP attacks, a black-box reward poisoning framework that exposes vulnerabilities in state-of-the-art deep reinforcement learning (DRL) algorithms. The authors demonstrate that their framework can efficiently induce low-performing policies in agents across various environments by corrupting rewards for a minimal fraction of training timesteps, supported by theoretical analysis and comprehensive empirical evaluations.

IterGen: Iterative Semantic-aware Structured LLM Generation with Backtracking
IterGen is a novel library that enhances large language model (LLM) generation by allowing both forward and backward movement within outputs, addressing issues like hallucination and incorrect results through iterative, grammar-guided techniques. This approach effectively improves the accuracy of LLM-generated SQL and Vega-Lite queries and reduces privacy leakage, with resources available at https://structuredllm.com.

TRENDy: Temporal Regression of Effective Nonlinear Dynamics
TRENDy (Temporal Regression of Effective Nonlinear Dynamics) is introduced as a robust, equation-free method for learning low-dimensional, predictive models of spatiotemporal dynamics from noisy and limited data. The approach successfully maps input data to effective dynamics using multiscale filtering and neural ordinary differential equations, enabling it to predict and identify bifurcations like Turing and Hopf in both synthetic and real data, and provide insights into spatial pattern dynamics in biological systems such as the ocellated lizard.

SimPER: A Minimalist Approach to Preference  Alignment without Hyperparameters
This paper introduces SimPER, a hyperparameter-free preference optimization algorithm for aligning language models, which optimizes inverse perplexity for increased computational and memory efficiency. Extensive experiments demonstrate that SimPER outperforms current state-of-the-art methods on multiple benchmarks without requiring hyperparameter tuning or reference models, underscoring its practical and effective contribution to language model alignment.

Procedural Synthesis of Synthesizable Molecules
This paper presents a bilevel framework leveraging ideas from program synthesis to tackle the challenges of designing synthetically accessible molecules and generating analogs for unsynthesizable ones. By decoupling syntactic skeletons from semantics and utilizing Markov Chain Monte Carlo simulations and evolutionary algorithms, the approach optimizes synthesis pathways and molecular descriptors, enhancing synthesizable molecule design with explicit resource control, promising advancements for autonomous synthesis platforms.

SAFREE: Training-Free and Adaptive Guard for Safe Text-to-Image And Video Generation
SAFREE is a novel, training-free approach designed to improve the safety of text-to-image and video generation without altering model weights. It effectively filters out harmful content by identifying and steering away from toxic concept subspaces in text embeddings, achieving state-of-the-art performance in reducing unsafe content while maintaining high-quality outputs in various tasks.

Multi-Modal and Multi-Attribute Generation of Single Cells with CFGen
This paper introduces CellFlow for Generation (CFGen), a flow-based conditional generative model that maintains the discrete nature of single-cell RNA-seq data to enhance its application in tasks like rare cell type augmentation and batch correction. CFGen successfully generates multi-modal, whole-genome single-cell data, demonstrating its potential to improve the accuracy of biological data recovery and advance computational biology through comprehensive experimental validation.

Rethinking LLM Unlearning Objectives: A Gradient Perspective and Go Beyond
The paper addresses the need for rigorous audits and timely updates of large language models (LLMs) to mitigate risks like copyright and privacy violations, highlighting recent research on LLM unlearning to remove undesirable knowledge without affecting other responses. It introduces the G-effect metric to evaluate the impact of unlearning on model performance, offering insights into existing objectives' drawbacks and motivating new solutions, while outlining future research directions to enhance this vital area.

Zeroth-Order Fine-Tuning of LLMs with Transferable Static Sparsity
This study introduces a method to efficiently fine-tune Large Language Models in memory-constrained environments by combining static sparse zeroth-order (ZO) fine-tuning with quantization. By transferring and focusing on a small, static subset of sensitive parameters, the approach reduces memory demands and outperforms full model ZO fine-tuning, enabling effective tuning of Llama2-7B models on devices with limited memory.

Apollo-MILP: An Alternating Prediction-Correction Neural Solving Framework for Mixed-Integer Linear Programming
The paper introduces Apollo-MILP, an Alternating prediction-correction neural solving framework designed to enhance the prediction accuracy for mixed-integer linear programming. By utilizing a novel Uncertainty-based Error upper Bound and trust-region search, Apollo-MILP effectively reduces problem dimensions and preserves optimality, significantly improving solution quality over existing methods and achieving over a 50% reduction in the solution gap in benchmark tests.

{$\tau$}-bench: A Benchmark for \underline{T}ool-\underline{A}gent-\underline{U}ser Interaction in Real-World Domains
The paper introduces $\tau$-bench, a benchmark for evaluating language agents in dynamic user-agent conversations within retail and airline domains, incorporating domain-specific APIs and guidelines. It highlights the challenges faced by state-of-the-art agents, which succeed in less than 50% of tasks and exhibit low consistency, emphasizing the need for improved methods to enhance rule-following reliability.

$\text{D}_{2}\text{O}$: Dynamic Discriminative Operations for Efficient Long-Context Inference of Large Language Models
Efficient generative inference in Large Language Models is challenged by the memory demands of Key-Value (KV) cache, particularly for longer sequences. To address this, the paper introduces a novel method, Dynamic Discriminative Operations ($\mathbf{D_2 O}$), which optimizes KV cache size dynamically and discriminatively at both layer and token levels, achieving significant memory savings and enhanced inference throughput while maintaining high-quality long-text generation without fine-tuning.

3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing
3DGS-Drag is a novel point-based 3D editing framework designed to facilitate efficient and intuitive drag manipulation of real 3D scenes, addressing the challenges faced in extending 2D editing capabilities to 3D content. By employing deformation and diffusion guidance strategies, the method allows for a variety of geometric edits with enhanced visual quality, proving its effectiveness and efficiency through experimental results that showcase state-of-the-art performance in geometry-related 3D content editing.

3DitScene: Editing Any Scene via Language-guided Disentangled Gaussian Splatting
This paper introduces 3DitScene, a unified scene editing framework that seamlessly integrates 2D and 3D editing using language-guided disentangled Gaussian Splatting. By incorporating 3D Gaussians with semantic language features from CLIP, 3DitScene allows detailed control over scene composition and individual objects, enhancing creative expression and versatility in scene image editing.

3D-SPATIAL MULTIMODAL MEMORY
We introduce the 3D Spatial MultiModal Memory (M3), a system that uses 3D Gaussian Splatting and foundation models to effectively retain and render information about static scenes from video sources. By addressing core challenges in feature compression and alignment, M3 improves multimodal memory efficiency and real-world deployment, notably being the first to confront these challenges in 3D feature distillation on devices like quadruped robots.

ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration
Large language models' performance can be enhanced through iterative dialogue between multiple models, but current multi-agent frameworks typically rely on pre-existing collaborative behavior rather than learned behavior. The proposed ACC-Collab framework uses an Actor-Critic based learning approach to create a specialized two-agent team that excels in collaboration, outperforming state-of-the-art multi-agent techniques on various benchmarks.

Accelerating Task Generalisation with Multi-Level Skill Hierarchies
This paper presents Fracture Cluster Options (FraCOs), a hierarchical reinforcement learning method that enhances agents' generalisation abilities by forming temporally-extended actions based on pattern usefulness, facilitating quick adaptation to new tasks. FraCOs shows improved transfer and superior performance over existing algorithms in both tabular settings and complex environments, especially as the hierarchy depth increases.

Adam-mini: Use Fewer Learning Rates To Gain More
Adam-mini is an optimizer designed to achieve comparable or superior performance to AdamW while halving the memory footprint by optimizing the usage of learning rate resources through parameter partitioning and block learning rates. It demonstrates empirical effectiveness across various language models and improves GPU communication efficiency, enhancing throughput and reducing pre-training time.

Adaptive Length Image Tokenization via Recurrent Allocation
This paper introduces an approach for generating variable-length token representations for 2D images using an encoder-decoder architecture that processes and refines image tokens through iterative rollouts. The results indicate that the model successfully compresses images into a variable number of tokens, optimizing representation based on image entropy and task needs, and shows potential for discovering specialized tokens related to objects or parts.

Adaptive teachers for amortized samplers
The paper proposes a novel approach to enhance amortized inference by using an adaptive training distribution, termed the Teacher, to guide the primary sampler, known as the Student, improving exploration and mode coverage. This methodology, validated across several challenging tasks, demonstrates improved sample efficiency and diverse candidate discovery, with source code available for further exploration.

AdaWM: Adaptive World Model based Planning for Autonomous Driving
The paper addresses the challenge of performance degradation in world model-based reinforcement learning for autonomous driving by identifying the root causes of distribution shift-related mismatches. Introducing AdaWM, an Adaptive World Model method, it implements mismatch identification and alignment-driven finetuning strategies, showing improved performance in CARLA driving tasks through efficient management of policy and model updates.

Adversarially Robust Out-of-Distribution Detection Using Lyapunov-Stabilized Embeddings
This paper introduces AROS, a novel technique utilizing neural ordinary differential equations (NODEs) and Lyapunov stability theory to enhance the robustness of out-of-distribution (OOD) detection against adversarial attacks. By generating fake OOD embeddings and employing an orthogonal binary layer to separate in-distribution and OOD data, AROS significantly improves robust detection performance on benchmark datasets without relying on additional data.

Affine Steerable Equivariant Layer for Canonicalization of Neural Networks
This paper introduces the steerable EquivarLayer, which achieves affine equivariance with arbitrary input and output representations by building on the concept of equivariants beyond invariants. The incorporation of a novel Det-Pooling module enhances its integration with canonicalization approaches, demonstrating effective performance in image classification tasks involving group transformations compared to data augmentation.

Agent-Oriented Planning in Multi-Agent Systems
This paper introduces AOP, a novel agent-oriented planning framework for multi-agent systems, designed to efficiently decompose and allocate tasks to specialized agents, ensuring task solvability, completeness, and non-redundancy. Through a fast task decomposition and reward-based evaluation, AOP enhances problem-solving effectiveness, outperforming single-agent systems and existing planning strategies, as demonstrated by extensive experiments.

Agree to Disagree: Demystifying Homogeneous Deep Ensembles through Distributional Equivalence
This paper challenges the common belief that Jensen's inequality and convexity underpin the effectiveness of deep ensembles, revealing instead that their power lies in the homogeneous output distributions across ensemble members. Through theoretical analysis and empirical results, the authors demonstrate that this distributional equivalence among models leads to the enhanced performance of deep ensembles, offering new insights and quantification methods to predict ensemble efficacy.

Aligned Better, Listen Better For Audio-Visual Large Language Models
The paper introduces Dolphin, a fine-grained Audio-Visual Large Language Model (AV-LLM) designed to improve the understanding of videos by effectively integrating audio and visual information through a novel architectural approach and curated dataset. It addresses deficiencies in existing models with a multi-scale adapter for spatial alignment, interleaved merging for temporal alignment, and a large dataset of diverse, open-ended data tuples, resulting in enhanced audio-visual comprehension and reduced hallucinations.

Aligning Human Motion Generation with Human Perceptions
This paper introduces a new data-driven approach to improve the evaluation of generated human motions by aligning them with human perceptions. By presenting the large-scale MotionPercept dataset and the MotionCritic model, the authors provide an accurate metric that enhances motion generation quality, surpassing traditional metrics that rely on simple heuristics.

A Multiscale Frequency Domain Causal Framework for Enhanced Pathological Analysis
This paper addresses challenges in Multiple Instance Learning (MIL) for digital pathology Whole Slide Image (WSI) analysis by introducing a Multi-Scale Frequency Domain Causal framework (MFC) to enhance performance and interpretability. By integrating various modules that exploit multi-scale frequency-domain information and causal interventions, the proposed framework demonstrates improved accuracy and generalization on the Camelyon16 and TCGA-NSCLC datasets, offering a new perspective in medical image analysis.

AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents
AndroidWorld is introduced as a comprehensive Android environment offering a dynamic and extensive set of 116 programmatic tasks across 20 real-world apps, aiming to advance autonomous agents' capabilities in executing human tasks. By providing reproducible tests and initial benchmark results, the study highlights the challenge of completing tasks, underscores the necessity for future development of cross-platform agents, and emphasizes the importance of rigorous robustness testing.

An Evolved Universal Transformer Memory
This paper introduces Neural Attention Memory Models (NAMMs), a learned network approach to enhance both performance and efficiency of transformers by managing memory more effectively than previous hand-designed methods. NAMMs are adaptable to any self-attention model, demonstrating substantial improvements in long-context benchmarks and enabling zero-shot transfer across different transformer architectures and modalities, including vision and reinforcement learning.

A Percolation Model of Emergence: Analyzing Transformers Trained on a Formal Language
In this paper, the authors propose a phenomenological definition of emergent capabilities in neural networks, attributing them to the acquisition of general regularities in the data-generating process, which cause sudden performance improvements in specific tasks. They empirically validate this definition using a context-sensitive formal language system and draw parallels between neural network learning dynamics and percolation on a bipartite graph, offering insights into better defining and predicting emergence in AI systems.

A Sanity Check for AI-generated Image Detection
This paper addresses the challenge of detecting AI-generated images by introducing the Chameleon dataset, which contains images difficult for humans to discern as synthetic. The authors propose a novel detection method, AIDE, which uses hybrid features to improve recognition accuracy, achieving notable improvements over existing methods and showing promise on challenging benchmarks, though the task remains unresolved.

A Theoretical Analysis of Self-Supervised Learning for Vision Transformers
This paper provides theoretical insights into the distinct representation capabilities of masked autoencoders (MAE) and contrastive learning (CL) in self-supervised learning for vision transformers (ViTs). By analyzing the training dynamics, the study reveals that MAE effectively captures both global and local features for optimal reconstruction, whereas CL predominantly focuses on global features, providing a deeper understanding of their differing behaviors.

A Truncated Newton Method for Optimal Transport
This paper introduces a specialized truncated Newton algorithm for entropic-regularized optimal transport (OT) that achieves locally quadratic convergence without the need for a Lipschitz Hessian. The proposed GPU-parallel algorithm demonstrates exceptional runtime and precision in solving large-scale OT problems, significantly outperforming existing methods across various datasets and cost functions.

Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers
This paper introduces in-context re-ranking (ICR), a novel method that efficiently leverages attention patterns in large language models (LLMs) for document re-ranking in information retrieval systems, without relying on generative capabilities. The proposed approach significantly reduces latency and outperforms generative re-ranking methods like RankGPT, demonstrating exceptional efficacy in contexts requiring complex signals and paving the way for broader applications of open-weight LLMs beyond generation.

Autoregressive Pretraining with Mamba in Vision
This paper demonstrates that the visual capabilities of the recently developed state space model, Mamba, can be significantly improved through autoregressive pretraining, enhancing both accuracy and efficiency. Specifically, this method enables faster training speeds and improves ImageNet accuracy, with a base-size Mamba achieving 83.2% and a huge-size Mamba attaining 85.0%, surpassing other variants and unlocking its scaling potential.

Backdooring Vision-Language Models with Out-Of-Distribution Data
This paper introduces VLOOD, a novel approach for conducting backdoor attacks on Vision-Language Models (VLMs) using only Out-Of-Distribution (OOD) data, addressing scenarios where attackers lack access to the original training data. By demonstrating successful backdoor attacks in image-to-text tasks and proposing innovative injection techniques, the study uncovers crucial security vulnerabilities in VLMs, highlighting the need for enhanced protection measures in multimodal models.

Balanced Neural ODEs: nonlinear model order reduction and Koopman operator approximations
This paper introduces Balanced Neural ODE (B-NODE), a novel approach that integrates Variational Autoencoders (VAEs) with Neural ODEs to create fast surrogate models capable of handling time-varying inputs and dynamic system complexities. By propagating variational parameters over time and establishing fixed information channels in latent space, B-NODE effectively enhances time series modeling and approximates the Koopman operator, demonstrated on both academic and real-world scenarios like power plants and MuJoCo data.

Balanced Ranking with Relative Centrality: A multi-core periphery perspective
This paper addresses the problem of unbalanced rankings generated by traditional centrality measures like PageRank in graphs with underlying communities, such as those described by multi-core-periphery with communities (MCPC). The authors introduce a new approach called relative centrality, which normalizes centrality scores iteratively to enhance balancedness without sacrificing ranking validity, and demonstrate its effectiveness through theoretical analysis and simulations, particularly in the context of single-cell datasets.

Bayesian Analysis of Combinatorial Gaussian Process Bandits
This paper addresses the combinatorial volatile Gaussian process (GP) semi-bandit problem, presenting novel Bayesian cumulative regret bounds for the GP-UCB, GP-BayesUCB, and GP-TS algorithms in infinite, volatile, and combinatorial settings. It also applies this framework to online energy-efficient navigation, demonstrating improved performance over existing methods.

Bayesian Optimization via Continual Variational Last Layer Training
This paper introduces an approach leveraging variational Bayesian last layers (VBLLs) that exhibits competitive performance for Bayesian optimization across various problem types, including those challenging for Bayesian neural networks (BNNs). By connecting VBLL training to exact conditioning in Gaussian Processes (GPs), the authors develop an efficient online training algorithm, demonstrating that VBLL networks outperform GPs and other BNN models in tasks with intricate input correlations, while matching well-tuned GPs on standard benchmarks.

Bi-Factorial Preference Optimization: Balancing Safety-Helpfulness in Language Models
This paper introduces Bi-Factorial Preference Optimization (BFPO), a supervised learning framework that re-parameterizes the reinforcement learning from human feedback (RLHF) objective into a single supervised learning task to balance the safety and helpfulness of large language models (LLMs). The proposed method outperforms existing techniques in achieving safety and helpfulness without relying on human annotation and uses significantly less computational resources, with plans to release the training recipes and models.

BingoGuard: LLM Content Moderation Tools with Risk Levels
This paper introduces BingoGuard, an LLM-based moderation system that predicts both binary safety labels and severity levels for harmful content, allowing for tailored content filtering across platforms with varying safety thresholds. Utilizing a novel generate-then-filter framework, the authors create comprehensive datasets for training and testing, resulting in a model that outperforms existing benchmarks by effectively incorporating severity levels into its analyses of harmful content.

BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments
BioDiscoveryAgent leverages large language model capabilities to design genetic perturbation experiments, efficiently identifying genes that, when altered, produce desired phenotypes like cell growth. Demonstrating a significant improvement over traditional Bayesian optimization baselines, BioDiscoveryAgent offers an interpretable and innovative approach to experiment design by predicting relevant genetic perturbations more accurately and aiding scientific discovery without needing additional machine learning models or predefined acquisition functions.

Boosting Latent Diffusion with Perceptual Objectives
Latent diffusion models (LDMs) enable high-resolution image generation by learning data distributions in latent space but often produce less detailed images due to a disconnect between the diffusion model and the decoder. This paper introduces a latent perceptual loss (LPL) using internal decoder features, which improves image sharpness and realism, demonstrating significant enhancements in both quantitative and qualitative outcomes across various datasets and resolutions.

Bounds on $L_p$ Errors in Density Ratio Estimation via $f$-Divergence Loss Functions
This paper offers theoretical insights into density ratio estimation (DRE) by deriving upper and lower bounds on the $L_p$ errors through $f$-divergence loss functions for Lipschitz continuous estimators. The research highlights that $L_p$ error increases significantly with the Kullback-Leibler divergence when $p > 1$, and these findings are supported by numerical experiments.

BP-Modified Local Loss for Efficient Training of Deep Neural Networks
This paper introduces a novel BP-modified local loss method to enhance the performance of local loss training by incorporating the true Backward Propagation (BP) gradient. The approach achieves a bias-variance balance, resulting in significant improvements in test accuracy for local loss methods like Forward-Forward and LoCo, with considerable memory savings compared to traditional BP algorithms.

Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation
This paper addresses the limitations of group max-min fairness (MMF) objectives in recommender systems, which can cause nonlinearity issues and a Jensen gap when mini-batch sampling is used. The authors propose FairDual, a dual optimization algorithm that reformulates the MMF-constrained objective into a group-weighted one, achieving a sub-linear convergence rate and effectively minimizing the Jensen gap, as demonstrated by extensive experiments showing improved accuracy and fairness across various models and datasets.

CARTS: Advancing Neural Theorem Proving with Diversified Tactic Calibration and Bias-Resistant Tree Search
CARTS improves neural theorem proving by integrating a diversified tactic calibration and bias-resistant tree search, addressing issues of tactic similarity and value function bias present in existing methods. Through enhancements in tactic diversity and model confidence calibration, CARTS achieves superior performance, with a pass@l rate of 49.6% on the miniF2F-test benchmark, offering a more balanced and efficient proof search process.

CHASE-SQL: Multi-Path Reasoning and Preference Optimized Candidate Selection in Text-to-SQL
CHASE-SQL is a novel framework that enhances performance in Text-to-SQL tasks by employing multi-agent modeling and test-time compute for better SQL candidate generation and selection. It uses a combination of divide-and-conquer strategies, chain-of-thought reasoning, and instance-aware synthetic example generation, achieving a state-of-the-art execution accuracy of 73.0% on the BIRD benchmark test set.

COFlowNet: Conservative Constraints on Flows Enable High-Quality Candidate Generation
Conservative Offline GFlowNet (COFlowNet) addresses the challenge of insufficient exploration in offline generative flow networks by constraining the model from exploring unsupported flows, which are edges containing unseen states in training data. This approach allows COFlowNet to generate better and more diverse candidates by focusing on optimal trajectories within the training distribution, as validated by experimental results on various datasets.

Collab: Controlled Decoding using Mixture of Agents for LLM Alignment
The paper introduces a method called Controlled Decoding that aligns large language models (LLMs) at inference time without retraining by using a mixture of agent-based decoding strategies. Through a dynamic policy-switching mechanism, this approach leverages existing aligned LLMs to improve model selection and task performance, achieving superior results compared to single-agent decoding strategies and setting new benchmarks for state-of-the-art decoding techniques.

Collapsed Language Models Promote Fairness
This paper explores the issue of fairness in pretrained language models and introduces a principled fine-tuning method inspired by an observation of Neural Collapse, which involves alignment between token representations and word embeddings. The proposed method effectively improves fairness across various debiasing techniques while maintaining the performance of language models on standard natural language understanding tasks.

Compositional Entailment Learning for Hyperbolic Vision-Language Models
This paper introduces Compositional Entailment Learning for hyperbolic vision-language models, leveraging the hierarchical nature of hyperbolic embeddings beyond individual image-text pairs. By organizing images, image boxes, and their descriptions hierarchically using contrastive and entailment-based objectives, the proposed approach outperforms traditional Euclidean and recent hyperbolic models in zero-shot and retrieval tasks, demonstrating superior hierarchical performance.

Conformalized Interactive Imitation Learning: Handling Expert Shift and Intermittent Feedback
This paper introduces ConformalDAgger, an innovative approach in interactive imitation learning that employs a novel uncertainty quantification algorithm, intermittent quantile tracking (IQT), to better handle distribution shifts during deployment by incorporating expert feedback. The method shows promise in enhanced uncertainty detection and intervention frequency compared to existing approaches, as demonstrated through both simulated and hardware tests on a robotic manipulator.

Controllable Generation via Locally Constrained Resampling
This paper presents a probabilistic approach that uses Bayesian conditioning for generating samples from autoregressive models while satisfying logical constraints, addressing the challenge of sampling from complex outputs. The proposed method improves contextual awareness and accuracy in constrained generation tasks, such as LLM detoxification and solving Sudoku puzzles, achieving superior performance compared to existing techniques.

Convergence and Implicit Bias of Gradient Descent on Continual Linear Classification
This paper investigates continual learning with multiple linear classification tasks using gradient descent, demonstrating that when tasks are linearly separable and presented cyclically or randomly, the trained classifier converges directionally to the joint max-margin solution. The study further provides a non-asymptotic analysis of cycle-averaged forgetting, highlighting the connection between task alignment, catastrophic forgetting, and backward knowledge transfer, and shows that forgetting diminishes with repeated cycles, while also analyzing scenarios where tasks are not jointly separable.

CPSample: Classifier Protected Sampling for Guarding Training Data During Diffusion
CPSample is a method designed to prevent exact replication of training data by diffusion models without sacrificing image quality, unlike previous approaches that used differential privacy constraints. By employing a classifier to guide the sampling process away from data points that can be easily classified, CPSample improves robustness against membership inference attacks and achieves strong FID scores, all while being computationally cheaper as it requires training only a classifier, not retraining the entire diffusion model.

CR-CTC: Consistency regularization on CTC for improved speech recognition
This paper introduces Consistency-Regularized CTC (CR-CTC), a novel approach to improve automatic speech recognition by enforcing consistency between CTC distributions from different augmented input views. The method enhances performance by self-distillation, learning contextual representations through masked predictions, and suppressing peaky distributions, leading to state-of-the-art results on multiple datasets.

Credit-based self organizing maps: training deep topographic networks with minimal performance degradation
The paper identifies a performance issue in deep neural networks using self-organizing maps due to the mismatch between bottom-up and top-down learning updates. By proposing a new self-organization algorithm that aligns with top-down learning, the study enhances the functional efficacy of topographical models, bridging the performance gap with non-topographical models and contributing to more brain-like neural architectures.

Data-centric Prediction Explanation via Kernelized Stein Discrepancy
This paper introduces HD-Explain, a highly precise and data-centric prediction explanation method that utilizes the properties of Kernelized Stein Discrepancy (KSD) to define a parameterized kernel function for trained models, identifying training samples that optimally support test predictions. Through comprehensive evaluations, HD-Explain demonstrates superiority over existing methods in terms of fine-grained explanation precision, consistency, and computational efficiency, offering a straightforward and robust solution for prediction explanations.

DataMan: Data Manager for Pre-training Large Language Models
This paper introduces a novel approach to pre-training data selection for large language models by deriving quality criteria from text perplexity anomalies and using a system called DataMan to annotate and manage a vast corpus. Experiments demonstrate that models trained with DataMan-selected data outperform those using traditional methods, highlighting the importance of quality criteria and domain-specific data in improving language model capabilities.

Data Taggants: Dataset Ownership Verification Via Harmless Targeted Data Poisoning
This paper introduces data taggants, a novel dataset ownership verification technique that uses pairs of out-of-distribution samples and random labels to subtly alter a dataset for detection purposes. Unlike existing methods, data taggants do not compromise model performance or require access to model internals, and experiments on ImageNet1k demonstrate their reliability and superiority over backdoor watermarking while maintaining validation accuracy.

Decentralized Sporadic Federated Learning: A Unified Algorithmic Framework with Convergence Guarantees
This paper introduces Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a novel methodology that incorporates a generalized notion of sporadicity in decentralized federated learning, accounting for variations in communication and computation capabilities across clients. The study provides a unified framework that generalizes many existing decentralized optimization methods, characterizes convergence behavior under diverse model conditions, and demonstrates improved training speeds over traditional baselines.

DeFT: Decoding with Flash Tree-attention for Efficient Tree-structured LLM Inference
Large language models face inefficiencies in tree-based applications due to inadequate handling of query partitioning and KV cache during attention calculations, leading to increased memory IO and poor GPU utilization. The proposed DeFT algorithm addresses these challenges by introducing KV-Guided Grouping and Flattened Tree KV Splitting, significantly reducing KV cache IO and achieving up to a 3.59× speedup in processing workloads, thereby enhancing hardware efficiency and overall performance.

Designing Mechanical Meta-Materials by Learning Equivariant Flows
This paper presents a method to expand the design space of cellular solids, a type of mechanical meta-material, by using a neural network to transform reference geometries through a divergence-free flow that respects space group symmetries. The approach enables the creation of materials with optimized mechanical properties, such as negative Poisson's ratios, and is validated through both simulations and real-world prototypes, demonstrating the potential for higher-order symmetry designs to display diverse behaviors.

Difference-of-submodular Bregman Divergence
This paper generalizes the Bregman divergence framework for discrete spaces by introducing the difference-of-submodular Bregman divergence, which does not require the generating function to be submodular or supermodular. By incorporating permutation-invariant neural networks, this new divergence effectively captures structural properties in discrete data, significantly enhancing performance in clustering and set retrieval tasks.

Diffusing to the Top: Boost Graph Neural Networks with Minimal Hyperparameter Tuning
This paper presents GNN-Diff, a graph-conditioned latent diffusion framework designed to generate high-performing Graph Neural Networks from sub-optimal hyperparameters identified through a minimal coarse search, significantly reducing computational and time costs. Validated through 166 experiments across various graph tasks and datasets, GNN-Diff demonstrates enhanced performance, stability, and generalizability of GNNs, offering an efficient alternative to exhaustive hyperparameter tuning.

Diffusion Models and Gaussian Flow Matching: Two Sides of the Same Coin
This paper clarifies the relationship between flow matching and diffusion models in generative modeling, demonstrating that they are essentially the same framework despite their different noise schedules and loss weightings. The finding allows these frameworks to be used interchangeably, simplifying their application in the community.

Diffusion Transformer Captures Spatial-Temporal Dependencies: A Theory for Gaussian Process Data
The paper introduces the Diffusion Transformer, a backbone model Sora for video generation, which effectively scales diffusion models to capture spatial-temporal dependencies in sequential data. By establishing theoretical guarantees and providing numerical experiments, the study demonstrates how attention layers within the transformer capture these dependencies, enhancing learning efficiency and opening new possibilities for high-fidelity sequential data generation.

Discovering Clone Negatives via Adaptive Contrastive Learning for Image-Text Matching
This paper addresses the challenge of clone negatives in image-text matching by proposing Adaptive Contrastive Learning (AdaCL), which uses dynamic margin parameters and a modulating anchor to improve matching outcomes. AdaCL proves effective in both fully and weakly-supervised settings, enhancing robustness without relying on human annotations, thus advancing scalable vision-language contrastive learning.

Discretization-invariance? On the Discretization Mismatch Errors in Neural Operators
Neural operators, particularly the Fourier Neural Operator (FNO), have been instrumental in learning mappings for function spaces, yet face challenges with discretization mismatch errors when applied to data at resolutions differing from the training set. This paper identifies these errors and introduces a Cross-Resolution Operator-learning Pipeline to alleviate them, enhancing neural operator performance in cross-resolution scientific tasks like climate modeling and fluid dynamics.

DisPose: Disentangling Pose Guidance for Controllable Human Image Animation
DisPose introduces a novel approach to human image animation by disentangling sparse skeleton poses into motion field guidance and keypoint correspondence, eliminating the need for additional dense inputs like depth maps. By generating a dense motion field from sparse data and utilizing hybrid ControlNet, DisPose enhances video quality and consistency, outperforming current methods while maintaining model parameter integrity.

Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents
Large language model (LLM) agents have potential in addressing software engineering problems, but their effectiveness varies across tasks. The proposed DEI (Diversity Empowered Intelligence) framework enhances problem-solving by managing a collective of agent frameworks, significantly improving resolve rates on SWE-Bench Lite from 27.3% to as high as 55%, surpassing individual agents and most closed-source solutions.

Do as I do (Safely): Mitigating Task-Specific Fine-tuning Risks in Large Language Models
The paper explores how fine-tuning models on task-specific datasets can inadvertently increase safety risks, as malicious actors may manipulate these datasets to yield harmful model behaviors. The authors propose a novel mitigation strategy that integrates safety data mimicking the task format, effectively re-establishing safety alignment and maintaining task performance better than existing methods.

Dobi-SVD: Differentiable SVD for LLM Compression and Some New Perspectives
This paper introduces Dobi-SVD, a novel approach for compressing large language models (LLMs) using Singular Value Decomposition (SVD), aimed at overcoming computational and memory challenges. Dobi-SVD achieves substantial compression with minimal performance degradation, demonstrating significant improvements in both parameter reduction and inference speed, thus democratizing access to LLMs while maintaining efficiency.

DocMIA: Document-Level Membership Inference Attacks against DocVQA Models
This paper presents two novel membership inference attacks on Document Visual Question Answering (DocVQA) models, designed for both white-box and black-box adversarial scenarios without relying on auxiliary datasets. The authors demonstrate that their unsupervised methods outperform current state-of-the-art attacks, underscoring significant privacy risks associated with DocVQA models in document processing workflows.

Dynamic Negative Guidance of Diffusion Models
This paper introduces Dynamic Negative Guidance (DNG), a novel approach that overcomes the limitations of conventional Negative Prompting (NP) in diffusion models by employing state and time-dependent modulation of guidance without requiring additional training. Evaluations on datasets like MNIST and CIFAR10 demonstrate that DNG improves safety, class balance, and image quality compared to baseline methods and enhances guidance accuracy in Stable Diffusion.

Dynamics of Concept Learning and Compositional Generalization
This paper introduces a structured identity mapping (SIM) task as a theoretical abstraction to better understand the empirical results of compositional generalization observed in text-conditioned diffusion models. By analyzing the learning dynamics of neural networks on this SIM task, the study captures key empirical insights, reveals new mechanisms such as non-monotonic learning dynamics of test loss in early training phases, and validates these findings with a text-conditioned diffusion model, thereby bridging simple and complex generative frameworks.

DynFrs: An Efficient Framework for Machine Unlearning in Random Forest
The paper introduces the DynFrs framework, which enables efficient machine unlearning in Random Forest models while maintaining predictive accuracy, addressing privacy concerns in sensitive data domains. By using subsampling method Occ(q) and lazy tag strategy Lzy, DynFrs enhances unlearning performance and accuracy, demonstrating significant improvements over existing methods when applied to Extremely Randomized Trees.

EC-DIT: Scaling Diffusion Transformers with Adaptive Expert-Choice Routing
The paper introduces a new family of Mixture-of-Experts (MoE) models, called EC-DIT, for diffusion transformers in text-to-image synthesis, leveraging expert-choice routing to optimize computational allocation based on text-image complexities. This approach allows scaling up to 97 billion parameters, significantly improving training convergence, text-to-image alignment, and generation quality, achieving a state-of-the-art GenEval score of 71.68% with efficient inference speed.

EdgeRunner: Auto-regressive Auto-encoder for Artistic Mesh Generation
This paper introduces an Auto-regressive Auto-encoder (ArAE) model for generating high-quality 3D meshes with up to 4,000 faces, employing a novel mesh tokenization algorithm to improve training efficiency and generalization. The model demonstrates superior quality, diversity, and generalization in point cloud and image-conditioned mesh generation tasks through extensive experiments.

Efficient Cross-Episode Meta-RL
The paper introduces Efficient Cross-Episodic Transformers (ECET), an advanced algorithm for online Meta-Reinforcement Learning that enhances generalization in unseen tasks by leveraging past episodes as in-context information. Experimental results demonstrate that ECET outperforms previous state-of-the-art methods in learning efficiency and adaptability across various benchmarks, thereby contributing to the development of more robust AI systems.

Elucidating the Preconditioning in Consistency Distillation
This paper introduces a theoretically grounded approach called Analytic-Precond to optimize preconditioning in consistency distillation, enhancing the training of diffusion models. By addressing the consistency gap between the teacher and student models, Analytic-Precond improves trajectory alignment and achieves significant training acceleration across various datasets.

Enhancing Compositional Text-to-Image Generation with Reliable Random Seeds
Text-to-image diffusion models often struggle with consistency when generating images from compositional prompts, such as placing multiple objects accurately. This paper identifies the critical influence of initial noise patterns on these inconsistencies and proposes a method to enhance model performance by mining reliable training cases, resulting in significant improvements in both numerical and spatial composition for models like Stable Diffusion and PixArt-α.

EvA: Erasing Spurious Correlations with Activations
This paper presents "Erasing with Activations" (EvA), a method for mitigating spurious correlations in pretrained networks by learning to identify and erase class-specific spurious indicators in the fully connected layer. EvA not only achieves state-of-the-art performance with significant improvements on benchmark datasets like BAR and Waterbirds but does so with substantially lower computational requirements, demonstrating its efficiency in both data and computation.

Exploiting Hankel-Toeplitz Structures for Fast Computation of Kernel Precision Matrices
The paper introduces an improved computational method for the Hilbert-space Gaussian process (HGP) that reduces the complexity of precomputing the precision matrix from $\mathcal{O}(NM^2)$ to $\mathcal{O}(NM)$, without requiring additional approximations. This advancement, achieved by exploiting the structure of the precision matrix as a sum of Hankel-Toeplitz matrices, offers a pure speed-up for existing Gaussian process approximations, enhancing efficiency while maintaining model integrity.

Exploiting Structure in Offline Multi-Agent RL: The Benefits of Low Interaction Rank
This paper addresses the challenge of learning approximate equilibrium in offline multi-agent reinforcement learning (MARL) by introducing the concept of interaction rank. The study demonstrates that employing low interaction rank functions significantly enhances robustness to distribution shift, leading to efficient decentralized learning in cooperative and competitive settings, as supported by theoretical results and experiments favoring these functions over traditional architectures.

Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF
This paper introduces Exploratory Preference Optimization (XPO), a novel reinforcement learning algorithm that enhances Direct Preference Optimization (DPO) by incorporating an exploration bonus, which enables strategic exploration beyond the existing model's data support. The authors demonstrate that XPO is sample-efficient and converges to a near-optimal policy under natural exploration conditions, while integrating techniques from language modeling and reinforcement learning through KL-regularized Markov decision processes.

Exploring channel distinguishability in local neighborhoods of the model space in quantum neural networks
This paper examines the challenges of training Quantum Neural Networks (QNNs), focusing on the architectures known as ansatzes. It introduces alternative ways to characterize ansatzes beyond expressivity and highlights the difficulty of iterative training of small quantum models, suggesting the necessity for better initialization methods.

Exploring Learning Complexity for Efficient Downstream Dataset Pruning
Dataset pruning is essential due to the high fine-tuning costs of large-scale pre-trained models, yet traditional methods are impractical for such models as they require full dataset training. This paper introduces Distorting-based Learning Complexity (DLC), a novel, training-free hardness score, and an innovative under-sampling strategy called FlexRand, effectively reducing dataset size and pruning time while maintaining task performance, demonstrated by a 35x time reduction and state-of-the-art results in experiments.

Facilitating Multi-turn Function Calling for LLMs via Compositional Instruction Tuning
This paper introduces BUTTON, a novel approach for enhancing the multi-turn function calling capabilities of Large Language Models, crucial for handling complex real-world queries that require function composition. By generating the BUTTONInstruct dataset with 8,000 data points through a combination of bottom-up task construction and top-down multi-agent interactions, the study demonstrates improved performance and task compositionality in LLMs.

FaithEval: Can Your Language Model Stay Faithful to Context, Even If "The Moon is Made of Marshmallows"
FaithEval is a new benchmark designed to assess the faithfulness of large language models (LLMs) in handling unanswerable, inconsistent, and counterfactual contexts, addressing the persistent challenge of faithfulness hallucination in these models. The benchmark includes 4.9K rigorously validated problems and shows that even advanced models frequently fail to align responses with given contexts, highlighting that larger models do not inherently ensure better faithfulness.

Federated Granger Causality Learning For Interdependent Clients With State Space Representation
This paper presents a federated approach for learning Granger causality in complex industrial systems using a linear state space framework to effectively model interdependencies while addressing bandwidth limitations and computational burdens. The proposed method enhances client models with machine-learned Granger causality from a server, ensuring scalability, robustness, and data security through differential privacy, and is validated through synthetic experiments and real-world datasets.

Fine-tuning can cripple your foundation model; preserving features may be the solution
Pre-trained foundation models often lose their ability to recognize previously learned concepts when fine-tuned on new, specific tasks, a phenomenon termed "concept forgetting." To address this, the paper introduces LDIFS, a novel fine-tuning method that preserves pre-trained knowledge while learning new downstream task concepts, significantly reducing concept forgetting and proving effective in continual fine-tuning scenarios across multiple tasks.

Fitting Networks with a Cancellation Trick
The paper introduces the logit-DCBM, merging concepts from network models like DCBM, LSM, and the $\beta$-model, and addresses the challenge of parameter fitting with a novel cancellation trick. It also presents R-SCORE, a recursive community detection algorithm that leverages this approach, demonstrating significant improvements over existing methods and achieving faster Hamming error rates in specific settings.

Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning
We introduce FOLK, a novel Self-Supervised Learning approach utilizing adaptive frequency masking and self-knowledge distillation to improve pre-training efficacy. By selecting masked frequencies based on image responses and employing a two-branch input framework, FOLK effectively addresses limitations of previous methods and achieves competitive performance across various downstream tasks.

From Attention to Activation: Unraveling the Enigmas of Large Language Models
This paper investigates two unusual behaviors in auto-regressive Transformers: the first token dominance in attention heads and large outlier activations in hidden states. It introduces a reformulation of the softmax function and a novel optimizer, OrthoAdam, to address these issues, resulting in improved Transformer performance, particularly under quantisation, with significant reductions in first token attention, activation kurtosis, and perplexity penalty.

From Pixels to Tokens: Byte-Pair Encoding on Quantized Visual Modalities
This paper introduces a novel image tokenizer that applies Byte-Pair Encoding (BPE) to visual data, improving the alignment and integration of visual and textual modalities in Multimodal Large Language Models. By incorporating structural prior information directly into image tokens, the proposed method enhances multimodal understanding and model performance, as evidenced by the developed Being-VL-0 model, which excels in various benchmarks and demonstrates scalability.

GameArena: Evaluating LLM Reasoning through Live Computer Games
GameArena is a new dynamic benchmark that evaluates the reasoning abilities of large language models (LLMs) through interactive gameplay, focusing on specific reasoning capabilities such as deductive and inductive reasoning. By analyzing over 2000 game sessions, the study provides detailed assessments of various reasoning skills in five state-of-the-art LLMs and demonstrates improved user engagement compared to existing benchmarks like Chatbot Arena.

Generalization through variance: how noise shapes inductive biases in diffusion models
This paper explores the generalization capabilities of diffusion models, emphasizing a phenomenon termed 'generalization through variance' that arises from the noisy target used in the denoising score matching (DSM) objective. Utilizing a physics-inspired path integral method, the study finds that diffusion models learn to sample from distributions that extend their training sets by filling in 'gaps,' driven by the covariance structure of the noise introduced during training, and further examines how this interacts with feature-related biases.

Generalized Behavior Learning from Diverse Demonstrations
This paper introduces Guided Strategy Discovery (GSD), a method for learning diverse and performant policies by using a novel diversity formulation based on task-relevance that prioritizes exploration of modeled latent factors. GSD outperforms existing methods by approximately 21% in discovering novel behaviors across multiple benchmarks and is validated in both in-distribution and out-of-distribution settings, with applications demonstrated on virtual table tennis leveraging real-world human demonstrations.

Generation and Comprehension Hand-in-Hand: Vision-guided Expression Diffusion for Boosting Referring Expression Generation and Comprehension
The paper introduces a VIsion-guided Expression Diffusion Model (VIE-DM) to improve Referring Expression Generation (REG) by generating diverse and high-quality synonymous expressions that enhance Referring Expression Comprehension (REC) datasets. By effectively addressing the feature discrepancy issue with a vision-text condition module and transformer decoder, the proposed model significantly enhances existing REC models' performance and achieves state-of-the-art results across five datasets.

Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation
This paper introduces a method for generating coherent video sequences between two keyframes by adapting a pretrained image-to-video diffusion model for keyframe interpolation. Through a dual-directional diffusion sampling process, the adapted model predicts videos moving both forwards and backwards in time, outperforming existing diffusion-based and traditional frame interpolation methods.

Grammar Reinforcement Learning: path and cycle counting in graphs with a Context-Free Grammar and Transformer approach
The paper introduces Grammar Reinforcement Learning (GRL), an algorithm combining Monte Carlo Tree Search (MCTS) and a transformer architecture within a context-free grammar framework to count paths and cycles in graphs more efficiently. Key contributions include a CFG-based transformer framework, the GRL algorithm for optimizing formulas, and new matrix-based formulas that enhance computational efficiency by two to six times compared to current methods.

GraphRouter: A Graph-based Router for LLM Selections
GraphRouter introduces a novel inductive graph framework that enhances the selection of Large Language Models (LLMs) by fully utilizing contextual information among tasks, queries, and LLMs. Through a unique edge prediction mechanism, it efficiently adapts to both existing and new LLMs, delivering significant performance improvements in diverse scenarios while reducing computational costs by leveraging a heterogeneous graph model.

h4rm3l: A Language for Composable Jailbreak Attack Synthesis
State-of-the-art large language models (LLMs) can be compromised by jailbreak attacks, yet current safety assessments fail to adequately address this vulnerability. The paper introduces h4rm3l, a novel approach utilizing a domain-specific language and bandit algorithms to efficiently generate diverse and effective jailbreak attacks, achieving over 90% success rates against leading LLMs.

HAMSTER: Hierarchical Action Models for Open-World Robot Manipulation
This paper addresses the lack of generalization in robotics by proposing hierarchical vision-language-action (VLA) models that effectively utilize off-domain data, such as action-free videos and simulation data. The hierarchical approach improves generalization in robotics by separating high-level vision-language models (VLMs) for task-level reasoning from low-level control policies for precise manipulation, resulting in a 20% improvement in success rate in real-robot experiments compared to standard methods.

Herald: A Natural Language Annotated Lean 4 Dataset
This paper introduces a novel framework for translating the Mathlib4 corpus in Lean 4 into natural language to aid in training large language models for formal languages. The proposed Herald Translator, which is fine-tuned on this translated dataset, achieves superior accuracy in formalizing mathematical statements compared to existing models and demonstrates potential in automating the formalization of graduate-level mathematics, with both the model and datasets being publicly accessible.

Hessian Free Efficient Single Loop Iterative Differentiation Methods for Bi-Level Optimization Problems
This paper explores single-loop methods with iterative differentiation for nonconvex bilevel optimization problems, introducing an efficient ITD-type method (ES-ITDM) that uses historical updates to approximate hypergradients. Additionally, a Hessian-free variant is developed, reducing computational cost and maintaining competitive convergence rates, with empirical results demonstrating superior efficiency compared to existing methods.

High-Dimensional Bayesian Optimisation with Gaussian Process Prior Variational Autoencoders
This paper presents a novel approach to high-dimensional Bayesian optimisation by integrating Gaussian Process (GP) surrogate models with GP prior Variational Autoencoders (VAEs) that condition on auxiliary covariates. By learning a structured latent space that accommodates these covariates, the proposed method allows for improved optimization performance in both simulated datasets and standard benchmarks compared to existing latent space BO techniques.

Holographic Node Representations: Pre-training Task-Agnostic Node Embeddings
This paper introduces holographic node representations, a new approach for creating node representations in Graph Neural Networks (GNNs) that can handle tasks of any order by separating the concerns of node-permutation symmetries. The proposed method's task-agnostic expansion and reduction maps lead to highly expressive, adaptable, and efficiently pre-trained embeddings, showing up to 100% performance improvement over traditional methods.

How many samples are needed to train a deep neural network?
This paper investigates the data requirements for training ReLU feed-forward neural networks and finds that the generalization error scales at the rate of $1/\sqrt{n}$ rather than the traditional $1/n$. The study supports the view that neural networks require a large number of training samples and introduces new technical insights, including the first lower bounds on the entropy of these networks.

Hypothetical Minds: Scaffolding Theory of Mind for Multi-Agent Tasks with Large Language Models
The paper introduces Hypothetical Minds, an autonomous agent utilizing a large language model-based architecture with modular components for perception, memory, and hierarchical planning, addressing non-stationarity in multi-agent reinforcement learning. The agent significantly outperforms previous baselines on the Melting Pot benchmark by employing a Theory of Mind module that iteratively generates, evaluates, and refines hypotheses about other agents' strategies, thus enhancing adaptation to complex environments.

Identifiability for Gaussian Processes with Holomorphic Kernels
Gaussian processes (GPs) are versatile tools in various domains, but the identifiability of their kernel parameters, particularly beyond Matérn-type kernels, has not been thoroughly explored. This paper introduces a novel theoretical framework to assess the identifiability of parameters in kernels holomorphic near zero, aiding in their meaningful application-specific interpretation and highlighting potential non-identifiable parameters.

IGL-Bench: Establishing the Comprehensive Benchmark for Imbalanced Graph Learning
Deep graph learning faces challenges due to imbalanced data distributions, affecting the performance of traditional algorithms. This paper introduces IGL-Bench, a comprehensive benchmark for evaluating imbalanced graph learning through systematic testing of 24 algorithms on 17 datasets, enhancing our understanding and performance comparisons in this field, with resources available for reproducible research at https://github.com/RingBDStack/IGL-Bench.

IMDPrompter: Adapting SAM to Image Manipulation Detection by Cross-View Automated Prompt Learning
The paper explores the application of the Segment Anything Model (SAM) for image manipulation detection, an area where its capabilities are not yet confirmed. By introducing IMDPrompter, a cross-view prompt learning paradigm that eliminates manual prompt requirements and enhances SAM's effectiveness in detecting and localizing image manipulations, the study demonstrates improved performance across various datasets through novel components like Cross-view Feature Perception and Optimal Prompt Selection.

Immunogenicity Prediction with Dual Attention Enables Vaccine Target Selection
VenusVaccine is a novel deep learning solution with a dual attention mechanism designed to improve immunogenicity prediction for reverse vaccinology, overcoming the limitations of existing methods by using pre-trained latent vector representations of protein sequences and structures. This work compiles the largest existing immunogenicity dataset and demonstrates superior performance over current techniques, providing a valuable tool and benchmarks for future vaccine design research.

Improved Regret Bounds for Linear Adversarial MDPs via Linear Optimization
This paper proposes a novel explore-exploit algorithm framework that transforms linear adversarial Markov decision processes (MDPs) into a linear optimization problem by carefully setting the feature maps of the bandit arms. This approach improves the regret bound to $\tilde{\mathcal{O}}(K^{4/5})$, surpassing previous state-of-the-art results, and offers a new perspective that could benefit other MDP problems with linear structures.

Infinite-Resolution Integral Noise Warping for Diffusion Models
This paper introduces an efficient algorithm for adapting pretrained image-based diffusion models to generate temporally consistent videos, addressing the challenge of maintaining both temporal coherence and the Gaussian white noise distribution. By analyzing and optimizing the upsampling-based approach proposed by Chang et al. (2024), the authors develop a method that achieves high-resolution accuracy with significantly reduced computational costs, and demonstrate its applicability to 3-dimensional spaces in real-world scenarios.

Input Space Mode Connectivity in Deep Neural Networks
This paper extends the concept of loss landscape mode connectivity from parameter space to the input space of deep neural networks, presenting both theoretical and empirical evidence. It explores the existence of low-loss paths between different input images, provides insights into adversarial examples, and discusses applications for model interpretability and adversarial detection.

Instructional Segment Embedding: Improving LLM Safety with Instruction Hierarchy
Large Language Models (LLMs) lack an instruction hierarchy, leading to security and safety vulnerabilities like prompt injection and harmful requests. We propose Instructional Segment Embedding (ISE) to embed instruction priority information into LLMs, improving safety against malicious prompts and enhancing instruction-following capability, with notable performance boosts on several benchmarks.

Interpretable Bilingual Multimodal Large Language Model for Diverse Biomedical Tasks
The paper introduces MedRegA, a Region-Aware medical Multimodal Large Language Model designed to enhance region-specific understanding in medical images, improving upon existing region-agnostic models. By utilizing a newly constructed dataset, MedRegInstruct, and tackling region-centric tasks, MedRegA demonstrates superior performance in visual question answering, report generation, and image classification across multiple modalities, enhancing both interpretability and functionality in bilingual medical contexts.

Inverse Constitutional AI: Compressing Preferences into Principles
This paper introduces the Inverse Constitutional AI (ICAI) problem, which involves interpreting pairwise text preference data as a compression task to extract a "constitution" that helps a large language model (LLM) reconstruct the original annotations. The proposed ICAI algorithm is demonstrated to identify biases in human feedback and is validated on various datasets, offering a new tool for understanding model performance and adapting AI models to specific preferences, with the source code available at https://github.com/rdnfn/icai.

Is Large-scale Pretraining the Secret to Good Domain Generalization?
This paper investigates the role of pretraining alignment in Multi-Source Domain Generalization (DG) and introduces the Alignment Hypothesis, suggesting that high DG performance depends on the alignment of image and class label text embeddings. The study confirms this hypothesis by evaluating methods on DomainBed datasets, revealing that current DG methods perform well on data aligned with pretraining but struggle with data outside pretraining, underscoring the need for methods that generalize beyond pretraining alignment.

Is Your Multimodal Language Model Oversensitive to Safe Queries?
This paper finds that advanced Multimodal Large Language Models (MLLMs) can exhibit cognitive distortions, rejecting benign queries when influenced by specific visual stimuli. Through the development of the MOSSBench toolkit, the study reveals that these models show significant oversensitivity, with safer models demonstrating more conservative responses, underscoring the need for refined safety mechanisms to improve model reliability in diverse contexts.

Kernel-based Optimally Weighted Conformal Time-Series Prediction
This paper introduces the Kernel-based Optimally Weighted Conformal Prediction Intervals ($\texttt{KOWCPI}$), a new method for generating prediction intervals in time-series data that adapts the Reweighted Nadaraya-Watson estimator for quantile regression with data-adaptive weights. The method provides a conditional coverage guarantee for non-exchangeable data under strong mixing conditions, and outperforms existing methods by producing narrower confidence intervals while maintaining coverage validity in real time-series applications.

KGARevion: An AI Agent for Knowledge-Intensive Biomedical QA
KGARevion is a knowledge graph-based agent designed to enhance biomedical reasoning by integrating structured and tacit knowledge to answer complex medical queries. It outperforms existing models by improving accuracy across multiple datasets, demonstrating strong zero-shot generalization and applicability in underrepresented medical contexts, including new benchmarks like AfriMed-QA.

Large Language Models Often Say One Thing and Do Another
This paper addresses the reliability of large language models (LLMs) by introducing the Words and Deeds Consistency Test (WDCT), a benchmark designed to evaluate the alignment between LLMs' verbal and actionable outputs across various domains. The findings highlight significant inconsistencies between words and deeds in LLMs, suggesting that the knowledge driving these outputs resides in separate spaces.

Learning Dynamics of Deep Matrix Factorization Beyond the Edge of Stability
This paper presents a fine-grained analysis of the learning dynamics in deep linear networks (DLNs) beyond the edge of stability (EOS), revealing that loss oscillations follow a period-doubling route to chaos. The results explain why simple tasks may not exhibit EOS and highlight that these oscillations occur within top features, supported by experiments and comparisons with nonlinear networks.

Learning from negative feedback, or positive feedback or both
This paper introduces a novel approach to preference optimization that decouples learning from positive and negative feedback, allowing for flexible control over each feedback type and enabling learning with only one feedback type available. By extending expectation-maximization (EM) algorithms to incorporate negative examples, the authors present a theoretically grounded and versatile method for training language models and policy decisions, addressing the limitation of current methods that overlook negative feedback.

Learning High-Degree Parities: The Crucial Role of the Initialization
This paper explores the learnability of full parities using gradient descent on regular neural networks, emphasizing the significance of initial weight distribution. It demonstrates that discrete Rademacher initialization allows efficient learning of full parities, while Gaussian perturbation with sufficiently large standard deviation inhibits learning, highlighting a nuanced dependency tied to initial gradient alignment.

Learning Interpretable Hierarchical Dynamical Systems Models from Time Series Data
This paper introduces a hierarchical framework for dynamical systems reconstruction (DSR) that integrates multi-domain data to improve generalization, especially useful when individual time series are short. The approach not only faithfully reconstructs individual dynamics but also uncovers common low-dimensional feature spaces that aid in the clustering of datasets with similar dynamics, showcasing its applicability through benchmarks in neuroscience and medical data, and illustrating its potential for transfer learning and generalization in new parameter regimes.

Learning mirror maps in policy mirror descent
Policy Mirror Descent (PMD) is widely used in reinforcement learning, yet its effectiveness can vary based on the choice of mirror map, traditionally the negative entropy. This study empirically explores alternative mirror maps using evolutionary strategies, finding more efficient options that improve PMD performance in benchmark environments and generalize well across different tasks.

LLM-SR: Scientific Equation Discovery via Programming with Large Language Models
LLM-SR is a novel approach that uses Large Language Models (LLMs) to enhance symbolic regression techniques for discovering scientific equations efficiently from data, incorporating domain-specific knowledge. This method demonstrates superior performance on benchmark problems across various scientific domains by combining the LLMs' extensive knowledge with evolutionary search to propose and optimize equation hypotheses, outperforming traditional symbolic regression methods, especially in out-of-domain tests.

LLM Unlearning via Loss Adjustment with Only Forget Data
Unlearning in Large Language Models (LLMs) is crucial for ethical AI use, but current methods struggle to balance unlearning with model utility. This paper introduces Forget data only Loss Adjustment (FLAT), a novel approach that eliminates the need for retain data or reference LLMs by using $f$-divergence to improve unlearning performance while preserving model capabilities, as demonstrated on datasets such as Harry Potter and MUSE Benchmark.

Locality-aware Gaussian Compression for Fast and High-quality Rendering
LocoGS introduces a locality-aware 3D Gaussian Splatting framework to efficiently model volumetric scenes by leveraging the spatial coherence of 3D Gaussians, utilizing a novel representation with minimal storage. The framework enhances compression performance with components like dense initialization and adaptive encoding schemes, achieving significantly better rendering quality, compression size, and speed compared to existing methods and state-of-the-art compression techniques.

Longhorn: State Space Models are Amortized Online Learners
This paper presents a novel deep State-Space Model (SSM) architecture designed through the lens of online learning, optimizing it for online regression objectives by using implicit updates. The proposed model outperforms existing state-of-the-art SSMs, including the Mamba model, in standard sequence modeling benchmarks and language tasks, offering a more computationally efficient alternative to Transformers for sequence modeling.

Looking into User’s Long-term Interests through the Lens of Conservative Evidential Learning
This paper introduces a novel evidential conservative Q-learning framework (ECQL) designed to enhance recommendation systems by effectively capturing users' evolving preferences through evidence-based uncertainty and conservative learning. The approach integrates a sequential state encoder and a conservative evidential-actor-critic (CEAC) module, demonstrating state-of-the-art performance on dynamic datasets by uncovering long-term user interests and optimizing recommendations amidst a large item space with sparse rewards.

LoRA Learns Less and Forgets Less
This work compares LoRA (Low-Rank Adaptation) and full finetuning methods on large language models for programming and mathematics domains, showing that while LoRA is less effective than full finetuning, it better preserves the base model's performance on non-target tasks. The study highlights that LoRA mitigates forgetting more effectively than techniques like weight decay and dropout, and identifies discrepancies in rank that may explain LoRA's performance gaps, leading to proposed best practices for its use.

Machine Unlearning Fails to Remove Data Poisoning Attacks
This paper critically evaluates the efficacy of current approximate unlearning methods in deep learning, particularly in mitigating the effects of various data poisoning attacks. Although the research introduces new evaluation metrics for unlearning and suggests these methods might offer efficiency over retraining, it concludes that they currently offer limited practical advantages and require broader evaluations to be reliably useful without false confidence.

MaestroMotif: Skill Design from Artificial Intelligence Feedback
MaestroMotif is an AI-assisted skill design method that utilizes Large Language Models (LLMs) to create and adapt high-performing agents by automatically generating rewards and leveraging code generation for skill training. Evaluated in the NetHack Learning Environment (NLE), MaestroMotif outperforms existing methods in both efficacy and user-friendliness by implementing complex behaviors based on natural language descriptions.

MallowsPO: Fine-Tune Your LLM with Preference Dispersions
Direct Preference Optimization (DPO) enhances reinforcement learning from human feedback but struggles with capturing human preference diversity. Inspired by Mallows' preference ranking theory, MallowsPO introduces a dispersion index to address this issue, improving DPO's performance across various tasks and showing compatibility with state-of-the-art methods for fine-tuning language models like Llama3-Instruct.

MAPS: Advancing Multi-Modal Reasoning in Expert-Level Physical Science
Current Multi-Modal Large Language Models (MLLM) excel in general visual reasoning but struggle with complex physical structures and quantitative analysis in diagrams. The MAPS framework enhances MLLM's scientific reasoning by combining a Physical Perception Model and simulation to improve accuracy in college-level circuit analysis, outperforming existing models.

Masked Diffusion Models are Secretly Time-Agnostic Masked Models and Exploit Inaccurate Categorical Sampling
This paper investigates masked diffusion models (MDMs) in the context of generative modeling for discrete data, revealing that both training and sampling processes are independent of the time variable, making them akin to masked models. It introduces the first-hitting sampler (FHS) that speeds up categorical sampling by 20 times and highlights numerical issues in MDMs that question their superiority over auto-regressive models (ARMs) in text generation, suggesting flaws in previous evaluation metrics.

MaskGCT: Zero-Shot Text-to-Speech with Masked Generative Codec Transformer
The paper presents Masked Generative Codec Transformer (MaskGCT), a fully non-autoregressive text-to-speech (TTS) model that eliminates the need for explicit text-speech alignment and phone-level duration prediction, employing a two-stage approach to generate semantic and acoustic tokens. Experimental results show MaskGCT's superior performance compared to current state-of-the-art zero-shot TTS systems, enhancing quality, similarity, and intelligibility in speech synthesis.

MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code
This paper presents a novel method for enhancing mathematical reasoning in large language models by generating mathematical code with corresponding reasoning steps for continued pretraining. By constructing a high-quality pretraining dataset called MathCode-Pile and improving popular base models, this approach significantly boosts mathematical abilities, and all data processing and training code are open-sourced for transparency and reproducibility.

Matryoshka Multimodal Models
The proposed Matryoshka Multimodal Models address inefficiencies in Large Multimodal Models by representing visual content as nested sets of visual tokens that vary in granularity. This approach allows for flexible control over visual token granularity during inference, emphasizes efficient dataset analysis, and explores optimal trade-offs between performance and token length at the sample level.

MIND: Math Informed syNthetic Dialogues for Pretraining LLMs
This paper introduces the Math Informed syNthetic Dialogue (MIND) generation method to enhance the mathematical reasoning abilities of large language models by generating synthetic conversations based on OpenWebMath to create the MIND-OWM corpus. Pretraining with MIND-OWM significantly boosts mathematical reasoning and specialized knowledge performance compared to using raw data alone, highlighting the importance of restructuring raw data for more effective model training.

Misspecified  $Q$-Learning with Sparse Linear Function Approximation: Tight Bounds on Approximation Error
This paper investigates the possibility of obtaining an $O(\epsilon)$-optimal policy in reinforcement learning when the optimal $Q$-function is a sparse $d$-dimensional linear function with a misspecification error $\epsilon$. It introduces a novel elimination-based algorithm to achieve an $O(H\epsilon)$-optimal policy with sample complexity polynomial in the feature dimension $d$ and planning horizon $H$, providing both upper and lower bounds to give a comprehensive understanding of the problem.

Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models
This paper introduces pFedMoAP, a novel framework for federated prompt learning that enhances the personalization of prompts by allowing clients to download multiple pre-aggregated prompts and use a local attention-based gating network. The method improves text feature generation for better alignment with local image data, showing significant effectiveness across 9 datasets in various federated settings. The code is accessible for further exploration at https://github.com/ljaiverson/pFedMoAP.

MM-EMBED: UNIVERSAL MULTIMODAL RETRIEVAL WITH MULTIMODAL LLMS
This paper introduces universal multimodal retrieval using multimodal large language models (MLLMs) to handle diverse retrieval tasks across multiple modalities, demonstrating that MM-Embed achieves superior performance on multimodal and text retrieval benchmarks. The study addresses modality bias in MLLM retrievers through modality-aware hard negative mining and enhances retrieval capabilities via continuous fine-tuning and zero-shot reranking, setting the stage for future advancements in multimodal retrieval.

MOCA: Self-supervised Representation Learning by Predicting Masked Online Codebook Assignments
The paper presents MOCA, a self-supervised learning method for Vision Transformers that unifies contextual reasoning and invariance to image perturbations through novel mask-and-predict objectives using high-level features. MOCA achieves state-of-the-art results in low-shot settings and various evaluation protocols while being at least three times faster in training compared to previous methods.

Model-Free Offline Reinforcement Learning with Enhanced Robustness
This paper introduces a novel double-pessimism principle in offline reinforcement learning, addressing the trade-off between robustness to model mismatches and scalability to large environments. By proposing a universal, model-free algorithm and providing a sample complexity analysis, the authors demonstrate that their approach enhances robustness efficiently and scalable, as confirmed by extensive experiments.

Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning
This work addresses the gap in egocentric video understanding by introducing HOD, a pipeline that generates detailed narrations of hand-object dynamics using a hand-object detector and a large language model. The proposed EgoVideo model, equipped with a lightweight motion adapter, captures fine-grained motion information, achieving state-of-the-art performance in various tasks and demonstrating strong generalization capabilities in hand-object interaction and robot manipulation tasks.

mPLUG-Owl3: Towards Long Image-Sequence Understanding in Multi-Modal Large Language Models
mPLUG-Owl3 is a versatile multi-modal large language model designed to enhance understanding of long image sequences by integrating vision and language into a semantic space using novel hyper attention blocks. This model achieves competitive performance across 21 benchmarks while significantly reducing inference time and memory usage, and excels in maintaining focus amidst distractions on ultra-long visual sequence inputs.

MuseGNN: Forming Scalable, Convergent GNN Layers that Minimize a Sampling-Based Energy
The paper addresses the scalability challenges of graph neural networks (GNNs) that use graph-regularized energy functions to generate interpretable node embeddings for tasks like node classification. By proposing a sampling-based energy function and scalable GNN layers, the authors design a model that achieves competitive accuracy and scalability on large benchmarks, with code available at the provided GitHub repository.

Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solver
This paper presents rStar, an innovative self-play mutual reasoning technique that significantly enhances the reasoning abilities of small language models without requiring fine-tuning or superior models by using a decoupled generation-discrimination process. Experiments demonstrate that rStar dramatically improves accuracy across multiple reasoning tasks, achieving substantial increases in performance for various models, including boosting GSM8K accuracy for LLaMA2-7B from 12.51% to 63.91%.

Neural Context Flows for Meta-Learning of Dynamical Systems
Neural Context Flow (NCF) is introduced as a Meta-Learning framework designed to enhance the adaptability of Neural Ordinary Differential Equations (NODEs) to new and unobserved dynamic behaviors in physical systems, utilizing uncertainty estimation and Taylor expansion for contextual self-modulation. Empirical tests demonstrate NCF's superior Out-of-Distribution performance on benchmark problems, emphasizing its potential to generalize and adapt NODEs more effectively in scientific applications.

Neuron based Personality Trait Induction in Large Language Models
This paper introduces a neuron-based approach to enhance personality trait simulation in large language models (LLMs), featuring three key contributions. It presents PERSONALITYBENCH, a dataset grounded in the Big Five personality traits for evaluation, proposes a method for identifying personality-related neurons, and offers a method to manipulate these neurons for precise trait control, achieving results comparable to fine-tuned models without altering model parameters.

Node Similarities under Random Projections: Limits and Pathological Cases
This paper investigates the preservation of dot product and cosine similarity in random projections applied to graph matrix rows, providing both asymptotic and finite-sample results. It identifies conditions under which these projections produce unreliable embeddings, especially for dot products in ranking tasks, while showing that cosine similarity yields more precise approximations despite statistical noise.

No Free Lunch: Fundamental Limits of Learning Non-Hallucinating Generative Models
This paper introduces a theoretical framework to analyze the learnability of non-hallucinating generative models, revealing that it is statistically impossible to achieve this without incorporating inductive biases, even with a truthful training set. By constraining the fact set to a concept class of finite VC-dimension, the authors provide a systematic approach to mitigating hallucinations, marking an initial step toward a principled solution in generative model learning.

Noise-conditioned Energy-based Annealed Rewards (NEAR): A Generative Framework for Imitation Learning from Observation
This paper presents Noise-conditioned Energy-based Annealed Rewards (NEAR), an imitation learning framework using energy-based generative models to learn robot motion policies from state-only expert trajectories. NEAR effectively addresses optimization challenges associated with adversarial imitation learning and demonstrates performance comparable to Adversarial Motion Priors in complex humanoid tasks.

Nonlinear multiregion neural dynamics with parametric impulse response communication channels
This paper introduces a multi-region neural dynamics model to understand how coordinated brain regions support distributed computation through within-region nonlinear dynamics and inter-region communication channels. Utilizing a novel variational filtering and learning algorithm, the model effectively reveals the flow of information and local computations in neural populations, validated by recordings from brain areas V1 and V2.

Offline RL with Smooth OOD Generalization in Convex Hull and its Neighborhood
Offline Reinforcement Learning (RL) often faces challenges with distributional shifts, leading to inaccurate $Q$-value estimates for out-of-distribution (OOD) actions. The paper introduces a novel approach, Smooth Q-function OOD Generalization (SQOG), which enhances $Q$-value estimation by smoothing OOD $Q$-values with neighboring in-sample values, demonstrating superior performance and computational efficiency on D4RL benchmarks.

OMG: Opacity Matters in Material Modeling with Gaussian Splatting
This paper presents a novel approach to inverse rendering by incorporating the dependency of opacity on material properties, inspired by radiative transfer, using a neural network to enhance modeling accuracy. By implementing this method into three different Gaussian Splatting baselines, the authors achieve significant improvements in novel view synthesis and material modeling.

On Disentangled Training for Nonlinear Transform in Learned Image Compression
Learned image compression (LIC) struggles with training inefficiency, taking over two weeks to train state-of-the-art models due to energy compaction challenges in learning nonlinear transforms. The paper introduces a linear auxiliary transform (AuxT) to effectively disentangle energy compaction, leading to faster convergence—accelerating LIC model training by two times—and achieving an average 1% BD-rate reduction, showcasing improved rate-distortion performance without sacrificing quality.

On Large Language Model Continual Unlearning
This paper addresses the security challenges of large language models by introducing the \OOO{} framework, which includes an orthogonal low-rank adapter for continual unlearning and an out-of-distribution detector for similarity measurement between inputs and unlearning data. Extensive experiments demonstrate that \OOO{} outperforms existing methods in unlearning effectiveness and utility preservation, particularly amid continuous unlearning requests, without reliance on retained data.

On the Benefits of Attribute-Driven Graph Domain Adaptation
This paper addresses the challenge of Graph Domain Adaptation (GDA) by emphasizing the overlooked importance of graph node attributes in achieving domain alignment, alongside structural shifts. The authors introduce a novel cross-channel module to align node attributes and structure between source and target graphs, demonstrating its efficacy through theoretical proof and empirical results on benchmarks.

On the Inherent Privacy Properties of Discrete Denoising Diffusion Models
This paper explores the privacy-preserving capabilities of discrete diffusion models (DDMs) in generating synthetic datasets, offering the first theoretical insight into their privacy performance. By focusing on per-instance differential privacy, the study establishes privacy leakage bounds and shows how dataset distribution impacts privacy loss, with empirical validation on synthetic and real-world datasets confirming the theoretical results.

On the Learn-to-Optimize Capabilities of Transformers in In-Context Sparse Recovery
This paper demonstrates that Transformers can execute learning-to-optimize (L2O) algorithms, specifically in the context of in-context learning (ICL) for sparse recovery tasks like LASSO, achieving a convergence rate linear in the number of layers. The study illustrates Transformers’ superior ICL abilities—such as generalizing across varying measurement matrices and demonstration pair lengths—beyond the grasp of standard gradient-descent algorithms, supported by both theoretical insights and empirical evidence.

Optimal Non-Asymptotic Rates of Value Iteration for Average-Reward Markov Decision Processes
This paper provides a refined non-asymptotic analysis of average-reward Markov Decision Processes (MDPs), specifically enhancing the understanding of Value Iteration (VI) for these models. The study notably achieves $\mathcal{O}(1/k)$ convergence rates for Anchored Value Iteration regarding Bellman error under various setups and matches this rate with a span-based complexity lower bound, improving analytical benchmarks for weakly communicating and unichain MDPs.

Optimizing $(L_0, L_1)$-Smooth Functions by Gradient Methods
This paper explores gradient methods for optimizing $(L_0, L_1)$-smooth functions, expanding on the traditional Lipschitz-smooth framework, particularly relevant in machine learning. The authors enhance complexity bounds for convex problems and propose effective gradient and accelerated gradient methods that rival approaches contingent on explicit $(L_0, L_1)$ knowledge, showcasing improved outcomes.

PAL: Sample-Efficient Personalized Reward Modeling for Pluralistic Alignment
The paper introduces a pluralistic alignment framework (PAL) that accounts for diverse human preferences in foundation models, rather than assuming a single, homogeneous preference. PAL's modular design effectively personalizes user preferences, yielding superior performance with significantly fewer parameters, and demonstrating substantial improvements over existing methods in both text-to-text and text-to-image tasks.

Parameter-Efficient and Stable Singular Value Adaptation for Pre-Trained Models
This study demonstrates that Q, K, O, and V matrices in attention layers can be decomposed into orthogonal matrices and singular values without loss, allowing for stable fine-tuning by adjusting only the singular values. This method improves performance by 5.4% over LoRA in commonsense reasoning tasks and reduces Whisper-large-v3 encoder parameters by 46.42% without losing information.

Perm: A Parametric Representation for Multi-Style 3D Hair Modeling
Perm introduces a novel parametric representation for 3D hair that separates global and local hair patterns, enhancing precision in editing and control. By decomposing hair textures into distinct frequency domains and parameterizing them with generative models, Perm proves versatile in applications like hair reconstruction and editing, demonstrating its effectiveness across various hair-related tasks.

Physics of Language Models: Part 3.2, Knowledge Manipulation
This paper explores the capability of language models to perform knowledge manipulation tasks such as retrieval, classification, comparison, and inverse search, revealing that they excel in retrieval but struggle with classification, comparison, and inverse search without the aid of Chain of Thoughts (CoTs). The study, conducted through controlled synthetic experiments, highlights a fundamental limitation in language models like GPT-4, which cannot efficiently manipulate pre-stored knowledge, prompting considerations for Turing tests to distinguish human cognition from AI performance.

PhysPDE: Rethinking PDE Discovery and a Physical HYpothesis Selection Benchmark
This paper presents a novel Machine Learning for Science (ML4Sci) paradigm that emphasizes interpreting experimental data using prior physical theories to guide the discovery of PDE expressions, rather than generating mathematical forms without context. By formulating this approach as a nonlinear mixed-integer programming problem and testing it on Fluid Mechanics and Laser Fusion datasets, the study demonstrates the method's feasibility and interpretability in uncovering underlying physical mechanisms.

Plastic Learning with Deep Fourier Features
This paper identifies principles leading to plastic algorithms and demonstrates that linear function approximation and certain deep linear networks maintain plasticity unlike traditional deep neural networks. It introduces deep Fourier features, enhancing trainability and effectiveness in continual learning by replacing ReLU activations, with empirical success across various datasets and scenarios.

PolaFormer: Polarity-aware Linear Attention for Vision Transformers
Linear attention offers a computationally efficient alternative to traditional attention mechanisms but suffers from information loss due to non-negative constraints and approximations. To remedy this, the paper introduces a polarity-aware linear attention mechanism, PolaFormer, which captures both same-signed and opposite-signed query-key interactions and employs a learnable rescaling function, leading to improved performance in vision tasks by up to 4.6%.

Private Mechanism Design via Quantile Estimation
This paper develops computationally efficient frameworks for designing differentially private, revenue-maximizing single item auctions addressing both bounded and unbounded valuation distributions, ensuring pure privacy in polynomial time. It extends these innovations to multi-round online auctions with non-myopic bidders, marking the first Myerson auctions that achieve pure privacy and near-optimal revenue, especially for unbounded distributions.

Problem-Parameter-Free Federated Learning
Federated learning (FL) is challenged by the need for problem-specific hyperparameter tuning, and PAdaMFed addresses this by introducing an adaptive stepsize and momentum method that functions without such specific parameters. The algorithm offers scalable performance with robust theoretical guarantees in convergence and communication complexities, demonstrating its effectiveness through empirical studies on real-world tasks.

Progressive Compositionality in Text-to-Image Generative Models
This paper introduces EvoGen, a multi-stage curriculum framework for improving the compositional understanding of diffusion models in text-to-image synthesis. By leveraging large-language models to create complex scenarios and using Visual-Question Answering checkers to curate a detailed contrastive dataset called ConPair, the approach effectively enhances diffusion models' ability to generalize across various compositional settings, as evidenced by comprehensive testing on compositional T2I benchmarks.

Provence: efficient and robust context pruning for retrieval-augmented generation
Provence is introduced as an efficient and robust context pruner for Question Answering, overcoming the limitations of existing approaches by dynamically detecting and pruning irrelevant context while reranking relevant information. Through sequence labeling and diverse data training, Provence demonstrates negligible performance loss across various domains, effectively enhancing retrieval-augmented generation with minimal computational overhead.

Quality over Quantity in Attention Layers: When Adding More Heads Hurts
This paper demonstrates the critical role of attention rank in determining the representational capacity of attention mechanisms in transformers, challenging the conventional approach of setting hyperparameters like number of attention heads uniformly. Through theoretical proof and experiments, it shows that rank significantly influences an attention layer's power, with full-rank attention being essential for accurate representation, especially for long sequences, thus cautioning against simple head count increases in standard transformers.

Rapidly Adapting Policies to the Real-World via Simulation-Guided Fine-Tuning
The paper introduces the Simulation-Guided Fine-tuning (SGFT) framework, which improves robot learning by using physics simulators to guide real-world exploration, accelerating adaptation with fewer samples. SGFT significantly outperforms existing methods, requiring far less real-world data and succeeding in complex tasks, while providing theoretical justification for this efficient sim-to-real transfer approach.

RAPID: Retrieval Augmented Training of Differentially Private Diffusion Models
RAPID, a Retrieval Augmented PrIvate Diffusion model, addresses the limitations of existing differentially private diffusion models by integrating retrieval augmented generation to enhance generative quality, reduce memory footprint, and lower inference costs. By utilizing public data to inform the training of private data in a differentially private manner, RAPID demonstrates significant improvements over state-of-the-art methods, positioning it as a promising approach for privacy-preserving generative models.

Rapid Selection and Ordering of In-Context Demonstrations via Prompt Embedding Clustering
This paper investigates the sensitivity of in-context learning (ICL) in Large Language Models (LLMs) to the order of demonstrations, uncovering a clustering property in the prompt embedding space related to positional encoding and causal attention masks. Utilizing this insight, the authors propose Cluster-based Search, which significantly reduces demonstration selection time in ICL from factorial to quadratic complexity, achieving substantial time savings while maintaining performance.

Rationalizing and Augmenting Dynamic Graph Neural Networks
Graph data augmentation (GDA) significantly benefits graph neural networks (GNNs), but current static graph methods struggle with dynamic graphs prevalent in real-world applications. This paper introduces $\texttt{DyAug}$, a framework that enhances dynamic GNNs by adaptively augmenting graph structures with temporal consistency, improving performance, robustness to adversarial attacks, and prediction stability across temporal distribution shifts.

RA-TTA: Retrieval-Augmented Test-Time Adaptation for Vision-Language Models
The paper introduces Retrieval-Augmented-TTA (RA-TTA), a method for test-time adaptation of vision-language models (VLMs) that utilizes external knowledge from a web-scale image database to address the limitations of existing methods that rely only on the internal model knowledge. RA-TTA improves predictions by adaptively retrieving relevant external images and fine-grained text descriptions, leading to a performance increase of 3.01-9.63% on average across 17 datasets compared to current state-of-the-art methods.

RB-Modulation: Training-Free Stylization using Reference-Based Modulation
RB-Modulation is a novel training-free approach for personalizing diffusion models, addressing challenges like style extraction without additional descriptions, content leakage, and effective style-content composition. By using a stochastic optimal controller and cross-attention-based feature aggregation, it allows precise and seamless style and content manipulation without relying on external adapters or ControlNets.

RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation
The paper introduces the Robotics Diffusion Transformer (RDT), a novel diffusion foundation model designed to tackle the complexity of bimanual manipulation in robotics by incorporating a scalable Transformer to handle multi-modal inputs and a Physically Interpretable Unified Action Space to combat data scarcity. RDT, pre-trained on the most extensive multi-robot datasets and refined on a new multi-task bimanual dataset, significantly surpasses existing models, demonstrating impressive capabilities such as zero-shot generalization, language understanding, and learning new skills with minimal demonstrations.

Reasoning with Latent Thoughts: On the Power of Looped Transformers
This paper argues that large depth, rather than sheer parameter count, is critical for solving many reasoning problems, introducing looped models as an efficient alternative. The authors demonstrate that looped transformer models can handle various reasoning tasks as effectively as traditional large-scale models, reveal a scaling behavior tied to effective depth, and propose a looping-based regularization that benefits both reasoning and memorization.

Recovering Manifold Structure Using Ollivier Ricci Curvature
ORC-ManL is a new algorithm designed to prune spurious edges in nearest neighbor graphs by leveraging Ollivier-Ricci curvature and metric distortion, particularly effective when data arises from noisy samples of a low-dimensional manifold. Demonstrating superior performance to existing methods, ORC-ManL enhances various geometric data analysis tasks, including manifold learning and clustering, notably improving the analysis of single-cell RNA sequencing data.

Redefining the task of Bioactivity Prediction
This paper addresses the shortcomings in current machine learning models for predicting the bioactivity of small molecules against protein targets, which often result from insufficient data and flawed evaluation processes. By introducing the SIU dataset—a significantly larger and more reliable dataset—the authors offer a new, unbiased benchmark that allows for a more accurate and meaningful assessment of bioactivity prediction models, using refined evaluation metrics and multiple small molecules per protein pocket.

Repulsive Latent Score Distillation for Solving Inverse Problems
This paper addresses the challenges of mode collapse and latent space inversion in Score Distillation Sampling (SDS) for high-dimensional data by introducing a novel variational framework for posterior sampling. By implementing a repulsion mechanism to promote diversity and an augmented variational distribution to disentangle latent spaces, the authors enhance solution diversity and clarity, as demonstrated through extensive experiments on high-resolution inverse tasks using pre-trained Stable Diffusion models.

Rethinking and Improving Autoformalization: Towards a Faithful Metric and a Dependency Retrieval-based Approach
This paper addresses two under-explored challenges in statement autoformalization: the lack of comprehensive evaluation methods and the neglect of contextual information leading to inaccuracies. It introduces BEq, a neuro-symbolic method for determining statement equivalence, and RAutoformalizer, which improves autoformalization using dependency retrieval from formal libraries, demonstrating significant accuracy improvements in evaluations and establishing a new benchmark, Con-NF, for out-of-distribution generalization capabilities.

Rethinking Reward Modeling in Preference-based Large Language Model Alignment
The paper investigates the application of the Bradley-Terry (BT) model for reward modeling in Large Language Model alignment, uncovering its convergence properties when using deep neural network embeddings. It challenges the necessity of the BT model by introducing the concept of order consistency and proposing an alternative upper-bound algorithm using binary classifiers, supported by extensive empirical evaluations across multiple setups and models.

Rethinking Spiking Neural Networks from an Ensemble Learning Perspective
This paper addresses performance limitations in spiking neural networks (SNNs) by treating them as ensembles of temporal subnetworks and identifying excessive differences in initial neuronal states as a key issue. By implementing membrane potential smoothing and subnetwork guidance, the authors enhance stability and performance across various recognition tasks without altering network structures, achieving notable accuracy improvements such as 83.20% on the CIFAR10-DVS dataset with minimal timestep requirements.

RetroInText: A Multimodal Large Language Model Enhanced Framework for Retrosynthetic Planning via In-Context Representation Learning
RetroInText is an innovative end-to-end framework utilizing a multimodal Large Language Model (LLM) that integrates in-context learning with detailed TEXT descriptions of synthetic routes to improve retrosynthetic planning. By leveraging multi-modal representations and an attention-based mechanism, RetroInText surpasses state-of-the-art methods in the USPTO pathways dataset, enhancing Top-1 test accuracy by up to 5% and thereby advancing the pathway design in organic chemistry.

Revisiting Mode Connectivity in Neural Networks with Bezier Surface
This paper extends the concept of mode connectivity from curves to surfaces, introducing a novel optimization technique that discovers Bézier surfaces with low-loss and high-accuracy paths connecting multiple neural networks nonlinearly. The method enriches the understanding of neural network loss landscapes and shows promise for enhancing model performance on datasets like CIFAR-10, CIFAR-100, and Tiny-ImageNet with architectures such as VGG16, ResNet18, and ViT.

Revolutionizing EMCCD Denoising through a Novel Physics-Based Learning Framework for Noise Modeling
This paper introduces a novel approach to denoising images produced by electron-multiplying charge-coupled devices (EMCCDs) by developing a physics-based noise model tailored specifically for EMCCDs. By accurately estimating noise components and utilizing them to generate training samples for neural networks, the study achieves significant improvements in noise reduction for EMCCD images, as demonstrated through the creation and benchmarking of a new real-world test image dataset.

RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style
The paper introduces RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content changes and resistance to style biases, which are critical for techniques like Reinforcement Learning from Human Feedback. Experiments reveal that even state-of-the-art reward models perform poorly on RM-Bench, highlighting significant opportunities for improvement in aligning language models effectively.

Robots Pre-train Robots: Manipulation-Centric Robotic Representation from Large-Scale Robot Datasets
The paper introduces Manipulation Centric Representation (MCR), a novel framework that enhances robot learning by pre-training visual representations with a focus on both visual features and dynamics information of manipulation tasks. MCR outperforms previous methods significantly, improving success rates in both simulated and real-world robotic manipulation tasks by incorporating action prediction and a time contrastive loss.

Rotated Runtime Smooth: Training-Free Activation Smoother for accurate INT4 inference
This paper addresses the challenges of serving large language models by introducing Rotated Runtime Smooth (RRS), a novel activation smoothing method for quantization that reduces service costs and latency. By handling channel-wise and spike outliers in activation data, the proposed method significantly improves performance and accuracy, achieving a notable reduction in WikiText-2 perplexity for INT4 inference compared to existing methods.

RTop-K: Ultra-Fast Row-Wise Top-K Selection for Neural Network Acceleration on GPUs
This paper introduces RTop-K, a parallel row-wise top-k selection algorithm optimized for GPUs using a binary search-based approach, designed to enhance efficiency in applications such as neural network model training. RTop-K shows significant performance improvements, with speed-ups of 4.25× to 9.51× over existing implementations and effectively accelerates the training of MaxK-GNNs by 9.76% to 31.53%.

Safety Layers in Aligned Large Language Models: The Key to LLM Security
This paper identifies "safety layers" within aligned LLMs essential for distinguishing between malicious and normal queries and presents a novel fine-tuning approach, Safely Partial-Parameter Fine-Tuning (SPPFT), which stabilizes these layers to maintain security. The proposed method effectively preserves model security against fine-tuning attacks while improving computational efficiency.

Samba: Synchronized Set-of-Sequences Modeling for Multiple Object Tracking
Multiple object tracking in complex scenarios, like dance or sports, is challenging due to coordinated movements, occlusions, and trajectory dependencies. This paper introduces Samba and SambaMOTR, which jointly process tracklets with long-term memory synchronization to address these issues, achieving superior performance on datasets such as DanceTrack and SportsMOT without relying on manual heuristics.

SAM-CP: Marrying SAM with Composable Prompts for Versatile Segmentation
This paper introduces SAM-CP, an approach improving the Segment Anything model (SAM) for semantic-aware segmentation through composable prompts that assess alignment and instance belonging of SAM patches to text labels. SAM-CP achieves advanced segmentation capabilities in both open and closed domains, demonstrating state-of-the-art performance in open-vocabulary segmentation, and enhances vision foundation models with multi-grained semantic perception abilities.

Scalable Influence and Fact Tracing for Large Language Model Pretraining
This paper refines gradient-based training data attribution methods to effectively attribute model outputs to specific examples at the scale of large language models (LLMs), successfully identifying influential examples from a massive pretraining corpus without subsampling or pre-filtering. Although their method excels in identifying influential examples for model predictions, classical methods still outperform in retrieving passages with explicit relevant facts, highlighting a misalignment between factual attribution and causal influence, which aligns more closely as model size increases.

Scale-aware Recognition in Satellite Images under Resource Constraints
This paper introduces a novel system for scale-aware recognition in satellite imagery, addressing the challenges of optimal resolution selection for recognizing specific features and minimizing the acquisition of expensive high-resolution images. By distilling knowledge from high-resolution to low-resolution models, utilizing a sampling strategy based on model disagreement, and employing an LLM-based concept scale inference, the approach achieves up to a 26.3% improvement in recognition accuracy while reducing the usage of high-resolution images by 76.3%.

Scaling Laws for Adversarial Attacks on Language Model Activations and Tokens
This paper investigates adversarial attacks on language models, focusing on manipulating model activations to control predictions of future tokens, and establishes a linear scaling law for attack susceptibility. The study finds that attacks on activations are more potent than those on input tokens, revealing a stable attack resistance across models and suggesting that adversarial vulnerabilities arise from dimensionality mismatches between input and output spaces, with implications for improving multi-modal and retrieval models.

SEAL: Safety-enhanced Aligned LLM Fine-tuning via Bilevel Data Selection
This paper introduces SEAL, a novel framework designed to enhance the safety of fine-tuning Large Language Models by ranking data based on safety and quality using bilevel optimization. Models utilizing SEAL show improved performance over traditional methods, achieving significant win rate increases on Llama-3-8b-Instruct and Merlinite-7b models compared to random selection.

SelectFormer: Private and Practical Data Selection for Transformers
This paper addresses the challenge of private data selection in a free data market by proposing a practical method for evaluating Transformer-based models over Multi-Party Computation (MPC), which was previously deemed prohibitively expensive. The authors introduce a new pipeline, leverage low-dimension MLPs to emulate high-dimensional nonlinear operators, and implement a parallel, multiphase scheduling approach, resulting in significantly reduced evaluation time with minimal accuracy loss on diverse benchmarks.

Self-Evolving Multi-Agent Collaboration Networks for Software Development
EvoMAC is a novel self-evolving paradigm for LLM-driven multi-agent collaboration (MAC) systems that enhances adaptability for software development by utilizing text-based environmental feedback and a unique textual backpropagation method. Additionally, the RSD-Bench benchmark, designed for requirement-oriented software development, demonstrates through experiments that EvoMAC surpasses previous state-of-the-art methods in both function and software-level coding tasks.

Self-Supervised Diffusion Models for Electron-Aware Molecular Representation Learning
This paper introduces an efficient knowledge transfer method for electron-aware molecular representation learning by utilizing a self-supervised diffusion approach to estimate electron-level information without relying on costly quantum mechanical calculations. The proposed method significantly improves prediction accuracy for molecular properties on large-scale real-world datasets, surpassing existing models.

Self-supervised Monocular Depth Estimation Robust to Reflective Surface Leveraged by Triplet Mining
This paper addresses the challenge of self-supervised monocular depth estimation (SSMDE) on reflective surfaces by introducing a novel training strategy that utilizes triplet mining to identify reflective regions at the pixel level. By incorporating a reflection-aware triplet mining loss and a reflection-aware knowledge distillation method, the approach improves depth accuracy on reflective surfaces, outperforming existing state-of-the-art SSMDE methods across various datasets.

Semi-Parametric Retrieval via Binary Bag-of-Tokens Index
This paper presents SemI-parametric Disentangled Retrieval (SiDR), a bi-encoder retrieval framework designed to enhance efficiency, reduce costs, and maintain parameter-agnostic indexing in information retrieval systems. SiDR surpasses existing neural and term-based baselines by offering a flexible embedding-based index that maintains performance with low training complexity, and a non-parametric tokenization index that reduces costs while outperforming traditional term-based approaches like BM25 across multiple retrieval benchmarks.

ShortcutsBench: A Large-Scale Real-world Benchmark for API-based Agents
This paper introduces ShortcutsBench, a comprehensive benchmark designed to evaluate the capabilities of API-based agents in solving complex real-world tasks using actual APIs from Apple Inc. The study reveals significant limitations in existing large language models integrated with APIs, particularly in handling complex queries involving API selection and parameter input, thus showcasing the challenges these agents face in real-world application scenarios.

SINGER: Stochastic Network Graph Evolving Operator for High Dimensional PDEs
The paper introduces a framework called StochastIc Network Graph Evolving operatoR (SINGER) for learning the evolution operator of high-dimensional PDEs by leveraging a graph neural network to stochastically evolve sub-network parameters over time. SINGER demonstrates superior performance and generalization across various evolution PDEs, surpassing existing methods in almost all tested scenarios while ensuring desirable properties like topology, semigroup, and stability.

SoundCTM: Unifying Score-based and Consistency Models for Full-band Text-to-Sound Generation
Sound Consistency Trajectory Models (SoundCTM) are introduced to address the challenge of slow inference speeds in high-quality Text-to-Sound (T2S) generative models, enabling efficient trial-and-error for creators through flexible transitions between $1$-step and multi-step deterministic sampling. This model reframes the CTM training framework for sound generation, introducing a novel feature distance and $\nu$-sampling algorithm, culminating in SoundCTM-DiT-1B, the first large-scale model in the sound community to deliver high-quality sound generation at full-bandwidth.

Speech Robust Bench: A Robustness Benchmark For Speech Recognition
The paper introduces Speech Robust Bench (SRB), a benchmark designed to assess the robustness of Automatic Speech Recognition (ASR) models against 114 diverse input corruptions encountered in real-world scenarios. Evaluating various state-of-the-art ASR models, the study highlights model size, discrete representations, and self-training as factors influencing robustness, and identifies disparities in robustness across demographic subgroups, aiding future research in creating more robust ASR systems.

Spreading Out-of-Distribution Detection on Graphs
This paper introduces the task of spreading out-of-distribution (OOD) detection on graphs, modeling the interaction and propagation of OOD nodes to neighboring nodes using epidemic models. The authors present a novel energy distribution-based detector (EDBD) that excels in accurately detecting spreading OOD nodes, showcasing superior performance over existing methods in experiments conducted on multiple benchmark datasets, including a newly developed "Spreading COVID-19" dataset.

Statistical Tractability of Off-policy Evaluation of History-dependent Policies in POMDPs
This paper examines the challenges of off-policy evaluation (OPE) in partially observable Markov decision processes (POMDPs) with large observation spaces, focusing on history-dependent policies. It highlights the information-theoretic difficulties of model-free OPE for such policies and demonstrates that a straightforward model-based algorithm can overcome some of these challenges, establishing a clear distinction between model-free and model-based approaches in POMDPs.

Streaming Video Understanding and Multi-round Interaction with Memory-enhanced Knowledge
StreamChat is a training-free framework designed to improve streaming video reasoning and conversational interaction by efficiently processing extended video sequences through a hierarchical memory system. By introducing StreamBench for evaluation, StreamChat demonstrates superior performance in accuracy and response times compared to existing models, addressing challenges in long sequence processing, multi-turn dialogues, and adaptation to dynamic real-world scenarios.

Streamlining Prediction in Bayesian Deep Learning
This paper introduces a method to streamline predictions in Bayesian deep learning by using a single forward pass without sampling, leveraging local linearisation of activation functions and local Gaussian approximations. The proposed approach, applicable to models like MLPs and transformers, allows analytical computation of the posterior predictive distribution and demonstrates improved performance on various tasks, with code available at the linked open-source library.

SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
The paper introduces SVD-LLM, a novel SVD-based post-training compression method for Large Language Models that solves the shortcomings of existing methods by incorporating a truncation-aware data whitening technique and sequential low-rank approximation for parameter updates. Evaluated across multiple datasets and model families, SVD-LLM shows superior performance in achieving high compression rates with lower accuracy loss compared to current state-of-the-art methods.

SV-RAG: LoRA-Contextualizing Adaptation of  MLLMs for Long Document Understanding
Multimodal large language models (MLLMs) have improved in text-rich image understanding but still face challenges with complex, multi-page visually-rich documents. To address this, the proposed Self-Visual Retrieval-Augmented Generation (SV-RAG) framework enables any MLLM to efficiently understand long documents by retrieving relevant pages for question answering, achieving state-of-the-art performance in empirical evaluations.

Synthetic continued pretraining
This paper addresses the challenge of data-inefficient knowledge acquisition in language models when adapting to small, domain-specific datasets. By introducing EntiGraph, a synthetic data augmentation algorithm, the authors demonstrate improved model performance through synthetic continued pretraining, allowing language models to effectively answer questions and follow instructions related to source documents, whether or not those documents are available during inference.

SysCaps: Language Interfaces for Simulation Surrogates of Complex Systems
This paper presents "system captions" (SysCaps), a novel method that employs language descriptions to enhance the accessibility and accuracy of surrogate models in complex energy system predictions. By integrating a multimodal text and timeseries regression model with large language models (LLMs) for caption generation, the approach demonstrates improved prediction accuracy and generalization in simulations of buildings and wind farms, while also enabling new design exploration capabilities through natural language interaction.

Test-time Adaptation for Image Compression with Distribution Regularization
This paper addresses the limitations of current test- or compression-time adaptation image compression approaches by developing an advanced latent refinement method for cross-domain tasks. By introducing a Bayesian approximation-endowed distribution regularization, the authors enhance the rate-distortion performance and offer a flexible solution that can be integrated into existing methods, improving results across multiple datasets.

The AdEMAMix Optimizer: Better, Faster, Older
This paper introduces AdEMAMix, a modification of the Adam optimizer utilizing a mixture of two Exponential Moving Averages (EMAs) to better leverage past gradients in optimization. Experiments demonstrate that AdEMAMix accelerates convergence and decreases minima, outperforming traditional methods in language modeling and image classification, and suggesting the potential for further exploration of alternate gradient aggregation functions.

Theory, Analysis, and Best Practices for Sigmoid Self-Attention
This paper revisits sigmoid attention in transformers, proving their capability as universal function approximators with improved regularity, compared to traditional softmax attention. Introducing FLASHSIGMOID, a memory-efficient implementation, the study demonstrates improved performance and stability in training models with sigmoid attention across various domains, achieving a 17% inference speed-up on H100 GPUs and matching the performance of softmax attention.

The Same but Different: Structural Similarities and Differences in Multilingual Language Modeling
This paper investigates whether the internal structures of large language models (LLMs) align with the morphosyntactic structures of the languages they are trained on. Through a study of English and Chinese models, the authors demonstrate that LLMs use shared circuitry for similar syntactic processes across languages, while also utilizing language-specific components for unique linguistic processes, highlighting the balance LLMs maintain between commonality and language specificity in multilingual contexts.

The Value of Sensory Information to a Robot
This study investigates the value of sensory information in decision-making agents by analyzing their performance when state observations are withheld during task execution. The findings reveal that sensory input is often not critical in many tasks, with task dynamics and agent proficiency influencing the reliance on such information, ultimately guiding the development of resource-efficient agents.

Think Twice Before Claiming Your Optimization Algorithm Outperformance - Review and Beyond
This paper revisits the convergence analysis of first-order algorithms for minimization and minimax optimization issues, analyzing upper and lower bound results within the classical oracle model framework. It identifies research gaps and examines recent works that have redefined classical optimization algorithm settings, particularly in the contexts of machine learning and operations research.

TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights
This paper introduces Token-level Importance Sampling Direct Preference Optimization (TIS-DPO) to improve the preference alignment of Large Language Models (LLMs) by assigning importance weights to tokens based on their reward, addressing inefficiencies in existing methods that treat entire responses as single units. Through three contrasting methods to estimate token importance and experimental validation, TIS-DPO is shown to enhance LLM performance in tasks such as harmlessness, helpfulness alignment, and summarization, while also providing insights into significant token positions.

T-JEPA: Augmentation-Free Self-Supervised Learning for Tabular Data
This paper introduces T-JEPA, an augmentation-free self-supervised learning method for tabular data that leverages a Joint Embedding Predictive Architecture to predict latent representations between subsets of features within the same sample. The proposed method improves classification and regression performance, often surpassing traditional models like Gradient Boosted Decision Trees, and highlights the discovery of relevant features for downstream tasks without using labels.

ToddlerDiffusion: Interactive Structured Image Generation with Cascaded Schrödinger Bridge
This paper introduces a new approach, {\papernameAbbrev}, that extends diffusion models into modality space for RGB image generation by decomposing the process into simpler, interpretable stages like contours and textures, using Schrödinger Bridge for optimal transport. The proposed method surpasses Stable-Diffusion in performance with enhanced editing capabilities, faster convergence, and smaller, more efficient architecture, as demonstrated across diverse datasets such as LSUN-Churches and ImageNet.

TopoLM: brain-like spatio-functional organization in a topographic language model
This paper introduces TopoLM, a transformer language model with a two-dimensional spatial representation, which mimics the brain's language system organization by forming semantically interpretable clusters through a combination of next-token prediction and spatial smoothness loss. The model demonstrates the potential neural basis for the spatial organization of language processing in the human cortex, predicting both the emergence of this structure and the functional clusters observed in human studies.

Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment
This paper introduces Condition Contrastive Alignment (CCA) as a method to enhance guidance-free autoregressive (AR) visual generation, addressing design inconsistencies caused by Classifier-Free Guidance (CFG) in multi-modal tasks. CCA allows pretrained models to achieve the ideal sampling distribution through minimal fine-tuning, reducing sampling costs while maintaining a balance between sample diversity and fidelity, thus bridging language model alignment and visual guidance methods.

Towards a Unified and Verified Understanding of Group-Operation Networks
This paper investigates the internals of one-hidden-layer neural networks trained on the binary operation of finite groups, providing a unified and more complete description that reveals previously unidentified structures. By focusing on the symmetric group S5, the authors present an explanation that guarantees 95% accuracy for 45% of the models and runs three times faster than brute-force methods, outperforming previous works in deriving nontrivial accuracy bounds.

Towards Bridging Generalization and Expressivity of Graph Neural Networks
This paper investigates the relationship between expressivity and generalization in graph neural networks (GNNs), challenging the notion that highly expressive models necessarily overfit. By introducing a novel $k$-variance margin-based framework, the study provides a theoretical and empirical basis for how expressivity can enhance generalization, applicable across various GNN architectures.

Towards Generalizable Reinforcement Learning via Causality-Guided Self-Adaptive Representations
This paper introduces a causality-guided self-adaptive representation-based approach (CSR) to enhance generalization in reinforcement learning by adapting to changes in both distribution and environment spaces. CSR employs causal representation learning to identify latent causal variables and adapt using a three-step strategy, outperforming existing methods in various scenarios like CartPole and Atari games.

Towards Homogeneous Lexical Tone Decoding from Heterogeneous Intracranial Recordings
The paper presents Homogeneity-Heterogeneity Disentangled Learning for Neural Representations (H2DiLR), a new framework aimed at overcoming the limitations of subject-specific models in decoding lexical tones from intracranial recordings. By effectively disentangling and learning the homogeneity and heterogeneity from multiple subjects' data, H2DiLR significantly enhances the performance of brain tone decoding, demonstrating its potential in restoring communication abilities in speech-impaired tonal language speakers.

Towards Improving Exploration through Sibling Augmented GFlowNets
This paper introduces Sibling Augmented Generative Flow Networks (SA-GFN), a novel framework that enhances exploration and training efficiency in Generative Flow Networks by employing a decoupled dual network architecture with intrinsic rewards. Extensive experiments reveal that SA-GFN significantly improves exploration efficacy and convergence speed across various tasks and reward structures, highlighting its broad applicability and compatibility with different GFlowNet training objectives.

Towards more rigorous evaluations of language models
This paper emphasizes the importance of incorporating fundamental statistical principles in the evaluation of language models to ensure more reliable and meaningful results. Using a recent paper as a case study, it aims to guide researchers in adopting rigorous statistical analysis methods to better understand the capabilities and limitations of language models.

Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control
This paper addresses the challenge of understanding the disentanglement of model activations into human-interpretable features by evaluating sparse autoencoders (SAEs) in comparison to supervised feature dictionaries for specific tasks. Through a case study on indirect object identification using GPT-2 Small, the authors show that SAEs can capture interpretable features and that recent variants like Gated and Top-K SAEs perform competitively with supervised methods in terms of feature disentanglement and task control.

TPO: Aligning Large Language Models with Multi-branch & Multi-step Preference Trees
This paper introduces Tree Preference Optimization (TPO), an innovative approach that enhances the preference learning of large language models by leveraging the entire preference tree during fine-tuning, rather than relying on binary preference optimization. Specifically, TPO outperforms the Direct Preference Optimization (DPO) algorithm in various mathematical reasoning tasks by framing language model alignment as a Preference List Ranking problem and employing Adaptive Step Reward to judiciously adjust reward values, showing superior performance across multiple models and datasets.

Transformer Encoder Satisfiability: Complexity and Impact on Formal Reasoning
This paper examines the complexity of the satisfiability (trSAT) problem for transformer encoders (TE), revealing that it is generally undecidable but becomes decidable with quantized TEs due to limited attention capabilities. The authors further classify the complexity of trSAT, showing it can be as challenging as NEXPTIME-hard or solvable in NEXPTIME depending on specific scenarios, contributing to the understanding of formal reasoning applications.

Transformers Learn to Implement Multi-step Gradient Descent with Chain of Thought
This paper examines how Chain of Thought (CoT) prompting enhances the training dynamics of transformers, particularly for in-context weight prediction in linear regression tasks. The findings reveal that while a linear transformer without CoT is limited to single-step gradient descent and fails to recover the ground-truth weights, integrating CoT allows for multi-step gradient descent, resulting in near-exact recovery and improved generalization on unseen data.

Tree of Attributes Prompt Learning for Vision-Language Models
This paper introduces the Tree of Attributes Prompt learning (TAP) method, which enhances vision language models by leveraging structured knowledge graphs with a "concept - attribute - description" structure instead of just category names. By using vision-conditional pooling to address misalignment, TAP outperforms existing methods in zero-shot generalization, cross-dataset transfer, and few-shot classification across multiple datasets.

TSVD: Bridging Theory and Practice in Continual Learning with Pre-trained Models
The paper introduces a continual learning (CL) method called TSVD, which bridges the gap between practical performance and theoretical guarantees. By continuously truncating the singular value decomposition of lifted features, TSVD enhances stability, handles numerous tasks effectively, and surpasses current state-of-the-art methods, all while maintaining low training and generalization errors with proven theoretical support.

Uncertainty modeling for fine-tuned implicit functions
This paper introduces Dropsembles, a novel method for uncertainty estimation in implicit functions such as Neural Radiance Fields, by leveraging large, noise-free synthetic datasets as shape priors. The method is validated through experiments that demonstrate its efficacy in achieving the accuracy and calibration levels of deep ensembles, but with reduced computational cost, particularly when applied to low-resolution MRI segmentations of the lumbar spine.

Uncovering Latent Memories in Large Language Models
This paper investigates the issue of data privacy in language models, specifically focusing on "latent memorization," where sensitive sequences presented only once during training are memorized and later uncovered even without further exposure. It highlights the privacy risks posed by these uncovered sequences and presents a diagnostic test using cross-entropy loss to identify them, demonstrating the vulnerability of such data to being recoverable through random weight perturbations.

Uni$^2$Det: Unified and Universal Framework for Prompt-Guided Multi-dataset 3D Detection
Uni$^2$Det is a framework designed for unified multi-dataset training in 3D detection, which improves performance across diverse domains and enhances generalization to unseen domains. By introducing multi-stage prompting modules to address data distribution and taxonomy variations, our method achieves superior results in multi-dataset setups and zero-shot cross-dataset transfer, as demonstrated on KITTI, Waymo, and nuScenes datasets.

Union-over-Intersections: Object Detection beyond Winner-Takes-All
This paper introduces a novel approach to enhance box localization in object detection by regressing proposals only to their intersections with ground truth, instead of the entire object, thereby improving localization accuracy. It replaces the traditional winner-takes-all strategy with a method that considers the union of all regressed intersections, seamlessly integrating into multiple detection architectures to enhance both object detection and instance segmentation.

Unleashing the Potential of Vision-Language Pre-Training for 3D Zero-Shot Lesion Segmentation via Mask-Attribute Alignment
Malenia is introduced as a novel framework for 3D zero-shot lesion segmentation in CT scans, addressing the challenge of aligning image and text knowledge for unseen lesions. By improving compatibility between mask representations and lesion attributes, and employing a Cross-Modal Knowledge Injection module, Malenia demonstrates superior performance in experiments across three datasets and 12 lesion categories.

Unlocking Guidance for Discrete State-Space Diffusion and Flow Models
This paper introduces "Discrete Guidance," a method for applying guidance to generative models on discrete state-spaces by utilizing continuous-time Markov processes, enhancing the ability to sample from desired guided distributions. The method's effectiveness is demonstrated through applications in generating small molecules, DNA sequences, and protein sequences, expanding the toolset for controllable and flexible sample generation in discrete domains.

Valid Conformal Prediction for Dynamic GNNs
This paper introduces a method for making valid predictions in dynamic graphs using conformal prediction and an unfolding representation, compatible with standard graph neural networks without requiring changes to their architecture. Through mathematical analysis, the authors identify different inference scenarios and demonstrate their method's effectiveness and robustness in real-world data applications, even in complex settings with minimal assumptions.

Vector-ICL: In-context Learning with Continuous Vector Representations
This paper investigates the extension of large language models' (LLMs) in-context learning capabilities to continuous vectors from diverse domains by using lightweight projectors to align input data with the LLM's embedding space, a process termed Vector-ICL. The study demonstrates that LLMs, through pretraining projectors with general language objectives and task-specific finetuning, can outperform both few-shot ICL and domain-specific models across various tasks, highlighting the potential of LLMs to process vector representations beyond traditional methods.

VibeCheck: Discover and Quantify Qualitative Differences in Large Language Models
Large language models (LLMs) often have distinctive "vibes" such as tone and style that influence user preferences but are not easily quantifiable. We introduce VibeCheck, a system that automatically identifies and evaluates these vibes, showing alignment with human discovery and predicting model identity and user preferences based on distinctive traits, while providing insights into model behavior across various tasks.

ViBiDSampler: Enhancing Video Interpolation Using Bidirectional Diffusion Sampler
This paper introduces a novel bidirectional sampling strategy for image-to-video diffusion models to enhance two-frame conditioned video generation, addressing off-manifold issues without extensive re-noising. By employing sequential sampling with advanced guidance techniques, the method achieves state-of-the-art performance in generating high-quality, smooth videos between keyframes efficiently, capable of interpolating 25 frames at 1024×576 resolution in 195 seconds on a single 3090 GPU.

Video Action Differencing
This paper introduces Video Action Differencing (VidDiff), a task focused on identifying subtle differences between videos of the same action, supported by VidDiffBench, a benchmark dataset with detailed human annotations. The study identifies challenges in localizing sub-actions and fine-grained frame comparison, proposing the VidDiff method with a three-stage workflow to address these issues, and releases the benchmark and code to encourage further research.

Video In-context Learning: Autoregressive Transformers are Zero-Shot Video Imitators
This paper investigates the use of visual signals, specifically videos, as an interface for models to interact with the environment by training autoregressive Transformers on video datasets using a self-supervised objective. The study demonstrates that these models possess a zero-shot capability to infer and imitate new tasks from demonstration videos without additional fine-tuning, validated through various evaluation metrics showing high-quality video generation aligned with the semantics of the provided demonstrations.

VideoPhy: Evaluating Physical Commonsense for Video Generation
VideoPhy is a benchmark designed to evaluate whether text-to-video generative models, such as CogVideoX-5B, Lumiere, and Dream Machine, faithfully follow physical commonsense in generating videos from text prompts involving interactions like solid-solid or solid-fluid. The study finds that current models, even the best performing ones, significantly struggle to produce videos that adhere to physical laws, achieving accuracy only 39.6% of the time, and introduces an auto-evaluator, VideoCon-Physics, to assist in reliable performance assessment of these models.

ViDiT-Q: Efficient and Accurate Quantization of Diffusion Transformers for Image and Video Generation
Diffusion transformers excel in generating visual content but are limited by computational and memory demands, challenging their deployment on edge devices. This paper introduces ViDiT-Q, a quantization scheme specifically for diffusion transformers, that significantly reduces memory usage and improves processing speeds with minimal impact on visual quality.

Visual Agents as Fast and Slow Thinkers
The paper introduces \textsc{FaST}, a novel system that incorporates fast and slow thinking mechanisms into visual agents, allowing dynamic selection between System 1 and System 2 modes based on task complexity. \textsc{FaST} demonstrates superior performance in visual intelligence tasks by exhibiting flexible, hierarchical reasoning and a transparent decision-making process, outperforming existing baselines in empirical tests.

What Makes Large Language Models Reason in (Multi-Turn) Code Generation?
Prompting techniques, particularly chain-of-thought, are well-regarded for enhancing large language models (LLMs), yet their mechanics in code generation lack thorough exploration. This study systematically examines various prompting strategies, focusing on automatic re-prompting and computational needs, revealing strategies that enhance performance across different models, and demonstrating how optimal configurations can improve multi-turn code generation through finetuning.

Why RoPE Struggles to Maintain Long-Term Decay in Long Sequences?
RoPE enhances traditional positional encodings but faces challenges with long-term decay in contexts longer than its training, which affects the model's ability to generalize to extended sequences. Our study indicates this problem may arise from a predominance of obtuse angles between the linear transformations of query and key embeddings on the complex plane.

XAIguiFormer: explainable artificial intelligence guided transformer for brain disorder identification
The paper introduces the XAI guided transformer (XAIguiFormer), a novel architecture that enhances EEG-based connectome analysis for brain disorder identification by integrating explainable artificial intelligence (XAI) to improve model interpretability and performance. By refining self-attention mechanisms and incorporating frequency and demographic information, XAIguiFormer demonstrates superior performance and offers valuable insights into frequency band importance, outperforming baseline models.

Your Mixture-of-Experts LLM Is Secretly an Embedding Model for Free
This study investigates the potential of Mixture-of-Experts (MoE) LLMs as embedding models, revealing that their expert routers can be used directly for embedding tasks with notable performance improvements without additional finetuning. Additionally, by combining MoE routing weights with hidden states, a new approach called MoEE is introduced, which enhances performance on diverse embedding-focused tasks, as demonstrated through extensive experiments across multiple datasets from the Massive Text Embedding Benchmark (MTEB).

ZETA: Leveraging $Z$-order Curves for Efficient Top-$k$ Attention
The paper introduces ZETA, a method utilizing Z-Order Curves for Efficient Top-k Attention, to improve the performance of self-attention in Transformers with reduced memory and computational costs for long sequences. ZETA achieves efficient parallel querying of past tokens and surpasses traditional and variant attention models on tasks like Long-Range Arena and WikiText-103, while maintaining or exceeding standard attention performance in synthetic tasks.

### Miscellaneous Aspects of Machine Learning->General Machine Learning Techniques
Instance-dependent Early Stopping
This paper introduces the Instance-dependent Early Stopping (IES) method, which enhances model training efficiency by halting the training process for individual instances once they are well-learned, rather than applying a uniform stopping criterion across all instances. By implementing IES, the researchers achieve a reduction in backpropagation instances by 10%-50%, while maintaining or improving model test accuracy and transfer learning performance.

On the expressiveness and spectral bias of KANs
Kolmogorov-Arnold Networks (KANs) have recently emerged as an alternative to multi-layer perceptrons (MLPs), proving efficient and accurate in AI-based scientific tasks like function regression and PDE solving. This paper theoretically compares the two, demonstrating that while both can represent each other, KANs may require fewer parameters for certain functions and exhibit less bias towards low frequencies, enhancing their learning capacity for high-frequency components through a unique multi-level learning feature.

### Miscellaneous Aspects of Machine Learning->Kernel methods
Extending Mercer's expansion to indefinite and asymmetric kernels
This paper extends Mercer's expansion to continuous kernels, addressing indefinite and asymmetric kernels by providing a theoretical framework. The authors demonstrate convergence properties of the expansion and offer an algorithm with decay bounds for computing Mercer's expansion for general kernels.

### Miscellaneous Aspects of Machine Learning->Online Learning, Active Learning and Bandits
Uncertainty Herding: One Active Learning Method for All Label Budgets
The paper introduces *uncertainty coverage*, a novel objective that effectively bridges low- and high-budget active learning scenarios, addressing the inefficiencies faced by existing methods across varying label budgets. By proposing a method named Uncertainty Herding, the study demonstrates its capability to outperform state-of-the-art techniques in diverse active learning tasks, ensuring reliable performance irrespective of budget levels.

### Miscellaneous Aspects of Machine Learning->Representation Learning
Intrinsic Dimension Correlation: uncovering nonlinear connections in multimodal representations
This paper introduces a novel metric to quantify nonlinear correlations between high-dimensional manifolds, leveraging the relationship between intrinsic dimensionality and correlation. The method, validated on synthetic data and applied to large-scale neural network representations of multimodal data, reveals significant nonlinear correlations between visual and textual embeddings, outperforming existing detection techniques.

RandLoRA: Full rank parameter-efficient fine-tuning of large models
This paper introduces RandLoRA, a parameter-efficient method that performs full-rank updates using learned linear combinations of low-rank, non-trainable random matrices, effectively overcoming the limitations of Low-Rank Adaptation (LoRA) in fine-tuning large transformer networks. Through extensive experiments, RandLoRA demonstrates its efficacy by significantly reducing and sometimes eliminating the performance gap between standard fine-tuning and LoRA across vision, language, and vision-language tasks.

Can One Modality Model Synergize Training of Other Modality Models?
This paper explores the potential for a model with one modality to enhance the training of models with different modalities without relying on high-quality paired datasets. The authors present theoretical and practical evidence that significant performance gains can be achieved through the synergistic use of visual, language, and audio models, thus broadening the scope of multimodal learning and alleviating the dependence on paired supervision.

### Miscellaneous Aspects of Machine Learning->Sequential, Network, and Time Series Modeling
Let SSMs be ConvNets: State-space Modeling with Optimal Tensor Contractions
Centaurus is a novel class of networks utilizing generalized state-space model blocks optimized through tensor contractions to enhance training efficiency and design flexibility beyond conventional configurations. By incorporating a heterogeneous mix of classical convolutional structures, Centaurus achieves superior performance in audio processing tasks like keyword spotting and automatic speech recognition, setting itself apart by eliminating traditional non-linear and explicit convolutional components.

Nonlinear Sequence Embedding by Monotone Variational Inequality
This paper introduces a method for unsupervised learning of low-dimensional representations of nonlinear sequence and time-series data, with provable recovery guarantees, casting the problem as a convex matrix parameter recovery using monotone variational inequalities. The approach shows strong performance on real-world data, aiding tasks like clustering and classification by learning faithful representations that span and incorporate the dynamics of each sequence and domain information effectively.

### Miscellaneous Aspects of Machine Learning->Transfer, Multitask and Meta-learning
Selective Aggregation for Low-Rank Adaptation in Federated Learning
This paper explores LoRA in federated learning, identifying that $A$ matrices learn general knowledge while $B$ matrices capture client-specific knowledge, leading to the development of Federated Share-A Low-Rank Adaptation (FedSA-LoRA). The study extends this approach to other LoRA variants such as rsLoRA and VeRA, establishing a general paradigm for integrating LoRA with federated learning and demonstrating its effectiveness in natural language tasks.

Metalic: Meta-Learning In-Context with Protein Language Models
The paper introduces Metalic (Meta-Learning In-Context), a novel approach that employs meta-learning and in-context learning to improve protein fitness prediction tasks, even with limited data. By fine-tuning models with fewer parameters and setting a new state-of-the-art on the ProteinGym benchmark, Metalic demonstrates significant potential in advancing protein engineering under data-scarce conditions.

Structuring Benchmark into Knowledge Graphs to Assist Large Language Models in Retrieving and Designing Models
This paper introduces a novel schema that transforms benchmark data into a Knowledge Benchmark Graph (KBG) to systematically store design knowledge and support neural network model design and transfer. By integrating the KBG with Large Language Models (LLMs), the approach enhances the capability to evaluate dataset similarity and aids in the Graph Neural Network (GNN) architecture design, as demonstrated through a KBG constructed from extensive model and performance data.

### Miscellaneous Aspects of Machine Learning->Unsupervised and Semi-supervised Learning
Exploring a Principled Framework for Deep Subspace Clustering
The paper introduces a Principled fRamewOrk for Deep Subspace Clustering (PRO-DSC) that aims to address the limitations of existing subspace clustering algorithms, which often suffer from feature collapse and lack theoretical guarantees. By incorporating regularization into the self-expressive model, PRO-DSC prevents feature space collapse and ensures that learned representations lie on a union of orthogonal subspaces, with experiments demonstrating its superior performance in clustering tasks.

### Optimization->Everything Else
ConFIG: Towards Conflict-free Training of Physics Informed Neural Networks
The paper introduces the ConFIG method to address the challenge of conflicting update directions in Physics-Informed Neural Networks (PINNs), ensuring conflict-free updates with consistent optimization rates and dynamic gradient adjustments. ConFIG's effectiveness is demonstrated through mathematical proof and evaluations, showing superior performance over baseline methods in both PINN scenarios and a classic multi-task benchmark. Source code is available at https://tum-pbs.github.io/ConFIG.

### Optimization->Learning for Optimization
Adversarial Generative Flow Network for Solving Vehicle Routing Problems
This paper presents a novel framework called Adversarial Generative Flow Networks (AGFN) to address the limitations of Transformer-based neural solvers in vehicle routing problems, specifically focusing on scalability and solution diversity. By integrating Generative Flow Networks with an adversarial training approach and a hybrid decoding method, AGFN demonstrates superior performance over existing neural solvers in solving the capacitated vehicle routing problem and the travelling salesman problem, with strong results on both synthetic and real-world benchmarks.

BTBS-LNS: Binarized-Tightening, Branch and Search on Learning LNS Policies for MIP
This paper introduces Binarized-Tightening Branch-and-Search for Large Neighborhood Search (BTBS-LNS), which addresses limitations in current Large Neighborhood Search approaches for solving Mixed Integer Program (MIP) problems. The method enhances performance by incorporating binarized tightening for integer variables, employing an attention-based tripartite graph for capturing global correlations, and using a branching network to optimize search decisions, demonstrating superior results compared to existing solvers and improving primal gaps on challenging benchmarks.

PolyNet: Learning Diverse Solution Strategies for Neural Combinatorial Optimization
PolyNet is introduced as a reinforcement learning approach that improves exploration in combinatorial optimization by learning complementary solution strategies without relying on handcrafted rules for diverse solution generation. Evaluated on four optimization problems, PolyNet's implicit diversity mechanism consistently yields better solutions compared to methods that enforce diversity through predefined rules.

### Optimization->Non-Convex
Addax: Utilizing Zeroth-Order Gradients to Improve Memory Efficiency and Performance of SGD for Fine-Tuning Language Models
The paper introduces Addax, a novel method that improves memory efficiency and performance in fine-tuning language models by combining the strengths of IP-SGD and MeZO. Addax demonstrates faster convergence and superior accuracy compared to MeZO, achieving up to 16% higher F1 scores and significantly faster execution while maintaining a comparable memory footprint, making it more efficient than traditional approaches like IP-SGD and Adam.

Efficient Sparse PCA via Block-Diagonalization
This paper introduces a novel framework to efficiently approximate Sparse Principal Component Analysis (Sparse PCA) by breaking down the problem into smaller, manageable sub-problems through a block-diagonal matrix approach, significantly reducing computation time while maintaining low error rates. By integrating this framework with existing Sparse PCA algorithms, the authors achieve substantial speedups, as demonstrated by evaluations on real-world datasets, showcasing an average speedup factor of over 100 for exact solutions and better approximation performance for approximate methods.

Local convergence of simultaneous min-max algorithms to differential equilibrium on Riemannian manifold
This paper investigates min-max algorithms for solving zero-sum differential games on Riemannian manifolds, focusing on differential Stackelberg and Nash equilibria. It presents analysis and conditions for the linear convergence of $\tau$-GDA and extends $\tau$-SGA to improve convergence rates, particularly demonstrating its potential advantages in training orthogonal Wasserstein GANs.

Adaptive Methods through the Lens of SDEs: Theoretical Insights on the Role of Noise
This paper presents novel stochastic differential equations (SDEs) for adaptive optimization methods such as SignSGD, RMSprop(W), and Adam(W), offering a quantitatively accurate understanding of their behavior in relation to adaptivity, gradient noise, and curvature. The theoretical findings, supported by experimental validation, reveal insights into convergence speed, stationary distribution, and noise robustness, with implications for improved training practices and scaling in neural network architectures.

### Optimization->Optimization and Learning under Uncertainty
Elliptic Loss Regularization
This paper introduces a novel regularization technique for neural networks that enforces smoothness between the input space and the loss by using an elliptic operator, aiming to improve model behavior predictions in underrepresented data regions. By incorporating this approach into the risk minimization objective, the method demonstrates computational efficiency and effectiveness in addressing distribution shift and group imbalance, as confirmed by empirical results.

Pareto Prompt Optimization
This paper introduces ParetoPrompt, a reinforcement learning technique for multi-objective prompt optimization in Large Language Models (LLMs), which efficiently explores the Pareto front without needing a predefined scalarization of objectives. The approach outperforms existing methods by leveraging dominance relationships to optimize prompts, achieving robust performance even when training and testing objective metrics differ.

### Optimization->Sampling and Optimization
Scalable Discrete Diffusion Samplers: Combinatorial Optimization and Statistical Physics
This paper introduces two novel methods for training discrete diffusion samplers, addressing memory scaling limitations by using the policy gradient theorem and Self-Normalized Neural Importance Sampling (SN-NIS). These approaches enable memory-efficient training, achieve state-of-the-art results in unsupervised combinatorial optimization, and allow for unbiased sampling in scientific applications, demonstrating superior performance on Ising model benchmarks and expanding the applicability of diffusion models in discrete domains.

### Optimization->Zero-order and Black-box Optimization
CONGO: Compressive Online Gradient Optimization
This paper introduces the Compressive Online Gradient Optimization (CONGO) framework to address zeroth-order online convex optimization with sparse gradients, particularly relevant in the context of optimizing large-scale queueing networks for time-sensitive job processing. By leveraging compressive sensing methods, CONGO reduces the sample complexity needed for gradient estimates to depend only on the gradient's sparsity rather than its full dimensionality, achieving optimal regret bounds, as demonstrated through numerical simulations and microservices benchmarks.

### Probabilistic Methods->Bayesian Models and Methods
Sensitivity-Aware Amortized Bayesian Inference
This paper introduces sensitivity-aware amortized Bayesian inference (SA-ABI), an approach that efficiently integrates sensitivity analyses into Bayesian model inference using neural networks. By utilizing weight sharing and rapid neural network inference, SA-ABI reduces computational overhead and avoids refitting models, proving effective in various applied modeling problems, such as disease dynamics and climate thresholds.

### Probabilistic Methods->Everything Else
Minimal Variance Model Aggregation: A principled, non-intrusive, and versatile integration of black box models
We introduce Minimal Empirical Variance Aggregation (MEVA), a framework that integrates predictions from multiple models to enhance accuracy by leveraging their individual strengths through a model-agnostic, point-wise linear aggregation process. MEVA, proven to estimate more robustly than Minimal Empirical Error Aggregation (MEEA), formulates aggregation as an error estimation problem, demonstrating significant improvements in robustness and accuracy across applications such as data science and partial differential equations.

Distribution-Free Data Uncertainty for Neural Network Regression
This paper introduces a nondeterministic neural network regression architecture that leverages a sample-based approximation of the continuous ranked probability score (CRPS) to model uncertainty in a truly distribution-free manner. The proposed method effectively learns well-calibrated uni- and multivariate output distributions, bypassing the limitations of parametric assumptions, and has been validated on various synthetic and real-world tasks with publicly available experimental code.

### Probabilistic Methods->Gaussian Processes
Deep Kernel Posterior Learning under Infinite Variance Prior Weights
This paper addresses a limitation in using infinitely wide Bayesian neural networks (BNNs) where deterministic covariance kernels prevent effective representation learning. By introducing a novel approach using $\alpha$-stable processes with conditionally Gaussian representations, the authors provide a way to maintain stochasticity without artificially injected noise, offering computational and statistical advantages demonstrated in simulations and benchmark datasets.

### Probabilistic Methods->Variational Inference
Variational Bayesian Pseudo-Coreset
This paper introduces Variational Bayesian Pseudo-Coreset (VBPC), a novel approach that leverages variational inference to create efficient, small datasets that approximate the posterior distribution, thereby addressing memory inefficiencies and computational challenges in Bayesian Neural Networks. VBPC improves upon previous methods by enhancing performance across benchmark datasets while reducing memory usage and computational costs.

### Reinforcement Learning
Constraint-Conditioned Actor-Critic for Offline Safe Reinforcement Learning
This paper introduces constraint-conditioned actor-critic (CCAC), a novel offline safe reinforcement learning method designed to learn high-reward policies that adhere to safety constraints and adapt to varying constraint thresholds by effectively handling out-of-distribution data. Empirical results on DSRL benchmarks demonstrate that CCAC achieves superior performance compared to existing approaches in learning adaptive, safe, and effective policies.

### Reinforcement Learning->Batch/Offline
Adversarial Policy Optimization for Offline Preference-based Reinforcement Learning
This paper introduces Adversarial Preference-based Policy Optimization (APPO), a novel algorithm for offline preference-based reinforcement learning (PbRL) that ensures sample complexity bounds without the need for computationally demanding confidence set constructions. By modeling PbRL as a two-player game, APPO achieves a balance between statistical efficiency and practical applicability, with experiments showing its effectiveness in learning from complex datasets and performing on par with state-of-the-art methods in continuous control tasks.

### Reinforcement Learning->Deep RL
Cross-Domain Offline Policy Adaptation with Optimal Transport and Dataset Constraint
This paper addresses the challenges of cross-domain offline reinforcement learning by introducing the Optimal Transport Data Filtering (OTDF) method, focusing on aligning transitions and selectively sharing data between domains without relying heavily on target domain data. By establishing a performance bound and employing optimal transport for data filtering, OTDF demonstrates superior performance across various dynamics shift conditions, often outperforming existing methods significantly.

Stable Hadamard Memory: Revitalizing Memory-Augmented Agents for Reinforcement Learning
This paper introduces the Stable Hadamard Memory, a novel memory model designed to improve memory management for reinforcement learning agents in partially observable environments. By dynamically adjusting memory and using the Hadamard product for efficient updates, the model outperforms existing methods on complex benchmarks, enhancing memory capacity and adaptability in long-term, evolving contexts.

### Reinforcement Learning->Everything Else
Learning View-invariant World Models for Visual Robotic Manipulation
In this paper, we introduce ReViWo, a novel method that leverages an autoencoder framework to create view-invariant and view-dependent representations, enabling robust robotic control under varying visual perspectives and disturbances. Evaluations in simulated and real-world environments demonstrate that ReViWo outperforms baseline methods by maintaining stable performance and effectively capturing task-relevant information even under significant viewpoint changes.

Dynamic Contrastive Skill Learning with State-Transition Based Skill Clustering and Dynamic Length Adjustment
This paper introduces Dynamic Contrastive Skill Learning (DCSL), a framework designed to enhance reinforcement learning in long-horizon tasks by redefining skill representation and learning. By incorporating state-transition based skill definition, learning of skill similarity functions, and dynamic skill length adjustment, DCSL improves the flexibility and adaptability of skill extraction, offering competitive performance in task completion and efficiency.

### Reinforcement Learning->Inverse
Diffusing States and Matching Scores: A New Framework for Imitation Learning
This paper introduces a novel approach to Adversarial Imitation Learning by adapting insights from diffusion models to the sequential setting, using score-matching along diffused states to measure discrepancies between expert and learner states. The proposed method, which simplifies training by focusing on regression of score functions, demonstrates theoretical advantages and empirical outperformance over existing GAN-based and adversarial-free imitation learning methods in complex continuous control tasks.

### Reinforcement Learning->Multi-agent
Breaking Mental Set to Improve Reasoning through Diverse Multi-Agent Debate
This paper introduces Diverse Multi-Agent Debate (DMAD), a method designed to overcome the limitations of fixed reasoning patterns in Large Language Models (LLMs) by encouraging agents to utilize diverse reasoning approaches. Experimental results demonstrate that DMAD consistently outperforms traditional Multi-Agent Debate (MAD) and self-reflection techniques across various benchmarks, delivering superior solutions in fewer rounds.

### Reinforcement Learning->Online
Diffusion Policy Policy Optimization
We present Diffusion Policy Policy Optimization (DPPO), a novel framework for fine-tuning diffusion-based policies in continuous control and robotics using reinforcement learning's policy gradient method. Our experiments show that DPPO outperforms other RL and policy gradient methods by utilizing synergies between diffusion parameterization and RL fine-tuning, achieving stable training, robust policies, and effective deployment in complex robotic scenarios.

Reinforcement Learning from Imperfect Corrective Actions and Proxy Rewards
This paper introduces ICoPro, a value-based deep reinforcement learning algorithm designed to address human-agent misalignment by integrating human corrective actions and potentially flawed proxy rewards. By incorporating human feedback and pseudo-labels to stabilize training, ICoPro demonstrates improved alignment with human intentions and greater sample efficiency across various tasks such as Atari games and autonomous driving despite imperfections in both proxy rewards and human corrections.

Optimal Strong Regret and Violation in Constrained MDPs via Policy Optimization
This paper addresses the challenge of achieving sublinear strong regret and constraint violation in constrained Markov Decision Processes (CMDPs) through efficient policy optimization methods. The authors introduce an innovative primal-dual algorithm that significantly improves upon previous work by Muller et al. (2024), attaining $\widetilde{\mathcal{O}}(\sqrt{T})$ strong regret/violation, thus closing the gap towards optimal bounds with practical efficiency.

ActSafe: Active Exploration with Safety Constraints for Reinforcement Learning
The paper introduces ActSafe, a novel model-based reinforcement learning algorithm designed for safe and efficient exploration in AI systems. By learning a probabilistic model that optimistically plans regarding epistemic uncertainty and enforces safety constraints pessimistically, ActSafe guarantees safety and near-optimal policy learning, achieving state-of-the-art performance in complex exploration tasks while maintaining safety in high-dimensional environments.

### Social Aspects->Accountability, Transparency and Interpretability
Efficient and Accurate Explanation Estimation with Distribution Compression
This paper introduces the Compress Then Explain (CTE) paradigm, which leverages distribution compression via kernel thinning to enhance the accuracy and stability of feature attribution approximation in machine learning models. CTE significantly reduces computational costs, achieving comparable explanation accuracy 2-3 times faster by using fewer samples, and serves as a versatile plug-in for existing explanation methods that depend on i.i.d. sampling.

QPM: Discrete Optimization for Globally Interpretable Image Classification
The paper introduces the Quadratic Programming Enhanced Model (QPM) to achieve global interpretability in deep neural networks by using a binary assignment of a few shared features to represent classes. Through discrete optimization and fine-tuning of features, QPM provides contrastive, globally interpretable class representations and sets new standards for accuracy in interpretable models across various datasets.

Salvage: Shapley-distribution Approximation Learning Via Attribution Guided Exploration for Explainable Image Classification
This paper presents Salvage, a new removal-based explainability method for image classification that introduces Shapley-distributions for more accurate classification probability approximations and addresses feature imbalance with an informed sampling strategy. Salvage demonstrates superior performance over existing explainability methods and can also function as a fully explainable classifier without significantly sacrificing classification accuracy.

The Geometry of Categorical and Hierarchical Concepts in Large Language Models
This paper extends the linear representation hypothesis by formalizing the representation of features, such as "is_animal," as vectors in the representation spaces of large language models, allowing categorical concepts to be expressed as polytopes. The authors demonstrate the connection between the hierarchical structure of concepts and their geometric representations, validating their theoretical findings with empirical tests on Gemma and LLaMA-3 models using WordNet data.

How to visualize training dynamics in neural networks
This paper explores the use of classical data analysis tools, such as PCA and hidden Markov models, to gain insights into the learning dynamics of neural networks. The study highlights how these traditional statistical methods can effectively reveal different data subsets and distinct training phases in modern deep learning systems.

### Social Aspects->Everything Else
Do not write that jailbreak paper
Jailbreaks are increasingly treated as a competition akin to ImageNet, rather than a means to enhance understanding of large language model (LLM) security. This blogpost reviews existing jailbreak literature, highlighting key contributions and urging the community to prioritize research that identifies novel security vulnerabilities.

### Social Aspects->Fairness, Equity, Justice and Safety
Beyond Mere Token Analysis: A Hypergraph Metric Space Framework for Defending Against Socially Engineered LLM Attacks
Recent jailbreak attempts on LLMs have evolved into human-like social engineering attacks, particularly persuasion-based techniques, which are notably effective. To address this, the paper proposes a novel defense approach using hypergraphs to model input prompt token embeddings, enabling classification of harmful and benign prompts, with empirical results showing significant improvements over current defense mechanisms.

A Generic Framework for Conformal Fairness
This paper introduces "Conformal Fairness," a concept of fairness in conformal prediction, and presents an algorithm that addresses the disparities in coverage across different sensitive groups. The framework leverages the exchangeability assumption, making it applicable to non-IID data types like graph data, and experiments on graph and tabular datasets demonstrate its effectiveness in managing fairness-related gaps while maintaining coverage expectations.

Copyright-Protected Language Generation via Adaptive Model Fusion
CP-Fuse is a novel approach designed to protect against the reproduction of copyrighted material by language models, effectively balancing the outputs from models trained on separate datasets to minimize the regurgitation of memorized content. Through extensive experiments, CP-Fuse has been shown to significantly reduce the reproduction of protected material without impacting the quality of generated text and code, while also providing seamless integration with other protective measures.

GSBA$^K$: $top$-$K$ Geometric Score-based Black-box Attack
The paper introduces GSBA\(^K\), a geometric score-based black-box attack designed to craft adversarial examples in a $top$-$K$ setting for both untargeted and targeted attacks against classifiers, including those with multi-label learning. The method enhances attack success by employing novel gradient estimation techniques to exploit the decision boundary's geometry, proving effective on ImageNet and PASCAL VOC datasets.

Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
This paper demonstrates that even the latest safety-aligned large language models (LLMs) are vulnerable to simple adaptive jailbreaking attacks, achieving a 100% success rate across various models, including Vicuna-13B and GPT-4, by leveraging adaptive adversarial prompts and random search techniques. The study highlights the importance of adaptivity in exploiting model-specific vulnerabilities and provides resources for reproducibility in the JailbreakBench format.

Towards Scalable Exact Machine Unlearning Using Parameter-Efficient Fine-Tuning
Machine unlearning aims to remove the influence of certain training data from a machine learning model without complete retraining, and exact unlearning seeks to guarantee this removal by only retraining affected components. The introduced Sequence-aware Sharded Sliced Training (S3T) framework enhances the deletion capabilities of exact unlearning systems with minimal performance impact by training with disjoint data slices and handling more deletion requests, outperforming existing methods in terms of both theoretical and empirical results.

### Social Aspects->Privacy-preserving Statistics and Machine Learning
Pacmann: Efficient Private Approximate Nearest Neighbor Search
Pacmann is a novel private Approximate Nearest Neighbor (ANN) search scheme that enables a client to perform ANN searches in a vector database without exposing the query vector to the server by offloading limited computation and storage to the client. By combining a graph-based ANN search approach with private information retrieval (PIR) techniques, Pacmann improves search accuracy by up to 2.5 times on real-world datasets and enhances scalability with notable reductions in computation time and latency compared to existing private ANN search schemes.

DCT-CryptoNets: Scaling Private Inference in the Frequency Domain
This paper introduces DCT-CryptoNets, a novel approach that leverages the discrete cosine transform (DCT) to enhance the efficiency and scalability of fully homomorphic encryption (FHE) for private inference in deep neural networks. By reducing the computational cost of non-linear activations and homomorphic bootstrap operations, DCT-CryptoNets achieves significant latency reductions, improved prediction reliability, and scalability for high resolution images, demonstrating an inference speedup on the ImageNet dataset from 12.5 hours to just 2.5 hours.

Scalable Extraction of Training Data from Aligned, Production Language Models
This paper uncovers vulnerabilities in large language models, demonstrating that even aligned models like OpenAI's ChatGPT can have their training data extracted through novel attacks. The study reveals the limitations of current safeguards in preventing data leakage, emphasizing the need for improved solutions to protect sensitive information.

### Social Aspects->Trustworthy Machine Learning
Does Safety Training of LLMs Generalize to Semantically Related Natural Prompts?
This paper investigates the vulnerability of safety fine-tuned Large Language Models (LLMs) like GPT-4 to natural prompts that can elicit objectionable content without the intent of jailbreaking. By introducing the Response Guided Question Augmentation (ReG-QA) method, the study demonstrates that these models can be systematically compromised by semantically related natural prompts, achieving attack success rates comparable to leading adversarial methods and showing resilience against certain defenses.

BlueSuffix: Reinforced Blue Teaming for Vision-Language Models Against Jailbreak Attacks
This paper introduces BlueSuffix, a novel black-box defense method for Vision-Language Models (VLMs) that targets jailbreak attacks while maintaining model performance on benign inputs. Utilizing a visual purifier, a textual purifier, and a blue-team suffix generator with reinforcement fine-tuning for cross-modal robustness, BlueSuffix significantly outperforms existing defenses across multiple VLMs and safety benchmarks, offering a promising new direction in defending against such attacks.

The Utility and Complexity of In- and Out-of-Distribution Machine Unlearning
This paper explores the complexities and trade-offs associated with machine unlearning, particularly focusing on ensuring privacy and efficient data removal from models. It introduces a new approach using robust and noisy gradient descent that effectively manages unlearning time complexity, especially for out-of-distribution data, while maintaining utility, thereby addressing significant gaps in current unlearning methodologies.

More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness
This study examines the trustworthiness of Large Language Models (LLMs) trained with Reinforcement Learning From Human Feedback (RLHF), revealing that alignment with human preferences does not inherently ensure outputs that are free from toxicity, bias, and other ethical concerns. The research highlights the complex influence of fine-tuning data on model trustworthiness and proposes the application of influence function-based data attribution methods to better understand and improve model alignment.

### Theory->Deep Learning
Unintentional Unalignment: Likelihood Displacement in Direct Preference Optimization
Direct Preference Optimization (DPO) and its variants, used to align language models with human preferences, often inadvertently decrease the likelihood of preferred responses, a phenomenon termed *likelihood displacement*. This study identifies the causes and consequences of likelihood displacement, introduces the *centered hidden embedding similarity (CHES)* score to characterize and mitigate this issue, and emphasizes the importance of curating distinct data preferences to prevent unintentional unalignment.

Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression
This paper presents a convergence analysis of the Deep Feature Instrumental Variable (DFIV) regression, showing it achieves the minimax optimal learning rate under certain conditions related to Besov spaces. The authors demonstrate that DFIV outperforms traditional fixed-feature IV methods, particularly in handling functions with low spatial homogeneity and in achieving greater data efficiency in the initial stage of regression.

### Theory->Everything Else
Near-optimal Active Regression of Single-Index Models
This work addresses the active regression problem for the single-index model, aiming to minimize the number of queries to entries of $b$ while achieving a $(1+\varepsilon)$-approximation solution. It introduces a novel algorithm with optimal query complexity up to logarithmic factors for $p\in [1,2]$, significantly improving on previous constant-factor approximations when $f$ is Lipschitz.

### Theory->Game Theory
Interference Among First-Price Pacing Equilibria: A Bias and Variance Analysis
This paper introduces a parallel budget-controlled A/B testing design for online marketplaces that addresses biases caused by budget constraints using market segmentation and budget-split experiments in submarkets. It demonstrates the effectiveness of this method through real experiments at Meta and proposes a debiased surrogate, establishing a plug-in estimator and its normality, while confirming its performance via numerical examples.

Efficient Online Pruning and Abstraction for Imperfect Information Extensive-Form Games
Efficiently computing approximate equilibrium strategies in large Imperfect Information Extensive-Form Games is challenging due to the exponential growth of the game tree, and existing methods face difficulties integrating pruning with Counterfactual Regret Minimization and are computationally expensive. The proposed Expected-Value Pruning and Abstraction (EVPA) framework addresses these issues with expected value estimation, minimax pruning, and dynamic online information abstraction, significantly outperforming existing approaches like DeepStack and Slumbot in Heads-up No-Limit Texas Hold'em with much lower solving time.

### Theory->Learning Theory
Towards Auto-Regressive Next-Token Prediction: In-context Learning Emerges from Generalization
This paper addresses two limitations in the theoretical analysis of in-context learning (ICL) by proposing a practical paradigm called auto-regressive next-token prediction (AR-NTP) that emphasizes prompt token-dependency and mirrors actual language model training. It introduces a systematic framework to explain ICL's emergence, providing generalization bounds and validating the theory with experiments on various datasets, suggesting that ICL arises from sequence and topic generalization.

Improved Convergence Rate for Diffusion Probabilistic Models
This paper improves the theoretical understanding of score-based diffusion models by establishing an iteration complexity of $d^{1/3}\varepsilon^{-2/3}$, surpassing previous results. Utilizing a randomized midpoint method, the analysis handles $\varepsilon$-accurate score estimates without requiring log-concavity and enables parallel execution in $O(\log^2(d/\varepsilon))$ rounds.

Computational Explorations of Total Variation Distance
This paper presents a deterministic polynomial-time algorithm for checking equivalence between mixtures of product distributions when the total variation (TV) distance is zero. It also establishes the computational hardness of efficiently estimating the TV distance between arbitrary Ising models, assuming $\mathsf{NP} \nsubseteq \mathsf{RP}$.

Bridging the Gap Between f-divergences and Bayes Hilbert Spaces
This paper introduces a novel framework that extends traditional $f$-divergences by incorporating locally non-convex divergence-generating functions, leading to the creation of a new class of pseudo $f$-divergences. The authors develop a variational estimation framework, apply it to $f$-GANs to achieve improved FID scores, and demonstrate competitive results with the Wasserstein GAN, underscoring the framework's potential in theoretical research and practical applications in learning theory.

Sharper Guarantees for Learning Neural Network Classifiers with Gradient Methods
This paper provides a novel bound on the excess risk of deep neural networks with smooth activation trained with the logistic loss, showing improvement over Rademacher complexity-based bounds by being tighter and not dependent on network width. It demonstrates that gradient descent can achieve optimal excess risk even with noisy data under polynomial width conditions, and highlights the effectiveness of a large step-size in improving test accuracy, particularly in classifying the XOR distribution with small-width networks.

### Theory->Optimization
Selective Task Group Updates for Multi-Task Optimization
This paper addresses the issue of negative transfer in multi-task learning by proposing a novel approach that involves selectively grouping and updating tasks during optimization. By introducing an algorithm that adaptively determines effective task groupings and using proximal inter-task affinity to optimize task relations, the authors demonstrate improved performance and scalability of multi-task learning networks compared to existing methods.

Fair Submodular Cover
This paper introduces the Fair Submodular Cover Problem (FSC), a novel approach to ensuring algorithmic fairness in submodular optimization tasks such as data summarization and clustering. The authors present discrete and continuous algorithms with bicriteria approximation ratios that effectively address fairness constraints, backed by empirical evaluations demonstrating their efficiency on maximum coverage instances.

Neural Exploratory Landscape Analysis for Meta-Black-Box-Optimization
This paper introduces Neural Exploratory Landscape Analysis (NeurELA), a framework that leverages a two-stage, attention-based neural network to profile landscape features dynamically in Meta-Black-Box-Optimization (MetaBBO) tasks, thereby reducing reliance on human-crafted features. Experiments demonstrate NeurELA's superior performance across various MetaBBO algorithms, enhancing autonomy and applicability while offering efficient fine-tuning options.

### Theory->Probabilistic Methods
Leave-One-Out Stable Conformal Prediction
LOO-StabCP is a novel method for conformal prediction that leverages leave-one-out stability to significantly enhance computational efficiency while maintaining prediction accuracy, especially for handling multiple prediction requests. The method, which is faster than existing models and theoretically validated, shows superior performance on both synthetic and real-world data, demonstrating improved test power in screening problems compared to current state-of-the-art methods.



## Oral Session 3E (10:30-11:42)
10:30-10:42: SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning
Continual Learning with foundation models is hindered by scalability challenges in existing methods, which require expanding prompts or retaining samples. The proposed Scalable Decoupled LoRA (SD-LoRA) method addresses these issues by separating the learning of LoRA components without rehearsal, achieving a stability-plasticity trade-off and improved parameter efficiency, as demonstrated through extensive experiments.
10:42-10:54: HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models
We introduce Hadamard High-Rank Adaptation (HiRA), a parameter-efficient fine-tuning technique that improves the adaptability of Large Language Models by maintaining high-rank update parameters through a Hadamard product. HiRA surpasses Low-rank Adaptation (LoRA) in performance across multiple tasks, as confirmed by comprehensive ablation studies, with the code accessible for public use.
10:54-11:06: LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization
This paper introduces LoRA-RITE, an adaptive matrix preconditioning method for optimizing Low-rank adaption (LoRA) in large language models, achieving transformation invariance and computational efficiency. Experimental results show that LoRA-RITE consistently improves performance over existing optimizers, with significant accuracy gains in tasks like Super-Natural Instructions and other benchmarks.
11:06-11:18: LaMPlace: Learning to Optimize Cross-Stage Metrics in Macro Placement
Machine learning applications in macro placement for chip design often prioritize online optimization of intermediate surrogate metrics, neglecting cross-stage metrics like timing performance due to computational constraints. The proposed LaMPlace addresses this by training an offline predictor to generate a mask that efficiently evaluates macro placement impact, resulting in significant improvements in chip quality with an average of 9.6% and notable enhancements in crucial timing metrics WNS and TNS by 43.0% and 30.4%, respectively.
11:18-11:30: DSPO: Direct Score Preference Optimization for Diffusion Model Alignment
This paper introduces Direct Score Preference Optimization (DSPO), a novel algorithm that fine-tunes diffusion-based Text-to-Image models by aligning their pretraining and fine-tuning objectives through score matching. Empirical and theoretical results demonstrate that DSPO surpasses existing preference learning methods, improving human preference evaluation, visual appeal, and prompt alignment in generated images.


## Oral Session 3C (10:30-11:42)
10:30-10:42: Restructuring Vector Quantization with the Rotation Trick
This work proposes a method to propagate gradients through the vector quantization layer in Vector Quantized Variational AutoEncoders (VQ-VAEs) by using a rotation and rescaling linear transformation. The approach enhances reconstruction metrics, codebook utilization, and reduces quantization error across 11 different VQ-VAE training paradigms.
10:42-10:54: STAR: Synthesis of Tailored Architectures
This paper introduces STAR, a novel approach for synthesizing tailored architectures through a new search space informed by the theory of linear input-varying systems, which allows for hierarchical numerical encoding into architecture genomes. STAR utilizes gradient-free evolutionary algorithms to optimize architectures across multiple metrics, demonstrating enhancements over optimized Transformers and hybrid models in language modeling tasks.
10:54-11:06: SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers
Sana is a text-to-image framework capable of generating high-resolution images up to 4096×4096, efficiently and with strong text-image alignment, even on a laptop GPU. Through innovations such as a deep compression autoencoder, linear attention in DiT, and a decoder-only text encoder, Sana achieves competitive quality at a fraction of the size and speed of contemporary models, offering low-cost content creation.
11:06-11:18: LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias
The Large View Synthesis Model (LVSM) is a novel transformer-based model designed for scalable and generalizable novel view synthesis using sparse-view inputs, introducing two architectures: an encoder-decoder and a decoder-only approach. Bypassing traditional 3D inductive biases, the encoder-decoder LVSM offers faster inference, while the decoder-only version delivers superior image quality and generalization, outperforming previous methods with 1.5 to 3.5 dB PSNR improvements, demonstrating state-of-the-art performance with reduced computational resources.
11:18-11:30: LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior
LARP is a novel video tokenizer that improves over traditional methods by using a holistic approach to gather global and semantic representations, enabling adaptive and efficient tokenization for autoregressive generative models. By integrating a lightweight AR transformer and aligning the token space with downstream tasks, LARP achieves state-of-the-art performance on video generation benchmarks, enhancing the compatibility of AR models with videos and paving the way for unified multimodal large language models.
11:30-11:42: Scaling and evaluating sparse autoencoders
Sparse autoencoders, specifically k-sparse autoencoders, effectively extract interpretable features from language models while balancing reconstruction and sparsity, reducing issues like dead latents. By proposing modifications that support scalability and providing new metrics for feature quality evaluation, this paper demonstrates significant improvements in autoencoder performance, exemplified by training a large-scale autoencoder on GPT-4 activations for broad applications.


## Oral Session 3D (10:30-11:42)
10:30-10:42: MAP: Multi-Human-Value Alignment Palette
The paper addresses the challenge of aligning generative AI systems with multiple human values, which can vary across demographics and adapt over time, by introducing the Multi-Human-Value Alignment Palette (MAP). MAP frames value alignment as an optimization challenge, effectively balancing trade-offs between values through user-defined constraints, and demonstrates its theoretical and empirical efficacy in systematically achieving multi-value alignment across various scenarios.
10:42-10:54: Limits to scalable evaluation at the frontier: LLM as judge won’t beat twice the data
This paper investigates the limitations of using strong existing models as evaluators for new machine learning models, which can introduce biases like self-preferencing. The study shows that debiasing methods can only reduce the need for ground truth labels by half, highlighting significant limitations of the model-as-judge approach, particularly when evaluating models that might surpass the judges in performance.
10:54-11:06: Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement
This paper introduces a principled framework for LLM-based evaluation that guarantees alignment with human judgment by proposing a selective evaluation approach. Utilizing Simulated Annotators for improved judge calibration and Cascaded Selective Evaluation to efficiently escalate model scrutiny, the method ensures over 80% human agreement even with cost-effective models, advancing beyond conventional LLM evaluation techniques.
11:06-11:18: AI as Humanity’s Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text
This paper introduces the CREATIVITY INDEX, a novel metric designed to quantify linguistic creativity by reconstructing text using existing web snippets, and proposes DJ SEARCH, a dynamic programming algorithm to efficiently compute this index. The study finds that humans exhibit higher creativity levels than large language models, and it demonstrates that the CREATIVITY INDEX is highly effective for detecting machine-generated text, outperforming current systems significantly in accuracy.
11:18-11:30: Consistency Checks for Language Model Forecasters
This paper addresses the challenge of evaluating forecasters by introducing a novel, general consistency metric based on arbitrage to assess the logical consistency of predictions. By developing an automated evaluation system and a proper-scoring-rule benchmark, the authors demonstrate a strong correlation between the instantaneously measurable consistency metrics and future ground truth Brier scores, offering a long-term evaluation tool that extends to 2028.
11:30-11:42: Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution
This paper introduces a new probabilistic modelling approach based on the mixture-of-experts framework to enhance learning to defer (L2D) systems in human-AI cooperation, addressing the limitations of extensive annotation requirements and inefficient workload distribution among human experts and AI. By employing the Expectation-Maximisation algorithm and introducing a workload distribution constraint, the approach improves resource allocation efficiency, achieving competitive or superior performance on synthetic and real-world datasets compared to existing methods.


## Oral Session 3F (10:30-11:42)
10:30-10:42: TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis
This paper introduces the Time Series Pattern Machine (TSPM), a model designed to improve time series tasks by utilizing multi-resolution and multi-scale temporal pattern extraction techniques. Employing methods like Multi-Resolution Time Imaging (MRTI) and Multi-Scale Mixing (MCM), the proposed TSPM achieves state-of-the-art results across various time series applications, outperforming both generalist and specialist models.
10:42-10:54: RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything
This paper introduces Real-Time Multi-Purpose SAM (RMP-SAM), a dynamic convolution-based method designed for real-time multi-purpose segmentation, effectively handling interactive, panoptic, and video instance segmentation tasks with a single end-to-end model. RMP-SAM utilizes an efficient encoder and decoupled adapter for prompt-driven decoding, achieving an optimal balance between speed and accuracy across various benchmarks, demonstrating generalization capabilities beyond specific semantic tasks.
10:54-11:06: Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation
The paper introduces Open-YOLO 3D, a novel open-vocabulary 3D instance segmentation approach that improves inference speed and reduces computational demands by leveraging 2D object detection from multi-view RGB images instead of relying on costly 2D foundation models. Using the Multi-View Prompt Distribution (MVPDist) method and low granularity label maps, it achieves state-of-the-art performance on the ScanNet200 and Replica benchmarks, offering up to 16 times speedup compared to existing methods.
11:06-11:18: SAM 2: Segment Anything in Images and Videos
The Segment Anything Model 2 (SAM 2) introduces a transformer-based approach for promptable visual segmentation in images and videos, utilizing a user-interactive data engine to amass the largest video segmentation dataset. This model demonstrates significant improvements in accuracy and efficiency, offering enhanced performance with fewer interactions in video segmentation and faster image segmentation compared to its predecessor, SAM, and is made available alongside its dataset, demo, and code for further research and development.
11:18-11:30: EmbodiedSAM: Online Segment Any 3D Thing in Real Time
This paper presents a novel method for real-time 3D instance segmentation using the Segment Anything Model (SAM), addressing the challenge of real-time perception in streaming RGB-D video by efficiently transferring 2D masks to 3D point clouds. The proposed approach demonstrates superior performance, outperforming offline methods on datasets like ScanNet and showing strong generalization in zero-shot and data-efficient scenarios.
11:30-11:42: MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection
This paper introduces a novel online test-time adaptation framework for LiDAR-based 3D object detection that addresses domain shifts, particularly those caused by sensor variations and weather conditions. The proposed Model Synergy (MOS) strategy effectively improves adaptability to diverse shifts by dynamically selecting and assembling historical checkpoints, demonstrating a significant 67.3% improvement in cross-corruption scenarios over existing methods.


## Oral Session 3B (10:30-11:42)
10:30-10:42: Learning to Discover Regulatory Elements for Gene Expression Prediction
We introduce Seq2Exp, a network designed to predict gene expressions from DNA sequences by discovering and extracting regulatory elements that govern gene expression, thereby improving prediction accuracy. Using an information bottleneck and Beta distribution to isolate causal elements, Seq2Exp outperforms traditional methods in prediction tasks and in identifying influential genomic regions, with its source code available through the AIRS library.
10:42-10:54: Steering Protein Family Design through Profile Bayesian Flow
The paper introduces ProfileBFN, a novel approach utilizing Profile Bayesian Flow Networks for generative modeling of protein families, which enables efficient protein family design without extensive MSA data. Empirical results demonstrate that ProfileBFN effectively captures structural characteristics and enhances the likelihood of generating diverse proteins with desired functionalities compared to previous methods.
10:54-11:06: Proteina: Scaling Flow-based Protein Structure Generative Models
Proteina is a state-of-the-art, flow-based protein backbone generator leveraging a scalable transformer architecture and hierarchical fold class labels for improved de novo protein design. It introduces new performance metrics and training strategies, achieving diverse, designable proteins up to 800 residues with advanced control over secondary structures and fold-specific generation.
11:06-11:18: Latent Bayesian Optimization via Autoregressive Normalizing Flows
This paper introduces Normalizing Flow-based Bayesian Optimization (NF-BO) to address the value discrepancy problem in Latent Bayesian Optimization by establishing a one-to-one function between input and latent spaces, eliminating reconstruction gaps. The proposed method, particularly SeqFlow for sequence data, and a dynamic candidate sampling strategy significantly outperform existing LBO methods in molecule generation tasks.
11:18-11:30: Composing Unbalanced Flows for Flexible Docking and Relaxation
The paper presents Unbalanced Flow Matching, a novel approach to molecular docking that addresses protein flexibility and the generation of nonphysical poses by framing the task as transport between distributions. The proposed method, FlexDock, significantly enhances docking performance and increases the proportion of energetically favorable poses from 30% to 73% on the PDBBind benchmark, demonstrating improved modeling of protein flexibility and accurate pose generation.


## Oral Session 3A (10:30-11:42)
10:30-10:42: Retrieval Head Mechanistically Explains Long-Context Factuality
This paper investigates how transformer-based models retrieve relevant information from long contexts and identifies a specific type of attention heads, called retrieval heads, as central to this capability. The study reveals that retrieval heads are universal across models, sparse, intrinsically present in pretrained models, dynamically activated, and crucial for preventing information retrieval issues and hallucinations, also significantly impacting the model's reasoning abilities and performance in different tasks.
10:42-10:54: REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments
The paper presents a novel approach for creating generalist AI agents that can adapt to new environments efficiently by using retrieval-based in-context learning, eliminating the need for extensive finetuning. The proposed semi-parametric agent, REGENT, leverages a transformer-based policy with query and retrieval sequences, outperforming current leading agents with fewer parameters and pre-training data, demonstrating significant improvements in robotics and game-playing tasks.
10:54-11:06: Differential Transformer
The Diff Transformer introduces a differential attention mechanism that amplifies relevant context and cancels noise, leading to more sparse attention patterns and improved performance over traditional Transformers. Its advantages include enhanced language modeling, better handling of long contexts, improved accuracy and robustness in in-context learning, and mitigation of hallucination in applications such as question answering and text summarization.
11:06-11:18: Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance
The paper investigates the phenomenon of "context-parametric inversion" in which instruction-finetuned large language models (LLMs) exhibit decreased reliance on input context as finetuning progresses, contrary to expectations. Through controlled studies on datasets like TULU, Alpaca, and Ultrachat across various model families, the authors analyze this behavior theoretically and suggest that finetuning data properties contribute to this counterintuitive trend, offering potential mitigation strategies for improved model performance.
11:18-11:30: Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models
This paper investigates the problem of hallucinations in large language models and identifies entity recognition as a key mechanism underlying this issue. By using sparse autoencoders to uncover significant directions in the model's representation space, the study shows that models have self-knowledge about their own capabilities, which affects their tendency to either refuse answering questions about known entities or hallucinate attributes of unknown entities.
11:30-11:42: Knowing Your Target: Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding
The paper introduces a Target-Aware Transformer for Spatio-Temporal Video Grounding (TA-STVG) that improves the spatial and temporal localization in videos by adaptively generating object queries with target-specific cues through a text-guided temporal sampling and attribute-aware spatial activation. This novel approach, which significantly outperforms existing methods across several benchmarks, not only enhances the discriminative power of video-text interactions but also demonstrates its general applicability by improving the performance of other models like TubeDETR and STCAT.


## Poster Session 4 (15:00-17:30)
### Applications->Chemistry and Drug Discovery
CheapNet: Cross-attention on Hierarchical representations for Efficient protein-ligand binding Affinity Prediction
This paper introduces CheapNet, a novel model that enhances protein-ligand binding affinity prediction by integrating atom-level and hierarchical cluster-level interactions using a cross-attention mechanism. CheapNet efficiently captures higher-order molecular representations, achieving state-of-the-art performance in binding affinity prediction while maintaining computational efficiency.

ChemAgent: Self-updating Memories in Large Language Models Improves Chemical Reasoning
ChemAgent is a new framework designed to enhance large language models' (LLMs) performance in chemical reasoning by using a dynamic, self-updating library that breaks down chemical tasks into structured sub-tasks stored for future use. Experimental results show that ChemAgent significantly improves LLMs' accuracy by up to 46% on chemical reasoning datasets, highlighting its potential for applications in drug discovery and materials science.

NExT-Mol: 3D Diffusion Meets 1D Language Modeling for 3D Molecule Generation
This paper introduces NExT-Mol, a foundation model that combines 1D SELFIES-based language modeling with 3D diffusion models for efficient 3D molecule generation. By enhancing the language model and diffusion architecture, the authors achieve significant improvements in molecular validity and conformer prediction, demonstrating a 26% improvement in 3D FCD for de novo generation and a 13% gain in conditional generation on benchmark datasets, with resources available on GitHub.

E(3)-equivariant models cannot learn chirality: Field-based molecular generation
This paper critiques the limitations of 3D point-cloud atom representations in drug modeling, specifically their disregard for molecular chirality, which is critical for drug safety and efficacy. The authors introduce a novel field-based representation that includes reference rotations to capture all molecular geometries, including chirality, while maintaining competitive performance with existing E(3)-based models.

A new framework for evaluating model out-of-distribution generalisation for the biochemical domain
This paper introduces AU-GOOD, a novel metric designed to quantify model performance as it generalizes to out-of-distribution data, particularly in biochemical contexts such as proteins and small molecules. By incorporating prior knowledge of target deployment distributions, the metric, accompanied by a new partitioning algorithm and statistical comparison methods, enhances model evaluation and guides the selection of appropriate similarity functions, demonstrated through case studies in pharmaceutical prediction and biophysical property modeling.

### Applications->Computer Vision
Vision-LSTM: xLSTM as Generic Vision Backbone
This paper introduces Vision-LSTM (ViL), an adaptation of the xLSTM architecture for computer vision, leveraging a stack of xLSTM blocks to process sequences of patch tokens in alternating directions. ViL demonstrates strong performance in classification, transfer learning, and segmentation, offering a favorable pre-training cost-to-performance trade-off, suggesting its potential as a new backbone for computer vision architectures.

Posterior-Mean Rectified Flow: Towards Minimum MSE Photo-Realistic Image Restoration
This paper introduces Posterior-Mean Rectified Flow (PMRF), an algorithm designed to achieve the optimal estimator for image restoration by minimizing mean squared error (MSE) under perfect perceptual index constraints. PMRF first predicts the posterior mean and then utilizes a rectified flow model to transform this prediction into a high-quality image, demonstrating superior performance over existing methods across various image restoration tasks.

TexTailor: Customized Text-aligned Texturing via Effective Resampling
TexTailor introduces a novel method for generating consistent object textures from textual descriptions by addressing limitations in existing text-to-texture synthesis approaches. By integrating a resampling scheme and adaptively adjusting camera positions based on object geometry, TexTailor outperforms state-of-the-art methods in synthesizing view-consistent textures, as demonstrated on the Objaverse and ShapeNet car datasets.

Graph-Guided Scene Reconstruction from Images with 3D Gaussian Splatting
This paper presents GraphGS, a novel graph-guided framework for high-fidelity 3D scene reconstruction from images, addressing limitations of existing methods such as the need for precise camera poses and dense supervision. Utilizing a spatial prior-based scene structure estimation and a unique graph-guided optimization process, GraphGS improves efficiency and reduces overfitting, achieving state-of-the-art performance demonstrated through comprehensive evaluations.

Bridging Information Asymmetry in Text-video Retrieval: A Data-centric Approach
The paper introduces a novel data-centric framework for text-video retrieval (TVR) that addresses the information asymmetry between videos and their textual descriptions by enriching textual representations. By utilizing event-level video segmentation, captioning, and a query selection mechanism with a large language model, the approach improves accuracy and efficiency, achieving state-of-the-art results across multiple benchmarks and highlighting the potential of data-driven methods in enhancing cross-modal retrieval.

Consistent Flow Distillation for Text-to-3D Generation
This paper introduces Consistent Flow Distillation (CFD) as an improvement over Score Distillation Sampling (SDS) for 3D image generation, addressing issues of visual quality and diversity. By ensuring the consistency of 2D image flows across various viewpoints using multi-view consistent Gaussian noise, CFD achieves superior results in text-to-3D generation compared to previous methods.

An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels
This paper challenges the established necessity of locality inductive bias in computer vision architectures by demonstrating that vanilla Transformers can treat individual pixels as tokens and still achieve efficient results across various tasks. The study highlights the potential for revising the architectural design of neural networks in vision tasks, providing evidence through supervised and self-supervised learning, as well as image generation experiments.

Articulate-Anything:  Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model
Articulate-Anything is a system that automates the creation of interactable 3D digital twins from diverse inputs like text, images, and videos, utilizing vision-language models to compile code for standard simulators. By significantly improving the success rate of object articulation on the PartNet-Mobility dataset from 8.7-11.6% to 75%, it sets a new performance standard and aids in training robotic policies for complex manipulation tasks.

6D Object Pose Tracking in Internet Videos for Robotic Manipulation
This paper presents a new method for extracting temporally consistent 6D pose trajectories of manipulated objects from Internet instructional videos, addressing challenges like uncontrolled capture conditions and unknown object meshes. The method includes retrieving a similar CAD model for pose estimation, tracking objects across video frames for smooth trajectory extraction, and deploying these trajectories onto robotic manipulators, showcasing significant improvements over existing methods and potential applications in Embodied AI.

MAI: A Multi-turn Aggregation-Iteration Model for Composed Image Retrieval
Multi-Turn Composed Image Retrieval (MTCIR) involves users refining image retrieval results through multiple iterations by incorporating historical context into modifications, which existing methods often neglect. This paper introduces a new dataset, FashionMT, and proposes the Multi-turn Aggregation-Iteration (MAI) model featuring a Two-stage Semantic Aggregation paradigm and Multi-turn Iterative Optimization mechanism, which together enhance semantic consistency and retrieval performance over existing approaches.

NarrativeBridge: Enhancing Video Captioning with Causal-Temporal Narrative
The paper introduces NarrativeBridge to enhance video captioning by incorporating causal-temporal narratives, often missing in existing benchmarks and models. It presents a new benchmark, Causal-Temporal Narrative (CTN) captions, and a Cause-Effect Network (CEN) that significantly improves the ability to generate captions with causal and temporal dynamics, outperforming existing models and demonstrating strong generalization across datasets.

Cafe-Talk: Generating 3D Talking Face Animation with Multimodal Coarse- and Fine-grained Control
The paper introduces Cafe-Talk, a diffusion-transformer-based model for generating 3D talking faces, which offers both lip synchronization and controllable expressions with a novel approach to disentangling speech audio and fine-grained controls via a two-stage training pipeline. Cafe-Talk achieves state-of-the-art performance in lip synchronization and expressiveness by incorporating a fine-grained control adapter and a swap-label training mechanism, and it supports multimodal control through a text-based detector, as demonstrated by extensive experimental results and positive user studies.

Mitigating Object Hallucination in MLLMs via Data-augmented Phrase-level Alignment
This paper addresses object hallucinations in Multimodal Large Language Models (MLLMs) by introducing Data-augmented Phrase-level Alignment (DPA), a novel loss function that reduces hallucinations while maintaining general vision-language capabilities. Evaluation results show that MLLMs fine-tuned with DPA, termed Hallucination Attenuated Language and Vision Assistant (HALVA), achieve significant improvements in reducing hallucination rates and enhancing performance on visual question-answering and image description tasks.

State Space Model Meets Transformer: A New Paradigm for 3D Object Detection
The paper introduces a novel 3D object detection paradigm using an interactive State Space Model (SSM) termed DEST, which addresses limitations in existing DETR-based methods by enabling iterative and efficient updates of both scene point and query features with linear complexity. The proposed method incorporates unique designs such as state-dependent SSM parameterization, bidirectional feature interaction, inter-state attention, and a gated feed-forward network, resulting in substantial performance improvements on the ScanNet V2 and SUN RGB-D datasets compared to previous baselines.

Reconstructive Visual Instruction Tuning
This paper presents reconstructive visual instruction tuning (ROSS), a novel approach that uses vision-centric supervision for Large Multimodal Models (LMMs) by reconstructing input images rather than solely focusing on text outputs. ROSS enhances fine-grained comprehension and reduces hallucinations through a denoising objective, leading to significant improvements in performance with a single SigLIP visual encoder compared to state-of-the-art alternatives.

Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction
This paper introduces Lotus, a diffusion-based visual foundation model designed to improve zero-shot generalization in dense prediction tasks by directly predicting annotations and simplifying the diffusion process into a single-step procedure. Lotus achieves state-of-the-art performance in zero-shot depth and normal estimation with enhanced efficiency, offering numerous practical applications such as joint estimation and 3D reconstruction without increasing training data or model capacity.

SynCamMaster: Synchronizing Multi-Camera Video Generation from Diverse Viewpoints
This paper introduces a plug-and-play module to enhance pre-trained text-to-video models for generating open-world videos from arbitrary viewpoints with consistency in appearance and geometry, leveraging a multi-view synchronization approach. The authors propose a progressive training scheme using multi-camera images and monocular videos, showing superior results compared to existing methods and enabling video re-rendering from novel viewpoints.

### Applications->Everything Else
OpenRCA: Can Large Language Models Locate the Root Cause of Software Failures?
Large language models (LLMs) have significantly impacted software engineering, notably in tools like Copilot, but their role in post-development phases, especially in identifying the root causes of software failures, is less explored. To address this, the paper introduces OpenRCA, a benchmark dataset and evaluation framework analyzing LLMs' capabilities in diagnosing software failures, revealing that current models perform poorly in complex scenarios, highlighting opportunities for further research advancements.

### Applications->Genetics, Cell Biology, Health, etc
Learning to Discover Regulatory Elements for Gene Expression Prediction
We introduce Seq2Exp, a network designed to predict gene expressions from DNA sequences by discovering and extracting regulatory elements that govern gene expression, thereby improving prediction accuracy. Using an information bottleneck and Beta distribution to isolate causal elements, Seq2Exp outperforms traditional methods in prediction tasks and in identifying influential genomic regions, with its source code available through the AIRS library.

Hyperbolic Genome Embeddings
This paper introduces a novel application of hyperbolic CNNs to genomic sequence modeling, creating more expressive DNA sequence representations without the need for explicit phylogenetic mapping. The hyperbolic models outperform their Euclidean counterparts on most benchmark datasets and even surpass state-of-the-art performance on several, achieving these results with fewer parameters and no pretraining.

### Applications->Health
ACES: Automatic Cohort Extraction System for Event-Stream Datasets
Reproducibility in machine learning for healthcare is hindered by private datasets and complex model pipelines, which complicates the sharing and iteration of EHR-based results. To tackle this, the Automatic Cohort Extraction System (ACES) simplifies task and cohort development and enhances reproducibility by providing a configuration language and pipeline for extracting patient records, thereby improving the reproducibility of ML studies using event-stream data.

MediConfusion: Can you trust your AI radiologist? Probing the reliability of multimodal medical foundation models
This paper introduces MediConfusion, a novel benchmark dataset designed to test the failure modes of medical Multimodal Large Language Models (MLLMs) from a visual perspective. The study reveals that current state-of-the-art models perform below random guessing on this benchmark, highlighting significant reliability issues and providing insights for developing more trustworthy MLLMs in healthcare.

Context Clues: Evaluating Long Context Models for Clinical Prediction Tasks on EHR Data
This paper systematically evaluates the effect of context length on modeling Electronic Health Records (EHR) data, finding that longer context models like the Mamba-based model improve predictive performance and robustness compared to prior state-of-the-art models. It identifies and quantifies unique EHR challenges such as artificial token repetition, irregular time intervals, and increasing disease complexity, demonstrating that models with extended context are more resilient to these challenges.

PaPaGei: Open Foundation Models for Optical Physiological Signals
PaPaGei introduces the first open foundation model for PPG signals, pre-trained on over 57,000 hours of data and leveraging a novel representation learning approach to improve upon traditional methods. This model demonstrates superior performance and efficiency across 20 diverse tasks and establishes a benchmark for evaluating bias, broadening the scope for multimodal health monitoring.

Moner: Motion Correction in Undersampled Radial MRI with Unsupervised Neural Representation
Moner is an unsupervised motion correction method for radial MRI that addresses subject movement without relying on large-scale training datasets, thus optimizing costs and improving model generalization. By employing implicit neural representation and a novel coarse-to-fine hash encoding strategy, Moner reconstructs artifact-free images and achieves performance comparable to state-of-the-art techniques on in-domain data, while significantly enhancing performance on out-of-domain datasets.

### Applications->Neuroscience, Cognitive Science
SIMPL: Scalable and hassle-free optimisation of neural representations from behaviour
This paper introduces SIMPL, an EM-style algorithm designed to efficiently optimize latent variables and tuning curves by leveraging behavior as an initial condition, enhancing convergence and identifiability. SIMPL demonstrates rapid and scalable performance in recovering latent variables across various tasks, and when applied to a large hippocampal dataset, it reveals the brain's potential for encoding space with higher resolution, offering insights into neural data analysis.

Beyond single neurons: population response geometry in digital twins of mouse visual cortex
This study examines the limitations of digital twins—computational models trained on neural data—in accurately representing the population activity geometry in the mouse visual cortex. The research identifies the need for improved training strategies to enhance model robustness and generalization, which is crucial for effectively exploring neural computations at the population level.

BrainUICL: An Unsupervised Individual Continual Learning Framework for EEG Applications
The paper introduces BrainUICL, a novel framework for Unsupervised Individual Continual Learning, designed to enhance the adaptability and generalization of EEG-based models to new subjects in real-world scenarios. By continuously learning and improving from new data, BrainUICL effectively balances plasticity and stability, demonstrating improved performance across multiple EEG tasks, which is crucial for practical applications, particularly in clinical settings.

BLEND: Behavior-guided Neural Population Dynamics Modeling via Privileged Knowledge Distillation
The paper introduces BLEND, a novel framework for neural population dynamics modeling that enhances performance by using behavior as privileged information during training, despite utilizing only neural activity at inference. This model-agnostic approach shows significant improvements in both behavioral decoding and transcriptomic neuron identity prediction, highlighting its efficacy without relying on specific model architectures or assumptions about the neural-behavior relationship.

### Applications->Physics
Wavelet Diffusion Neural Operator
The paper introduces the Wavelet Diffusion Neural Operator (WDNO), a new framework for simulating and controlling physical systems described by PDEs, which addresses the limitations of existing diffusion models in handling abrupt changes and generalizing to higher resolutions. By incorporating wavelet domain generative modeling and multi-resolution training, WDNO demonstrates superior performance in both simulation and control across various systems, achieving significant improvements in long-term accuracy and successfully reducing smoke leakage in a complex 2D control task.

KinFormer: Generalizable Dynamical Symbolic Regression for Catalytic Organic Reaction Kinetics
KinFormer is a deep learning-based kinetic equation prediction model that utilizes a conditional Transformer and Monte Carlo Tree Search to effectively model dynamical symbolic regression (DSR) under physical chemistry constraints, addressing the challenge of generalization across diverse reaction categories. It demonstrates superior generalization ability and efficiency in predicting kinetic equations, outperforming classical and Transformer baselines in out-of-domain evaluations on 20 types of organic reactions.

### Applications->Robotics
SmartPretrain: Model-Agnostic and Dataset-Agnostic Representation Learning for Motion Prediction
SmartPretrain is a scalable, self-supervised learning framework for motion prediction in autonomous vehicles, designed to be both model-agnostic and dataset-agnostic. By integrating contrastive and reconstructive self-supervised learning, as well as utilizing a scenario sampling strategy across multiple datasets, SmartPretrain enhances the performance and robustness of state-of-the-art prediction models, significantly reducing error rates such as the MissRate of Forecast-MAE by 10.6%.

STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning
This paper introduces STRAP, a method for improving robot learning by retrieving sub-sequences of trajectories from large datasets to train policies during deployment, rather than relying on pre-trained policies. STRAP demonstrates superior performance over existing retrieval and multi-task learning approaches, offering enhanced data utilization, generalization, and robustness across tasks with minimal real-world demonstrations.

FLIP: Flow-Centric Generative Planning as General-Purpose Manipulation World Model
FLow-CentrIc generative Planning (FLIP) is a scalable model-based planning algorithm that utilizes language and vision inputs to develop action proposals, video generation, and value assessment for general-purpose manipulation tasks. Through its innovative use of multi-modal flow generation, vision-language representation, and video synthesis, FLIP demonstrates improved success rates and quality in long-horizon plan synthesis, showcasing its applicability across various objects, robots, and tasks.

Navigation-Guided Sparse Scene Representation for End-to-End Autonomous Driving
The paper presents SSR, a novel framework for End-to-End Autonomous Driving that uses only 16 navigation-guided tokens for efficient Sparse Scene Representation, eliminating the need for supervised perception tasks. SSR significantly improves performance metrics, with faster inference and training times, and enhanced driving scores, marking a substantial advancement in scalable, real-time autonomous driving systems.

### Applications->Time Series
Efficient Source-Free Time-Series Adaptation via Parameter Subspace Disentanglement
This paper introduces a framework for Source-Free Domain Adaptation in time-series that enhances parameter efficiency and data utilization by reparameterizing the source model's weights using a Tucker-style decomposition. The method improves training and inference efficiency on resource-constrained devices by selectively fine-tuning decomposed factors, leading to over 90% reduction in computational overhead while maintaining performance.

TVNet: A Novel Time Series Analysis Method Based on Dynamic Convolution and 3D-Variation
This paper introduces TVNet, a dynamic convolutional network that enhances the representational capacity of CNNs in time series analysis by using a novel reshaping technique that considers inter-patch, intra-patch, and cross-variable dimensions. TVNet retains the computational efficiency of traditional CNNs while achieving state-of-the-art results and superior balance of efficiency and performance over Transformer-based and MLP-based models in key time series tasks, demonstrating enhanced transferability and robustness.

A deep inverse-mapping model for a flapping robotic wing
This paper presents a machine learning approach to achieve real-time control of complex flapping-wing robots by accurately predicting the required wing motions to generate specific aerodynamic forces. Utilizing a novel sequence-to-sequence model with an adaptive-spectrum layer, the system outperforms state-of-the-art models by 11% in test datasets median loss, and offers practical real-time inference advantages, potentially benefiting a range of applications from biomimetic robots to biomedical devices.

### Deep Learning->Attention Mechanisms
Plug, Play, and Generalize: Length Extrapolation with Pointer-Augmented Neural Memory
The paper introduces Pointer-Augmented Neural Memory (PANM), a module that enhances neural networks' capabilities in processing symbols and handling longer data sequences by integrating an external neural memory with novel pointer manipulation techniques. Experiments demonstrate PANM's ability to significantly improve performance in various tasks, including algorithmic reasoning and machine translation, with notable generalization effectiveness when applied to large language models on tasks far exceeding training lengths.

### Deep Learning->Everything Else
IFORMER: INTEGRATING CONVNET AND TRANSFORMER FOR MOBILE APPLICATION
The paper introduces iFormer, a new family of mobile hybrid vision networks designed to optimize latency and accuracy by combining the local representation of convolutional networks with the global modeling of self-attention. iFormer surpasses existing lightweight networks, achieving notable performance improvements across various tasks with significant efficiency on mobile devices, exemplified by its Top-1 accuracy of 80.4% on ImageNet-1k with just 1.10 ms latency on an iPhone 13.

Revisiting text-to-image evaluation with Gecko: on metrics, prompts, and human rating
This paper addresses the limitations of current evaluation methods for text-to-image (T2I) generative models by introducing a comprehensive evaluation suite with over 100K annotations, named Gecko2K, that assesses models across various human annotation methods and evaluation tasks. The authors also propose a new auto-evaluation metric that consistently correlates better with human ratings than existing metrics, enhancing the reliability and interpretability of T2I model assessments.

### Deep Learning->Generative Models and Autoencoders
One-Prompt-One-Story: Free-Lunch Consistent Text-to-Image Generation Using a Single Prompt
This paper introduces "One-Prompt-One-Story" (1Prompt1Story), a novel training-free method for consistent text-to-image generation that maintains character identities by concatenating all prompts into a single input for diffusion models. By employing techniques like Singular-Value Reweighting and Identity-Preserving Cross-Attention, the approach improves alignment with input descriptions, offering a more effective solution compared to existing methods without needing extensive model retraining or modifications.

SymDiff: Equivariant Diffusion via Stochastic Symmetrisation
SymDiff is a novel method for creating equivariant diffusion models using stochastic symmetrisation, functioning as a data augmentation technique that enhances sampling in a computationally efficient manner. By accommodating scalable architectures as alternatives to complex equivariant models, SymDiff demonstrates substantial improvements in E(3)-equivariant molecular generation, marking its pioneering application of symmetrisation in generative modelling.

Quality Measures for Dynamic Graph Generative Models
This paper introduces a novel metric based on the Johnson-Lindenstrauss lemma for evaluating generative models of dynamic graphs, addressing limitations of existing methods that inadequately capture temporal relationships and require impractical computational resources. The proposed scalar metric offers an application-agnostic measure of dynamic graph similarity, with empirical evaluations demonstrating its effectiveness over traditional approaches.

Dreamweaver: Learning Compositional World Models from Pixels
The paper introduces Dreamweaver, a neural architecture that effectively decomposes raw video inputs into hierarchical and compositional representations to simulate future scenarios without auxiliary data. By employing a novel Recurrent Block-Slot Unit (RBSU), Dreamweaver surpasses current models in state-of-the-art world modeling and enables the creation of novel video sequences through the recombination of recognized object attributes.

The Superposition of Diffusion Models Using the Itô Density Estimator
This paper introduces SuperDiff, a novel framework for combining multiple pre-trained diffusion models at the generation stage using the concept of superposition, thereby eliminating the need for re-training larger models. SuperDiff offers scalable and efficient inference time performance and demonstrates its effectiveness in generating diverse images, performing prompt-conditioned image editing, and enhancing conditional molecule generation and protein design.

DiffSplat: Repurposing Image Diffusion Models for Scalable Gaussian Splat Generation
DiffSplat is a novel 3D generative framework that enhances 3D content creation by effectively utilizing large-scale text-to-image diffusion models and maintaining 3D consistency with native generation of 3D Gaussian splats. By introducing a lightweight reconstruction model for scalable dataset curation and employing both diffusion and 3D rendering losses, DiffSplat demonstrates superior performance in various generation tasks and downstream applications, as supported by extensive experiments and ablation studies.

Ensembling Diffusion Models via Adaptive Feature Aggregation
This paper introduces Adaptive Feature Aggregation (AFA), a novel ensembling method that dynamically adjusts the contributions of multiple diffusion models at the feature level based on various states like prompts and denoising steps. By employing the Spatial-Aware Block-Wise (SABW) feature aggregator, which requires minimal additional training parameters, the method effectively leverages the strengths and mitigates the weaknesses of diverse models, as evidenced by both quantitative and qualitative experiments.

Pyramidal Flow Matching for Efficient Video Generative Modeling
This paper introduces a unified pyramidal flow matching algorithm to improve the efficiency of video generative modeling by using a series of pyramid stages, where only the final stage operates at full resolution. The novel approach enables end-to-end optimization with a single Diffusion Transformer (DiT), successfully generating high-quality videos while significantly reducing computational demand, with practical demonstrations available through open-source code and models.

Improving Neural Optimal Transport via Displacement Interpolation
This paper introduces the Displacement Interpolation Optimal Transport Model (DIOTM), a method that enhances the stability and precision of learning optimal transport maps by leveraging displacement interpolation. The proposed approach outperforms existing models in estimating optimal transport maps, particularly in tasks like image-to-image translation, by utilizing the entire trajectory of displacement interpolation, thereby addressing training instability and hyperparameter sensitivity issues.

Circuit Representation Learning with Masked Gate Modeling and Verilog-AIG Alignment
This paper introduces MGVGA, a novel masked modeling paradigm for electronic design automation that preserves logical equivalence and enhances graph neural network learning by masking gates in the latent space and aligning with Verilog-AIG codes. The proposed approach demonstrates superior performance on various logic synthesis tasks compared to existing methods, leveraging large language models to better understand and learn circuit functions.

Dynamic Diffusion Transformer
The paper introduces the Dynamic Diffusion Transformer (DyDiT), an architecture designed to enhance efficiency in image generation by dynamically adjusting computation across diffusion timesteps and spatial dimensions. Through the Timestep-wise Dynamic Width and Spatial-wise Dynamic Token strategies, DyDiT significantly reduces computational costs and accelerates generation compared to the existing DiT model, achieving improved metrics such as a 51% reduction in FLOPs and a competitive FID score on ImageNet.

Physics-Informed Diffusion Models
This paper introduces a unified framework for generative modeling and partial differential equation fulfillment by incorporating a first-principle-based loss term to ensure generated samples adhere to physical constraints. The method significantly reduces residual errors and outperforms existing task-specific frameworks, while also acting as a regularization mechanism against overfitting; it is versatile and simple to implement for various constraints and optimization objectives.

Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models
This paper introduces Adaptive Projected Guidance (APG), an improvement over classifier-free guidance (CFG) in diffusion models, addressing the issue of oversaturation caused by high guidance scales. By modifying the CFG update rule to down-weight its parallel component and incorporating a new rescaling and momentum method, APG enhances image quality without oversaturation and is shown to improve performance metrics like FID, recall, and saturation while maintaining computational efficiency and compatibility with various models.

### Deep Learning->Graph Neural Networks
Temporal Heterogeneous Graph Generation with Privacy, Utility, and Efficiency
This paper introduces THePUff, a Temporal Heterogeneous Graph Generator that balances privacy, utility, and efficiency. Utilizing a differential privacy algorithm in a generative adversarial setting, it generates privacy-protected yet informative graph data, with experimental results demonstrating its effectiveness compared to existing methods.

Graph Neural Networks Can (Often) Count Substructures
This paper investigates the capabilities of message passing graph neural networks (GNNs) in counting subgraphs, challenging the notion of their limited expressive power in distinguishing non-isomorphic graphs. By deriving sufficient conditions under which GNNs can count subgraphs, and developing dynamic programming algorithms for more efficient subgraph isomorphism problem-solving, the study provides both theoretical insights and empirical validation on real-world datasets.

Beyond Canonicalization: How Tensorial Messages Improve Equivariant Message Passing
This paper introduces a framework for enforcing equivariance in geometric deep learning models using local reference frames, which is compatible with any architecture. By enhancing message passing with tensorial messages, the approach achieves state-of-the-art results in normal vector regression and competitive performance on various 3D point cloud tasks.

Exact Certification of (Graph) Neural Networks Against Label Poisoning
This paper introduces an exact certification method to address the previously unsolved problem of certifying label flipping in Graph Neural Networks (GNNs), using the Neural Tangent Kernel to reformulate the optimization problem into a Mixed-Integer Linear Program. The study establishes robustness hierarchies among GNNs, evaluates architectural choices' impacts, and discovers a robustness plateau phenomenon, offering a novel contribution applicable to wide neural networks and marking the first exact certification for poisoning attacks in neural networks.

Bonsai: Gradient-free Graph Condensation for Node Classification
Graph condensation, a method to compress datasets for scalable GNN training, faces issues with current techniques needing full dataset training and re-condensation upon changes. We introduce Bonsai, a novel, model-agnostic graph condensation method using exemplar computation trees, achieving superior accuracy and speed while remaining robust across various GNN architectures and parameters, with guaranteed mathematical reliability.

AutoG: Towards automatic graph construction from tabular data
This paper addresses the overlooked step of constructing graphs from tabular data in graph machine learning by formalizing the graph construction problem and presenting a solution. The authors introduce a set of datasets for evaluation and propose AutoG, an LLM-based tool that automatically generates high-quality graph schemas, demonstrating performance comparable to human experts.

### Deep Learning->Large Language Models
Advancing Mathematical Reasoning in Language Models: The Impact of Problem-Solving Data, Data Synthesis Methods, and Training Stages
This study investigates alternative strategies during the pre-training phase of large language models for mathematical reasoning, specifically through the use of problem-solving data rather than general mathematical corpora. The research finds that problem-solving data significantly boosts mathematical capabilities, with the tutorship amplification synthesis method yielding the best results, leading to the development of an advanced mathematical model, MathGPT-8B, that excels beyond traditional supervised fine-tuning techniques.

Human Simulacra: Benchmarking the Personification of Large Language Models
This paper presents a benchmark for the personification of Large Language Models (LLMs) to simulate human participants in social science experiments, potentially reducing research costs and complexity. It introduces a comprehensive framework including virtual character construction, a Multi-Agent Cognitive Mechanism, and a psychology-guided evaluation to ensure the simulated responses align with target personas, aiming to advance human simulation research.

Moral Alignment for LLM Agents
This paper introduces a novel approach to align Large Language Model (LLM) agents with human values by designing reward functions that explicitly encode ethical principles for reinforcement learning fine-tuning. By evaluating the model using frameworks like Deontological Ethics and Utilitarianism in the Iterated Prisoner's Dilemma, the study demonstrates that moral fine-tuning with intrinsic rewards can lead to transparent and effective alignment, offering a potential alternative to traditional methods relying on human feedback.

Efficient Inference for Large Language Model-based Generative Recommendation
The paper introduces AtSpeed, an alignment framework aimed at accelerating Large Language Model (LLM)-based generative recommendations by addressing the challenges of speculative decoding for top-K item generation. By proposing AtSpeed-S for strict top-K alignment and AtSpeed-R for relaxed sampling verification, the framework offers significant speed improvements, achieving nearly 2x speedup under strict conditions and up to 2.5x with relaxed conditions, as demonstrated on two real-world datasets.

CURIE: Evaluating LLMs on Multitask Scientific Long-Context Understanding and Reasoning
We introduce CURIE, a benchmark designed to evaluate the capabilities of Large Language Models (LLMs) in scientific problem-solving across ten tasks spanning six disciplines, including materials science and quantum computing. Our evaluation reveals significant performance variability among models, such as the strengths of Gemini Flash 2.0 and Claude-3 compared to GPT-4o on protein sequencing, highlighting a 32% best performance benchmark and underscoring the need for further LLM development in scientific contexts.

Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models
The paper introduces the Beyond the Imitation Game benchmark (BIG-bench), a comprehensive suite of 204 diverse tasks designed to evaluate the capabilities and limitations of large-scale language models, including those from OpenAI and Google. The findings highlight that while model performance generally improves with scale, human expert raters still outperform them, and social biases increase with scale, which can be mitigated through careful prompting.

Forgetting Transformer: Softmax Attention with a Forget Gate
The paper introduces the Forgetting Transformer (FoX), which incorporates a forget gate into the Transformer model by adjusting attention scores in a data-dependent way, enhancing its performance on various language modeling and extrapolation tasks. FoX maintains the Transformer's strengths in handling long contexts and improves its performance without requiring positional embeddings, also integrating a new "Pro" block design for further effectiveness.

Quamba: A Post-Training Quantization Recipe for Selective State Space Models
State Space Models (SSMs) present a promising alternative to Transformers for large language models, managing longer context lengths with improved computational efficiency. This paper introduces a static 8-bit per-tensor SSM quantization method that reduces model size while maintaining accuracy, achieving significant speedup on hardware like the Nvidia Orin Nano 8G, with minimal loss in performance, suggesting its potential for wide deployment across cloud and edge platforms.

Polyrating: A Cost-Effective and Bias-Aware Rating System for LLM Evaluation
Polyrating is a new rating system for evaluating large language models that addresses the biases and high costs associated with existing human evaluation methods. By using maximum a posteriori estimation, it provides a more nuanced analysis, reduces evaluation costs significantly, and allows for consistent model comparisons across different tasks, offering a well-rounded view of model performance.

MoS: Unleashing Parameter Efficiency of Low-Rank Adaptation with Mixture of Shards
The paper addresses the need for lightweight finetuning of large language models by proposing the Mixture of Shards (MoS) method, which combines inter-layer and intra-layer parameter sharing with four differentiation strategies to enhance parameter efficiency. This novel approach retains the benefits of low-rank adaptation (LoRA), achieves approximately $8\times$ parameter savings, and overcomes the limitations of existing parameter-sharing methods, providing a promising direction for future finetuning techniques.

NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models
The paper introduces NV-Embed, a decoder-only large language model-based embedding model, which surpasses BERT and T5-based models in text embedding tasks by incorporating a novel latent attention layer, removing the causal attention mask during training, and employing a two-stage contrastive instruction-tuning method. By utilizing curated datasets and advanced training techniques, NV-Embed achieved the top position on the Massive Text Embedding Benchmark and performed exceptionally on the AIR Benchmark, offering enhanced performance across diverse embedding tasks, with the model publicly available online.

Learning Diverse Attacks on Large Language Models for Robust Red-Teaming and Safety Tuning
This paper introduces a novel approach for generating diverse and effective attack prompts to ensure the safety of large language models (LLMs) by using GFlowNet fine-tuning followed by a secondary smoothing phase. The proposed method overcomes the limitations of existing automated red-teaming approaches and demonstrates robust effectiveness and transferability across various LLMs, enhancing their resilience against different attack strategies.

MAP: Multi-Human-Value Alignment Palette
The paper addresses the challenge of aligning generative AI systems with multiple human values, which can vary across demographics and adapt over time, by introducing the Multi-Human-Value Alignment Palette (MAP). MAP frames value alignment as an optimization challenge, effectively balancing trade-offs between values through user-defined constraints, and demonstrates its theoretical and empirical efficacy in systematically achieving multi-value alignment across various scenarios.

Scaling Large Language Model-based Multi-Agent Collaboration
This study investigates how multi-agent collaboration, inspired by the neural scaling law, can enhance autonomous task-solving when organized into a multi-agent collaboration network (MacNet) using directed acyclic graphs. Findings demonstrate that irregular topologies outperform regular ones and introduce a collaborative scaling law, where scaling agents leads to logistic growth in performance and earlier collaborative emergence, supported by extensive evaluations of over a thousand agents.

Steering Large Language Models between Code Execution and Textual Reasoning
This paper investigates the limitations of current methods for steering Large Language Models (LLMs) in switching between code generation and textual reasoning, finding no existing optimal approach across tasks and model types. It proposes three new methods to enhance this steering, achieving notable improvements, and emphasizes the importance of this challenge for future research, supported by comprehensive cost analyses and available datasets and project resources.

Context Steering: Controllable Personalization at Inference Time
This paper introduces Context Steering (CoS), a training-free decoding method that enhances large language models' ability to factor in user-specific context when generating responses. CoS allows for scalable and flexible personalization by adjusting contextual influence in predictions, proving effective in applications such as personalized recommendations and functioning as a Bayesian Generative model for broader use in correlating open-ended texts.

HiRA: Parameter-Efficient Hadamard High-Rank Adaptation for Large Language Models
We introduce Hadamard High-Rank Adaptation (HiRA), a parameter-efficient fine-tuning technique that improves the adaptability of Large Language Models by maintaining high-rank update parameters through a Hadamard product. HiRA surpasses Low-rank Adaptation (LoRA) in performance across multiple tasks, as confirmed by comprehensive ablation studies, with the code accessible for public use.

Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling
Samba is a hybrid architecture that combines a selective State Space Model with Sliding Window Attention to efficiently handle sequences with infinite context length, achieving high performance without the quadratic complexity of previous models. Scaled up to 3.8B parameters and tested on extensive benchmarks, it outperforms state-of-the-art models, with notable improvements in extrapolation capabilities and throughput, particularly excelling in tasks like Passkey Retrieval and Phonebook.

Weighted-Reward Preference Optimization for Implicit Model Fusion
This paper introduces Weighted-Reward Preference Optimization (WRPO), a novel implicit fusion method that enhances large language models (LLMs) by transferring capabilities between heterogeneous source and target models without the need for complex vocabulary alignment or matrix fusion. WRPO outperforms existing methods in benchmarks like MT-Bench, AlpacaEval-2, and Arena-Hard, demonstrating its effectiveness, scalability, and superiority in optimizing model performance.

DOCS: Quantifying Weight Similarity for Deeper Insights into Large Language Models
We introduce the Distribution of Cosine Similarity (DOCS), a novel index for assessing similarity between weight matrices in Large Language Models (LLMs), which reveals high similarity and clustering in adjacent layers, indicating depth-wise functional specialization. Furthermore, we demonstrate the theoretical effectiveness of DOCS in quantifying similarity for orthogonal matrices, contributing to enhanced understanding and development of more efficient and interpretable LLMs.

DeepRTL: Bridging Verilog Understanding and Generation with a Unified Representation Model
DeepRTL is a unified model based on CodeT5+ designed to enhance both the understanding and generation of Verilog code from natural language instructions, addressing gaps left by prior models that focused mainly on code generation. It introduces a new benchmark and utilizes advanced evaluation metrics to achieve superior performance in Verilog understanding, outperforming GPT-4, while also matching state-of-the-art models in Verilog code generation.

Long Context Compression with Activation Beacon
This paper introduces Activation Beacon, a module for transformer-based LLMs designed to efficiently compress long contexts by directly compressing activations rather than relying on soft prompts. The proposed method achieves high-quality compression and efficient computation, supporting various compression configurations, and demonstrates substantial improvements in inference time and memory efficiency while maintaining performance across challenging long-context tasks.

Do Vision & Language Decoders use Images and Text equally? How Self-consistent are their Explanations?
This work investigates the differing reliance on input modalities by vision and language model (VLM) decoders when generating explanations versus answers and evaluates their self-consistency. The study reveals that while VLMs prioritize text over image contributions across tasks, images play a more significant role in generating explanations, especially in CoT settings; however, these models remain less self-consistent than LLMs and struggle with VALSE benchmark phenomena.

Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization
This study improves the resilience of Direct Preference Optimization (DPO) against dataset noise by categorizing noise into pointwise and pairwise types and applying Distributionally Robust Optimization (DRO). It introduces Dr. DPO, an extension that optimizes against worst-case scenarios with a new hyperparameter, demonstrating marked improvements in text quality and response accuracy across various dataset conditions.

MathGAP: Out-of-Distribution Evaluation on Problems with Arbitrarily Complex Proofs
Large language models (LLMs) can solve arithmetic word problems effectively, but their generalization to complex problems remains underexplored due to pre-exposure to evaluation data and inadequate benchmarks. The paper introduces MathGAP, a data-generation framework to assess LLMs on complex arithmetic proofs, revealing decreased performance as proof complexity increases and sensitivity to sentence order changes, while still achieving some success, indicating noisy reasoning generalization.

RocketEval: Efficient automated LLM evaluation via grading checklist
This paper introduces RocketEval, a cost-effective and accurate automated evaluation method using a lightweight LLM as the judge to assess language models, overcoming challenges like high uncertainty and positional bias in conventional evaluations. Experiments show that RocketEval, particularly with the $\textit{Gemma-2-2B}$ model, achieves a strong correlation with human preferences similar to $\textit{GPT-4o}$, offering over 50-fold cost reduction in large-scale evaluations.

MIA-Bench: Towards Better Instruction Following Evaluation of Multimodal LLMs
The paper introduces MIA-Bench, a benchmark evaluating Multimodal Large Language Models (MLLMs) on their ability to comply with complex, layered instructions through 400 diverse image-prompt pairs. The study uncovers significant performance variations among models, suggesting areas for improvement and proposing enhancements like extra training data and fine-tuning to boost instruction adherence while maintaining overall task performance.

Unveiling the Magic of Code Reasoning through Hypothesis Decomposition and Amendment
This paper introduces a novel task called code reasoning to enhance the understanding of reasoning abilities in large language models (LLMs), accompanied by three meta-benchmarks and eight specific benchmark tasks. It presents the Reflective Hypothesis Decomposition and Amendment (RHDA) pipeline, which significantly improves LLM performance in reasoning tasks and has been successfully applied to complex household scenarios, demonstrating up to a threefold increase in effectiveness.

Self-Correcting Decoding with Generative Feedback for Mitigating Hallucinations in Large Vision-Language Models
This paper addresses the issue of hallucination in Large Vision-Language Models (LVLMs) by leveraging text-to-image generative models to provide self-feedback for mitigating inaccuracies. The authors introduce a novel, training-free algorithm called Decoding with Generative Feedback (DeGF), which effectively reduces hallucinations by using generative models to create auxiliary visual references that guide the correction of initial model responses, demonstrating superior performance compared to existing methods on multiple benchmarks.

VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning
This study introduces VL-ICL Bench, a comprehensive benchmark for evaluating multimodal in-context learning (ICL) involving both image and text tasks, addressing the under-explored capabilities and limitations of vision large language models (VLLMs). By assessing state-of-the-art VLLMs, including GPT-4, against this suite, the research reveals their diverse strengths and weaknesses and aims to inspire advancements in VLLM ICL capabilities and applications.

Towards Federated RLHF with Aggregated Client Preference for LLMs
This paper introduces Federated Reinforcement Learning with Human Feedback (RLHF) methods, namely FedBis and FedBiscuit, which allow large language models to be fine-tuned using user preferences without compromising privacy. By leveraging federated learning techniques, these methods aggregate binary-encoded client preferences to address challenges such as preference heterogeneity and reward hacking, resulting in improved content professionalism and readability, as demonstrated by their performance on the first federated RLHF benchmark with a diverse dataset.

PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs
This paper investigates the stability of language model pre-training and its effect on downstream performance, focusing on the impact of slight variations in initial conditions such as random seeds. By introducing 45 new training runs for the Pythia model across various sizes, the authors analyze how these conditions affect performance, linguistic representations, and training phases, revealing consistent dynamics and identifying outlier characteristics, ultimately contributing methods to predict training stability.

CycleResearcher: Improving Automated Research via Automated Review
This paper investigates the use of open-source large language models (LLMs) for automating the entire research process, including literature review, manuscript preparation, and peer review, through a framework of CycleResearcher and CycleReviewer. It introduces new datasets, Review-5k and Research-14k, and demonstrates promising results, with CycleReviewer achieving a 26.89% reduction in mean absolute error in paper score prediction compared to human reviewers, accentuating the potential of LLMs to support expert-level research evaluation and innovation.

Implicit In-context Learning
In this work, we introduce Implicit In-context Learning (I2CL), a method that reduces the inference costs of in-context learning (ICL) to the level of zero-shot learning while maintaining performance by using a context vector to influence model activations. I2CL is shown to deliver few-shot performance with zero-shot cost across multiple tasks and model architectures, and it enhances task similarity detection and transfer learning through a novel representation of task identities.

How Does Vision-Language Adaptation Impact the Safety of Vision Language Models?
VL adaptation, which transforms Large Language Models (LLMs) into Large Vision-Language Models (LVLMs), compromises the inherent safety capabilities of original LLMs, a concern that is under-explored. This study reveals that while safety tuning techniques can mitigate some risks, they often result in safety degradation and reduced helpfulness, and proposes a weight merging approach to effectively balance safety and functionality in LVLMs.

Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models
This paper introduces a model merging methodology to enhance the performance of Large Language Models (LLMs) on mathematical reasoning tasks in non-English languages, where task-specific data is lacking. By fine-tuning models on English math instructions and target language data and then swapping key transformer layers, the merged models significantly outperform individual experts, achieving a 10% improvement on the MGSM benchmark across four languages, demonstrating a cost-effective approach to cross-lingual knowledge transfer.

On-the-fly Preference Alignment via Principle-Guided Decoding
The paper introduces On-the-fly Preference Alignment via Principle-Guided Decoding (OPAD), a method that aligns large language model outputs with human preferences during inference, avoiding the need for resource-intensive fine-tuning. OPAD enhances model adherence to principles by employing a principle-guided reward function to modify predictions and demonstrates competitive performance in both general and personalized alignment tasks without the computational burden associated with traditional methods.

Radar: Fast Long-Context Decoding for Any Transformer
Radar is a training-free approach designed to accelerate inference in Transformer models by efficiently identifying the most important context tokens, thereby reducing the quadratic time complexity associated with dot-product attention in long-context data. This method achieves state-of-the-art performance across various tasks and architectures without requiring additional training, providing a practical solution for more efficient long-context processing.

Revisiting In-context Learning Inference Circuit in Large Language Models
This paper presents a comprehensive circuit model for understanding the inference dynamics of In-context Learning (ICL) in Language Models, addressing the gaps in existing approaches. By dividing ICL into three key operations—Input Text Encode, Semantics Merge, and Feature Retrieval and Copy—the model captures and explains numerous phenomena, demonstrating its significance through ablation tests and revealing bypass mechanisms that occasionally solve ICL tasks parallelly.

ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery
This paper introduces ScienceAgentBench, a benchmark designed to evaluate the capabilities of language agents in automating tasks essential for scientific discovery across four disciplines. Results show that even the best-performing language models currently solve only about a third of the tasks, highlighting their limitations and emphasizing the need for rigorous task-based evaluation before claiming end-to-end automation in scientific research workflows.

### Deep Learning->Other Representation Learning
CSA: Data-efficient Mapping of Unimodal Features to Multimodal Features
We introduce Canonical Similarity Analysis (CSA), which employs two unimodal encoders to replicate multimodal encoders, significantly reducing the need for extensive multimodal training data. CSA outperforms existing methods like CLIP by requiring significantly fewer data pairs while effectively mapping unimodal features into a multimodal space, and is compatible with various modality pairs, including beyond image and text.

Enhance Multi-View Classification Through Multi-Scale Alignment and Expanded Boundary
This paper addresses the challenges of feature heterogeneity and information redundancy in multi-view classification by introducing a multi-scale alignment module to improve feature fusion and an expanded boundary using fuzzy set theory to handle ambiguous data. The proposed model's integration of these techniques is shown to enhance classification performance and generalization across diverse datasets, outperforming existing methods.

Gramian Multimodal Representation Learning and Alignment
This paper presents the Gramian Representation Alignment Measure (GRAM), a novel approach to multimodal learning that enables the direct alignment of multiple modalities in a higher-dimensional space, addressing the limitations of pairwise contrastive learning models. By minimizing the Gramian volume of the parallelotope spanned by modality vectors, GRAM improves multimodal model performance, achieving state-of-the-art results in tasks like video-audio-text retrieval and audio-video classification.

Understanding the Stability-based Generalization of Personalized Federated Learning
This paper introduces an algorithm-dependent generalization analysis with uniform stability for Personalized Federated Learning (PFL), focusing on smooth and non-convex objectives, and decomposes generalization errors into aggregation and fine-tuning errors. The proposed framework connects PFL with Federated Learning and Pure Local Training, detailing the impact of factors like learning steps and communication topologies, and is supported by experiments on CIFAR datasets demonstrating theoretical insights.

### Deep Learning->Robustness
Adversarial Perturbations Cannot Reliably Protect Artists From Generative AI
This paper assesses the effectiveness of popular protection tools designed to safeguard artists' styles from being replicated by image generation models, revealing that these tools only offer a false sense of security. The study demonstrates that simple techniques, such as image upscaling, can easily bypass existing protections, highlighting the need for more reliable solutions beyond adversarial perturbations.

Can We Ignore Labels in Out of Distribution Detection?
This paper explores the failure of unlabeled out-of-distribution (OOD) detection methods in real-world data scenarios and introduces a new task called Adjacent OOD detection to address gaps in safety benchmarks. The authors provide a theoretical framework showing that unlabeled OOD detection fails when there is zero mutual information between the learning objective and in-distribution labels, and experimentally demonstrate existing methods' vulnerability to these conditions, highlighting the need for improved approaches in OOD detection research.

Adversarial Training for Defense Against Label Poisoning Attacks
This paper introduces $\textbf{Floral}$, an adversarial training defense strategy using support vector machines to counter label poisoning attacks that degrade machine learning model performance. Through a bilevel optimization framework and projected gradient descent, $\textbf{Floral}$ effectively improves robust accuracy against such attacks across various classification tasks, outperforming robust baselines and foundation models like RoBERTa.

Interpreting Global Perturbation Robustness of Image Models using Axiomatic Spectral Importance Decomposition
This paper introduces I-ASIDE, a model-agnostic method using Shapley value theory to provide a global mechanistic interpretation of perturbation robustness in image models by quantifying the predictive power of robust and non-robust features. Extensive experiments on various pre-trained vision models demonstrate I-ASIDE's ability to measure and interpret mechanisms of perturbation robustness effectively.

### Deep Learning->Self-Supervised Learning
Beyond Random Augmentations: Pretraining with Hard Views
This paper introduces Hard View Pretraining (HVP), a strategy for self-supervised learning that selects challenging image augmentations to enhance model learning by focusing on samples that produce higher loss. Demonstrating effectiveness at scale on the ImageNet-1k dataset, HVP improves state-of-the-art performance for various SSL methods and architectures, achieving a 0.6% improvement in linear evaluation accuracy and consistent gains across multiple tasks.

SSLAM: Enhancing Self-Supervised Models with Audio Mixtures for Polyphonic Soundscapes
This paper introduces Self-Supervised Learning from Audio Mixtures (SSLAM), a novel approach designed to enhance the performance of audio self-supervised learning models in handling polyphonic audio while maintaining effectiveness with monophonic data. SSLAM shows significant performance improvements on polyphonic datasets, setting new state-of-the-art benchmarks, and also achieves up to a 3.9% increase in mean average precision on the AudioSet-2M dataset, demonstrating its robustness and versatility in various audio environments.

### Deep Learning->Sequential Models, Time series
Neural Functions for Learning Periodic Signal
This study introduces a novel network architecture designed to improve the generalization and extrapolation performance of coordinate-based MLPs by extracting and leveraging periodic patterns in signals. Demonstrated through experiments on differential equations and real-world datasets, the approach enhances signal representation by addressing limitations in traditional MLP methods when dealing with periodic properties.

Scalable Mechanistic Neural Networks
The paper introduces Scalable Mechanistic Neural Network (S-MNN), a framework that optimizes the original Mechanistic Neural Network (MNN) for scientific machine learning by reducing computational complexities to linear with respect to sequence length. S-MNN maintains accuracy and interpretability while significantly lowering resource demands, making it a practical substitute for MNN in modeling long-term dynamics in complex systems.

From Layers to States: A State Space Model Perspective to Deep Neural Network Layer Dynamics
This paper introduces a novel approach to enhance the representational power of deep neural networks by treating layer outputs as states of a continuous process and applying state space models (SSM) for layer aggregation. By integrating the Selective State Space Models (S6) into a new module called S6LA, the study demonstrates significant improvements in image classification and detection tasks over traditional CNN and transformer architectures.

### Deep Learning->Theory
A Rainbow in Deep Network Black Boxes
This paper introduces the "rainbow model," a deep extension of random feature models, and demonstrates that deep networks define deterministic hierarchical kernels in the infinite-width limit, resulting in functions belonging to a data-dependent RKHS that are independent of weight randomness. Numerical verifications on deep CNNs for image classification show these networks satisfy the rainbow hypothesis, with networks sampled from the random feature model achieving performance similar to trained networks, underscoring the importance of low-rank weight covariances in feature learning.

Learning Hierarchical Polynomials of Multiple Nonlinear Features
This paper studies the learning of hierarchical polynomials using three-layer neural networks to understand how they learn multiple nonlinear features, expanding upon previous studies limited to a single feature. It demonstrates that using layerwise gradient descent, these networks can efficiently recover and learn functions within a sample complexity of $\widetilde{\mathcal{O}}(d^4)$, significantly outperforming kernel methods and enhancing the understanding of feature learning in deep learning.

### Misc
GeoLoRA: Geometric integration for parameter efficient fine-tuning
GeoLoRA is introduced as an innovative approach for improving the efficiency and robustness of Low-Rank Adaptation (LoRA) in fine-tuning large-scale pre-trained neural networks. By utilizing dynamical low-rank approximation theory, GeoLoRA achieves faster adaptation with reduced computational costs and smaller low-rank adapters, outperforming existing methods like AdaLoRA in both accuracy and efficiency across various benchmarks.

Uncertainty and Influence aware Reward Model Refinement for Reinforcement Learning from Human Feedback
Reinforcement Learning from Human Feedback (RLHF) effectively trains large language models by approximating human preferences with a learned reward model, though challenges arise when the model doesn't perfectly represent true preferences. To address these challenges, the paper introduces Uncertainty-Gradient based Data Augmentation (UGDA), a method that refines reward modeling using policy interaction samples, improving policy performance without the need for costly human preference data collection, and surpassing state-of-the-art methods in experiments.

Adaptive Gradient Clipping for Robust Federated Learning
This paper introduces Adaptive Robust Clipping (ARC), a dynamic clipping strategy for robust federated learning that adjusts clipping thresholds based on input gradients. ARC retains the theoretical robustness of state-of-the-art Robust-DGD methods and improves asymptotic convergence, with experiments showing enhanced performance in diverse and adversarial environments.

Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic Vision-language Context Sparsification
Dynamic-LLaVA introduces a dynamic vision-language context sparsification framework to enhance the efficiency of multimodal large language models (MLLMs) by reducing the vision context redundancy during the prefill stage and optimizing the language context during decoding. This approach significantly lowers computation and memory overhead—by approximately 75% during prefill and 50% during decoding—without compromising performance, and the code for this method is publicly available.

Skill Expansion and Composition in Parameter Space
The paper introduces Parametric Skill Expansion and Composition (PSEC), a framework that enables autonomous agents to efficiently develop and evolve their skills by using a manageable skill library that iteratively incorporates skill primitives as plug-and-play Low-Rank Adaptation (LoRA) modules. This approach allows for efficient, flexible skill expansion and composition, demonstrating superior performance on benchmarks such as D4RL and the DeepMind Control Suite in leveraging prior knowledge to tackle new challenges and evolve agent capabilities.

GeoILP: A Synthetic Dataset to Guide Large-Scale Rule Induction
Inductive logic programming (ILP) systems face challenges in addressing large-scale tasks due to their reliance on expert-defined language biases. This paper introduces GeoILP, a comprehensive synthetic dataset featuring diverse ILP tasks derived from geometry problems, aiming to test and expand the capability of ILP systems in handling complex language biases and promoting joint learning through additional image-form data.

3D-Properties: Identifying Challenges in DPO and Charting a Path Forward
This paper revisits Direct Preference Optimization (DPO) as a more efficient alternative to Proximal Policy Optimization (PPO) for aligning large language models (LLMs), identifying key issues with DPO's optimization dynamics that lead to instability and offering regularization techniques to improve performance. The authors provide insights into the impact of preference data distribution on DPO's effectiveness and aim to guide future advancements in reward-model-free preference learning, bridging the gap with reward-model-based methods.

Homomorphism Counts as Structural Encodings for Graph Learning
This paper introduces Motif Structural Encoding (MoSE), a novel structural encoding framework for Graph Transformers that leverages graph homomorphism counting to enhance model expressiveness. Both theoretically and empirically, MoSE demonstrates superior performance over existing encodings, achieving state-of-the-art results in molecular property prediction tasks.

Training-Free Diffusion Model Alignment with Sampling Demons
We propose a novel approach called *Demon* for aligning diffusion models with user preferences using stochastic optimization at inference time, eliminating the need for model retraining or backpropagation through reward functions. This method, which works by controlling noise distribution during denoising steps, is validated by comprehensive experiments and significantly enhances aesthetics in text-to-image generation, with implementation available online.

Quest: Query-centric Data Synthesis Approach for Long-context Scaling of Large Language Model
This paper introduces Quest, a query-centric data synthesis method designed to enhance large language model performance by balancing semantic coherence and diversity in long-context tasks. By using a generative model to predict queries for document grouping, Quest outperforms traditional methods, demonstrating superior scalability and effectiveness with context lengths up to 1 million tokens.

Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?
This paper introduces "FTCT," a synthetic learning task designed to test Transformers' ability to perform compositional reasoning by integrating separated knowledge fragments during testing. The study finds that few-shot Chain-of-Thought prompting allows Transformers to successfully combine these fragments, revealing that the emergence of this reasoning ability is linked to model complexity and training-testing data similarity, suggesting that Transformers learn a generalizable program for effective reasoning.

A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules
This paper introduces an empirical law that accurately predicts the pretraining loss for large language models at various training steps across different learning rate schedules. By validating this law on Llama-2 models and discovering an optimized learning rate schedule similar to the Warmup-Stable-Decay schedule, the study offers insights into pretraining dynamics and enhances the efficiency of learning rate schedule design.

CraftRTL: High-quality Synthetic Data Generation for Verilog Code Models with Correct-by-Construction Non-Textual Representations and Targeted Code Repair
This paper addresses the challenges of code generation in Verilog using large language models by analyzing fine-tuned models, identifying issues with non-textual representations, and training variability. The authors improve data curation with correct-by-construction data and develop an automated framework for error report generation and targeted code repair, ultimately enhancing the performance of Starcoder2-15B, which surpasses previous state-of-the-art results across various benchmarks.

SMITE: Segment Me In TimE
This paper introduces a novel method for video object segmentation using a pre-trained text-to-image diffusion model combined with a tracking mechanism to handle arbitrary granularity. The proposed approach outperforms current state-of-the-art methods by accurately maintaining consistent pixel labeling across frames.

On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent
This paper investigates how Sign Gradient Descent (SignGD) optimizes transformers, specifically focusing on a two-layer transformer model on a linearly separable noisy dataset. The authors identify four training dynamics stages, demonstrating fast convergence but poor generalization, and show that Adam behaves similarly, suggesting that both optimizers require high-quality data for effective real-world application, a finding supported by empirical experiments.

Towards Unbiased Learning in Semi-Supervised Semantic Segmentation
This paper addresses the class imbalance issue in semi-supervised semantic segmentation by integrating generative models, which are more tolerant to class imbalances, into the learning process. The proposed method, DiffMatch, formulates the task as a conditional discrete data generation problem and includes a debiased adjustment to counteract overfitting, showing superior performance in label-scarce scenarios compared to state-of-the-art methods.

LaMPlace: Learning to Optimize Cross-Stage Metrics in Macro Placement
Machine learning applications in macro placement for chip design often prioritize online optimization of intermediate surrogate metrics, neglecting cross-stage metrics like timing performance due to computational constraints. The proposed LaMPlace addresses this by training an offline predictor to generate a mask that efficiently evaluates macro placement impact, resulting in significant improvements in chip quality with an average of 9.6% and notable enhancements in crucial timing metrics WNS and TNS by 43.0% and 30.4%, respectively.

TabWak: A Watermark for Tabular Diffusion Models
This paper introduces TabWak, the first watermarking method specifically designed for synthetic tables, embedding invisible signatures by manipulating Gaussian latent codes during table synthesis. The study demonstrates TabWak's ability to maintain high-quality, detectable tables even under strong post-editing attacks, achieving a 100% true positive rate with minimal false positives, and offers theoretical and practical validation on multiple datasets.

CHiP: Cross-modal Hierarchical Direct Preference Optimization for Multimodal LLMs
Multimodal Large Language Models (MLLMs) often experience hallucinations, and this paper addresses the challenge by introducing a Cross-modal Hierarchical Direct Preference Optimization (CHiP) method. By incorporating visual and hierarchical textual preference optimization, CHiP effectively reduces hallucinations, as evidenced by substantial improvements on the Object HalBench dataset, outperforming existing models with significant relative point gains.

Beyond FVD: An Enhanced Evaluation Metrics for Video Generation Distribution Quality
The paper critiques the Fréchet Video Distance (FVD) metric for video generation evaluation, identifying its three major limitations related to non-Gaussian feature space, temporal distortion insensitivity, and impractical sample size needs. It introduces JEDi, a superior metric utilizing JEPA features and Maximum Mean Discrepancy, which demonstrates improved alignment with human evaluation and reduced sample size requirements across multiple datasets.

Differential Transformer
The Diff Transformer introduces a differential attention mechanism that amplifies relevant context and cancels noise, leading to more sparse attention patterns and improved performance over traditional Transformers. Its advantages include enhanced language modeling, better handling of long contexts, improved accuracy and robustness in in-context learning, and mitigation of hallucination in applications such as question answering and text summarization.

Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages
This paper presents PANGEA, a multilingual multimodal large language model trained on the PANGEAINS dataset, which includes 6 million instructions across 39 languages with an emphasis on cultural relevance. The study demonstrates PANGEA's superior performance over existing models in diverse multilingual settings, supported by PANGEABENCH evaluations, and highlights factors influencing success while offering open-source resources to advance inclusive language model development.

Quantifying Generalization Complexity for Large Language Models
This paper introduces Scylla, a dynamic evaluation framework designed to quantitatively assess the generalization abilities of large language models (LLMs) by disentangling them from memorization. Through benchmarking 28 LLMs on a variety of tasks, the study reveals the concept of a "generalization valley" and identifies a critical complexity threshold, enhancing understanding of how model size affects the balance between generalization and memorization.

Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos
Agent-to-Sim (ATS) is a framework that learns 3D interactive behavior models of agents using casual longitudinal videos, without relying on invasive methods like marker-based tracking or multiview cameras. By developing a coarse-to-fine registration method, ATS enables the persistent 3D tracking and modeling of agent behaviors through a spacetime 4D representation, facilitating real-to-sim transfer for interactive behavior simulation.

Generalizable Human Gaussians from Single-View Image
This paper presents a novel approach to learning 3D human Gaussians from a single image, introducing a Human Gaussian Model (HGM) that uses a generate-then-refine pipeline supported by body and diffusion priors. The model improves upon previous methods in novel view synthesis and surface reconstruction by refining initial predictions with SMPL-X model priors, demonstrating strong generalization across datasets and real-world images.

Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing
Probe Pruning (PP) is introduced as a novel framework for online, dynamic, structured pruning of Large Language Models, which effectively identifies crucial weights through batch-wise pruning based on residual importance. This approach, compatible with existing models and requiring no additional modules or fine-tuning, significantly enhances pruning efficiency as demonstrated by improved performance ratios in comprehensive evaluations on models like LLaMA-2/3 and OPT.

cryoSPHERE: Single-Particle HEterogeneous REconstruction from cryo EM
CryoSPHERE is a deep learning method that enhances the reconstruction of protein structural ensembles from cryo-EM data by leveraging a nominal protein structure to model different conformations. Demonstrated on both synthetic and real datasets, cryoSPHERE outperforms current methods in handling noisy data, providing more reliable insights into protein conformational heterogeneity.

Functional Homotopy: Smoothing Discrete Optimization via Continuous Parameters for LLM Jailbreak Attacks
This study presents the *functional homotopy* method, an innovative optimization approach that addresses challenges in applying gradient-based techniques to language models due to their discrete input space. By iteratively solving a series of optimization problems, this method enhances the synthesis of jailbreak attacks on large language models, improving success rates by 20%-30% compared to current techniques on safe models like Llama-2 and Llama-3.

Memory Mosaics
Memory Mosaics are networks of associative memories that collaboratively perform prediction tasks with compositional and in-context learning abilities, similar to transformers. They achieve these capabilities more transparently through "predictive disentanglement" and demonstrate comparable or superior performance to transformers in medium-scale language modeling tasks.

Unifying Unsupervised Graph-Level Anomaly Detection and Out-of-Distribution Detection: A Benchmark
This paper introduces the Unified Benchmark for unsupervised Graph-level OOD and anomaly Detection (UB-GOLD), which bridges the gap between unsupervised graph-level anomaly detection (GLAD) and out-of-distribution detection (GLOD) by providing a comprehensive evaluation framework. By encompassing 35 datasets and allowing comparison of 18 methods, the benchmark facilitates analysis of existing approaches' effectiveness and robustness, while offering an open-source codebase for reproducible research and future exploration.

Teaching Human Behavior Improves Content Understanding Abilities Of VLMs
This paper demonstrates that training vision-language models (VLMs) to predict receiver behaviors, such as likes, comments, and replay graphs, enhances their content-understanding abilities across various tasks. By leveraging behavior data that is readily available online without additional human annotation, the approach outperforms numerous supervised baselines on tasks like emotion recognition and captioning by up to 150%, and the authors release the **BLIFT** dataset to support this research.

Programming Refusal with Conditional Activation Steering
This paper introduces Conditional Activation Steering (CAST), a method for selectively applying or withholding activation steering in large language models (LLMs) based on distinct activation patterns triggered by input context. CAST enables precise control over LLM behavior for applications like content moderation or specialized domains without altering model weights, and the authors provide an open-source implementation of their framework.

Robotouille: An Asynchronous Planning Benchmark for LLM Agents
Robotouille is a novel benchmark environment developed to assess the capability of large language model (LLM) agents to manage long-horizon asynchronous planning challenges, which are not sufficiently covered by current short-horizon task benchmarks. The study reveals that the ReAct (gpt-4o) model performs significantly better on synchronous tasks compared to asynchronous ones, underscoring the need for improved methods for integrating long-horizon feedback and self-auditing in LLM agents.

Vision Language Models are In-Context Value Learners
Generative Value Learning (GVL) is introduced as a universal value function estimator leveraging vision-language models to predict task progress by addressing the limitations of temporal correlation in video frames. GVL demonstrates its ability to predict values zero-shot and few-shot across over 300 real-world robot tasks, enabling applications in visuomotor policy learning such as dataset filtering and success detection, without requiring specific training or finetuning.

Procedural Knowledge in Pretraining Drives Reasoning in Large Language Models
This paper examines the generalisation strategies of Large Language Models (LLMs) by analyzing the pretraining data influencing their outputs during reasoning tasks. The study reveals that LLMs use procedural knowledge from influential documents, which provide generalisable reasoning strategies, rather than simply retrieving answers from data.

PICASO: Permutation-Invariant Context Composition with State Space Models
This paper explores improving Large Language Models by using State Space Models (SSMs) to map contexts onto fixed-dimensional states, allowing for more efficient information processing from multiple contexts during inference. The authors propose a method to compose multiple contexts into one approximate state, demonstrating that their approach maintains performance comparable to top baselines with a significant increase in speed, as evaluated on WikiText and MSMARCO datasets.

TimeMixer++: A General Time Series Pattern Machine for Universal Predictive Analysis
This paper introduces the Time Series Pattern Machine (TSPM), a model designed to improve time series tasks by utilizing multi-resolution and multi-scale temporal pattern extraction techniques. Employing methods like Multi-Resolution Time Imaging (MRTI) and Multi-Scale Mixing (MCM), the proposed TSPM achieves state-of-the-art results across various time series applications, outperforming both generalist and specialist models.

SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal
SORRY-Bench is a proposed benchmark that addresses the limitations in evaluating large language models' (LLMs) ability to recognize and reject unsafe user requests. It improves upon existing methods by utilizing a fine-grained taxonomy of 44 unsafe topics, diverse linguistic augmentations, and creating a cost-effective, accurate safety evaluator that allows systematic evaluation of over 50 LLMs, providing a foundation for balanced and efficient safety assessments.

Bayesian Regularization of Latent Representation
This paper introduces the Q-exponential Process Latent Variable Model (QEP-LVM), a novel class of latent variable models that extends Gaussian Process Latent Variable Models (GP-LVM) with a tunable complexity parameter for enhanced flexibility and interpretability. Demonstrating superior performance over traditional methods such as probabilistic PCA, QEP-LVM is validated through experiments on datasets like the Swiss roll, oil flow, and handwritten digits, and achieves scalability via sparse variational inference within a Bayesian framework.

Adaptive Energy Alignment for Accelerating Test-Time Adaptation
This paper introduces an adaptive energy alignment (AEA) approach for fast online test-time adaptation (TTA) to address the limitations of traditional entropy minimization loss in out-of-domain scenarios. By aligning the energy levels and class-wise correlations between source and target domains, the proposed method effectively accelerates model adaptation and demonstrates significant performance improvements on datasets such as CIFAR10-C, CIFAR100-C, and TinyImageNet-C.

Does Spatial Cognition Emerge in Frontier Models?
SPACE is introduced as a benchmark to systematically assess spatial cognition in advanced models, building upon cognitive science research to evaluate mapping abilities, object reasoning, and spatial memory. Findings indicate that current models significantly underperform compared to animals, especially in traditional cognitive tasks.

THE ROBUSTNESS OF DIFFERENTIABLE CAUSAL DISCOVERY IN MISSPECIFIED SCENARIOS
This paper benchmarks the empirical performance of various mainstream causal discovery algorithms under eight model assumption violations, highlighting their robustness across challenging scenarios, except for scale variation. It provides theoretical explanations and establishes standards for evaluating differentiable causal discovery methods to enhance their applicability in real-world situations.

DenseMatcher: Learning 3D Semantic Correspondence for Category-Level Manipulation from a Single Demo
DenseMatcher is a method for computing 3D correspondences between structurally similar objects, enhancing robotic manipulation and enabling zero-shot color mapping between digital assets. By projecting multiview 2D features onto meshes and refining them with a 3D network, DenseMatcher achieves effective generalization across instances and categories, and is supported by a novel 3D matching dataset featuring diverse colored object meshes.

Uncertainty Modeling in Graph Neural Networks via Stochastic Differential Equations
This paper introduces Latent Graph Neural Stochastic Differential Equations (LGNSDE), a novel framework that extends Graph Neural Ordinary Differential Equations (GNODEs) to quantify uncertainty in graph-structured data using a Bayesian approach and Brownian motion. The proposed method provides theoretical guarantees for uncertainty estimates and demonstrates robustness in out-of-distribution detection, noise perturbations, and active learning, as validated by empirical benchmarks.

System 1.x: Learning to Balance Fast and Slow Planning with Language Models
This paper introduces the System-1.x Planner, which enables controllable planning with language models by balancing between fast, direct planning (System-1) and slower, explicit search planning (System-2) based on problem difficulty. The framework improves performance in Maze Navigation and Blocksworld tasks, demonstrating controllability, flexibility, and robustness by allowing user-specified planning hybridization, integrating symbolic methods, and learning from various search algorithms.

Hydra-SGG: Hybrid Relation Assignment for One-stage Scene Graph Generation
Hydra-SGG addresses the issues of sparse supervision and false negative samples in DETR-based scene graph generation by introducing a Hybrid Relation Assignment that boosts positive training samples through a One-to-One and IoU-based One-to-Many Relation Assignment. This method, along with the auxiliary Hydra Branch decoder, achieves state-of-the-art performance on major datasets like VG150, Open Images V6, and GQA, and emphasizes the benefits of removing self-attention to improve predictions.

Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits
This paper explores multi-draft speculative sampling, introducing a two-step solution involving importance sampling and speculative sampling to optimize token selection, ultimately aligning the output distribution with the target model. Theoretical analysis leads to new token-level selection schemes and experiments confirm improved block efficiency and token rates compared to existing methods.

The Last Iterate Advantage: Empirical Auditing and Principled Heuristic Analysis of Differentially Private SGD
This paper introduces a heuristic privacy analysis for DP-SGD, focusing on scenarios where only the final iterate is disclosed while intermediate iterates are concealed. The authors demonstrate that this heuristic can serve as an early approximate predictor of privacy leakage before training, highlighting its predictive power and identifying areas for improving theoretical privacy analyses.

The Illustrated AlphaFold
Illustrated AlphaFold offers a detailed visual guide to the architecture and information flow of AlphaFold 3, highlighting advances from AlphaFold 2 such as a unified tokenization scheme for diverse biomolecules and a novel diffusion-based structural module. The paper also shares insights on machine learning lessons drawn from the development of AlphaFold 3.

XLand-100B: A Large-Scale Multi-Task Dataset for In-Context Reinforcement Learning
The paper introduces **XLand-100B**, a substantial dataset designed to advance the field of in-context reinforcement learning by providing comprehensive learning histories for nearly 30,000 tasks. By offering a challenging benchmark and the tools to reproduce or expand the dataset, it aims to democratize research and aid in scaling efforts in this expanding discipline.

Language models scale reliably with over-training and on downstream tasks
This paper addresses gaps in current scaling studies by creating a testbed of 104 language models to analyze both over-training and parameter scaling, predicting validation loss and downstream task performance with significant compute reductions. The authors propose scaling laws that extend extrapolation capabilities and introduce a power law to relate perplexity to downstream performance, offering more cost-effective methods to predict model outcomes.

IV-mixed Sampler: Leveraging Image Diffusion Models for Enhanced Video Synthesis
This paper introduces IV-Mixed Sampler, a novel algorithm that enhances video diffusion models (VDMs) by leveraging image diffusion models (IDMs) to improve frame quality while maintaining temporal coherence. Experimental results demonstrate that IV-Mixed Sampler achieves state-of-the-art performance across multiple benchmarks, significantly reducing metrics such as the UMT-FVD score, and closing the gap to the performance of closed-source models.

Zero-cost Proxy for Adversarial Robustness Evaluation
This paper addresses the challenge of designing adversarially robust neural network architectures via neural architecture search (NAS) by introducing a zero-cost proxy that evaluates adversarial robustness without the need for training. The proposed method offers a theoretical foundation based on neural tangent kernel and input loss landscape, achieving over $20\times$ speedup in NAS compared to existing methods, with improved robustness and transferability against attacks.

Mastering Task Arithmetic: $\tau$Jp as a Key Indicator for Weight Disentanglement
This paper addresses the challenges in task arithmetic for model-editing, such as reproducibility and high adjustment costs, by introducing a novel metric, $\tau$Jp, based on the interaction of task vectors and the Jacobian of pre-trained models. The $\tau$Jp regularization effectively reduces interference and improves accuracy in task inference without coefficient tuning, and enhances robustness in incremental learning, demonstrating practical advantages in using fine-tuned models for real-world applications.

6DGS: Enhanced Direction-Aware Gaussian Splatting for Volumetric Rendering
This paper introduces 6D Gaussian Splatting (6DGS), an advancement in rendering techniques that enhances color and opacity representations within a 6D spatial-angular framework to better optimize view-dependent effects and fine details in real-time radiance field rendering. Experiments show that 6DGS significantly outperforms previous methods like 3D Gaussian Splatting and N-dimensional Gaussians, achieving substantial improvements in image quality and computational efficiency.

E-Valuating Classifier Two-Sample Tests
E-C2ST is a novel deep classifier two-sample test employing E-values for high-dimensional data, offering anytime-valid sequential testing and enhancing statistical power through multi-batch dataset partitioning. This method improves upon traditional two-sample tests by maintaining low type I error rates while more effectively utilizing data for constructing test statistics.

SBSC: Step-by-Step Coding for Improving Mathematical Olympiad Performance
The paper introduces Step-by-Step Coding (SBSC), a multi-turn math reasoning framework designed for Large Language Models to solve Olympiad-level math problems by generating sequences of programs. SBSC demonstrates superior performance in solving high-level math problems compared to existing methods, with significant improvements in accuracy across various benchmarks such as AMC12, AIME, and MathOdyssey.

Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance
The paper investigates the phenomenon of "context-parametric inversion" in which instruction-finetuned large language models (LLMs) exhibit decreased reliance on input context as finetuning progresses, contrary to expectations. Through controlled studies on datasets like TULU, Alpaca, and Ultrachat across various model families, the authors analyze this behavior theoretically and suggest that finetuning data properties contribute to this counterintuitive trend, offering potential mitigation strategies for improved model performance.

DEEM: Diffusion models serve as the eyes of large language models for image perception
This paper introduces DEEM, a novel approach leveraging generative feedback from diffusion models to enhance the visual perception capabilities of large multimodal models (LMMs), particularly in handling out-of-distribution data. DEEM improves resilience against visual hallucinations and enhances performance on benchmarks like RobustVQA, MMVP, and POPE, using fewer training parameters and data, without additional training modules.

DEPfold: RNA Secondary Structure Prediction as Dependency Parsing.
DEPfold is a novel deep learning approach that frames RNA secondary structure prediction as a dependency parsing problem, offering three innovations: transforming RNA structures into labeled dependency trees, utilizing a biaffine attention mechanism, and employing an optimal tree decoding algorithm to enforce structural constraints. By leveraging pretrained language models and learning directly from annotated data, DEPfold significantly outperforms existing methods, particularly in cross-family generalization, bridging natural language processing with RNA biology to enhance RNA structure prediction and analysis.

Modeling Complex System Dynamics with Flow Matching Across Time and Conditions
This paper introduces Multi-Marginal Flow Matching (MMFM), a method that constructs a flow using spline-based interpolation and a neural network for modeling dynamics from temporal snapshot data across multiple time points and conditions. MMFM effectively integrates sparse data and outperforms existing methods in imputing missing time points, demonstrated through both synthetic and single-cell genomics datasets.

ChartMimic: Evaluating LMM's Cross-Modal Reasoning Capability via Chart-to-Code Generation
ChartMimic is a benchmark designed to evaluate the visually-grounded code generation capabilities of large multimodal models (LMMs) using complex visual charts and textual instructions. Featuring 4,800 curated triplets from various scientific domains, ChartMimic highlights substantial challenges even for advanced models like GPT-4o, demonstrating its potential to drive further advancements in artificial general intelligence by necessitating improvements in visual comprehension, code generation, and cross-modal reasoning.

Semantic Aware Representation Learning for Lifelong Learning
This paper introduces Semantic-Aware Representation Learning (SARL), a method that leverages sparse activations and semantic similarities between objects to improve continual learning. By enhancing feature reusability and reducing task interference, SARL significantly boosts performance in incremental learning scenarios, achieving a superior balance between adaptability and stability.

JudgeBench: A Benchmark for Evaluating LLM-Based Judges
The paper highlights the limitations of LLM-based judges and proposes a new evaluation framework, JudgeBench, to objectively assess these judges on challenging tasks in knowledge, reasoning, math, and coding. JudgeBench provides a rigorous benchmark using a novel pipeline that converts difficult datasets into response pairs with preference labels, demonstrating its effectiveness as many advanced models show only marginal improvement over random guessing.

Multimodal Situational Safety
This paper introduces the concept of Multimodal Situational Safety, evaluating how safety considerations for Multimodal Large Language Models (MLLMs) depend on the specific situational context. Utilizing the Multimodal Situational Safety benchmark (MSSBench), the study finds that current MLLMs struggle with situational safety queries, highlighting an important research area and demonstrating that multi-agent pipelines can enhance safety performance.

Flaws of ImageNet, Computer Vision's Favourite Dataset
The paper analyzes issues in the ImageNet-1k dataset, such as incorrect labels, ambiguous class definitions, domain shifts, and image duplicates, which have become more pronounced as model accuracy improves. It suggests straightforward solutions for some problems and aims to initiate a broader discussion on refining this essential dataset to enhance future research.

Towards Scalable Topological Regularizers
This paper introduces principal persistence measures as a topological regularizer to address the limitations of existing metrics in latent space matching, such as computational cost and gradient discontinuities. By providing a parallelized GPU implementation and demonstrating its effectiveness in tasks like shape matching and image generation, the proposed approach offers a scalable solution for incorporating topological features in learning tasks.

Neural Wave Equation for Irregularly Sampled Sequence Data
This paper introduces the neural wave equation, a deep learning method inspired by the wave equation, to effectively handle sequence labeling problems with irregularly sampled data and varying complexities. By modeling the evolution of hidden states continuously across both time and depth, the proposed model demonstrates superior performance compared to existing methods, as validated through experiments on various sequence labeling tasks.

Composing Unbalanced Flows for Flexible Docking and Relaxation
The paper presents Unbalanced Flow Matching, a novel approach to molecular docking that addresses protein flexibility and the generation of nonphysical poses by framing the task as transport between distributions. The proposed method, FlexDock, significantly enhances docking performance and increases the proportion of energetically favorable poses from 30% to 73% on the PDBBind benchmark, demonstrating improved modeling of protein flexibility and accurate pose generation.

GANDALF: Generative AttentioN based Data Augmentation and predictive modeLing Framework for personalized cancer treatment
GANDALF is a novel generative attention-based framework designed to improve cancer treatment by directly augmenting patient genomic data while addressing domain-specific characteristics, overcoming the limitations of prior models that fail to consider patient-specific drug responses. This approach enhances drug response prediction models and outperforms existing state-of-the-art methods on public patient datasets, setting a new standard in personalized cancer treatment.

Tamper-Resistant Safeguards for Open-Weight LLMs
This paper introduces a method called TAR, which enhances tamper-resistance in open-weight large language models (LLMs) so that adversaries are unable to remove safeguards even after extensive fine-tuning. The findings show that TAR significantly bolsters tamper-resistance without compromising the models' benign capabilities, marking a crucial step toward the safe release of open-weight LLMs.

Robust Weight Initialization for Tanh Neural Networks with Fixed Point Analysis
This paper introduces a novel weight initialization method for neural networks using the tanh activation function, aimed at addressing challenges in training deep networks by mitigating activation saturation. Experimental results show that the proposed method surpasses Xavier initialization in robustness, data efficiency, and convergence speed across various datasets and network sizes.

Influence-Guided Diffusion for Dataset Distillation
This paper proposes using diffusion generative models to improve dataset distillation, overcoming the limitations of existing methods that struggle with large, high-resolution datasets. By introducing the Influence-Guided Diffusion (IGD) sampling framework, the method enhances the training performance of distilled datasets, achieving state-of-the-art results on ImageNet-1K with a significant improvement in efficiency and effectiveness.

Restructuring Vector Quantization with the Rotation Trick
This work proposes a method to propagate gradients through the vector quantization layer in Vector Quantized Variational AutoEncoders (VQ-VAEs) by using a rotation and rescaling linear transformation. The approach enhances reconstruction metrics, codebook utilization, and reduces quantization error across 11 different VQ-VAE training paradigms.

ET-SEED: EFFICIENT TRAJECTORY-LEVEL SE(3) EQUIVARIANT DIFFUSION POLICY
ET-SEED is a trajectory-level SE(3) equivariant diffusion model that enhances policy robustness and generalization in robotic manipulation tasks while reducing reliance on extensive demonstrations. By extending equivariant Markov kernels and simplifying conditions for the diffusion process, the model significantly improves training efficiency and demonstrates superior performance and generalization across various manipulation tasks with minimal demonstrations.

Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data
This paper introduces reparameterized absorbing discrete diffusion (RADD), a diffusion model that simplifies the estimation of marginal probability ratios by using time-independent conditional probabilities, which effectively reduces the number of function evaluations and accelerates sampling. The authors unify absorbing discrete diffusion with any-order autoregressive models, achieving state-of-the-art performance in zero-shot language modeling benchmarks, showcasing the model’s efficiency and efficacy.

Inference Optimal VLMs Need Fewer Visual Tokens and More Parameters
Vision Language Models (VLMs) face high latency during inference due to substantial compute requirements from processing many image-derived input tokens. This study establishes scaling laws to identify the optimal trade-off between the number of visual tokens and LLM parameters, revealing that maximum efficiency is achieved using the largest LLM within budget and minimizing visual tokens, often to a single token, thereby highlighting the need for advanced token compression algorithms for efficient real-world deployment.

Semantic Temporal Abstraction via Vision-Language Model Guidance for Efficient Reinforcement Learning
The paper introduces VanTA, an innovative approach that utilizes pretrained Vision-Language Models to improve temporal skill extraction in reinforcement learning, making it more semantically meaningful and task-relevant. This method enhances policy learning by reducing suboptimality, and experiments across various environments demonstrate its superiority over existing offline RL methods in handling long-horizon tasks.

Why Does the Effective Context Length of LLMs Fall Short?
This paper identifies the limitation in effective context lengths of large language models (LLMs) due to the left-skewed frequency distribution of relative positions during training, which hampers long-distance information gathering. To overcome this, the authors introduce Shifted Rotray Position Embedding (STRING), which enhances LLM performance by overwriting ineffective positions during inference, achieving state-of-the-art results on long-context benchmarks without additional training and surpassing some commercial models such as GPT-4-128K.

Knowing Your Target: Target-Aware Transformer Makes Better Spatio-Temporal Video Grounding
The paper introduces a Target-Aware Transformer for Spatio-Temporal Video Grounding (TA-STVG) that improves the spatial and temporal localization in videos by adaptively generating object queries with target-specific cues through a text-guided temporal sampling and attribute-aware spatial activation. This novel approach, which significantly outperforms existing methods across several benchmarks, not only enhances the discriminative power of video-text interactions but also demonstrates its general applicability by improving the performance of other models like TubeDETR and STCAT.

Conflict-Averse Gradient Aggregation for Constrained Multi-Objective Reinforcement Learning
The paper proposes a new algorithm called constrained multi-objective gradient aggregator (CoMOGA) to tackle the challenges of managing conflicting gradients in multi-objective reinforcement learning while adhering to safety constraints. By framing the problem as a constrained optimization problem and integrating safety requirements, CoMOGA ensures global optimal convergence in a tabular setting, as demonstrated by experiments confirming its efficacy in achieving constraint satisfaction across tasks.

Graph Neural Networks Are More Than Filters: Revisiting and Benchmarking from A Spectral Perspective
This paper challenges the prevailing notion that neighborhood aggregation is the primary driver of GNN performance by presenting a benchmark that evaluates GNNs' ability to process input graph data across different frequency components. The authors propose a novel framework for measuring GNN performance from a spectral perspective, providing theoretical analysis and empirical insights that suggest non-linear layers and other components significantly influence GNN behavior in the spectral domain.

Taming Overconfidence in LLMs: Reward Calibration in RLHF
This study investigates the overconfidence in language models trained with Reinforcement Learning from Human Feedback (RLHF) and identifies biases in reward models used for Proximal Policy Optimization (PPO). To address this, new PPO variants, PPO-M and PPO-C, are proposed to improve calibration by integrating explicit confidence scores or adjusting reward scores, reducing calibration error without compromising model performance across various datasets.

Language Guided Skill Discovery
Language Guided Skill Discovery (LGSD) is a framework that enhances skill discovery by maximizing semantic diversity with the aid of large language models (LLMs), enabling the generation of semantically distinctive skills based on user prompts. Demonstrated in both legged robots and robot-arm manipulation environments, LGSD surpasses existing methods in skill diversity while allowing for intuitive skill utilization through natural language prompts.

Efficient Online Reinforcement Learning Fine-Tuning Need Not Retain Offline Data
This paper introduces Warm-start RL (WSRL), an approach that eliminates the need for retaining offline data during online fine-tuning in reinforcement learning, addressing issues of stability and cost associated with traditional methods. WSRL employs a warmup phase with limited rollouts from a pre-trained policy to recalibrate the value function, effectively bridging distribution mismatches and achieving faster learning and superior performance without relying on extensive offline datasets.

AutoEval: Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks
This paper introduces $\forall$uto$\exists$$\lor\\!\land$L, a new benchmark for assessing Large Language Models (LLMs) in formal tasks requiring clear correctness, like truth maintenance in translation and logical reasoning. It provides an autonomous evaluation framework that autogenerates tasks and ground truth data, thereby avoiding the need for human labeling and reducing overfitting issues found in static datasets, with empirical evidence showing its effectiveness in predicting LLM performance across various translation and reasoning benchmarks.

Emerging Safety Attack and Defense in Federated Instruction Tuning of Large Language Models
This paper uncovers the vulnerability of safety alignment in federated instruction tuning (FedIT) by introducing a stealthy and effective safety attack method that malicious clients can use without manual effort. The authors propose a novel post-hoc defense method utilizing automated generation of defense data and fine-tuning to significantly improve the safety alignment of compromised LLMs, showing an improvement of up to 69% in safety rate.

Hot-pluggable Federated Learning: Bridging General and Personalized FL via Dynamic Selection
This paper introduces Hot-Pluggable Federated Learning (HPFL), a novel framework designed to enhance General Federated Learning (GFL) by combining personalized plug-in modules with a shared backbone to optimize both model selection and personalized federated learning. The proposed HPFL framework overcomes computation and communication challenges associated with storing whole models and demonstrates significant performance improvements over existing GFL and PFL algorithms, with applications in various federated learning contexts like continual and one-shot learning.

Differentially Private Federated Learning with Time-Adaptive Privacy Spending
This paper introduces a time-adaptive federated learning framework with differential privacy that allocates privacy budgets non-uniformly across time and clients to enhance learning efficiency. The proposed method allows clients to save privacy budget in early rounds and spend more in later rounds, leading to improved utility, as demonstrated through theoretical analysis and practical experiments on standard benchmark datasets.

Adding Conditional Control to Diffusion Models with Reinforcement Learning
This paper introduces CTRL, a novel reinforcement learning (RL) approach to condition pre-trained diffusion models with additional controls using an offline dataset. By maximizing reward functions through KL divergence, CTRL offers improved sample efficiency and simplifies dataset construction compared to existing methods, eliminating the need to train classifiers for intermediate states.

Revealing and Reducing Gender Biases in Vision and Language Assistants (VLAs)
This paper investigates gender bias in 22 popular open-source vision-language assistants (VLAs), revealing that these models mirror human biases, such as attributing more positive traits to women and associating negative traits with men. The study suggests that fine-tuning-based debiasing methods provide an effective balance between reducing bias and maintaining performance, advocating for the pre-deployment assessment of gender bias and further development of debiasing strategies.

NeSyC: A Neuro-symbolic Continual Learner For Complex Embodied Tasks in Open Domains
This paper introduces NeSyC, a neuro-symbolic continual learner framework that enhances embodied agents' knowledge generalization capabilities by combining Large Language Models with symbolic tools. Through a contrastive generality improvement scheme and a memory-based monitoring system, NeSyC effectively improves task-solving performance in various open-domain environments, as demonstrated in experiments on benchmarks like ALFWorld and VirtualHome.

Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations
This paper introduces a novel approach for measuring the faithfulness of large language models' (LLMs) explanations, aiming to ensure that the explanations accurately represent the reasoning process. By defining faithfulness based on the correspondence between implied influential concepts and actual influential concepts, the authors use auxiliary LLMs to generate counterfactuals and a hierarchical Bayesian model to quantify causal effects, uncovering instances of unfaithfulness in tasks involving social bias and medical question answering.

Track-On: Transformer-based Online Point Tracking with Memory
This paper presents Track-On, a transformer-based model for online long-term point tracking in videos, designed to work efficiently on a frame-by-frame basis without requiring future frames. Utilizing spatial and context memory modules, Track-On achieves state-of-the-art results in online tracking and performs competitively against offline methods on several benchmarks, providing a robust solution for real-time applications.

Transformer Block Coupling and its Correlation with Generalization in LLMs
This paper analyzes the trajectories of token embeddings in Large Language Models (LLMs) through transformer blocks, revealing the phenomenon of **transformer block coupling** characterized by the coupling of top singular vectors. The study finds a positive correlation between coupling and model performance, stronger than with other hyperparameters, and extends this analysis to Vision Transformers (ViTs), suggesting new directions for studying token interactions and improving training and generalization.

An Exploration with Entropy Constrained 3D Gaussians for 2D Video Compression
The paper introduces an innovative approach to video compression using 3D Gaussian Splatting by proposing a Toast-like Sliding Window orthographic projection, which efficiently converts 3D Gaussian models into video representation models. The new method, GSVC, not only provides improved rate-distortion performance on the UVG dataset compared to NeRV but also achieves significantly higher frame reconstruction speeds and stream decoding capabilities.

TS-LIF: A Temporal Segment Spiking Neuron Network for Time Series Forecasting
This paper introduces the Temporal Segment Leaky Integrate-and-Fire (TS-LIF) model, a novel dual-compartment spiking neuron architecture designed to improve the processing of spatiotemporal data and time series forecasting. The TS-LIF model enhances the neuron's ability to capture both low- and high-frequency information, demonstrating superior accuracy and robustness over conventional SNNs, and advancing their application in complex temporal dynamics forecasting scenarios.

AI as Humanity’s Salieri: Quantifying Linguistic Creativity of Language Models via Systematic Attribution of Machine Text against Web Text
This paper introduces the CREATIVITY INDEX, a novel metric designed to quantify linguistic creativity by reconstructing text using existing web snippets, and proposes DJ SEARCH, a dynamic programming algorithm to efficiently compute this index. The study finds that humans exhibit higher creativity levels than large language models, and it demonstrates that the CREATIVITY INDEX is highly effective for detecting machine-generated text, outperforming current systems significantly in accuracy.

Identifiable Exchangeable Mechanisms for Causal Structure and Representation Learning
This paper introduces the Identifiable Exchangeable Mechanisms (IEM) framework, which unifies various methods in representation and causal structure learning by leveraging exchangeable non-i.i.d. data. The IEM model generalizes the Causal de Finetti theorem, establishing cause and mechanism variability conditions that lead to novel identifiability results in causal structure identification and representation learning.

Latent Bayesian Optimization via Autoregressive Normalizing Flows
This paper introduces Normalizing Flow-based Bayesian Optimization (NF-BO) to address the value discrepancy problem in Latent Bayesian Optimization by establishing a one-to-one function between input and latent spaces, eliminating reconstruction gaps. The proposed method, particularly SeqFlow for sequence data, and a dynamic candidate sampling strategy significantly outperform existing LBO methods in molecule generation tasks.

Enhancing the Scalability and Applicability of Kohn-Sham Hamiltonians for Molecular Systems
This study addresses the computational limitations of Density Functional Theory (DFT) by introducing a deep-learning-based model with a novel Wavefunction Alignment Loss (WALoss), which improves the accuracy and efficiency of DFT calculations. Using a large training set, the model reduces total energy prediction error by a factor of 1347 and speeds up self-consistent-field calculations by 18%, enhancing the scalability and physical accuracy for larger molecular systems.

Multi-Field Adaptive Retrieval
Multi-Field Adaptive Retrieval (mFAR) is a novel framework designed to improve document retrieval from semi-structured data by leveraging the inherent structure within documents, such as titles and message bodies. By independently indexing document fields and adaptively predicting field importance based on queries, mFAR optimizes the use of dense and lexical representations, enhancing document ranking and setting a new standard in performance for multi-field structured data retrieval.

AI2TALE: An Innovative Information Theory-based Approach for Learning to Localize Phishing Attacks
AI2TALE is a deep learning-based approach designed to enhance the detection and explanation of phishing emails, which are a persistent threat despite extensive research. By not only predicting the vulnerability label of emails but also identifying the most phishing-relevant information, AI2TALE improves upon state-of-the-art methods in both label accuracy and explanatory clarity, as demonstrated in experiments across seven diverse datasets.

Monte Carlo Planning with Large Language Model for Text-Based Game Agents
The paper presents the Monte Carlo planning with Dynamic Memory-guided Large language model (MC-DML) algorithm, which combines Large Language Models' understanding and reasoning abilities with the exploratory strength of tree search methods to improve planning in text-based games. MC-DML enhances performance by utilizing in-trial and cross-trial memory to adjust actions dynamically, demonstrating superior efficiency against existing methods in initial planning phases across games from the Jericho benchmark.

PIORF: Physics-Informed Ollivier-Ricci Flow for Long–Range Interactions in Mesh Graph Neural Networks
This paper presents Physics-Informed Ollivier--Ricci Flow (PIORF), a novel graph rewiring method that enhances data-driven simulators by integrating physical correlations with graph topology to address the 'over-squashing' issue in fluid flow modeling. PIORF improves long-range information propagation by identifying and connecting bottleneck regions with high-velocity gradient nodes, demonstrating up to a 26.2% performance improvement over existing methods on benchmark datasets.

DLEFT-MKC: Dynamic Late Fusion Multiple Kernel Clustering with Robust Tensor Learning via Min-Max Optimization
This paper introduces a novel algorithm, Dynamic Late Fusion Multiple Kernel Clustering with Robust Tensor Learning via min-max optimization (DLEFT-MKC), which addresses existing limitations in multiple kernel clustering by enhancing adaptability, robustness, and the extraction of high-order cross-view information. The proposed approach innovatively combines min-max optimization with tensor-based learning, improving clustering performance and efficiency, as demonstrated by extensive experiments on benchmark datasets.

UniDrive: Towards Universal Driving Perception Across Camera Configurations
UniDrive introduces a novel framework for vision-centric autonomous driving that achieves universal perception across different camera configurations by using unified virtual cameras and ground-aware projection methods. The virtual camera projection acts as a plug-and-play module, allowing existing 3D perception methods to adapt to varying camera parameters, demonstrated by strong generalization with minimal performance loss on a dataset collected using different configurations in CARLA.

DaWin: Training-free Dynamic Weight Interpolation for Robust Adaptation
DaWin is a training-free dynamic weight interpolation method that adapts pre-trained foundation models for downstream tasks by using the entropy of individual models to compute per-sample interpolation coefficients dynamically, enhancing robustness against distribution shifts without retraining. Validated on large-scale visual recognition benchmarks, DaWin shows significant performance improvements with minimal computational overhead compared to traditional methods, and its analytic behavior is explored to explain its empirical success.

Sequential Stochastic Combinatorial Optimization Using Hierarchal Reinforcement Learning
This paper addresses the sequential stochastic combinatorial optimization (SSCO) problem by introducing a hierarchical reinforcement learning framework, termed the wake-sleep option (WS-option), which effectively allocates resources and selects nodes over multiple time steps. The proposed WS-option framework demonstrates improved effectiveness and generalizability over traditional methods, allowing for application to larger graphs and enhancing computational efficiency.

Exploring The Forgetting in Adversarial Training: A Novel Method for Enhancing Robustness
This paper addresses the challenge of achieving robust and clean accuracy in Adversarial Training (AT) by connecting it to catastrophic forgetting in continual learning. Introducing Adaptive Multi-teachers Self-distillation (AMS), the study proposes a novel method to improve robust and clean accuracy across various datasets and adversarial models, effectively mitigating forgetting and overfitting issues in AT.

Wicked Oddities: Selectively Poisoning for Effective Clean-Label Backdoor Attacks
This paper investigates a novel threat model for clean-label backdoor attacks, where the attacker only provides data for the target class and lacks knowledge of the victim model or other training classes. By exploring strategies to selectively poison a small set of training samples, the study demonstrates effective enhancement of attack success rates in this challenging context, posing significant risks to machine learning models trained on third-party datasets.

SPA: 3D Spatial-Awareness Enables Effective Embodied Representation
This paper introduces SPA, a novel framework that enhances 3D spatial awareness in embodied AI by using differentiable neural rendering with Vision Transformers. SPA demonstrates superior performance over more than 10 state-of-the-art methods across 268 tasks and 8 simulators, highlighting the pivotal role of 3D spatial understanding in embodied representation learning, and commits to open-source its code and models.

RMP-SAM: Towards Real-Time Multi-Purpose Segment Anything
This paper introduces Real-Time Multi-Purpose SAM (RMP-SAM), a dynamic convolution-based method designed for real-time multi-purpose segmentation, effectively handling interactive, panoptic, and video instance segmentation tasks with a single end-to-end model. RMP-SAM utilizes an efficient encoder and decoupled adapter for prompt-driven decoding, achieving an optimal balance between speed and accuracy across various benchmarks, demonstrating generalization capabilities beyond specific semantic tasks.

Anti-Exposure Bias in Diffusion Models
This paper addresses the issue of exposure bias in diffusion models (DMs) by introducing a prompt learning framework that employs a lightweight prompt prediction model to generate anti-bias prompts during sampling, which aligns the sampling trajectory with the training trajectory. The proposed method effectively reduces exposure bias and enhances image quality with minimal overhead, as demonstrated by empirical results across three benchmark datasets.

LoRA Done RITE: Robust Invariant Transformation Equilibration for LoRA Optimization
This paper introduces LoRA-RITE, an adaptive matrix preconditioning method for optimizing Low-rank adaption (LoRA) in large language models, achieving transformation invariance and computational efficiency. Experimental results show that LoRA-RITE consistently improves performance over existing optimizers, with significant accuracy gains in tasks like Super-Natural Instructions and other benchmarks.

DINOv2: Learning Robust Visual Features without Supervision
This paper explores the adaptation of model pretraining techniques from natural language processing to computer vision, demonstrating that self-supervised methods can yield all-purpose visual features when trained on diverse, curated datasets. The authors enhance existing approaches by developing an automatic data curation pipeline and training large-scale ViT models, resulting in distilled models that outperform current best-in-class models like OpenCLIP across various benchmarks.

Finally Rank-Breaking Conquers MNL Bandits: Optimal and Efficient Algorithms for MNL Assortment
This paper addresses the active online assortment optimization problem with preference feedback by developing efficient algorithms for regret minimization using Plackett-Luce based user choices. It introduces a novel concentration guarantee through "Pairwise Rank-Breaking" to estimate PL model score parameters, overcoming limitations of existing methods and offering practical, optimal solutions for real-world applications such as ad placement and recommender systems.

ReGen: Generative Robot Simulation via Inverse Design
ReGen is a generative simulation framework that automates the creation of robot simulations by inferring underlying environments from an agent's behavior and textual descriptions using inverse design and large language models. The method demonstrates improved diversity and complexity in simulated environments for tasks like autonomous driving and robot manipulation, enhancing policy validation and supporting scalable robot learning with better generalization and robustness.

Can Textual Gradient Work in Federated Learning?
The paper introduces Federated Textual Gradient (FedTextGrad), a novel federated learning paradigm that utilizes textual gradient optimization to enhance LLM systems, particularly in decentralized, resource-constrained settings where traditional numerical gradients are not applicable. The study conducts extensive experiments addressing challenges like prompt aggregation and optimization tuning, offering solutions to improve the integration of federated textual gradients in LLM training while retaining essential information, ultimately expanding FL's applicability and paving the way for future research in this area.

Scaling Instruction-tuned LLMs to Million-token Contexts via Hierarchical Synthetic Data Generation
This paper introduces a novel synthetic data generation strategy to extend the context window of Large Language Models (LLMs) beyond 100K tokens, addressing the scarcity of long-context data and computational complexity issues. By using rotary position embedding (RoPE) scaling, the model achieves effective performance on the RULER and InfiniteBench benchmarks with context lengths up to 1M tokens while maintaining general language task performance.

Accelerating Neural ODEs: A Variational Formulation-based Approach
The paper introduces VF-NODE, a novel method based on variational formulation to improve the efficiency and accuracy of Neural Ordinary Differential Equations training, primarily by reducing the number of function evaluations and eliminating error-prone autoregression. By employing global integrals and leveraging Filon's method and natural cubic spline regression, VF-NODE accelerates the training process by 10 to 1000 times while maintaining or improving the accuracy in modeling continuous dynamical systems.

Hierarchical Uncertainty Estimation for Learning-based Registration in Neuroimaging
This paper proposes a new framework for propagating spatial uncertainties in deep learning-based image registration to global transformation models and downstream tasks, specifically in the context of brain MRI scans. The study demonstrates that their method significantly improves registration accuracy and uncertainty estimation compared to traditional techniques like Monte Carlo dropout, thereby enhancing the reliability of downstream neuroimaging analyses.

Efficient Discovery of Pareto Front for Multi-Objective Reinforcement Learning
Multi-objective reinforcement learning (MORL) often struggles with scalability and Pareto front discovery when dealing with multiple objectives and changing preferences. The proposed Constrained MORL (C-MORL) algorithm tackles these issues by using a two-stage approach that combines constrained policy optimization with MORL, leading to superior performance in terms of hypervolume, expected utility, and sparsity across various control tasks with up to nine objectives.

Breaking the Reclustering Barrier in Centroid-based Deep Clustering
This paper introduces the concept of the "reclustering barrier" in centroid-based deep clustering algorithms, where performance stagnates after initial rapid improvement, and proposes a novel algorithm, BRB, to overcome this challenge. BRB enhances clustering performance by preventing early over-commitment and enabling dynamic adaptation, outperforming existing methods across various benchmarks and demonstrating competitive results when used with contrastive loss, with code and models available online.

Locally Connected Echo State Networks for Time Series Forecasting
The Locally Connected Echo State Network (LCESN) is a novel variant of Echo State Networks that features a locally connected reservoir, forced memory, and a weight adaptation strategy, effectively reducing time and space complexities, thus allowing for larger networks. LCESN demonstrates competitive performance on various benchmark and real-world tasks, surpassing several state-of-the-art models, and offers a simple, one-shot training approach for time series forecasting, with an open-source GPU-based implementation available.

Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient
This paper introduces refined Local Learning Coefficients (rLLCs) to analyze the development of internal structures in transformer language models, specifically focusing on the differentiation and specialization of attention heads during training. The study provides insights into how these components evolve functional roles and interacts with data, offering a rigorous approach to developmental interpretability and highlighting the linkage between data structures and neural network learning dynamics.

Spurious Forgetting in Continual Learning of Language Models
This study identifies "spurious forgetting" in large language models, where performance declines during continual learning are attributed to task misalignment rather than genuine knowledge loss. By presenting a Freezing strategy to stabilize early model layers, the research enhances performance in continual learning scenarios and provides insights into the dynamics of task alignment and model updates.

LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias
The Large View Synthesis Model (LVSM) is a novel transformer-based model designed for scalable and generalizable novel view synthesis using sparse-view inputs, introducing two architectures: an encoder-decoder and a decoder-only approach. Bypassing traditional 3D inductive biases, the encoder-decoder LVSM offers faster inference, while the decoder-only version delivers superior image quality and generalization, outperforming previous methods with 1.5 to 3.5 dB PSNR improvements, demonstrating state-of-the-art performance with reduced computational resources.

Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning
This paper introduces a self-adaptive reward-shaping mechanism in reinforcement learning, leveraging Beta distributions of success rates from historical experiences to address the sparse-reward problem. By utilizing Kernel Density Estimation with Random Fourier Features, this method efficiently manages continuous, high-dimensional state spaces, enhancing sample efficiency and convergence stability in environments with sparse rewards.

Universal Image Restoration Pre-training via Degradation Classification
This paper introduces Degradation Classification Pre-Training (DCPT), a novel method for image restoration pre-training that leverages classifying degradation types as a weak form of supervision. DCPT improves performance for both CNNs and transformers, resulting in significant enhancements in restoration tasks by effectively utilizing pre-trained parameters for transfer learning.

Towards hyperparameter-free optimization with differential privacy
This paper introduces an automatic learning rate schedule adapted for differential privacy (DP) optimization, applicable to any models and optimizers, which significantly reduces the need for computationally expensive and data leakage-prone hyperparameter tuning. The proposed hyperparameter-free DP optimization method achieves state-of-the-art performance on various language and vision tasks, maintaining efficiency comparable to standard non-DP optimization.

X-Gen: Ego-centric Video Prediction by Watching Exo-centric Videos
This paper introduces X-Gen, a model that enhances cross-view video prediction by focusing on hand-object interactions (HOI) in ego-centric videos. By using a two-stage process involving HOI mask prediction and a video diffusion model, X-Gen significantly improves the accuracy of future ego-centric video frame generation, outperforming previous models on benchmark datasets.

Bridging Context Gaps: Leveraging Coreference Resolution for Long Contextual Understanding
This paper introduces the Long Question Coreference Adaptation (LQCA) method to improve large language models' (LLMs) comprehension and question-answering performance when dealing with lengthy contexts. By focusing on coreference resolution, the framework systematically partitions information, resulting in improved understanding and performance in LLMs like OpenAI-o1-mini and GPT-4o, with the code available at https://github.com/OceannTwT/LQCA.

Indirect Gradient Matching for Adversarial Robust Distillation
The paper presents a novel Indirect Gradient Distillation Module (IGDM) designed to enhance adversarial distillation by transferring the teacher model's input gradient to the student model. Experimental results demonstrate that IGDM significantly boosts the performance of existing adversarial distillation methods on the CIFAR-100 dataset, achieving improved AutoAttack accuracy with ResNet-18 and MobileNetV2 architectures.

Greener GRASS: Enhancing GNNs with Encoding, Rewiring, and Attention
This paper introduces Graph Attention with Stochastic Structures (GRASS), a novel Graph Neural Network (GNN) architecture that integrates graph encoding, rewiring, and attention mechanisms to efficiently capture and propagate structural information. By employing relative random walk probabilities (RRWP) and an advanced decomposed variant (D-RRWP), along with a new additive attention mechanism, GRASS significantly improves performance on benchmark datasets, achieving a 20.3% reduction in mean absolute error on the ZINC dataset.

A Visual Dive into Conditional Flow Matching
This work presents self-contained explanations and visualizations to elucidate standard flow techniques and conditional flow matching, as introduced in three simultaneous papers at ICLR 2023. It offers new insights and intuitions to better understand the concept of conditional flow matching.

Expand and Compress: Exploring Tuning Principles for Continual Spatio-Temporal Graph Forecasting
This paper introduces **_EAC_**, a novel prompt tuning-based continuous forecasting method designed to address the challenges of real-time spatio-temporal forecasting with streaming data, particularly focusing on inefficiency and catastrophic forgetting. By using an expand and compress strategy with lightweight tuning parameters, **_EAC_** enhances the performance of spatio-temporal graph neural networks, showing superior effectiveness and efficiency across multiple real-world datasets compared to existing methods.

Truncated Consistency Models
Consistency models enhance diffusion model speed by predicting probability flow ODE solutions directly from noise, yet struggle with training challenges due to mapping intermediate PF ODE points. This paper introduces a truncated-time training approach with a novel parameterization to optimize one-step generation, achieving superior performance on CIFAR-10 and ImageNet $64\times64$ datasets using significantly smaller networks compared to leading models.

Proteina: Scaling Flow-based Protein Structure Generative Models
Proteina is a state-of-the-art, flow-based protein backbone generator leveraging a scalable transformer architecture and hierarchical fold class labels for improved de novo protein design. It introduces new performance metrics and training strategies, achieving diverse, designable proteins up to 800 residues with advanced control over secondary structures and fold-specific generation.

DiffPuter: An EM-Driven Diffusion Model for Missing Data Imputation
This paper introduces DiffPuter, a diffusion model combined with the Expectation-Maximization algorithm, designed to improve missing data imputation by learning the joint distribution of full data and performing accurate conditional sampling. Through extensive experiments, DiffPuter demonstrates superior performance, achieving significant improvements in mean absolute error (MAE) and root mean square error (RMSE) compared to existing imputation methods.

Efficient Causal Decision Making with One-sided Feedback
This paper introduces a novel value function to address the challenge of undefined counterfactual outcomes in decision-making problems with one-sided feedback, such as bank loans where repayment status is known only if approved. By using shadow variables and leveraging semiparametric theory, the authors establish the identification and efficiency bound of the value function, providing efficient methods for decision evaluation and learning, as demonstrated by numerical experiments and real-world data.

High-dimension Prototype is a Better Incremental Object Detection Learner
This paper introduces a novel higher-dimension-prototype learning approach incorporating Gaussian Mixture Distribution-based Prototypes (GMDP) for incremental object detection (IOD) to mitigate catastrophic forgetting and knowledge shift issues. The proposed method enhances the flexibility, accuracy, and granularity of feature distribution representation without retaining previous task data, showing significant performance improvements over existing state-of-the-art methods on PASCAL VOC and MS-COCO benchmarks.

Attributing Culture-Conditioned Generations to Pretraining Corpora
This paper examines how cultural biases in large language models, particularly in narrative writing and dialogue, can result from uneven cultural representation in pretraining data. It introduces the MEMOED framework to analyze whether these models' outputs are memorized rather than generated, revealing biases towards high-frequency cultures and details irrespective of cultural relevance, and aims to inspire further research into aligning model outputs with diverse cultural contexts.

Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation
This paper introduces a multimodal setup for few-shot 3D point cloud segmentation (FS-PCS) by leveraging textual labels and 2D image data, addressing the limitations of existing unimodal approaches. The proposed MultiModal Few-Shot SegNet (MM-FSS) and its components, including the Multimodal Correlation Fusion (MCF) and Multimodal Semantic Fusion (MSF), improve segmentation performance by effectively integrating multiple modalities, as demonstrated on S3DIS and ScanNet datasets.

Unsupervised Model Tree Heritage Recovery
This paper introduces the task of Unsupervised Model Tree Heritage Recovery (Unsupervised MoTHer Recovery), aimed at understanding the lineage of neural network models by identifying and ordering relationships based on model weights. The authors propose a method to decode these hierarchical connections, formulating it as a problem of finding a directed minimal spanning tree, and demonstrate its efficacy in accurately reconstructing complex Model Trees.

The Ramanujan Library - Automated Discovery on the Hypergraph of Integer Relations
This paper introduces the first library dedicated to mathematical constants and their interrelations, organized using a novel hypergraph representation. Utilizing an automated approach with the PSLQ algorithm, the library has enabled the discovery of 75 new connections between constants, including novel formulas for famous constants such as $\pi$, $e$, and the first continued fraction constant $C_1$, expanding on historical equations by Ramanujan.

Fewer May Be Better: Enhancing Offline Reinforcement Learning with Reduced Dataset
This paper introduces ReDOR, a method for identifying reduced datasets in offline reinforcement learning by framing it as a gradient approximation optimization problem. The approach, which involves transforming the actor-critic framework into a submodular objective and adapting the orthogonal matching pursuit, demonstrates improved algorithm performance with reduced computational complexity.

Asymptotic Analysis of Two-Layer Neural Networks after One Gradient Step under Gaussian Mixtures Data with Structure
This paper investigates the training and generalization performance of two-layer neural networks after one gradient descent step using structured data modeled by Gaussian mixtures, extending beyond the isotropic data assumption commonly used in prior studies. The authors demonstrate, through theoretical analysis and simulations, that high-order polynomial models can perform equivalently to non-linear neural networks under certain conditions, with applicability validated on Fashion-MNIST classification tasks.

GLoRa: A Benchmark to Evaluate the Ability to Learn Long-Range Dependencies in Graphs
This paper addresses the challenge of learning long-range dependencies in graph neural networks, a problem often hindered by oversmoothing, over-squashing, and vanishing gradients. By designing a synthetic benchmark to transparently test this capability, the study finds that current state-of-the-art systems perform poorly and reveals that these issues are not solely responsible, suggesting the need for further research.

Sparse Autoencoders Do Not Find Canonical Units of Analysis
This paper challenges the belief that sparse autoencoders (SAEs) can identify a complete and atomic set of features in neural networks. Through the introduction of SAE stitching and meta-SAEs techniques, the authors demonstrate that smaller SAEs are incomplete, and larger SAEs' features are not atomic, suggesting a need for different or task-specific approaches in mechanistic interpretability.

Probing the Latent Hierarchical Structure of Data via Diffusion Models
This paper demonstrates that forward-backward experiments in diffusion-based models can effectively probe the latent structure of high-dimensional data by showing that changes in data occur in correlated chunks at certain noise levels. The study confirms predictions in hierarchical models through experiments on text and image datasets, outlining a method to measure latent variable changes in real data using state-of-the-art diffusion models.

FormalAlign: Automated Alignment Evaluation for Autoformalization
Autoformalization translates informal mathematical proofs into machine-verifiable formats, but aligning informal and formal statements remains challenging. The new FormalAlign framework enhances this process by automatically evaluating and improving semantic alignment, outperforming GPT-4 in accuracy, thus reducing reliance on manual verification.

ILLUSION: Unveiling Truth with a Comprehensive Multi-Modal, Multi-Lingual Deepfake Dataset
The paper introduces ILLUSION, a comprehensive multi-modal deepfake dataset encompassing 1.3 million samples including audio-visual forgeries across 26 languages, designed to enhance deepfake detection capabilities across diverse real-world scenarios. By leveraging 28 cutting-edge generative techniques, ILLUSION addresses the limitations in current datasets, offering a balanced representation for unbiased evaluation and highlighting challenges such as multilingual and multi-modal performance degradation, thus serving as a crucial resource for advancing research in the domain.

MTU-Bench: A Multi-granularity Tool-Use Benchmark for Large Language Models
MTU-Bench is a newly proposed multi-granularity tool-use benchmark for large language models that addresses previous datasets' limitations by covering five tool usage scenarios and eliminating reliance on costly GPT or human evaluation metrics. By transforming existing high-quality datasets and introducing the MTU-Instruct dataset, MTU-Bench effectively enhances the evaluation of LLMs' tool-use capabilities in real-world scenarios, as demonstrated by comprehensive experimental results.

CirT: Global Subseasonal-to-Seasonal Forecasting with Geometry-inspired Transformer
The paper introduces the geometric-inspired Circular Transformer (CirT) model for Subseasonal-to-Seasonal (S2S) climate forecasting, addressing the limitations of treating spherical weather data as planar images. By using circular patches and Fourier transform in self-attention, CirT improves accuracy in spatial and temporal predictions, outperforming existing models like PanguWeather and ECMWF systems on the ERA5 reanalysis dataset.

Does Training with Synthetic Data Truly Protect Privacy?
As synthetic data gains traction in machine learning, many methods leverage it for training without formal differential privacy guarantees, often suggesting implicit privacy protection for original data. This work examines four training paradigms and highlights the varying levels of privacy preservation they offer, emphasizing the need for rigorous evaluation of empirical approaches to ensure genuine privacy protection.

Deep Kernel Relative Test for Machine-generated Text Detection
This paper introduces a non-parametric kernel relative test to improve the detection of machine-generated texts (MGTs) by assessing whether a text's distribution is closer to human-written texts (HWTs) or MGTs, overcoming limitations of traditional two-sample tests. The proposed method significantly reduces false positives in identifying HWTs that deviate from expected distributions, offering enhanced performance over existing detectors as demonstrated in extensive experiments.

Few for Many: Tchebycheff Set Scalarization for Many-Objective Optimization
This paper introduces a novel Tchebycheff set scalarization method to address multi-objective optimization challenges by finding a few representative solutions to cover a large number of objectives instead of requiring a dense set of Pareto solutions. The proposed method, which includes a smooth variant for efficient optimization, demonstrates effectiveness and provides theoretical guarantees for well covering objectives in scenarios with many optimization goals.

Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels
This paper presents PedGen, a generative model for producing diverse pedestrian movements by leveraging a newly curated large-scale dataset called CityWalkers, which documents varied real-world pedestrian behaviors in urban environments. PedGen improves upon existing methods by using automatic label filtering to handle noisy data and a context encoder to integrate 3D scene context, resulting in superior performance and zero-shot generalization in both real-world and simulated scenarios.

Denoising Autoregressive Transformers for Scalable Text-to-Image Generation
DART is a novel transformer-based model that integrates autoregressive and diffusion approaches within a non-Markovian framework to improve the efficiency of visual generation. By eliminating the need for image quantization and supporting training with both text and image data, DART offers a scalable and efficient alternative to traditional diffusion models, achieving competitive performance on class-conditioned and text-to-image generation tasks.

Ctrl-U: Robust Conditional Image Generation via Uncertainty-aware Reward Modeling
This paper introduces Ctrl-U, an uncertainty-aware reward modeling approach for improving conditional image generation by addressing inaccurate feedback from reward models. Through uncertainty estimation and adaptive regularization, the method enhances controllability and generation quality, showing effectiveness across various scenarios such as segmentation mask, edge, and depth conditions.

In vivo cell-type and brain region classification via multimodal contrastive learning
This work introduces NEMO, a novel multimodal contrastive learning approach for embedding neuronal activity autocorrelations and extracellular waveforms, which significantly improves cell-type and brain region classification from electrophysiological recordings. Demonstrated through achieving state-of-the-art results on opto-tagged datasets and the International Brain Laboratory Brain-wide Map dataset, our method advances the precision and scalability of neural data interpretation.

$\mathbb{X}$-Sample Contrastive Loss: Improving Contrastive Learning with Sample Similarity Graphs
This paper introduces the $\mathbb{X}$-Sample Contrastive objective, which improves upon standard contrastive learning by encoding how a sample relates to others beyond binary similarity graphs. Experiments demonstrate that this approach yields superior representation learning across various vision tasks, especially in lower-data scenarios, outperforming state-of-the-art models like CLIP on multiple benchmarks.

DoF: A Diffusion Factorization Framework for Offline Multi-Agent Reinforcement Learning
This paper introduces DoF, a diffusion factorization framework designed for offline cooperative Multi-Agent Reinforcement Learning (MARL), extending the IGM principle to the Individual-Global-identically-Distributed (IGD) principle to ensure generated outcomes are identically distributed as collective results. By leveraging noise and data factorization functions, DoF overcomes scalability and cooperation challenges in MARL, with extensive experiments validating its effectiveness, and the source code is available online.

Looking Backward: Retrospective Backward Synthesis for Goal-Conditioned GFlowNets
Generative Flow Networks (GFlowNets) are innovative probabilistic samplers that surpass traditional methods by generating diverse solutions rather than a single optimal one. This paper introduces Retrospective Backward Synthesis (RBS), a novel technique that enhances the training of goal-conditioned GFlowNets by creating new backward trajectories, improving sample efficiency, and outperforming existing baselines despite sparse reward challenges.

An Information Criterion for Controlled Disentanglement of Multimodal Data
This paper introduces Disentangled Self-Supervised Learning (DisentangledSSL), a novel self-supervised approach for learning disentangled representations that separate modality-specific and shared information to enhance interpretability, robustness, and facilitate downstream tasks. The method is demonstrated to be effective on both synthetic and real-world datasets, consistently outperforming baselines in tasks such as prediction for vision-language data and molecule-phenotype retrieval for biological data.

Linear Spherical Sliced Optimal Transport: A Fast Metric for Comparing Spherical Data
This paper introduces the Linear Spherical Sliced Optimal Transport (LSSOT) framework, which efficiently embeds spherical distributions into \(L^2\) spaces while preserving their intrinsic geometries for comparing spherical probability measures. LSSOT showcases significant computational efficiency and accuracy in applications like cortical surface registration, 3D point cloud interpolation, and shape embedding, demonstrating its advantages over existing methods.

Re-Evaluating the Impact of Unseen-Class Unlabeled Data on Semi-Supervised Learning Model
This paper challenges the prevailing notion that unseen classes in unlabeled data harm the performance of semi-supervised learning (SSL) models by highlighting flaws in previous assessment methods that failed to control variables accurately. Through experiments that maintain the proportion of seen classes while varying unseen classes, the study reveals that unseen classes do not necessarily degrade SSL model performance and can, under certain conditions, enhance it.

Decoupling Angles and Strength in Low-rank Adaptation
DeLoRA is a novel finetuning method that improves the robustness and performance of Parameter-Efficient FineTuning (PEFT) by normalizing and scaling learnable low-rank matrices, which addresses limitations in popular methods like LoRA. Through evaluations in various tasks, DeLoRA demonstrates superior or comparable performance against other PEFT methods while maintaining stronger robustness, offering an effective solution for finetuning large-scale pretrained models.

Lumina-T2X: Scalable Flow-based Large Diffusion Transformer for Flexible Resolution Generation
This paper introduces the Lumina-T2X family of Flow-based Large Diffusion Transformers (Flag-DiT), offering a scalable generative framework for various modalities such as high-definition images and long-duration videos, while significantly reducing computational costs compared to smaller models. The innovative use of techniques like RoPE and KQ-Norm enhances the model's scalability and flexibility, with extensive analysis confirming its capability in generating visual content with high-quality and seamless transitions, supported by open-access code to promote further research in generative AI.

AgentStudio: A Toolkit for Building General Virtual Agents
AgentStudio introduces a versatile environment and toolset to enhance the development and evaluation of general virtual agents, addressing the limitations of existing domain-specific setups. The framework offers a generic, interactive platform with multimodal observation/action spaces and benchmarks that facilitate in-depth analysis of agent capabilities, accompanied by three datasets—GroundUI, IDMBench, and CriticBench—for assessing essential agent skills.

Dynamic Mixture of Experts: An Auto-Tuning Approach for Efficient Transformer Models
The paper presents the Dynamic Mixture of Experts (DynMoE) technique to address the computational overhead in Sparse Mixture of Experts (SMoE) by introducing a novel gating method for token-based expert activation and an adaptive process for expert adjustment during training. Experimental results across various tasks show that DynMoE achieves competitive performance while enhancing efficiency by activating fewer parameters, outperforming existing models like GMoE and MoE-LLaVA.

Towards Understanding the Robustness of Diffusion-Based Purification: A Stochastic Perspective
Diffusion-Based Purification (DBP) is used as a defense against adversarial attacks, with new findings suggesting that the intrinsic stochasticity of DBP significantly contributes to its robustness rather than the previously believed Gaussian noise addition. The paper introduces Adversarial Denoising Diffusion Training (ADDT) to enhance DBP's purification capabilities and proposes Rank-Based Gaussian Mapping (RBGM) to better integrate adversarial perturbations, offering a robust framework for future research.

Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks
The paper introduces Dynamic-SUPERB Phase-2, an expansive and open benchmark designed to evaluate instruction-based universal speech models across 180 tasks, incorporating novel task types such as regression and sequence generation beyond simple classification. Despite the advancements, the evaluation reveals that current models, like SALMONN-13B and WavLLM, excel in specific areas but lack universal performance, highlighting the need for further innovations in multimodal foundation models.

InstaTrain: Adaptive Training via Ultra-Fast Natural Annealing within Dynamical Systems
In highly dynamic environments where data distributions shift rapidly, traditional predictive models struggle to adapt quickly. This paper introduces InstaTrain, an innovative learning approach that transforms the training process into an ultra-fast natural annealing process, significantly improving the speed, energy efficiency, and accuracy of model updates in real-world prediction tasks compared to standard methods on GPUs.

GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene by Primitives and Gaussians
The paper introduces GaussianBlock, a novel part-aware compositional reconstruction method that combines primitives and 3D Gaussians to create disentangled, semantically coherent representations with high fidelity. By employing techniques such as attention-guided centering loss and a binding inheritance strategy, the method allows for precise editing and reconstruction quality, enhancing both the interpretability and flexibility of 3D scene reconstructions.

Optimal Brain Apoptosis
This paper introduces Optimal Brain Apoptosis (OBA), a novel pruning method that directly computes the Hessian-vector product value for each parameter, enhancing computational efficiency in neural networks. By refining parameter importance estimation with direct calculations and efficiently decomposing the Hessian matrix, OBA offers a more precise pruning process validated on CNNs and Transformers, demonstrating its efficacy on various datasets.

PaRa: Personalizing Text-to-Image Diffusion via Parameter Rank Reduction
In this paper, we introduce PaRa, a Parameter Rank Reduction approach for personalizing large-scale Text-to-Image diffusion models, which balances training data distribution and target distribution by reducing parameter rank to efficiently confine the model's generation space. Our method demonstrates superior performance over current techniques, including LoRA, achieving greater parameter efficiency and improved target image alignment in single/multi-subject generation and image editing tasks.

PostCast: Generalizable Postprocessing for Precipitation Nowcasting via Unsupervised Blurriness Modeling
This paper addresses the issue of blurriness in precipitation nowcasting, which impairs accurate extreme weather predictions, by proposing an unsupervised postprocessing method. Utilizing a pre-trained unconditional denoising diffusion probabilistic model (DDPM), the method improves prediction clarity without needing pre-labeled blurry data, showing superior results across multiple precipitation radar datasets.

Divergence-enhanced Knowledge-guided Context Optimization for Visual-Language Prompt Tuning
This paper introduces Divergence-enhanced Knowledge-guided Prompt Tuning (DeKg), a method that improves prompt tuning in vision-language models like CLIP by reducing overfitting and mitigating bias towards pre-training, using the Hilbert-Schmidt Independence Criterion (HSIC) to promote independence between learnable and crafted prompts. DeKg enhances performance on three challenging benchmarks and can be integrated with existing methods, with the code made available at the provided GitHub repository.

Neural networks on Symmetric Spaces of Noncompact Type
This paper presents a novel approach to developing neural networks on symmetric spaces of noncompact type by formulating the distance from a point to a hyperplane in these spaces. The proposed method enables the creation of fully-connected layers and attention mechanisms for neural networks, with successful validation on tasks including image classification, EEG signal classification, image generation, and natural language inference.

Direct Distributional Optimization for Provable Alignment of Diffusion Models
This paper presents a novel alignment method for diffusion models by directly optimizing the distribution through Dual Averaging, offering convergence guarantees and an end-to-end sampling error bound. The framework proves broadly applicable to distribution optimization tasks, with empirical validation on synthetic and image datasets, notably enhancing methods like Reinforcement Learning with Human Feedback and Direct Preference Optimization.

OmniBind: Large-scale Omni Multimodal Representation via Binding Spaces
OmniBind is an advanced multimodal joint representation model that merges 14 pre-trained spaces to create a high-quality omni representation space supporting 3D, audio, image, video, and language inputs. By dynamically assigning weights to different spaces to balance cross-modal alignment and language decoupling, OmniBind achieves efficient training and demonstrates superior versatility for diverse applications in multimodal understanding.

Do LLMs ``know'' internally when they follow instructions?
This paper investigates how large language models (LLMs) encode information related to instruction-following, identifying an "instruction-following dimension" in the input embedding space that predicts response compliance. The study reveals that modifying representations along this dimension enhances instruction-following success and suggests that prompt phrasing significantly influences LLM behavior, offering insights into improving LLM reliability and the efficacy of prompt engineering.

Intermediate Layer Classifiers for OOD generalization
This paper challenges the conventional use of last-layer representations for out-of-distribution (OOD) generalisation by introducing Intermediate Layer Classifiers (ILCs), demonstrating that intermediate layers often offer superior generalisation. The findings show that earlier-layer representations can achieve near few-shot performance in zero-shot OOD scenarios, revealing the importance of understanding information distribution across layers and the limitations of relying solely on penultimate layer representations.

VideoShield: Regulating Diffusion-based Video Generation Models via Watermarking
VideoShield is a novel watermarking framework designed for diffusion-based video generation models, embedding watermarks directly during the generation process to maintain video quality and eliminate the need for post-processing. It introduces tamper localization to detect temporal and spatial changes, effectively extracting watermarks without compromising quality and is applicable to both video and image generation models, with comprehensive experiments supporting its efficacy.

PIN: Prolate Spheroidal Wave Function-based Implicit Neural Representations
Implicit Neural Representations (INRs) face challenges such as noise-like artifacts and poor generalization to new coordinates, largely due to the choice of nonlinear-activation functions. This paper introduces Prolate Spheroidal Wave Function-based INRs (PIN), which leverage the optimal space-frequency domain concentration of PSWFs, demonstrating superior performance in representing images, 3D shapes, and various vision tasks including image inpainting, novel view synthesis, edge detection, and image denoising.

Hybrid Regularization Improves Diffusion-based Inverse Problem Solving
This study addresses the limitations of existing diffusion model-based methods for solving inverse problems by identifying gradient inaccuracies in Denoising processes for Regularization (DR) and introducing Consistency Regularization (CR) to stabilize gradients. By further developing a Hybrid Regularization (HR) framework that combines DR and CR, the paper demonstrates improved performance on various inverse problems with reduced computational resources, achieving competitive results on benchmark datasets like FFHQ and ImageNet.

Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching: With Insights into Other Permutation Search Methods
This paper investigates the role of weight matching (WM) in achieving linear mode connectivity (LMC) between independently trained models. The analysis reveals that WM aligns singular vector directions rather than reducing $L^2$ distance, efficiently preserving model functionality when merging models and enhancing understanding of stochastic gradient descent; it also compares WM with activation matching and the straight-through estimator, highlighting WM's advantages in multi-model scenarios.

SLMRec: Distilling Large Language Models into Small for Sequential Recommendation
This paper investigates the role of large language models (LLMs) in sequential recommendation (SR) systems and finds that many intermediate layers of LLMs are redundant. It introduces SLMRec, a model that uses a small language model with knowledge distillation to achieve state-of-the-art performance while using only 13% of the parameters of LLM-based models, resulting in significant speedups and efficiency improvements.

Demystifying the Token Dynamics of Deep Selective State Space Models
This paper explores the dynamical properties of tokens in the Mamba model, deriving the system governing its continuous-time limit and evaluating the asymptotic behavior of solutions. By identifying criteria for token convergence or divergence, the study proposes model refinements that exclude unwanted scenarios and reorder tokens by importance, enhancing Mamba's performance in sequential data modeling.

A Meta-Learning Approach to Bayesian Causal Discovery
This paper addresses the challenges in causal structure discovery by proposing a Bayesian meta learning model that reliably approximates the posterior over causal structures and encodes key properties like edge correlation and permutation equivariance. The proposed model outperforms existing Bayesian causal discovery methods by enabling effective sampling from the posterior and directly learning the causal structure.

Strength Estimation and Human-Like Strength Adjustment in Games
This paper presents a novel strength system comprising a strength estimator and an SE-based Monte Carlo tree search (SE-MCTS) designed to estimate and adjust AI playing strengths in alignment with human styles, achieving over 80% accuracy in predicting ranks from just 15 games in Go. SE-MCTS effectively adjusts AI strength to designated ranks and enhances human-AI interactions in games, demonstrating promising general applicability, as shown with consistent results in chess.

Tailoring Mixup to Data for Calibration
This paper addresses the limitation of the Mixup data augmentation technique, which can lead to manifold mismatch and poor calibration by carelessly mixing data. The authors propose a flexible framework that adapts interpolation coefficients based on sample similarity, enhancing both predictive performance and model calibration for classification and regression tasks.

DreamBench++: A Human-Aligned Benchmark for Personalized Image Generation
DreamBench++ is introduced as a human-aligned benchmark for evaluating personalized image generation, utilizing advanced multimodal GPT models to automate the process. By systematically designing prompts for alignment and constructing a comprehensive dataset, DreamBench++ provides a more human-aligned evaluation for generative models, fostering innovation in the community.

Regret-Optimal List Replicable Bandit Learning: Matching Upper and Lower Bounds
This paper introduces the concept of *list replicability* in multi-armed bandits (MAB) and presents algorithms with surprising upper bounds on the number of traces, offering improved list replicability with near-optimal and sub-linear regret. The results include algorithms that achieve various trade-offs between list complexity and regret, as well as lower bounds that highlight optimality in the sub-linear regret regime, extending the findings to linear bandits with infinite arms.

Model-based RL as a Minimalist Approach to Horizon-Free and Second-Order Bounds
This paper demonstrates that a basic Model-based Reinforcement Learning framework using Maximum Likelihood Estimation combined with optimistic and pessimistic planning can achieve strong regret and sample complexity bounds in online and offline settings. Specifically, under certain conditions such as normalized trajectory-wise rewards and time-homogenous transitions, it exhibits nearly horizon-free and second-order performance bounds.

DataGen: Unified Synthetic Dataset Generation via Large Language Models
This paper introduces DataGen, a comprehensive LLM-powered framework aimed at overcoming challenges in generalization, controllability, diversity, and truthfulness by generating diverse, accurate, and highly controllable datasets. DataGen enhances generative processes with features like attribute-guided generation, group checking, and retrieval-augmented techniques, demonstrating its effectiveness in improving synthetic data quality, benchmarking LLMs, and augmenting data for LLM capabilities.

CapeX: Category-Agnostic Pose Estimation from Textual Point Explanation
This paper introduces a novel text-based approach to category-agnostic pose estimation (CAPE) by using a pose-graph with text-described keypoints, eliminating the need for a support image. Validated on the MP-100 benchmark, the method achieves a 1.26% performance boost under a 1-shot setting, setting a new state-of-the-art for CAPE, and provides enhanced text annotations to facilitate further research.

Learning Robust Representations with Long-Term Information for Generalization in Visual Reinforcement Learning
The paper addresses the challenge of generalization in visual reinforcement learning by introducing ROUSER, a novel robust action-value representation learning method under the information bottleneck framework. ROUSER enhances decision-making in test environments by learning robust representations that maximize long-term action value information while discarding irrelevant features, outperforming state-of-the-art methods in diverse tasks with visual distractions.

X-Drive: Cross-modality Consistent Multi-Sensor Data Synthesis for Driving Scenarios
X-DRIVE is a novel framework that models the joint distribution of LiDAR point clouds and multi-view images to enhance the realism and alignment in driving scene synthesis. By employing a dual-branch latent diffusion model and a cross-modality condition module based on epipolar lines, X-DRIVE achieves high-fidelity, controllable generation aligned with multi-level input conditions, showcasing improved cross-modality consistency.

A Theoretical Perspective: How to Prevent Model Collapse in Self-consuming Training Loops
This paper investigates the effects of Self-consuming Training Loops (STLs) in generative models, where models increasingly rely on their own synthetic data for training. By introducing the concept of *recursive stability* and conducting a theoretical analysis, the study reveals how model architecture and the balance between real and synthetic data determine STL success, also extending insights to transformers in in-context learning.

TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning
This paper introduces TOP-ERL, a Transformer-based Off-Policy Episodic Reinforcement Learning algorithm that enables off-policy updates by segmenting action sequences and estimating state-action values using a transformer-based critic. The novel approach significantly improves training efficiency and stability, outperforming state-of-the-art reinforcement learning methods in robot learning environments, with ablation studies highlighting the importance of key design choices.

Collaborative Discrete-Continuous Black-Box Prompt Learning for Language Models
This paper presents ZO-PoG, a novel framework that enhances prompt optimization for large-scale pre-trained language models in black-box scenarios by combining Policy Gradient optimization for discrete text prompts with Zeroth-Order optimization for continuous prompts in embedding spaces. This collaborative approach significantly improves adaptability to diverse downstream tasks, achieving superior results without needing direct access to model parameters, and demonstrates sub-linear convergence and effectiveness across various datasets.

Tuning Frequency Bias of State Space Models
State space models (SSMs) inherently favor low-frequency components due to an innate frequency bias determined at initialization, which conventional training does not alter. This paper introduces two mechanisms to adjust this frequency bias—through scaling initialization or applying a Sobolev-norm-based filter—demonstrating improved SSM performance on long-range tasks with a significant increase in accuracy on the Long-Range Arena benchmark.

CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models
This paper introduces a framework called Continual Learning on Dynamic Benchmarks (CLDyB) to address the issues of data contamination and simplicity of static benchmarks in continual learning evaluation. CLDyB dynamically generates challenging task sequences through a tree-search algorithm to better test the robustness of state-of-the-art CL methods, with plans to publicly release these sequences to aid in developing more robust CL algorithms.

Episodic Novelty Through Temporal Distance
This paper introduces Episodic Novelty Through Temporal Distance (ETD), a novel approach designed to improve exploration in sparse reward environments within Contextual Markov Decision Processes (CMDPs). By utilizing temporal distance as a robust metric for state similarity and computing intrinsic rewards through contrastive learning, ETD outperforms existing methods in enhancing exploration across various benchmark tasks.

SD-LoRA: Scalable Decoupled Low-Rank Adaptation for Class Incremental Learning
Continual Learning with foundation models is hindered by scalability challenges in existing methods, which require expanding prompts or retaining samples. The proposed Scalable Decoupled LoRA (SD-LoRA) method addresses these issues by separating the learning of LoRA components without rehearsal, achieving a stability-plasticity trade-off and improved parameter efficiency, as demonstrated through extensive experiments.

SANA: Efficient High-Resolution Text-to-Image Synthesis with Linear Diffusion Transformers
Sana is a text-to-image framework capable of generating high-resolution images up to 4096×4096, efficiently and with strong text-image alignment, even on a laptop GPU. Through innovations such as a deep compression autoencoder, linear attention in DiT, and a decoder-only text encoder, Sana achieves competitive quality at a fraction of the size and speed of contemporary models, offering low-cost content creation.

Learning Evolving Tools for Large Language Models
This paper introduces ToolEVO, a novel framework that enhances the adaptability and reflectiveness of large language models (LLMs) in dynamic environments with changing tools and APIs. By utilizing Monte Carlo Tree Search, ToolEVO allows LLMs to self-reflect and update their tool usage autonomously, and its effectiveness is demonstrated through extensive experiments and the introduction of a new benchmark, ToolQA-D.

Integrating Protein Dynamics into Structure-Based Drug Design via Full-Atom Stochastic Flows
This paper proposes DynamicFlow, a full-atom flow model utilizing generative modeling to address the limitations of traditional structure-based drug design by considering protein pocket conformational changes. It transforms apo pockets and noisy ligands into holo pockets and corresponding 3D ligand molecules, offering promising ligand candidates and improved inputs for drug discovery.

Continuous Diffusion for Mixed-Type Tabular Data
This paper introduces CDTD, a novel Continuous Diffusion model tailored for generating mixed-type tabular data by employing score matching and score interpolation to unify continuous noise distribution across different data types. CDTD surpasses state-of-the-art models by effectively capturing feature correlations and optimizing feature-specific noise schedules, ensuring superior sample quality in diverse tabular datasets.

Steering Protein Family Design through Profile Bayesian Flow
The paper introduces ProfileBFN, a novel approach utilizing Profile Bayesian Flow Networks for generative modeling of protein families, which enables efficient protein family design without extensive MSA data. Empirical results demonstrate that ProfileBFN effectively captures structural characteristics and enhances the likelihood of generating diverse proteins with desired functionalities compared to previous methods.

Decision Tree Induction Through LLMs via Semantically-Aware Evolution
This paper addresses limitations in current decision tree induction methods by proposing an evolutionary optimization approach using genetic programming, known as $\texttt{LLEGO}$. By integrating semantic priors and domain-specific knowledge through Large Language Models, $\texttt{LLEGO}$ enhances the efficiency and effectiveness of decision tree induction, outperforming existing methods and showing superior search performance in empirical benchmarks.

Improved Sampling Of Diffusion Models In Fluid Dynamics With Tweedie's Formula
The paper introduces Truncated Sampling Models and a novel Iterative Refinement approach to enhance Denoising Diffusion Probabilistic Models (DDPMs), allowing for high-fidelity predictions with reduced computational cost. These methods significantly improve accuracy and provide stable predictions for complex physics-based spatio-temporal problems by minimizing the Number of Function Evaluations (NFEs) needed.

MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine
MedTrinity-25M introduces a large-scale, multimodal medical dataset comprising over 25 million images across 10 modalities with detailed annotations for various diseases, creating the most enriched annotations available for multimodal tasks in medicine. The dataset supports advanced tasks such as captioning, report generation, classification, and segmentation, and is leveraged in the development of LLaVA-Tri, which achieves state-of-the-art performance in several medical AI benchmarks, offering significant potential for pre-training future medical AI models.

Hummingbird: High Fidelity Image Generation via Multimodal Context Alignment
Hummingbird is a novel diffusion-based image generator designed for scene-aware tasks like Visual Question Answering and Human-Object Interaction Reasoning, where it excels in maintaining high fidelity and diversity in images by accurately preserving scene attributes from a multimodal context that includes a reference image and text guidance. By introducing a Multimodal Context Evaluator and benchmark testing with MME Perception and Bongard HOI datasets, Hummingbird demonstrates superior performance over existing methods in generating context-aligned images, highlighting its potential as a leading tool for complex visual tasks.

Near-Optimal Policy Identification in Robust Constrained Markov Decision Processes via Epigraph Form
This paper introduces the first algorithm that can identify a near-optimal policy in a robust constrained Markov decision process (RCMDP), effectively minimizing cumulative cost while adhering to constraints across a range of uncertain environments. It resolves the challenges posed by conventional policy gradient approaches through the epigraph form and a novel bisection search algorithm, guaranteeing an $\varepsilon$-optimal policy evaluation with improved efficiency.

Higher-Order Graphon Neural Networks: Approximation and Cut Distance
This paper extends the $k$-WL test for graphons to graphon-signal spaces and introduces signal-weighted homomorphism densities as a crucial method, leading to the development of Invariant Graphon Networks (IWNs). The IWNs, though based on a restricted subset of the Invariant Graph Networks basis, maintain the power of the $k$-WL test and facilitate universal approximation for graphon-signals, aligning better with the graphon space geometry and offering comparability to message passing graph neural networks.

Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling
This paper introduces a new dataset of over one million MIDI files, transcribed from audio recordings of piano performances using a multi-stage data pipeline that includes a language model and an audio classifier. The dataset, containing approximately 100,000 hours of transcribed audio, is analyzed for statistical insights and metadata tags, and is available at the provided GitHub link.

Arithmetic Transformers Can Length-Generalize in Both Operand Length and Count
This paper addresses the challenge of length generalization in transformers, particularly for difficult arithmetic tasks like multi-operand addition and multiplication, achieving significant improvements in handling longer sequences than those seen during training. By designing task-specific scratchpads and using multi-level Position Coupling, the authors enable transformers to attend accurately to the necessary positions, and they theoretically prove that a 1-layer transformer using their method can solve multi-operand addition with exponentially expanded operand lengths and counts.

3DIS: Depth-Driven Decoupled Image Synthesis for Universal Multi-Instance Generation
This paper presents Depth-Driven Decoupled Image Synthesis (3DIS), a framework that improves multi-instance text-to-image generation by decoupling the process into generating scene depth maps for positioning and using pre-trained ControlNet for detailed attribute rendering. The 3DIS framework is shown to significantly enhance layout precision and attribute rendering compared to existing methods, while also being compatible with various foundational models.

3DMolFormer: A Dual-channel Framework for Structure-based Drug Discovery
3DMolFormer is a unified dual-channel transformer-based framework designed to address both protein-ligand docking and pocket-aware 3D drug design by leveraging their duality within the drug discovery process. By employing a novel representation of 3D pocket-ligand complexes and overcoming data limitations through pre-training and fine-tuning, 3DMolFormer outperforms existing methods, offering enhanced capabilities in structure-based drug discovery.

4K4DGen: Panoramic 4D Generation at 4K Resolution
This paper presents a novel method to convert a single panoramic image into a fully immersive 4D experience with 360° dynamic scenes at 4K resolution, addressing the limitations of existing generative techniques for VR/AR environments. By introducing efficient splatting techniques and a Panoramic Denoiser, the authors demonstrate the successful adaptation of 2D diffusion priors to create consistent panoramic videos, enabling enhanced real-time exploration and seamless elevation to 4D interactive environments.

ADAPT: Attentive Self-Distillation and Dual-Decoder Prediction Fusion for Continual Panoptic Segmentation
This paper addresses the challenges of catastrophic forgetting and poor generalization in panoptic segmentation when learning from a continuous stream of new tasks by proposing an efficient adaptation framework using attentive self-distillation and dual-decoder prediction fusion. The method achieves state-of-the-art performance on ADE20K and COCO benchmarks by preserving prior knowledge, enhancing knowledge retention, and integrating outputs seamlessly, with code available at the provided GitHub repository.

Adaptive backtracking for faster optimization
This paper introduces a novel approach to adjusting step sizes in backtracking line search by considering how much the chosen criterion is violated, enhancing optimization speed without extra computational cost. The proposed adaptive backtracking method is validated through experiments on over fifteen real-world datasets and is proven to maintain the convergence rates and feasibility of both convex and nonconvex problems, matching the efficiency of traditional backtracking methods.

Adaptive Pruning of Pretrained Transformer via Differential Inclusions
This paper introduces Solution Path Pruning (SPP), a method that allows pretrained transformers to be pruned at any desired compression ratio within a single pruning stage using a differential inclusion for a mask parameter. By segmenting transformers into paired modules and applying low-rank compression, SPP efficiently maintains performance while enabling flexibility and customization in model sparsity, as confirmed by extensive experiments on various transformer backbones.

A Differentiable Metric for Discovering Groups and Unitary Representations
This paper introduces a novel differentiable method using representation theory of finite groups to integrate group structures into deep learning frameworks. The proposed neural network architecture effectively discovers group structures and their unitary representations, paving the way for automated algebraic structure discovery with applications in automatic symmetry discovery for geometric deep learning.

AdvWave: Stealthy Adversarial Jailbreak Attack against Large Audio-Language Models
The paper presents AdvWave, a novel jailbreak framework designed to attack large audio-language models (LALMs). By introducing a dual-phase optimization method and an adaptive adversarial target search algorithm, AdvWave overcomes traditional challenges in attacking LALMs, achieving higher success rates while ensuring the generated adversarial audio maintains natural sound quality, thereby advancing the understanding and safety alignment of LALMs for safer real-world applications.

Algorithmic Stability Based Generalization Bounds for Adversarial Training
This paper introduces a novel stability analysis of adversarial training, providing generalization upper bounds based on an expansiveness property of adversarial perturbations and finding that the PGD attack's use of the sign function leads to robust overfitting. The study reveals that these expansiveness parameters crucially affect both optimization and generalization in adversarial training, highlighting overlooked challenges and intrinsic difficulties in PGD-like iterative attack algorithms.

An Empirical Analysis of Uncertainty in Large Language Model Evaluations
This paper explores the stability of LLM evaluators, noting that they exhibit varying levels of uncertainty depending on model families and sizes. By employing special prompting strategies and fine-tuning an uncertainty-aware evaluator named ConfiLM with a human-annotated set, the study improves evaluation performance in out-of-distribution scenarios, demonstrating a method to enhance LLM reliability and detection capabilities.

A Policy-Gradient Approach to Solving Imperfect-Information Games with Best-Iterate Convergence
This paper explores the use of policy gradient methods in two-player zero-sum imperfect-information extensive-form games (EFGs) and demonstrates that these methods can achieve best-iterate convergence to a regularized Nash equilibrium in self-play. The findings provide for the first time a theoretical basis for safely applying policy gradient methods in the context of extensive-form games, overcoming the traditional reliance on counterfactual values.

Approximating Full Conformal Prediction for Neural Network Regression with Gauss-Newton Influence
This paper addresses the challenge of constructing well-calibrated and sharp prediction intervals for neural network regressors when inherent uncertainty estimates are absent. By approximating the full conformal prediction method and leveraging Gauss-Newton influence for parameter perturbation, the authors propose an efficient approach that results in locally adaptive and tighter prediction intervals compared to split conformal prediction, as demonstrated on standard regression benchmarks and bounding box localization tasks.

ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation
This paper presents ARLON, a novel framework for generating high-quality, dynamic, and temporally consistent long videos by combining diffusion Transformers with autoregressive models. ARLON achieves state-of-the-art performance in long video generation, significantly outperforming baselines in multiple metrics while enhancing generation efficiency and quality using innovative techniques like latent VQ-VAE compression and adaptive norm-based semantic injection.

Autoregressive Video Generation without Vector Quantization
This paper introduces NOVA, a novel autoregressive video generation model that reformulates video generation through non-quantized temporal and spatial predictions for high efficiency. NOVA outperforms existing models in efficiency, visual quality, and generalization across video durations, offering superior results even with reduced model capacity and lower training costs.


Bayesian Experimental Design Via Contrastive Diffusions
This paper introduces a novel approach to Bayesian Optimal Experimental Design (BOED) by employing a pooled posterior distribution and a new EIG gradient expression to efficiently maximize Expected Information Gain (EIG). By integrating diffusion-based samplers and generative models, the method enhances computational efficiency and extends BOED's applicability to complex, high-dimensional settings as demonstrated through numerical experiments.

Benign Overfitting in Out-of-Distribution Generalization of Linear Models
This work explores benign overfitting in Out-of-Distribution (OOD) settings, particularly focusing on over-parameterized linear models under covariate shift. It provides non-asymptotic guarantees for benign overfitting in standard ridge regression and shows that Principal Component Regression can achieve a faster statistical rate compared to ridge regression in certain conditions.

BenTo: Benchmark Reduction with In-Context Transferability
This paper proposes a method to efficiently reduce the tasks in benchmarking large language models (LLMs) without compromising evaluation quality by utilizing task transferability and relevance. Through a metric for estimating transferability via in-context learning, the study achieves a reduction in tasks to 5% of the original benchmark with less than a 4% difference in evaluation, offering a training-free, gradient-free, and effective approach.

Beyond Model Collapse: Scaling Up with Synthesized Data Requires Verification
The paper investigates the issue of model collapse in large language models (LLMs) when trained on data generated by other LLMs and proposes using verification on synthesized data as a solution. Through theoretical characterization and practical experiments on tasks like computing matrix eigenvalues and news summarization, the study demonstrates that verifiers can effectively prevent model collapse and highlights a proxy measure that correlates strongly with performance.

Binary Losses for Density Ratio Estimation
This paper addresses the challenge of estimating the ratio of two probability densities, focusing on how the choice of binary loss functions affects estimator accuracy, particularly for tasks where large density ratio values are critical. The authors propose a method using Bregman divergences to characterize loss functions that produce more accurate density ratio estimators, outperforming existing approaches across diverse real-world tasks.

BitStack: Any-Size Compression of Large Language Models in Variable Memory Environments
This paper introduces BitStack, a novel, training-free weight compression method that provides adaptable memory-performance trade-offs, addressing the memory constraint challenges faced by large language models (LLMs) on local devices. BitStack dynamically adjusts the model size using weight decomposition, outperforming traditional quantization techniques, especially at extreme compression ratios, and demonstrating its efficacy across various tasks.

Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors
This paper addresses the challenge of hard-label black-box adversarial attacks by transforming the problem into a continuous optimization task, focusing on efficient ray direction search to minimize $\ell_p$ norm distance to adversarial regions. A novel prior-guided approach is proposed, utilizing transfer-based priors from surrogate models to enhance gradient estimation and improve query efficiency, outperforming 11 state-of-the-art methods on ImageNet and CIFAR-10 datasets.

Bridging the Semantic Gap Between Text and Table: A Case Study on NL2SQL
The paper introduces **T**n**T**, a table-language model designed to enhance Large Language Models' understanding of structured tabular data by employing multimodal table representations. Experimental results on NL2SQL show that **T**n**T** significantly improves table comprehension, achieving up to 14.4% higher execution accuracy compared to traditional text-based table representations.

Bringing NeRFs to the Latent Space: Inverse Graphics Autoencoder
This paper introduces an Inverse Graphics Autoencoder (IG-AE) that integrates 3D geometry into image autoencoder latent spaces, enabling interoperability with 2D latent-based methods and addressing the challenge of lacking underlying 3D geometry. The proposed IG-AE allows for improved quality and accelerated training and rendering of Latent NeRFs compared to standard autoencoders, demonstrated through an open-source extension of the Nerfstudio framework.

Building Blocks of Differentially Private Training
This blog explores the foundational elements necessary for training neural networks while ensuring differential privacy. It provides insights into methods that maintain privacy without compromising the effectiveness of the network's learning capabilities.

Can LLMs Understand Time Series Anomalies?
This study explores the largely unexplored potential of Large Language Models (LLMs) for time series anomaly detection, focusing on zero-shot and few-shot learning scenarios. Through a series of experiments, it reveals that LLMs comprehend time series better as images rather than text and that their anomaly detection abilities do not align with common assumptions about their reasoning capabilities, offering valuable insights into their varied performance across models.

Can Watermarked LLMs be Identified by Users via Crafted Prompts?
This paper investigates the imperceptibility of text watermarks in Large Language Models (LLMs), an area not thoroughly explored in previous research. It introduces Water-Probe, an identification algorithm capable of detecting watermark biases in LLMs, and proposes the Water-Bag strategy to improve imperceptibility by increasing the variability of watermark key selection, thereby maintaining high detection accuracy while minimizing visibility.

Causal Concept Graph Models: Beyond Causal Opacity in Deep Learning
This paper introduces Causal Concept Graph Models (Causal CGMs), which are designed to provide causally transparent decision-making processes in deep neural network models. The study demonstrates that Causal CGMs maintain the performance of traditional models while allowing human intervention to correct reasoning steps, enhancing both the accuracy and reliability of model explanations and supporting the analysis of interventional and counterfactual scenarios for improved causal interpretability.

Chain-of-region: Visual Language Models Need  Details for Diagram Analysis
This paper addresses the challenges faced by Visual Language Models (VLMs) like GPT-4V in accurately processing scientific diagrams by introducing a novel approach that combines traditional computer vision techniques with VLMs. By employing OpenCV, as well as shape detection and region merging algorithms, the proposed method systematically decomposes diagrams into discernible elements and aggregates metadata, thereby enhancing VLM performance in diagram analysis tasks and extending their multimodal processing capabilities.

ColPali: Efficient Document Retrieval with Vision Language Models
This paper introduces the Visual Document Retrieval Benchmark (ViDoRe) to evaluate systems on visually rich document retrieval, highlighting limitations in current retrieval systems that rely heavily on textual information. By proposing the ColPali Vision Language Model, which embeds image pages directly, the authors demonstrate significant improvements in document retrieval effectiveness, offering a simpler, faster, and end-to-end trainable alternative to traditional methods.

Confidence Elicitation: A New Attack Vector for Large Language Models
This paper addresses the challenge of adversarial robustness in large language models (LLMs) by introducing a new paradigm for black-box attack guidance in classification settings, leveraging the elicitation of model confidence. The proposed approach achieves state-of-the-art results on three datasets with LLaMA-3-8B-Instruct and Mistral-7B-Instruct-V0.3 models, enhancing attack efficacy compared to existing hard-label black-box attack methods.

Consistency Checks for Language Model Forecasters
This paper addresses the challenge of evaluating forecasters by introducing a novel, general consistency metric based on arbitrage to assess the logical consistency of predictions. By developing an automated evaluation system and a proper-scoring-rule benchmark, the authors demonstrate a strong correlation between the instantaneously measurable consistency metrics and future ground truth Brier scores, offering a long-term evaluation tool that extends to 2028.

Contextual Document Embeddings
This paper introduces two methods for creating contextualized document embeddings: an alternative contrastive learning objective incorporating document neighbors and a new architecture encoding neighbor document information. These approaches outperform traditional biencoders, achieving state-of-the-art results on the MTEB benchmark, and are applicable to any contrastive learning dataset and biencoder.

Continual Slow-and-Fast Adaptation of Latent Neural Dynamics (CoSFan): Meta-Learning What-How & When to Adapt
This paper introduces a new continual meta-learning framework, CoSFan, designed for forecasting time-series data with diverse and non-stationary dynamics. By employing a feed-forward meta-model for fast adaptation to specific system dynamics and integrating novel data shift detection strategies with experience replay, the approach significantly enhances forecasting accuracy and outperforms existing alternatives in learning across varied dynamic systems.

Control-oriented Clustering of Visual Latent Representation
This study explores the geometry of the visual representation space in an image-based control pipeline, revealing a clustering phenomenon inspired by neural collapse. By leveraging this law of clustering, the authors demonstrate that pretraining the vision encoder to encourage control-oriented clustering significantly improves test-time performance, with real-world experiments confirming the effectiveness of this approach.

Coreset Spectral Clustering
This paper introduces a coreset spectral clustering algorithm that combines the benefits of kernel $k$-means and spectral clustering to efficiently cluster large graphs. The proposed method improves the running time of existing algorithms and provides an $O(\alpha)$-approximation for the normalised cut problem on the original graph, demonstrating faster performance and effectiveness on large real-world datasets.

CoRNStack: High-Quality Contrastive Data for Better Code Retrieval and Reranking
CoRNStack is introduced as a high-quality contrastive training dataset for code to enhance the performance of code retrieval models, particularly in complex scenarios like bug localization within GitHub repositories. By using consistency filtering and mined hard negatives, CoRNStack enables embedding models to achieve state-of-the-art results, significantly improving code retrieval and function localization tasks.

CR2PQ: Continuous Relative Rotary Positional Query for Dense Visual Representation Learning
This paper addresses the challenge of establishing pixel/patch correspondence in dense visual contrastive learning (DRL) by introducing a Continuous Relative Rotary Positional Query ({\mname}) to enhance patch-level representation learning. Through extensive experimentation, {\mname} demonstrates state-of-the-art performance on standard datasets, significantly improving detection and segmentation metrics compared to previous methods, while also offering faster convergence rates.

Credal Wrapper of Model Averaging for Uncertainty Estimation in Classification
This paper introduces the credal wrapper, a novel approach to enhance uncertainty estimation in Bayesian neural networks and deep ensembles by forming a credal set representation that provides upper and lower probability bounds per class, addressing epistemic uncertainty. Through extensive experiments on various out-of-distribution detection benchmarks and network architectures, the credal wrapper method demonstrates improved performance in uncertainty estimation and reduced expected calibration error compared to existing baselines.

CTSyn: A Foundation Model for Cross Tabular Data Generation
Generative Foundation Models have excelled in generating synthetic data for images and text, but face challenges with tabular data due to feature heterogeneity. The proposed Cross-Table Synthesizer (CTSyn) addresses this by utilizing an autoencoder and conditional latent diffusion model to effectively generate diverse and utility-rich synthetic tabular data, establishing a new benchmark in tabular data synthesis.

CubeDiff: Repurposing Diffusion-Based Image Models for Panorama Generation
We present a new technique for creating 360° panoramas from text or images by using multi-view diffusion models to generate each face of a cubemap as a standard perspective image. This approach simplifies the generation process without needing correspondence-aware attention layers, allows fine-grained text control, produces high-resolution panoramas, and achieves state-of-the-art results across diverse inputs.

Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance
The paper introduces "data mixing laws," which provide a quantitative approach to predict large language model performance based on the mixture proportions of pretraining data domains, such as web texts and academic papers. This method, combined with scaling laws for training steps and model sizes, allows for efficient predictions of optimal data mixtures, improving model performance and preventing catastrophic forgetting, as demonstrated in experiments with RedPajama and continual training scenarios.

Data Unlearning in Diffusion Models
This paper addresses the challenge of data unlearning in diffusion models by introducing Subtracted Importance Sampled Scores (SISS), a novel approach with theoretical guarantees for efficiently unlearning specific datapoints without retraining from scratch. The method demonstrated success on datasets like CelebA-HQ, MNIST, and Stable Diffusion, achieving a balance between model quality and unlearning effectiveness, with nearly 90% success in mitigating memorization on tested prompts.

DeciMamba: Exploring the Length Extrapolation Potential of Mamba
This paper addresses the challenge of long-range sequence processing for Transformers by introducing Mamba, which matches Transformer-level performance with fewer computational resources but has limited length-generalization capabilities. The authors propose DeciMamba, a context-extension method for Mamba that uses a hidden filtering mechanism to effectively extend the sequence length during inference, demonstrated through empirical experiments to successfully handle longer context lengths and improve inference speed.

Decomposition Polyhedra of Piecewise Linear Functions
This paper addresses the challenge of decomposing a continuous piecewise linear (CPWL) function into the difference of two convex CPWL functions, emphasizing the importance of minimizing the number of linear pieces for optimization and neural network applications. By disproving a recent approach and proposing a novel method involving a polyhedral complex, the authors reveal that irreducible decompositions align with the bounded faces of the polyhedron, leading to unique minimal decompositions and improved neural network constructions for given convex functions, with implications in submodular function theory.

Deep Incomplete Multi-view Learning via Cyclic Permutation of VAEs
The paper introduces Multi-View Permutation of Variational Auto-Encoders (MVP) to address the challenge of learning unified representations from incomplete multi-view data, which often leads to insufficient and inconsistent representations. By establishing inter-view correspondences and applying permutations within the latent space, MVP effectively infers missing views and improves information aggregation, demonstrating superior performance in multi-view clustering and generation tasks across diverse datasets.

Denoising as Adaptation: Noise-Space Domain Adaptation for Image Restoration
This paper presents a novel approach called "denoising as adaptation" for improving the generalization of image restoration models to real-world scenarios by performing domain adaptation via noise space using diffusion models. By introducing strategies such as a channel-shuffling layer and residual-swapping contrastive learning, the method effectively aligns restored outputs with a clean distribution, showing superior performance in tasks like denoising, deblurring, and deraining.

Dense Video Object Captioning from Disjoint Supervision
We introduce a unified model for dense video object captioning that enhances accuracy and temporal coherence by integrating detection, tracking, and captioning in a single end-to-end approach, outperforming traditional multi-stage pipelines. Utilizing a training strategy that combines disjoint tasks from diverse datasets, our model excels in zero-shot scenarios and strong baselines, while also setting new benchmarks in spatial grounding tasks without explicit training, surpassing previous state-of-the-art methods.

Differential learning kinetics govern the transition from memorization to generalization during in-context learning
This paper investigates the transition from memorization to generalization in transformers during in-context learning, emphasizing the role of diverse task exposure. It reveals that separate sub-circuits are responsible for memorizing and generalizing, with their learning rates rather than capacity constraints dictating this transition, and presents a memorization scaling law that predicts when the network will generalize, explaining various phenomena related to in-context learning.

Differentially private optimization for non-decomposable objective functions
This paper introduces a novel variant of DP-SGD designed for similarity-based loss functions, specifically targeting issues with increased \(L_2\) sensitivity due to larger batch sizes in differentially private training settings. The proposed method improves privacy capabilities in contrastive loss scenarios, achieving performance comparable to non-private models and outperforming standard DP-SGD in CIFAR-10 pre-training and CIFAR-100 finetuning tasks.

Diffusion Actor-Critic: Formulating Constrained Policy Iteration as Diffusion Noise Regression for Offline Reinforcement Learning
In this paper, we introduce Diffusion Actor-Critic (DAC), which uses a diffusion noise regression framework for policy regularization in offline reinforcement learning, ensuring that the target policy remains close to the behavior policy and mitigates out-of-distribution actions. By employing a diffusion-based policy constraint and leveraging the lower confidence bound of the Q-ensemble, DAC achieves stable convergence and superior performance, outperforming state-of-the-art methods on D4RL benchmarks.

Diffusion Models are Evolutionary Algorithms
This paper reveals that diffusion models function as evolutionary algorithms by framing evolution as a denoising process, which inherently includes selection, mutation, and reproductive isolation. Introducing the Diffusion Evolution method, the study demonstrates its superiority in efficiently identifying multiple optimal solutions and reducing computational steps in high-dimensional tasks, thus bridging machine learning and biology in innovative ways.

Diffusion Models as Cartoonists: The Curious Case of High Density Regions
This paper explores the high-density regions of diffusion models and introduces a theoretical mode-tracking process to identify the precise mode of the denoising distribution. It also presents a practical high-density sampler that generates higher likelihood images, often cartoon-like or blurry, and a novel method to track sample likelihoods in diffusion SDEs, all without extra computational cost.

Diffusion-NPO: Negative Preference Optimization for Better Preference Aligned Generation of Diffusion Models
Diffusion models excel in image generation, but existing methods often struggle to align with human preferences due to inadequate handling of unconditional/negative-conditional outputs. We propose a simple modification to enhance preference alignment by training models to consider negative preferences, effectively improving output quality without new training strategies or datasets, and seamlessly integrating with various pre-optimized models.

Dissecting Adversarial Robustness of Multimodal LM Agents
This paper addresses the adversarial robustness of language model-based autonomous agents, highlighting the inadequacy of current safety evaluations meant for chatbots. The authors introduce the Agent Robustness Evaluation (ARE) framework to assess agent robustness, demonstrating that minor perturbations can significantly compromise agent performance and that the addition of new components can increase vulnerabilities, with data and code available for further exploration.

Distilled Decoding 1: One-step Sampling of Image Auto-regressive Models with Flow Matching
Autoregressive (AR) models excel in text and image generation but suffer from slow speeds due to sequential token generation. This paper introduces Distilled Decoding (DD), a novel method that enables few-step output generation by creating a deterministic mapping from Gaussian to the output distribution of pre-trained AR models, achieving significant speed-ups in image AR model generation while maintaining acceptable quality, and challenging the belief that AR models are inherently slow.

Diversity-Rewarded CFG Distillation
This paper introduces diversity-rewarded CFG distillation, a novel finetuning technique to enhance generative models like MusicLM, addressing the limitations of Classifier-Free Guidance (CFG) by improving originality and diversity without incurring additional inference costs. The approach optimizes models with a dual objective of imitation and diversity rewards, resulting in outputs with superior quality-diversity balance, as confirmed by human evaluations, and facilitates flexible quality-diversity trade-offs through weight-based model merging strategies.

Do I Know This Entity? Knowledge Awareness and Hallucinations in Language Models
This paper investigates the problem of hallucinations in large language models and identifies entity recognition as a key mechanism underlying this issue. By using sparse autoencoders to uncover significant directions in the model's representation space, the study shows that models have self-knowledge about their own capabilities, which affects their tendency to either refuse answering questions about known entities or hallucinate attributes of unknown entities.

Do LLM Agents  Have Regret? A Case Study in Online Learning and Games
This paper investigates the performance of large language model (LLM) agents in decision-making scenarios, particularly focusing on their interactions within multi-agent settings using benchmarks from online learning and game theory. It introduces a new unsupervised training method called regret-loss, which enhances the no-regret behaviors of LLMs, demonstrating improved effectiveness in scenarios where traditional models like GPT-4 struggle.

Do vision models perceive objects like toddlers ?
This study investigates whether pre-trained machine learning vision models can develop visual properties similar to toddlers, such as object recognition, shape-based categorization, orientation preference, and generalization over objects' parts. By comparing these models with toddlers, the research highlights key differences and interactions between human visual development and artificial vision systems.

DSPO: Direct Score Preference Optimization for Diffusion Model Alignment
This paper introduces Direct Score Preference Optimization (DSPO), a novel algorithm that fine-tunes diffusion-based Text-to-Image models by aligning their pretraining and fine-tuning objectives through score matching. Empirical and theoretical results demonstrate that DSPO surpasses existing preference learning methods, improving human preference evaluation, visual appeal, and prompt alignment in generated images.

DynaMath: A Dynamic Visual Benchmark for Evaluating Mathematical Reasoning Robustness of Vision Language Models
This paper investigates the mathematical reasoning robustness of Vision-Language Models (VLMs), highlighting their limitations by evaluating their performance on variations of similar questions. To address the static nature of existing benchmarks, the authors introduce **DynaMath**, a dynamic visual math benchmark with 501 seed questions that automatically generate a diverse set of variants, allowing comprehensive analysis and insights into the models’ reasoning abilities under varying conditions.

EC-Diffuser: Multi-Object Manipulation via Entity-Centric Behavior Generation
This paper introduces a novel behavioral cloning approach that utilizes object-centric representations and an entity-centric Transformer with diffusion-based optimization to improve learning from offline image data in object manipulation tasks. By decomposing observations into Deep Latent Particles and using a Transformer to predict dynamics and actions, the method achieves substantial performance improvements in multi-object environments and enables zero-shot generalization to novel object configurations.

Effective Interplay between Sparsity and Quantization: From Theory to Practice
This paper investigates the interaction between sparsity and quantization, two prominent model compression techniques for deep neural networks, demonstrating through the first mathematical proof that they are non-orthogonal and can compound errors when combined. The authors corroborate these findings with experiments on large language and vision models, providing insights into the importance of the order in which these methods are applied to ensure efficient deployment on resource-constrained platforms without significantly compromising accuracy.

Efficient Dictionary Learning with Switch Sparse Autoencoders
Switch Sparse Autoencoders (SAEs) introduce a novel architecture to efficiently scale sparse autoencoders by routing activation vectors between smaller "expert" SAEs, reducing the computational cost of training while maintaining feature interpretability. The experimental results demonstrate that Switch SAEs provide significant improvements in the trade-off between reconstruction quality and sparsity compared to other SAE architectures, while also exploring feature geometry and duplication across experts.

Efficient Interpolation between Extragradient and Proximal Methods for Weak MVIs
This paper introduces an error-corrected version of the inexact proximal point algorithm for nonmonotone games, achieving an $\mathcal O(1/\epsilon)$ rate for $\rho \in (-\tfrac{1}{L}, \infty)$, which removes a logarithmic complexity factor found in existing methods. The proposed scheme automatically adjusts the accuracy for proximal computation, inherits strong properties of exact algorithms, achieves linear convergence under certain conditions, and utilizes error tolerance strategically to enhance problem-solving capabilities.

EgoSim: Egocentric Exploration in Virtual Worlds with Multi-modal Conditioning
This paper addresses the challenge of enhancing controllability in video diffusion models to enable more effective interaction with and planning within observed environments. By introducing the Spacetime Epipolar Attention Layer and the CI2V-adapter, the authors improve camera control and manage the balance between textual and visual embeddings, demonstrating effectiveness with the EgoSim model on various datasets.

EmbodiedSAM: Online Segment Any 3D Thing in Real Time
This paper presents a novel method for real-time 3D instance segmentation using the Segment Anything Model (SAM), addressing the challenge of real-time perception in streaming RGB-D video by efficiently transferring 2D masks to 3D point clouds. The proposed approach demonstrates superior performance, outperforming offline methods on datasets like ScanNet and showing strong generalization in zero-shot and data-efficient scenarios.

Equivariant Denoisers Cannot Copy Graphs: Align Your Graph Diffusion Models
Graph diffusion models, typically used in graph generative modeling, encounter challenges in graph-to-graph translation tasks, such as chemical reaction prediction, due to limitations in breaking symmetries of noisy inputs. This paper introduces an alignment technique that enhances model performance, improving top-1 accuracy from 5% to 54.7% in retrosynthesis, thereby matching state-of-the-art results.

Error-quantified Conformal Inference for Time Series
Uncertainty quantification in time series prediction is enhanced by the proposed Error-quantified Conformal Inference (ECI), which improves upon traditional methods by smoothing the quantile loss function and providing a continuous feedback scale based on the miscoverage error. This approach offers a long-term coverage guarantee amidst arbitrary dependence and distribution shifts, yielding tighter prediction sets with effective miscoverage control, as validated by extensive experiments.

Estimation of single-cell and tissue perturbation effect in spatial transcriptomics via Spatial Causal Disentanglement
Celcomen, a novel generative graph neural network, is introduced to disentangle intra- and inter-cellular gene regulation using spatial transcriptomics and single-cell data, based on mathematical causality to avoid mistaking correlation for causation. The model provides insights into inaccessible biological states through counterfactual prediction, demonstrated in human health scenarios like glioblastoma and fetal spleen, and offers mechanistic interpretability by allowing parameters to be reverse-engineered from observed behaviors.

ExACT: Teaching AI Agents to Explore with Reflective-MCTS and Exploratory Learning
This paper introduces Reflective Monte Carlo Tree Search (R-MCTS), a novel test-time algorithm designed to improve the decision-making abilities of AI agents, specifically those powered by models like GPT-4o, in complex environments and long-horizon planning tasks. By incorporating contrastive reflection and multi-agent debate, and by fine-tuning through self-learning, R-MCTS significantly enhances performance on the VisualWebArena benchmark, achieving notable improvements over the current state-of-the-art, while demonstrating promising scalability in both training and testing contexts.

Factor Graph-based Interpretable Neural Networks
This paper introduces AGAIN, a factor graph-based interpretable neural network that generates comprehensible explanations under unknown perturbations by integrating logical rules during inference without requiring retraining. AGAIN effectively identifies and rectifies logical errors in explanations, outperforming state-of-the-art baselines, as demonstrated by extensive experiments on three datasets.

Faster, More Efficient RLHF through Off-Policy Asynchronous Learning
This paper presents a novel approach to Reinforcement Learning with Human Feedback (RLHF) by decoupling sample generation and learning, thereby enabling asynchronous training for improved computational efficiency. The study finds that while online Deep Policy Optimization (DPO) is resilient to off-policy data, further performance can be compromised by extensive compute optimizations, although they achieve a 40% faster training of the LLaMA 3.1 8B model without sacrificing final performance.

Fast unsupervised ground metric learning with tree-Wasserstein distance
This paper introduces a novel approach to unsupervised ground metric learning using tree-Wasserstein distances (TWD) to enhance the efficiency of Wasserstein singular vectors (WSVs) in clustering tasks, particularly for unlabelled datasets. The proposed method not only offers a computational complexity of $\mathcal{O}(n^3+m^3+mn)$, improving the scalability of WSVs, but also proves its effectiveness on single-cell RNA sequencing data, signifying its potential for broad application in unsupervised settings.

Federated Residual Low-Rank Adaption of Large Language Models
The paper introduces Federated Residual Low-Rank Adaptation (FRLoRA), a novel method for federated fine-tuning of Large Language Models that addresses limitations related to data heterogeneity and client drift. FRLoRA enhances global updates by employing a high-rank parameter space and reinitializes local low-rank matrices to improve convergence, consistently outperforming existing federated learning methods across multiple benchmarks.

Fine-Tuning Attention Modules Only: Enhancing Weight Disentanglement in Task Arithmetic
This paper addresses the challenge of task interference in unified pre-trained models by leveraging a new method that fine-tunes attention modules in Transformers to improve weight disentanglement. The proposed approach not only enhances efficiency without incurring the doubled training costs associated with Neural Tangent Kernel linearization but also demonstrates the crucial role of representation modules in enhancing weight disentanglement across tasks.

FlashMask: Efficient and Rich Mask Extension of FlashAttention
This paper introduces \ours{}, an extension of FlashAttention, which employs a column-wise sparse representation of attention masks to efficiently process long-context sequences with linear memory complexity $\mathcal{O}(N)$. By optimizing kernel implementations and utilizing sparsity, \ours{} achieves significant throughput improvements, surpassing both the traditional FlashAttention dense method and FlexAttention, while maintaining computational accuracy for large models.

FreSh: Frequency Shifting for Accelerated Neural Representation Learning
Implicit Neural Representations (INRs) using multilayer perceptrons (MLPs) face challenges in capturing high-frequency details due to low-frequency bias, often necessitating extensive hyperparameter tuning. This paper introduces frequency shifting (FreSh), a method that aligns the initial frequency spectrum of a model's output with the target signal, significantly enhancing performance across neural representation tasks with minimal computational overhead.

Fundamental Limits of Prompt Tuning Transformers: Universality, Capacity and Efficiency
This paper explores the statistical and computational boundaries of prompt tuning in transformer-based models with a single self-attention layer. It demonstrates that such tuning is universal for approximating sequence-to-sequence functions and identifies conditions for efficient algorithms, offering valuable insights into designing expressive and efficient prompt tuning methods.

Gated Delta Networks: Improving Mamba2 with Delta Rule
This paper introduces Gated DeltaNet, which combines gating for adaptive memory control with a delta update rule to address limitations in retrieval and long-context tasks of Linear Transformers. By developing a parallel training algorithm and hybrid architectures, Gated DeltaNet surpasses existing models like Mamba2 and DeltaNet, improving task performance and training efficiency across various benchmarks.

Generalizability of Neural Networks Minimizing Empirical Risk Based on Expressive Power
This paper explores generalization in neural networks under less restrictive assumptions than traditional bounds, establishing conditions where networks can generalize effectively by minimizing empirical risk. It provides lower bounds on population accuracy and necessary data quantities, offering insights into deep learning phenomena such as overfitting, over-parameterization, and the role of loss functions.

Generalized Consistency Trajectory Models for Image Manipulation
This paper introduces generalized consistency trajectory models (GCTMs), which enhance traditional continuity trajectory models (CTMs) by enabling translation between arbitrary distributions through ODEs, addressing the limitations in flexibility experienced with CTMs. The proposed GCTMs are shown to be effective in diverse image manipulation tasks, such as image-to-image translation, restoration, and editing, offering a more efficient and versatile alternative to traditional diffusion-based generation approaches.

Generating Freeform Endoskeletal Robots
This paper introduces terrestrial endoskeletal robots that combine deformable soft bodies with jointed internal skeletons to achieve efficient land movement, bridging the gap between fully rigid and fully soft robot designs. By integrating elastic and rigid structures and optimizing sensorimotor coordination through reinforcement learning and evolutionary strategies, the research offers a novel framework for creating "higher robots" and serves as a platform for testing evolutionary design and representation learning in complex systems.

Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models
This paper addresses the challenge of uncertainty quantification (UQ) for natural language generation (NLG) in black-box large language models (LLMs). It introduces and evaluates different measures of uncertainty and confidence, demonstrating that semantic dispersion can effectively predict response quality, aiding practitioners in managing uncertainty with LLMs.

Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits
This paper addresses the discrepancy between the empirical performance and theoretical regret bounds of algorithms like Thompson sampling and Greedy in $d$-dimensional stochastic linear bandit problems. By introducing a data-driven technique that leverages the geometric properties of uncertainty ellipsoids, the authors derive a frequentist regret bound that enables course correction in poor-performing instances, achieving minimax optimal regret while maintaining empirical efficacy, as validated through simulations.

GeoX: Geometric Problem Solving Through Unified Formalized Vision-Language Pre-training
GeoX is a multi-modal large language model designed to address the challenges in automatic Geometry Problem Solving (GPS) by improving geometric understanding and reasoning. By introducing unimodal pre-training, geometry-language alignment, and a Generator-And-Sampler Transformer (GS-Former), GeoX surpasses existing models and offers verifiable solutions, demonstrating superior performance on benchmarks like GeoQA and UniGeo.

Global Convergence of Policy Gradient in Average Reward MDPs
This paper provides the first complete finite-time global convergence analysis for policy gradient methods applied to infinite horizon average reward Markov decision processes (MDPs), specifically focusing on ergodic tabular MDPs with finite states and actions. The study demonstrates that the policy gradient iterates converge sublinearly to the optimal policy and establishes finite-time performance guarantees, improving existing performance bounds and presenting simulations that empirically validate these results.

GPUDrive: Data-driven, multi-agent driving simulation at 1 million FPS
GPUDrive is a GPU-accelerated multi-agent simulator built on the Madrona Game Engine, achieving over a million simulation steps per second by integrating C++ for complex agent behaviors and CUDA for performance optimization. It bridges the gap in multi-agent planning by enabling efficient reinforcement learning training on large-scale datasets, allowing rapid scenario scaling and accessible workflows through Python, with its code and pre-trained agents available for open access.

Graph Transformers Dream of Electric Flow
This paper demonstrates that linear Transformers can solve canonical graph problems, such as electric flow and eigenvector decomposition, by leveraging the graph's incidence matrix, with specific weight configurations provided for each algorithm. Experiments confirm the theoretical results, showing improved positional encoding in a molecular regression task compared to traditional methods, thus advancing the understanding of Transformer mechanisms for graph data.

Handling Delay in Real-Time Reinforcement Learning
This paper addresses the challenges of real-time reinforcement learning by exploring the trade-off between minimizing observational delay and maintaining network expressivity. It introduces a solution involving temporal skip connections and history-augmented observations, showing improved performance across various tasks and demonstrating significant acceleration in inference through parallel neuron computation.

HelpSteer2-Preference: Complementing Ratings with Preferences
This paper releases preference annotations for the HelpSteer2 dataset to allow a direct comparison between Bradley-Terry and Regression style reward models, traditionally constrained by incompatible data formats. It introduces a novel combination of both paradigms, achieving state-of-the-art performance on RewardBench and Arena Hard, and openly provides the dataset and trained models.

High-Precision Dichotomous Image Segmentation via Probing Diffusion Capacity
DiffDIS is a diffusion-driven segmentation model utilizing pre-trained U-Nets from diffusion models for high-resolution, fine-grained object segmentation, achieving detailed generation and faster inference with a task-specific stable one-step denoising approach. Enhanced by an auxiliary edge generation task, DiffDIS excels in generating accurate, detailed binary maps quickly, as demonstrated by state-of-the-art results on the DIS5K dataset.

HiSplat: Hierarchical 3D Gaussian Splatting for Generalizable Sparse-View Reconstruction
HiSplat introduces a novel hierarchical framework in generalizable 3D Gaussian Splatting to improve 3D scene reconstruction from sparse input views by generating hierarchical 3D Gaussians using a coarse-to-fine strategy. This method enhances both large-scale structures and fine texture details, and outperforms previous single-scale approaches, demonstrating robust cross-dataset generalization and improved novel view synthesis with only two-view reference images.

How Gradient descent balances features: A dynamical analysis for two-layer neural networks
This paper explores the regression task of learning $k$ neurons from Gaussian input using two-layer ReLU neural networks with width $m$, trained via gradient descent, and provides a detailed analysis of its learning dynamics in three phases: alignment, tangential growth, and local convergence. The authors demonstrate global convergence at a rate of $\mathcal{O}(T^{-3})$ and reveal an implicit bias towards a minimum balanced $\ell_2$-norm, extending prior work by addressing teacher-student interactions and contributing to the understanding of neural network training dynamics and implicit biases.

How to Evaluate Reward Models for RLHF
We introduce Preference Proxy Evaluations (PPE), a novel benchmark for reward models linked explicitly to post-RLHF real-world human preference performance, aimed at predicting the downstream performance of language models without the high costs of a full RLHF training pipeline. By evaluating reward models on proxy tasks using large-scale human preference data and verifiable correctness metrics across various domains, we provide an open-source tool for improving language model development and evaluation.

Hymba: A Hybrid-head Architecture for Small Language Models
Hymba is a family of small language models utilizing a hybrid-head parallel architecture that combines attention mechanisms and state space models (SSMs) for complementary input processing. The models achieve state-of-the-art performance for small LMs with strategies like learnable meta tokens and cross-layer key-value sharing, resulting in significant reductions in cache size and increases in throughput, outperforming larger models such as Llama-3.2-3B.

I2VControl-Camera: Precise Video Camera Control with Adjustable Motion Strength
This paper introduces I2VControl-Camera, a novel camera control method that enhances controllability and precision in video generation by using point trajectory in the camera coordinate system and modeling higher-order video trajectory components. The proposed method demonstrates superior performance in both static and dynamic scenes, offering improved control over subject motion compared to previous approaches.

Improving Instruction-Following in Language Models through Activation Steering
This paper introduces a method for deriving instruction-specific vector representations from language models to steer their behavior, enhancing their adherence to constraints like format, length, and word inclusion. By computing activation differences with and without instructions, the approach allows for modular and scalable control over instruction following, demonstrating its effectiveness across models and the potential to improve base models through transferability of steering vectors.

Incremental Causal Effect for Time to Treatment Initialization
This paper explores the incremental causal effect of intervening on the timing of treatment initiation, commonly relevant in preventive medicine and non-fatal health conditions. The authors propose a novel estimation framework using inverse probability weighting, bypassing the usual positivity assumption, and demonstrate its application through simulations and a rheumatoid arthritis study.

Information Theoretic Text-to-Image Alignment
This paper presents a novel approach to aligning Text-to-Image (T2I) diffusion models with user intentions by using Mutual Information (MI) for model alignment instead of intricate linguistic analyses or human annotations. The proposed self-supervised fine-tuning method, which only requires the pre-trained T2I model's denoising network, shows superior alignment improvements without compromising image quality, offering a streamlined solution for enhancing model performance.

Integrative Decoding: Improving Factuality via Implicit Self-consistency
Integrative Decoding (ID) introduces a novel method for incorporating self-consistency in open-ended generation tasks by aggregating predictions from multiple inputs, each with a previously sampled response. This approach significantly enhances factual accuracy across various benchmarks, demonstrating substantial performance gains as the number of sampled responses increases.

Interpretable Compressed Descriptions For Image Generation
This paper addresses the challenge of controlling generative models by proposing a framework that uses succinct, informative, and interpretable data representations, grounded in information-theoretic principles, to access and manipulate semantic factors effectively. The efficacy of this framework is demonstrated through extensive experiments, contributing to advancements in the controllability and interpretability of generative processes. Code available at github.com/ArmandCom/InCoDe.

IPDreamer: Appearance-Controllable 3D Object Generation with Complex Image Prompts
IPDreamer is a novel method that enhances 3D object generation by capturing intricate appearance features from complex image prompts, allowing for high-fidelity, appearance-controllable synthesis. The approach successfully aligns generated 3D objects with both textual and image prompts, providing a significant advancement in creating detailed and predictable 3D models from complex images.

Iterative Dual-RL: An Optimal Discriminator Weighted Imitation Perspective for Reinforcement Learning
We introduce Iterative Dual Reinforcement Learning (IDRL), a method that iteratively approaches the optimal visitation distribution ratio without needing an additional expert dataset, and effectively beats existing Primal-RL and Dual-RL methods in both performance and stability. IDRL achieves this by using a correction method to update the dataset each iteration, thereby creating a curriculum that optimizes visitation distribution ratios towards the optimal discriminator weight, as demonstrated on various offline datasets.

Kolmogorov-Arnold Transformer
This paper introduces the Kolmogorov–Arnold Transformer (KAT), which replaces traditional MLP layers in transformers with Kolmogorov-Arnold Network (KAN) layers to boost model expressiveness and performance. The authors address challenges related to base function optimization, parameter efficiency, and weight initialization by proposing rational functions for faster computation, Group KAN for reduced computational load, and variance-preserving initialization, leading to superior performance in tasks such as image recognition, object detection, and semantic segmentation compared to standard transformers.

LARP: Tokenizing Videos with a Learned Autoregressive Generative Prior
LARP is a novel video tokenizer that improves over traditional methods by using a holistic approach to gather global and semantic representations, enabling adaptive and efficient tokenization for autoregressive generative models. By integrating a lightweight AR transformer and aligning the token space with downstream tasks, LARP achieves state-of-the-art performance on video generation benchmarks, enhancing the compatibility of AR models with videos and paving the way for unified multimodal large language models.

Latent Radiance Fields with 3D-aware 2D Representations
This paper introduces a novel framework for improving latent 3D reconstruction by integrating 3D awareness into the 2D latent space to overcome the domain gap between 2D features and 3D representations. The proposed approach includes a correspondence-aware autoencoding method, a latent radiance field, and a VAE-Radiance Field alignment strategy, collectively enhancing synthesis performance and cross-dataset generalizability, achieving photorealistic 3D reconstruction from 2D latent representations for the first time.

Latent Safety-Constrained Policy Approach for Safe Offline Reinforcement Learning
This paper proposes a novel method for safe offline reinforcement learning that uses Conditional Variational Autoencoders to model latent safety constraints, addressing the challenge of balancing performance with adherence to safety constraints. The approach, supported by theoretical analysis and empirical evaluations on benchmark datasets, effectively maximizes cumulative rewards while maintaining safety compliance, outperforming existing methods.

Learning-Augmented Search Data Structures
This paper investigates the integration of machine learning advice into skip lists and KD trees to enhance search query efficiency, addressing limitations found in binary search trees. The authors construct learning-augmented data structures that maintain optimal expected search time despite inaccuracies in oracle predictions, demonstrating both theoretical robustness and empirical performance improvements over traditional methods.

Learning Clustering-based Prototypes for Compositional Zero-Shot Learning
ClusPro is a clustering-based prototype mining framework developed for Compositional Zero-Shot Learning (CZSL) that defines the boundaries of primitive concepts through diversified prototypes, overcoming the limitations of oversimplified data assumptions. By employing prototype-based contrastive and decorrelation learning, ClusPro dynamically updates prototypes without additional computational costs, demonstrating superior performance over existing methods on multiple benchmarks in both closed-world and open-world settings.

Learning Graph Quantized Tokenizers
The paper introduces the Graph Quantized Tokenizer (GQT), which enhances Graph Transformers by decoupling tokenizer training from Transformer training through multi-task graph self-supervised learning, resulting in robust graph tokens. By employing Residual Vector Quantization (RVQ), GQT reduces memory needs and improves generalization, achieving state-of-the-art results on 20 out of 22 benchmarks and outperforming existing models in various graph learning tasks.

Learning Partial Graph Matching via Optimal Partial Transport
This paper introduces a novel optimization framework for partial graph matching, inspired by optimal partial transport, that balances matched and unmatched nodes using a weighted total variation divergence function for efficient, exact solutions. The approach establishes a connection between partial graph matching and the linear sum assignment problem, and it proposes a deep graph matching architecture with a new partial matching loss, demonstrating effectiveness through empirical evaluations on standard benchmarks.

Learning Successor Features with Distributed Hebbian Temporal Memory
This paper introduces the Distributed Hebbian Temporal Memory (DHTM) algorithm for effective online sequence learning in non-stationary, partially observable environments, leveraging a factor graph framework and neurophysiological inspirations. DHTM demonstrates superior performance over existing models like LSTM and HMM on non-stationary datasets, indicating its potential for improved decision-making and planning in dynamic settings.

Let Me Grok for You: Accelerating Grokking via Embedding Transfer from a Weaker Model
The paper introduces GrokTransfer, a novel method designed to eliminate delayed generalization in neural network training by leveraging data embeddings from a preliminary model to accelerate learning in the target model. Through both theoretical proof and empirical studies, GrokTransfer demonstrates its effectiveness in preventing delayed generalization, reshaping training dynamics across various tasks and neural network architectures.

Leveraging Driver Field-of-View for Multimodal Ego-Trajectory Prediction
This paper introduces RouteFormer, a novel multimodal network for predicting the ego-vehicle's trajectory by integrating the driver's gaze with GPS and environmental data, thereby addressing the often-overlooked factor of driver attention. The study, supported by the new GEM dataset and Path Complexity Index (PCI) metric, demonstrates that RouteFormer significantly enhances prediction accuracy, particularly in complex scenarios, by effectively modeling the driver's field-of-view and attention.

Leveraging Flatness to Improve Information-Theoretic Generalization Bounds for SGD
This paper presents a new information-theoretic generalization bound that better leverages the flatness bias of Stochastic Gradient Descent (SGD), indicating that models generalize better when large-variance directions in the weight covariance have small local curvatures in the loss landscape. Through the introduction of the "omniscient trajectory" technique, the study shows a numerically tighter bound for deep neural networks and improves IT bounds' rates in minimax excess risk scenarios, bypassing memorization-generalization trade-offs.

Lightweight Predictive 3D Gaussian Splats
This paper introduces a novel representation for 3D objects and scenes that significantly reduces storage requirements while maintaining or improving rendering quality compared to traditional 3D Gaussian splats. By utilizing a hierarchical tree structure and adaptive tree manipulation, the proposed method achieves up to 20x storage reduction, facilitating practical applications on resource-constrained devices such as mobile phones and AR glasses.

Linear combinations of latents in generative models: subspaces and beyond
This paper introduces Linear combinations of Latent variables (LOL), a general-purpose method for forming linear combinations of latent variables within generative models. LOL simplifies the creation of expressive low-dimensional representations, enabling more control in tasks like experimental design and creative applications, and is notable for its ease of implementation and adaptability across various models and data modalities.

LiveBench: A Challenging, Contamination-Limited LLM Benchmark
LiveBench is a new benchmark for large language models (LLMs) designed to prevent test set contamination and counter the biases of LLM and human-judging approaches by using frequently-updated, objectively-scored questions from recent sources. It covers a variety of challenging tasks across multiple domains, and aims to distinguish LLM capabilities more effectively over time, with top models currently achieving below 70% accuracy.

Locality Alignment Improves Vision-Language Models
This paper addresses the spatial reasoning limitations of vision language models (VLMs) by introducing a post-training stage called locality alignment for vision transformers (ViTs). By leveraging existing pre-trained models for scalable self-supervision, the proposed approach enhances the capture of local and global image semantics, improving performance in spatial understanding benchmarks without requiring new supervision.

LoLCATs: On Low-Rank Linearizing of Large Language Models
This paper introduces Low-rank Linear Conversion via Attention Transfer (LoLCATs), a novel method to improve the quality of linearized large language models (LLMs) by training linear attentions to approximate softmax attentions and adapting low-rank adjustments. LoLCATs achieves significant improvements in model quality and scalability for subquadratic LLMs, enabling the creation of the first linearized 70B and 405B LLMs, while using substantially fewer model parameters and training tokens compared to existing methods.

Long-Context Linear System Identification
This paper explores long-context linear system identification, showing that learning systems with longer context windows can achieve a sample complexity bound comparable to _i.i.d._ parametric rates, despite the absence of rapid mixing. It extends previous findings by revealing that long-context autoregressive models are not limited by slow mixing and demonstrates improvements in estimation rates with low-rank representations and benefits of shorter contexts in stable systems with misspecified context lengths.

LoRA-Pro: Are Low-Rank Adapters Properly Optimized?
This paper introduces LoRA-Pro, a method that enhances the performance of Low-rank adaptation (LoRA) by strategically adjusting low-rank matrix gradients to closely approximate full fine-tuning gradients. The authors demonstrate through extensive experiments across diverse tasks that LoRA-Pro significantly improves LoRA's performance, reducing the disparity with full fine-tuning methods.

LoR-VP: Low-Rank Visual Prompting for Efficient Vision Model Adaptation
This study proposes a novel visual prompt design called LoR-VP, which utilizes low-rank matrix multiplication to improve interaction between visual prompts and image patches, addressing limitations of existing techniques. Experiments across multiple architectures and datasets show LoR-VP significantly enhances performance and efficiency, achieving faster training, fewer parameters, and better accuracy compared to current methods.

MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference Optimization
As large language models (LLMs) advance, aligning them with human values is crucial, especially in situations where they outperform humans, posing a weak-to-strong alignment challenge. The proposed multi-agent contrastive preference optimization (MACPO) framework addresses this by promoting mutual learning between weak teachers and strong students, improving alignment performance on the HH-RLHF and PKU-SafeRLHF datasets, with enhanced outcomes as more weak teachers and iterations are added.

MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba
This paper explores parameter-efficient fine-tuning (PEFT) methods for Mamba, a State Space Model-based alternative to Transformers, focusing on adapting pre-trained models to downstream tasks. The authors modify existing PEFT methods and propose new Mamba-specific techniques, demonstrating that PEFT is more effective for Mamba than for Transformers, and provide a framework that surpasses previous approaches, with code to be released post-publication.

MamBEV: Enabling State Space Models to Learn Birds-Eye-View Representations
This paper introduces MamBEV, a Mamba-based framework for 3D visual perception tasks in autonomous driving, which utilizes linear spatio-temporal SSM-based attention to learn unified Bird's Eye View (BEV) representations. MamBEV significantly enhances computational and memory efficiency while delivering superior performance across various metrics, outperforming existing models in input scaling efficiency.

Mask in the Mirror: Implicit Sparsification
Continuous sparsification strategies are effective for reducing inference costs and memory demands of neural networks due to an implicit $L_1$ regularization that arises from learning mask and weight variables together. This paper provides a theoretical foundation for this phenomenon, proposes a method to control the implicit bias dynamically, introduces the PILoT approach, and demonstrates its superior performance compared to traditional baselines.

Mentored Learning: Improving Generalization and Convergence of Student Learner
The paper introduces "Mentored Learning," a paradigm where learning is guided by monitoring disagreement between a learner and a teacher function, leading to easier convergence analysis and improved generalization bounds compared to traditional active learning. By implementing an optimal teaching hypothesis, the approach reduces the generalization error and label complexity upper bounds, with experimental results showing enhanced performance over existing active learning strategies.

metabench - A Sparse Benchmark of Reasoning and Knowledge in Large Language Models
This study analyzes data from over 5000 Large Language Models (LLMs) to distill a sparse benchmark called metabench from six existing benchmarks, significantly reducing the number of items while retaining essential information. The new sparse benchmark can effectively estimate original benchmark scores with minimal error, revealing a strong common underlying factor and enhancing the efficiency of evaluating LLM abilities.

MetaOOD: Automatic Selection of OOD Detection Models
The paper introduces MetaOOD, an innovative zero-shot, unsupervised framework that automatically selects an optimal out-of-distribution (OOD) detection model using meta-learning. By utilizing historical performance data and language model-based embeddings to assess task similarities, MetaOOD outperforms existing methods in selecting suitable models for diverse tasks without requiring labeled data, as demonstrated through extensive testing and validation.

MindSimulator: Exploring Brain Concept Localization via Synthetic fMRI
The paper introduces a data-driven approach utilizing the MindSimulator, which employs advanced generative technologies to statistically localize concept-selective regions in the brain by synthesizing extensive brain activity recordings. This method overcomes limitations of traditional experiment-driven approaches by providing simulated brain recordings that accurately reflect neural responses, allowing for the validation and exploration of concept-selective regions, thereby offering new insights for future neuroscience research.

Minimax Optimal Two-Stage Algorithm For Moment Estimation Under Covariate Shift
This paper addresses the problem of covariate shift, specifically focusing on the minimax lower bound for estimating an unknown function's moment when source and target distributions are known. The authors propose a two-stage algorithm with a truncated estimator, demonstrating double robustness and effectiveness through theoretical findings and numerical studies, overcoming the instability of likelihood ratio estimation.

(Mis)Fitting Scaling Laws: A Survey of Scaling Law Fitting Techniques in Deep Learning
This paper examines the reliance on scaling laws for making training decisions in modern foundation models and highlights inconsistencies in conclusions due to variations in factors such as equations, training setups, and optimization methods. It presents a survey of over 50 studies, identifies issues in reproducibility due to under-reporting of essential details, and proposes a checklist to improve the rigor and clarity of scaling law research.

Model Equality Testing: Which Model is this API Serving?
This paper introduces Model Equality Testing, a method for detecting output distribution changes in large language models accessed via APIs, potentially altered through quantization, watermarking, or finetuning. By utilizing Maximum Mean Discrepancy tests with a string kernel, the study demonstrates high effectiveness in identifying such distortions, revealing that 11 out of 31 tested commercial endpoints differ from the reference models provided by Meta.

Modeling Future Conversation Turns to Teach LLMs to Ask Clarifying Questions
This paper introduces a new preference labeling method for large language models (LLMs) that emphasizes the importance of asking clarifying questions in response to ambiguous user requests, rather than presupposing a single interpretation. The proposed method, which simulates outcomes in future conversation turns, improves LLM performance on open-domain QA tasks by 5% in F1 score and enhances the accuracy of appropriately asking clarifying questions and direct responses by 3% compared to standard methods.

MOS: Model Synergy for Test-Time Adaptation on LiDAR-Based 3D Object Detection
This paper introduces a novel online test-time adaptation framework for LiDAR-based 3D object detection that addresses domain shifts, particularly those caused by sensor variations and weather conditions. The proposed Model Synergy (MOS) strategy effectively improves adaptability to diverse shifts by dynamically selecting and assembling historical checkpoints, demonstrating a significant 67.3% improvement in cross-corruption scenarios over existing methods.

Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning
This paper presents MPC$^2$, a hierarchical model-based learning algorithm that effectively controls high-dimensional, nonlinear systems in zero-shot and near-real-time scenarios. By leveraging a model predictive controller and a morphology-aware proportional controller, MPC$^2$ facilitates robust motion control for complex dynamical systems, such as human musculoskeletal models, while reducing the dependency on manual reward engineering through black-box optimization.

Multi-LLM-Agents Debate - Performance, Efficiency, and Scaling Challenges
The paper explores Multi-Agent Debate (MAD) frameworks, where multiple large language model agents collaborate to enhance test-time performance without further training. Evaluating five MAD frameworks across nine benchmarks, the study finds that these methods often do not surpass simpler single-agent strategies, indicating that current MAD designs do not effectively leverage additional computation during inference.

Multimodal Lego: Model Merging and Fine-Tuning Across Topologies and Modalities in Biomedicine
This paper introduces Multimodal Lego (MM-Lego), a flexible fusion framework that transforms any set of unimodal encoders into a competitive multimodal model with minimal fine-tuning. MM-Lego achieves competitive performance with end-to-end models and surpasses benchmarks across several datasets by harmonizing modality representations in the frequency domain, addressing limitations of current multimodal approaches.

Multimodal Unsupervised Domain Generalization by Retrieving Across the Modality Gap
This paper addresses the multimodal unsupervised domain generalization (MUDG) problem, utilizing a large task-agnostic unlabeled source dataset during finetuning without explicit relationships with the target task. The authors propose a framework that includes a new paired k-means clustering algorithm, adaptive text augmentation for target labels, and enhancements for downstream target accuracy, demonstrating improved accuracy over state-of-the-art methods on 20 diverse datasets.

Multi-objective Differentiable Neural Architecture Search
This paper introduces a novel neural architecture search (NAS) algorithm that incorporates user preferences to effectively trade-off performance and hardware metrics, generating diverse architectures across multiple devices with a single search run. By leveraging a hypernetwork conditioned on hardware features and preference vectors, the method provides zero-shot transferability and outperforms existing multi-objective optimization NAS approaches across various search spaces and datasets without additional costs.

Narrowing Information Bottleneck Theory for Multimodal Image-Text Representations Interpretability
This paper introduces the Narrowing Information Bottleneck Theory, a new framework that enhances the interpretability of multimodal models like CLIP by redefining traditional bottleneck approaches to satisfy contemporary attribution axioms. The proposed method improves image interpretability by 9%, text interpretability by 58.83%, and accelerates processing speed by 63.95%, outperforming state-of-the-art techniques.

NEAR: A Training-Free Pre-Estimator of Machine Learning Model Performance
This paper introduces the zero-cost proxy Network Expressivity by Activation Rank (NEAR) for Neural Architecture Search, which identifies optimal neural networks without the need for training. NEAR effectively correlates network expressivity with model accuracy and assists in selecting hyperparameters, such as layer sizes, activation functions, and weight initialization schemes.

NeurFlow: Interpreting Neural Networks through Neuron Groups and Functional Interactions
This paper introduces NeurFlow, a novel framework that enhances the interpretability of neural networks by focusing on functional interactions between neuron groups rather than individual neurons. By identifying and clustering core neurons into groups, NeurFlow constructs a hierarchical circuit that improves network interpretability and is validated through empirical studies, demonstrating its practicality in applications like image debugging and automatic concept labeling.

New Algorithms for the Learning-Augmented k-means Problem
This paper proposes new algorithms for clustering in a learning-augmented setting, leveraging oracle-predicted labels to enhance clustering accuracy. By utilizing sampling-based strategies instead of traditional sorting methods, the proposed algorithms achieve a (1+O(α))-approximation efficiently with linear runtime, showing improved scalability and comparable clustering quality in empirical tests compared to existing solutions.

N-ForGOT: Towards Not-forgetting and Generalization of Open Temporal Graph Learning
This paper addresses the open-set problem in Temporal Graph Neural Networks (TGNNs), which struggle with adapting to new class labels and preserving learned knowledge over time. The proposed N-ForGOT method enhances TGNNs by implementing two modules that retain inter-class connectivity and efficiently measure distribution discrepancies, significantly improving prediction accuracy, memory retention, and adaptability to new classes, as demonstrated on four datasets.

Not-So-Optimal Transport Flows for 3D Point Cloud Generation
This paper addresses the challenges of learning permutation-invariant generative models for 3D point clouds, highlighting the limitations of equivariant OT flows in scalability and complexity. The authors propose a more efficient approach using not-so-optimal transport flows with precomputed OT and hybrid coupling, which demonstrates superior performance in unconditional generation and shape completion tasks on the ShapeNet benchmark compared to existing methods.

NovelQA: Benchmarking Question Answering on Documents Exceeding 200K Tokens
This paper introduces NovelQA, a specialized benchmark designed to evaluate large language models (LLMs) on their ability to understand complex and extended narratives, using English novels as the source material. The findings highlight significant challenges LLMs face with multi-hop reasoning and detail-oriented questions when handling extremely long inputs, underscoring the necessity for further advancements in the models to improve their long-context comprehension capabilities.

Online-to-Offline RL for Agent Alignment
This paper addresses the challenge of adapting online-trained reinforcement learning policies to align with human preferences, which is often hindered by the limited availability of human behavior data. The authors propose ALIGN-GAP, an approach utilizing a reward model and curriculum-based preference learning, achieving effective alignment of game AI agents with human preferences demonstrated through experiments in various environments.

On Targeted Manipulation and Deception when Optimizing LLMs for User Feedback
This study examines the risks of training large language models (LLMs) to maximize human feedback, revealing that such approaches can lead to manipulative behaviors as models exploit vulnerable users to obtain positive feedback. The findings caution against using gameable feedback sources like user feedback for reinforcement learning, as mitigation strategies can inadvertently result in subtler manipulative tactics.

On the Adversarial Risk of Test Time Adaptation: An Investigation into Realistic Test-Time Data Poisoning
This paper examines the vulnerabilities of test-time adaptation (TTA) when faced with adversarial risks from test-time poisoned data, introducing more realistic assumptions for these scenarios. It proposes a novel attack method that effectively generates poisoned samples without benign data and demonstrates that TTA methods are more robust than previously thought, while also exploring defense strategies to enhance their robustness.

OpenMathInstruct-2: Accelerating AI for Math with Massive Open-Source Instruction Data
This paper addresses the challenge of mathematical reasoning in large language models (LLMs) by creating the OpenMathInstruct-2 dataset, facilitating significant improvements in fine-tuning effectiveness. The study highlights the importance of solution format, teacher-generated data quality, and question diversity, ultimately leading to a substantial 15.9% performance increase in mathematical reasoning tasks when fine-tuning Llama-3.1-8B models, with resources released under an open-source license for broader research access.

Open-YOLO 3D: Towards Fast and Accurate Open-Vocabulary 3D Instance Segmentation
The paper introduces Open-YOLO 3D, a novel open-vocabulary 3D instance segmentation approach that improves inference speed and reduces computational demands by leveraging 2D object detection from multi-view RGB images instead of relying on costly 2D foundation models. Using the Multi-View Prompt Distribution (MVPDist) method and low granularity label maps, it achieves state-of-the-art performance on the ScanNet200 and Replica benchmarks, offering up to 16 times speedup compared to existing methods.

Optimal Flow Transport and its Entropic Regularization: a GPU-friendly Matrix Iterative Algorithm for Flow Balance Satisfaction
This paper introduces Optimal Flow Transport (OFT) as an extension of the Sinkhorn algorithm by considering general graph settings with flow balance constraints, using entropic regularization and virtual flows for isolated nodes. The proposed OFT-Sinkhorn and Capacity-Constrained EOFT-Sinkhorn algorithms are shown to efficiently calculate flow solutions with GPU-friendly iterations, enhancing computational efficiency compared to traditional Minimum Cost Flow (MCF) algorithms across various problem scales.

Optimization with Access to Auxiliary Information
We introduce two novel algorithms to optimize a target function $f(x)$ with limited gradient availability by leveraging a more accessible auxiliary side function $h(x)$, relevant in settings like SGD, transfer learning, and federated learning. Our analysis shows that these algorithms provide advantages when there's a small Hessian similarity between the functions, and also potentially benefit from stochasticity if the auxiliary noise correlates with the target function's noise.

Param$\Delta$ for Direct Mixing: Post-Train Large Language Model At Zero Cost
This paper introduces Param$\Delta$, a novel approach that transfers knowledge from an existing post-trained model to a newly updated base model, effectively eliminating the need for additional post-training. By achieving a 98% transfer efficiency, Param$\Delta$ offers a cost-effective and sustainable solution for iterative model development, enhancing the use of models in the open-weight community.

Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models
This study explores whether small language models can effectively prune large-scale text datasets to enhance the performance of larger models, demonstrating that using a 125 million parameter model for perplexity-based pruning improves the average performance of a 3 billion parameter model on downstream tasks. The findings show that this method not only boosts performance by up to 2.04 but also reduces pretraining steps significantly, with benefits persisting even in over-trained and data-constrained scenarios.

PINP: Physics-Informed Neural Predictor with latent estimation of fluid flows
This paper introduces a novel physics-informed learning approach that enhances the prediction of fluid dynamics by integrating discretized physical equations into the model architecture and loss function. The method demonstrates superior temporal extrapolation and spatial generalization capabilities, achieving state-of-the-art performance in spatiotemporal prediction tasks on both numerical simulations and real-world data.

Planning Anything with Rigor: General-Purpose Zero-Shot Planning with LLM-based Formalized Programming
This paper introduces LLMFP, a general-purpose framework leveraging large language models (LLMs) to tackle complex planning problems by framing them as optimization tasks. Applying LLMFP to various planning problems shows significant improvement in optimal rates, outperforming existing baselines, without requiring task-specific examples, demonstrating its cross-task generalization capability.

Positive-Unlabeled Diffusion Models for Preventing Sensitive Data Generation
This paper introduces positive-unlabeled diffusion models to prevent the generation of sensitive data by leveraging a small set of labeled sensitive data alongside unlabeled training data. The approach approximates the evidence lower bound (ELBO) for normal data using only unlabeled and sensitive data, effectively generating only normal data without labeled normal data, as demonstrated through experiments across various datasets without compromising image quality.

Proximal Mapping Loss: Understanding Loss Functions in Crowd Counting & Localization
This paper introduces Proximal Mapping Loss (PML), a novel density regression method for crowd counting and localization that addresses the inconsistency of traditional counting methods by eliminating the "intersection" hypothesis. PML enhances accuracy and robustness by dynamically constructing learning targets through proximal mapping, is theoretically versatile across various existing loss functions, and demonstrates improved performance and resilience to annotation noise in experiments.

Pushing the Limits of All-Atom Geometric Graph Neural Networks: Pre-Training, Scaling, and Zero-Shot Transfer
This paper investigates the use of pre-trained geometric graph neural networks (Geom-GNNs) as zero-shot transfer learners to effectively model protein systems at all-atom granularity, addressing existing computational constraints. The study reveals unique scaling behaviors of Geom-GNNs for molecular representation learning, differing from conventional power-law scaling, and demonstrates how pre-trained graph embeddings can enhance protein modeling by synergizing with other architectures.

QA-Calibration of Language Model Confidence Scores
This paper introduces QA-calibration, a generalized calibration method for generative question-and-answering (QA) systems that ensures calibration across various question-and-answer groups rather than on average. The authors propose discretized posthoc calibration schemes and demonstrate their effectiveness with distribution-free guarantees on multiple QA benchmarks and large language models.

Quantum (Inspired)  $D^2$-sampling with Applications
This paper presents a quantum algorithm for $D^2$-sampling in the QRAM model, enabling a quantum implementation of $k$-means++ with a running time of $\tilde{O}(\zeta^2 k^2)$ and preserving the $O(\log{k})$ approximation guarantee. Additionally, the algorithm can be dequantized to produce a classical implementation, named QI-$k$-means++, which demonstrates promising results on large datasets and offers a quantum-inspired approach with practical applications in clustering tasks.

Real-Time Video Generation with Pyramid Attention Broadcast
Pyramid Attention Broadcast (PAB) is a novel, training-free method for DiT-based video generation that achieves real-time, high-quality output by leveraging a U-shaped pattern in attention redundancy. The approach, which introduces pyramid-style broadcasting and sequence parallelism for efficiency, significantly accelerates generation speeds by up to 10.5x compared to baselines, offering a promising foundation for future video generation research and applications.

REBIND: Enhancing Ground-state Molecular Conformation Prediction via Force-Based Graph Rewiring
ReBIND is a novel framework designed to improve the prediction of ground-state 3D molecular conformations from 2D molecular graphs by enhancing the modeling of inter-atomic forces, particularly for atoms with low coordination numbers affected by non-bonded interactions. By rewiring molecular graphs with edges informed by the Lennard-Jones potential, ReBIND achieves up to a 20% reduction in prediction error compared to state-of-the-art deep learning methods, providing a significant enhancement in computational chemistry predictions.

RECAST: Reparameterized, Compact weight Adaptation for Sequential Tasks
The paper introduces RECAST, a novel method for incremental learning that significantly reduces task-specific trainable parameters to under 50, making it highly efficient for resource-constrained environments. By utilizing shared weight templates and module-specific coefficients, along with a novel weight reconstruction pipeline called Neural Mimicry, RECAST enhances adaptability without extensive pretraining, outperforming state-of-the-art methods across diverse datasets and architectures.

Regularizing Energy among Training Samples for Out-of-Distribution Generalization
This paper highlights the importance of considering energy differences among in-distribution data samples in energy-based models, enhancing their application beyond out-of-distribution (OOD) scenarios. By introducing a unified framework that applies implicit energy regularization across subpopulation shift and OOD generalization scenarios, the study demonstrates its effectiveness through experiments on long-tail datasets and benchmarks.

REMEDY: Recipe Merging Dynamics in Large Vision-Language Models
Model merging is a powerful technique for unifying task-specific vision models, but large vision-language models (LVLMs) present challenges due to their scale and multi-modal nature. The REMEDY method addresses these challenges by introducing reusable modules called "recipes" and a modality-aware allocator that dynamically generates weights, leading to improved performance in both multi-task learning and zero-shot generalization scenarios.

RESuM: A Rare Event Surrogate Model for  Physics Detector Design
This paper addresses the challenge of optimizing detector designs for neutrinoless double-beta decay experiments to reduce background contamination, which is crucial for understanding the matter-antimatter imbalance in the universe. The authors introduce the Rare Event Surrogate Model (RESuM), which leverages a Conditional Neural Process in a Multi-Fidelity Gaussian Process framework, demonstrating significant computational efficiency and improved performance in reducing neutron backgrounds compared to traditional methods, with implications for broader applications in rare-event searches.

Retri3D: 3D Neural Graphics Representation Retrieval
Learnable 3D Neural Graphics Representations (3DNGR) offer promising capabilities for reconstructing 3D scenes from 2D images, yet existing systems lack a framework for accurate retrieval of these models from large data stores. Retri3D is introduced as a solution, providing a novel retrieval framework that uses Neural Field Artifact Analysis and a Smart Camera Movement Module to efficiently retrieve 3D scenes based on text queries, demonstrating compatibility with any NGR and significant improvements in retrieval accuracy and speed on datasets like LERF and ScanNet++.

Retrieval Head Mechanistically Explains Long-Context Factuality
This paper investigates how transformer-based models retrieve relevant information from long contexts and identifies a specific type of attention heads, called retrieval heads, as central to this capability. The study reveals that retrieval heads are universal across models, sparse, intrinsically present in pretrained models, dynamically activated, and crucial for preventing information retrieval issues and hallucinations, also significantly impacting the model's reasoning abilities and performance in different tasks.

Revisiting a Design Choice in Gradient Temporal Difference Learning
This paper revisits the intermediate algorithm $A^\top$TD, initially deemed inferior during the development of Gradient Temporal Difference learning (GTD), and introduces its variant $A_t^\top$TD as a solution to the deadly triad in reinforcement learning. $A_t^\top$TD requires only one set of parameters and one learning rate, simplifying tuning compared to GTD, and the paper provides both asymptotic and finite sample analysis to demonstrate its effectiveness.

REVISITING MULTI-PERMUTATION EQUIVARIANCE THROUGH THE LENS OF IRREDUCIBLE REPRESENTATIONS
This paper develops an alternative methodology based on irreducible representations and Schur’s lemma to characterize equivariant linear layers for permutations and related groups, providing a simpler derivation for models such as Deep Weight Space (DWS) networks. By extending this approach to unaligned symmetric sets, the authors fully characterize layers requiring equivariance to the wreath product of groups, revealing numerous non-Siamese layers that enhance performance in tasks like graph anomaly detection and learning Wasserstein distances.

RNNs are not Transformers (Yet):  The Key Bottleneck on In-Context Retrieval
This paper explores the representational differences between Transformers and Recurrent Neural Networks (RNNs), highlighting that RNNs, even with Chain-of-Thought (CoT) prompting, cannot match the information retrieval abilities of Transformers. However, by incorporating techniques like Retrieval-Augmented Generation (RAG) and a single Transformer layer, RNNs' in-context retrieval capabilities can be enhanced, enabling them to solve all polynomial-time tasks comparable to Transformers.

Robust Transfer of Safety-Constrained Reinforcement Learning Agents
This paper addresses the challenge of transferring reinforcement learning agents from controlled environments to real-world applications where safety dynamics may differ. The proposed methodology strengthens agents in controlled settings and provides a provably safe transfer to new environments, ensuring robust safety even when dynamics shift, as demonstrated empirically.

RTDiff: Reverse Trajectory Synthesis via Diffusion for Offline Reinforcement Learning
RTDiff introduces a novel diffusion-based data augmentation technique for offline reinforcement learning that generates trajectories in reverse, moving from unknown to known states, to naturally mitigate overestimation and improve reliability. This approach, combined with enhanced trajectory length control and efficient noise management, significantly boosts the performance of state-of-the-art offline RL algorithms by effectively overcoming distribution shift.

Safety-Prioritizing Curricula for Constrained Reinforcement Learning
This paper introduces Safe Curriculum Generation (SCG), a novel approach that aligns curriculum learning with constrained reinforcement learning to improve safety and sample efficiency during training. The approach generates task sequences prioritizing minimal safety violations, and empirical results show SCG achieves optimal performance with fewer constraint violations compared to other curriculum learning methods.

SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations
SafeWatch is an efficient MLLM-based video guardrail model that provides multi-label outputs with content-specific explanations in a zero-shot manner, addressing the limitations of current video guardrails that are either simplistic or inefficient. By introducing a policy-aware visual token pruning algorithm and parallel encoding of safety policies, SafeWatch significantly improves efficiency, accuracy, and generalizability, outperforming state-of-the-art models by 28.2% on the SafeWatch-Bench and by 13.6% on existing benchmarks while reducing inference costs by 10%.

SAM 2: Segment Anything in Images and Videos
The Segment Anything Model 2 (SAM 2) introduces a transformer-based approach for promptable visual segmentation in images and videos, utilizing a user-interactive data engine to amass the largest video segmentation dataset. This model demonstrates significant improvements in accuracy and efficiency, offering enhanced performance with fewer interactions in video segmentation and faster image segmentation compared to its predecessor, SAM, and is made available alongside its dataset, demo, and code for further research and development.

Scaling and evaluating sparse autoencoders
Sparse autoencoders, specifically k-sparse autoencoders, effectively extract interpretable features from language models while balancing reconstruction and sparsity, reducing issues like dead latents. By proposing modifications that support scalability and providing new metrics for feature quality evaluation, this paper demonstrates significant improvements in autoencoder performance, exemplified by training a large-scale autoencoder on GPT-4 activations for broad applications.

Self-Improving Robust Preference Optimization
The paper introduces Self-Improving Robust Preference Optimization (SRPO), an advanced offline RLHF framework designed to overcome the limitations of existing methods like PPO and DPO, particularly their inability to self-correct errors at inference and generalize across tasks. SRPO formulates learning from human preferences as a self-improvement process using a min-max objective, which is task-independent and efficiently optimized through supervised learning, resulting in significant performance improvements in AI Win-Rate against human completions on the XSum dataset and challenging prompts.

Sensor-Invariant Tactile Representation
This paper introduces a novel method for extracting Sensor-Invariant Tactile Representations (SITR) to facilitate zero-shot transfer across different optical tactile sensors. Using a transformer-based architecture trained on a diverse dataset, the approach allows for effective model transferability with minimal calibration, overcoming challenges posed by sensor design and manufacturing variations.

Separation Power of Equivariant Neural Networks
This paper analyzes the separation power of equivariant neural networks, like convolutional and permutation-invariant networks, and provides a comprehensive characterization of inputs that these architectures cannot distinguish. It concludes that while non-polynomial activations all have equivalent expressivity, depth enhances separation power only to a certain level, and architectural choices like representation types and block decomposition of hidden layers significantly affect separability.

Shapley-Guided Utility Learning for Effective Graph Inference Data Valuation
Graph Neural Networks (GNNs) struggle with evaluating the importance of testing nodes' neighbors due to the lack of test labels. The proposed Shapley-Guided Utility Learning (SGUL) framework addresses this by leveraging Shapley values to estimate test accuracy without ground truth labels, enhancing generalization and computational efficiency while outperforming existing methods on various graph datasets.

Simple, Good, Fast: Self-Supervised World Models Free of Baggage
This paper presents SGF, a Simple, Good, and Fast world model that utilizes self-supervised representation learning and frame-action stacking to avoid traditional RNNs and transformers while maintaining robust performance. The model's effectiveness is demonstrated through extensive ablation studies and quantitative comparisons on the Atari 100k benchmark, with code accessible at the provided GitHub link.

Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation
SubgraphRAG enhances Knowledge Graph-based Retrieval-Augmented Generation by optimizing retrieval effectiveness and efficiency through a novel approach that incorporates a lightweight multilayer perceptron and a parallel triple-scoring mechanism. This method allows flexible subgraph retrieval and achieves competitive results with smaller language models and state-of-the-art accuracy with larger ones, all without the need for fine-tuning, as demonstrated in evaluations on the WebQSP and CWQ benchmarks.

Simulating Training Dynamics to Reconstruct Training Data from Deep Neural Networks
This paper addresses the challenge of determining whether deep neural networks (DNNs) memorize training data by proposing a novel method called Simulation of training Dynamics (SimuDy). SimuDy effectively reconstructs training data from DNNs by simulating and optimizing training dynamics, significantly outperforming previous methods in handling non-linear dynamics and successfully reconstructing most training samples from a trained ResNet's parameters.

Sketch2Diagram: Generating Vector Diagrams from Hand-Drawn Sketches
We introduce SketikZ, a dataset of 3,231 pairs of hand-drawn sketches and their corresponding TikZ codes and reference diagrams, to tackle the largely unexplored challenge of converting sketches into high-quality vector diagrams. Additionally, we present ImgTikZ, an image-to-TikZ model that performs comparably to GPT-4o, showcasing significant improvements in sketch-to-diagram conversion and setting the stage for future research in image-to-code generation tasks.

SleepSMC: Ubiquitous Sleep Staging via Supervised Multimodal Coordination
SleepSMC is a novel framework designed to improve sleep staging accuracy in scenarios where only unimodal data is available by utilizing a method called supervised multimodal coordination. The framework achieves state-of-the-art cross-subject performance on multiple datasets and enhances the reliability of unimodal sleep monitoring through the use of adaptive weights and uncertainty estimates.

SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation
The paper introduces SlowFast-VGen, a dual-speed learning system for generating action-driven long videos that bridges slow learning of general world dynamics with fast storage of episodic memory. By employing a masked conditional video diffusion model and an inference-time fast learning strategy, the system outperforms existing models, notably achieving better FVD scores and improved consistency in video generation across various metrics, and enhances performance in long-horizon planning tasks.

Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling
This study examines whether using high-quality synthetic data from stronger language models is the most compute-optimal strategy for improving language model reasoning abilities under a fixed inference budget. The researchers find that data generated by weaker but cheaper models lead to better performance in finetuning across various benchmarks, challenging the convention of relying on stronger models for synthetic data generation.

Small Models are LLM Knowledge Triggers for Medical Tabular Prediction
This paper introduces SERSAL, a self-prompting synergy learning method that combines large language models (LLMs) with smaller models to enhance LLM performance on structured tabular data prediction in an unsupervised manner. By iteratively using soft noisy annotations from LLMs and feedback from small models, SERSAL significantly improves tabular data prediction without gold labels, as demonstrated in medical domain datasets, and highlights a new direction for utilizing LLMs in tabular learning tasks.

Small-to-Large Generalization: Training Data Influences Models Consistently Across Scale
The paper examines how training data distribution impacts model behavior, specifically comparing the effects on small and large-scale models. It finds a high correlation in prediction behavior across scales and explores how this understanding can improve data attribution and dataset selection in proxy model applications.

Solving New Tasks by Adapting Internet Video Knowledge
This paper explores adaptation techniques for video generative models, integrating in-domain robotic behavior data with large-scale pretrained video models to enhance text-conditioned generalization for novel robotic tasks. The authors introduce a novel strategy, Inverse Probabilistic Adaptation, which achieves robust generalization across various robotic environments, even when only suboptimal demonstration data is available.

SPaR: Self-Play with Tree-Search Refinement to Improve Instruction-Following in Large Language Models
The paper introduces SPaR, a self-play framework utilizing tree-search self-refinement, to create valid and comparable preference pairs, thereby enhancing language models’ instruction-following capabilities by reducing irrelevant content variations. Experiments show that using SPaR, the LLaMA3-8B model outperformed GPT-4-Turbo on the IFEval benchmark while maintaining general capabilities, with promising scalability improvements observed in larger models like GLM-4-9B and LLaMA3-70B.

Spatial-Mamba: Effective Visual State Space Models via Structure-Aware State Fusion
Spatial-Mamba is introduced as a novel selective state space model designed to address the challenges of applying SSMs to 2D vision tasks by directly establishing neighborhood connectivity in the state space. By leveraging dilated convolutions and a structure-aware state fusion equation, Spatial-Mamba efficiently captures spatial dependencies, enhancing visual contextual information flow, leading to improved performance in image classification, detection, and segmentation over existing SSM-based models.

SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models
The paper introduces SPORTU, a benchmark designed to evaluate the reasoning abilities of Multimodal Large Language Models (MLLMs) in sports contexts through two components: SPORTU-text for text-based rules and strategy understanding, and SPORTU-video for assessing reasoning in video-based sports scenarios. The study finds that while models like GPT-4o show potential, with maximum accuracies of 71% on text and 57.8% on video, they still lag behind human performance, underscoring the need for improvement in complex sports reasoning tasks.

SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks
The paper introduces SRSA (Skill Retrieval and Skill Adaptation), a framework designed to enable robots to efficiently learn contact-rich assembly tasks by leveraging a pre-existing skill library. By predicting transfer success for skills and focusing on zero-shot success rates, SRSA outperforms existing methods, with significant improvements in success rate, lower variance, and fewer required samples, while also achieving a 90% mean success rate in real-world deployments.

Standardizing Structural Causal Models
This paper introduces internally-standardized structural causal models (iSCMs), which modify traditional SCMs by standardizing each variable during the generative process to avoid increasing variances and pairwise correlations along the causal ordering. iSCMs mitigate artifacts exploited by causal learning algorithms and, unlike standard SCMs, retain non-deterministic relationships in large systems, offering a more robust model for causal inference.

STAR: Synthesis of Tailored Architectures
This paper introduces STAR, a novel approach for synthesizing tailored architectures through a new search space informed by the theory of linear input-varying systems, which allows for hierarchical numerical encoding into architecture genomes. STAR utilizes gradient-free evolutionary algorithms to optimize architectures across multiple metrics, demonstrating enhancements over optimized Transformers and hybrid models in language modeling tasks.

Straight to Zero: Why Linearly Decaying the Learning Rate to Zero Works Best for LLMs
This study demonstrates that a linear decay-to-zero (D2Z) learning rate schedule consistently outperforms traditional cosine decay schedules across various model and dataset sizes, leading to significant compute savings. By framing AdamW as an exponential moving average, the paper highlights how D2Z effectively balances early and late training demands, achieving as much as 60% savings in computational resources without sacrificing performance.

Streaming Video Question-Answering with In-context Video KV-Cache Retrieval
ReKV is a novel, training-free approach for streaming video question-answering (StreamingVQA) that integrates with existing Video Large Language Models to provide efficient, prompt responses to user queries on long videos. By incorporating a sliding-window attention mechanism and storing processed video key-value caches efficiently, ReKV allows for real-time analysis and question-answering across different processes and GPUs, significantly improving the efficiency and applicability of VideoQA models.

Synergy and Diversity in CLIP: Enhancing Performance Through Adaptive Backbone Ensembling
This paper investigates the differences in representations, classification performance, and robustness properties among various CLIP-trained vision architectures, such as vision transformers and convolutional networks. By exploiting the unique strengths of each backbone, the authors develop an adaptive ensemble method that significantly enhances classification accuracy, obtaining up to a 39.1% increase over the best single backbone on multiple datasets using minimal labeled data.

Targeted Attack Improves Protection against Unauthorized Diffusion Customization
This paper introduces a novel method using targeted attacks to enhance protection against unauthorized diffusion customization by significantly outperforming untargeted attacks in degrading image quality and poisoning diffusion models. Experiments demonstrate the effectiveness of this method across mainstream diffusion model customization techniques, and the paper provides insights into the mechanics of attack-based protections, revealing diffusion models' vulnerabilities to targeted attacks for the first time.

Test-Time Ensemble via Linear Mode Connectivity: A Path to Better Adaptation
This paper introduces Test-Time Ensemble (TTE), a method to enhance test-time adaptation by leveraging ensemble strategies like adaptive parameter averaging and dropout to improve model representation diversity. By integrating seamlessly with existing adaptation approaches, TTE demonstrates superior performance across challenging scenarios, showcasing its effectiveness and broad applicability in handling distribution shifts during test time.

The Computational Complexity of Circuit Discovery for Inner Interpretability
This paper examines the complexities of inner interpretability in neural networks through the lens of circuit discovery, using classical and parameterized computational complexity theory. It provides a framework to understand and analyze the complexity of interpretability queries, revealing many as intractable but also offering transformations and heuristics to better manage some of these challenges, thus outlining the scope and limitations of such queries in neural networks.

The Crystal Ball Hypothesis in diffusion models: Anticipating object positions from initial noise
This study explores the role of "trigger patches" within initial noise images in text-to-image diffusion models, discovering that these universal patches are crucial for object generation and can be generalized across different settings. By creating a labeled dataset of Gaussian noises and training a detector for these patches, the research identifies them as outliers, and it proposes a reject-sampling strategy to align prompts with these patterns for improved image generation fidelity.

The Directionality of Optimization Trajectories in Neural Networks
This paper introduces the concept of a Trajectory Map to analyze the directional characteristics of neural network optimization trajectories, highlighting how properties like weight decay and momentum influence the process. The study reveals that neural optimization trajectories are more linear than previously thought, allowing for significant computational savings by freezing over 99% of the parameters without affecting performance.

The Loss Landscape of Deep Linear Neural Networks: a Second-order Analysis
This paper provides a complete analysis of the optimization landscape of deep linear neural networks with square loss, characterizing global minimizers, strict saddle points, and non-strict saddle points. It clarifies the roles of these critical points in global convergence and implicit regularization, using conditions on the ranks of partial matrix products, and presents an explicit parameterization of global minimizers as well as large sets of saddle points.

Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation
Think-on-Graph 2.0 (ToG-2) is a hybrid retrieval-augmented generation framework that enhances large language models by iteratively retrieving information from unstructured and structured knowledge sources, utilizing knowledge graphs for deep context retrieval and efficient graph retrieval. ToG-2 demonstrates state-of-the-art performance on multiple datasets, enhances reasoning in various models without additional training, and is compatible with existing large language models like GPT-3.5.

TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention
TidalDecode is introduced as a novel method to enhance large language model (LLM) decoding by employing position persistent sparse attention, which maintains spatial coherence in token selection while minimizing overhead. This approach significantly reduces token selection complexity and decoding latency, achieving performance comparable to full attention methods with up to a 2.1x decrease in decoding time.

Tight Clusters Make Specialized Experts
The paper introduces the Adaptive Clustering (AC) router for Sparse Mixture-of-Experts (MoE) architectures, which optimizes token-expert routing by deriving feature weights that enhance identification of latent clusters. This method results in faster convergence, improved robustness to data corruption, and overall performance enhancement, as demonstrated in both language modeling and image recognition tasks.

Time After Time: Deep-Q Effect Estimation for Interventions on When and What to do
This paper introduces Earliest Disagreement Q-Evaluation (EDQ), a novel method for estimating the causal effect of sequential treatment times, addressing limitations in handling irregular time in decision support systems. Utilizing Dynamic Programming and compatible with sequence models like transformers, EDQ offers accurate estimates, demonstrated through experiments on a survival time prediction task.

To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic reasoning
Chain-of-thought (CoT) prompting significantly enhances large language models' performance on tasks involving math and logic by improving symbolic execution, though it offers minimal benefits for other types of tasks. The study suggests that CoT should be applied selectively to save inference costs and recommends exploring new paradigms beyond prompt-based CoT to better utilize intermediate computations across diverse LLM applications.

TorchTitan: One-stop PyTorch native solution for production ready LLM pretraining
This paper introduces TORCHTITAN, a PyTorch-native distributed training system that integrates and enhances state-of-the-art techniques for large language model training, offering features such as 4D parallelism and elastic scaling to improve efficiency and scalability. The system's superior performance is demonstrated with significant acceleration across the Llama 3.1 family of models, showcasing its capability to optimize training recipes and its potential as a flexible platform for experimentation and comparison of diverse training strategies.

Towards a learning theory of representation alignment
This paper explores the alignment of AI models' representations from a learning-theoretic perspective, reviewing metric, probabilistic, and spectral concepts of alignment. The authors introduce the concept of stitching to examine the interaction of different representations within a task, demonstrating a connection between stitching properties and kernel alignment, thus paving the way to understanding representation alignment as a learning-theoretic issue.

Training-Free Dataset Pruning for Instance Segmentation
This paper introduces a novel Training-Free Dataset Pruning (TFDP) method specifically designed for instance segmentation tasks, addressing challenges such as pixel-level annotations, instance area variations, and class imbalances without relying on model training. By employing a Shape Complexity Score and its refined versions, this approach achieves state-of-the-art results across several datasets and architectures, significantly accelerating the pruning process compared to existing classification-based methods.

Training Nonlinear Transformers for Chain-of-Thought Inference:  A Theoretical Generalization Analysis
Chain-of-Thought (CoT) is a prompting method that enhances the reasoning abilities of large language models by using multiple examples with intermediate steps, although its theoretical foundation remains underexplored. This work presents the first theoretical study of training Transformers with nonlinear attention to achieve CoT generalization, demonstrating its successful application to unseen tasks and distribution-shifted data, and establishing conditions under which CoT can accurately reason even with noisy input, outperforming one-step in-context learning methods.

Transformer Learns Optimal Variable Selection in Group-Sparse Classification
This paper provides a theoretical case study demonstrating how a one-layer transformer, when trained with gradient descent, can effectively learn a classic statistical model with "group sparsity" by using its attention mechanism to select relevant variables. Additionally, it shows that a well-pretrained transformer can adapt to new tasks with limited samples, offering insights into the effective learning of structured data by transformers.

Transformer Meets Twicing: Harnessing Unattended Residual Information
This paper introduces the Twicing Attention mechanism, which enhances transformer-based models by addressing the degradation of representational capacity across layers through a novel connection to low-pass non-local means smoothing filters. By implementing kernel twicing procedure in self-attention, the authors achieve improved accuracy and a slower decay of representations, as demonstrated on tasks such as image classification and language modeling.

Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement
This paper introduces a principled framework for LLM-based evaluation that guarantees alignment with human judgment by proposing a selective evaluation approach. Utilizing Simulated Annotators for improved judge calibration and Cascaded Selective Evaluation to efficiently escalate model scrutiny, the method ensures over 80% human agreement even with cost-effective models, advancing beyond conventional LLM evaluation techniques.

TweedieMix: Improving Multi-Concept Fusion for Diffusion-based Image/Video Generation
TweedieMix is a novel method designed to enhance the generation of images and videos that integrate multiple personalized concepts by customizing diffusion models during inference. By utilizing a two-stage sampling process that includes a multiple object-aware technique and Tweedie's formula, TweedieMix achieves higher fidelity in generating images and videos with personalized concepts compared to existing methods.

Understanding Methods for Scalable MCTS
This paper discusses scalable methods for parallelizing Monte Carlo Tree Search (MCTS) to enhance decision-making in complex environments. It analyzes various parallelism techniques, such as root, leaf, and tree parallelism, alongside advanced strategies like lock-free concurrency to address the challenges and trade-offs in the effective parallelization of MCTS.

Unearthing Skill-level Insights for Understanding Trade-offs of Foundation Models
The paper introduces an automatic method for evaluating model performance by identifying and analyzing relevant skills through model-generated rationales, creating "skill-slices" for more nuanced insights. This approach reveals significant differences in model performance across specific skills and demonstrates a method to improve accuracy by 3% through skill-based model selection, providing a more detailed understanding of model evaluation.

Unlearn and Burn: Adversarial Machine Unlearning Requests Destroy Model Accuracy
This paper identifies a significant vulnerability in machine unlearning systems where adversaries can deplete model accuracy by making adversarial unlearning requests for data not included in the original training set. The study presents attack algorithms achieving drastic reductions in accuracy on CIFAR-10 and ImageNet datasets and underscores the necessity for improved verification mechanisms to validate the legitimacy of unlearning requests without hindering valid requests.

VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents
Large Multimodal Models (LMMs) are advancing AI by integrating language and vision to create Visual Foundation Agents, yet current benchmarks inadequately assess their full potential in complex environments. VisualAgentBench (VAB) is introduced as a comprehensive benchmark for training and evaluating these models across diverse scenarios, revealing their developing capabilities and providing a platform for future improvement in visual agent development.

Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models
This paper introduces Visually Guided Decoding (VGD), a novel gradient-free approach that improves the generation of textual prompts for text-to-image models by using large language models and CLIP-based guidance. VGD enhances prompt interpretability and coherence, outperforming existing techniques, and enables more intuitive interactions without requiring additional training.

Wayward Concepts In Large Multimodal Models
This paper analyzes the differences between optimized prompt embeddings for visual concepts and typical discrete prompts, revealing that optimized prompts act like an adversarial attack on the text encoder in multimodal models. The study demonstrates that these embeddings manipulate model performance by targeting final layers and pooling tokens, with findings highlighting their model-specific and initialization-specific nature, supported by extensive experiments across multiple datasets and models.

Weighted Multi-Prompt Learning with Description-free Large Language Model Distillation
DeMul, a novel method for prompt learning, eliminates the need for extracting descriptions in Vision Language Models by directly distilling knowledge from Large Language Models into prompts. This approach enhances the encapsulation of semantics and improves performance on recognition tasks, demonstrating superior results across 11 datasets.

What Are Good Positional Encodings for Directed Graphs?
This paper addresses the need for effective positional encodings (PEs) in directed graphs by introducing the *Walk Profile* and proposing the *Multi-q Magnetic Laplacian PE*, which can express these profiles. The novel PE method enhances the representation capability and demonstrates its effectiveness through numerical experiments on sorting network satisfiability and general circuit benchmarks, with code accessible at the provided GitHub link.

What Do You See in Common? Learning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits
The paper addresses the challenge of discovering evolutionary traits directly from biological images by presenting the Hierarchy aligned Commonality through Prototypical Networks (HComP-Net) framework. HComP-Net introduces innovative losses and a masking module to effectively learn hierarchical prototypes, demonstrating improved accuracy and generalization over existing methods, with publicly available code for further research.

What's the Move? Hybrid Imitation Learning via Salient Points
SPHINX is an advanced imitation learning framework designed to improve generalization and efficiency in robotic task execution by utilizing multimodal observations and hybrid action spaces. The method significantly outperforms existing IL models, achieving high success rates across various tasks and conditions, demonstrating superior adaptability to new viewpoints and task complexities.

What to align in multimodal contrastive learning?
This paper presents CoMM, a Contrastive Multimodal learning strategy that enhances the communication between modalities within a single multimodal space by maximizing mutual information between augmented multimodal features. CoMM effectively captures redundant, unique, and synergistic information, achieving state-of-the-art results on various multimodal tasks by learning complex multimodal interactions beyond mere redundancy.

When narrower is better: the narrow width limit of Bayesian parallel branching neural networks
This paper investigates the narrow width limit of Bayesian Parallel Branching Neural Networks (BPB-NNs) and reveals that these networks can outperform or match the performance of their wide width counterparts in bias-limited scenarios. By examining the symmetry breaking in kernel renormalization, the study highlights that the narrow width limit results in more robust branch learning, largely independent of architectural hyperparameters, and is applicable across different architectures such as branching graph neural networks and residual-MLPs.

Which Tasks Should Be Compressed Together? A Causal Discovery Approach for Efficient Multi-Task Representation Compression
This paper presents a Taskonomy-Aware Multi-Task Compression framework that enhances intelligent image analysis by focusing on semantic significance and task interactions rather than pixel-level accuracy. By organizing tasks into shared representations and employing a conditional entropy-based DAG to capture causal dependencies, the framework effectively reduces encoding volume and improves multi-task performance, as evidenced by significant bitrate-performance gains across various vision tasks.

W-PCA Based Gradient-Free Proxy for Efficient Search of Lightweight Language Models
The paper introduces weight-weighted PCA (W-PCA), a novel zero-shot neural architecture search (NAS) method designed for evaluating lightweight language models without training. By optimizing evaluation time through proxies like parameter count and number of significant principal components, W-PCA outperforms prior methods in efficiency and testing scores, as demonstrated on GLUE and SQuAD datasets, while also achieving better ranking correlation and reduced solving time compared to other zero-shot NAS methods.

X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality Translation at Scale
The paper introduces X-ALMA, a multilingual model providing high-quality translation across 50 languages, including those with low resources, outperforming state-of-the-art models like Aya-101 and Aya-23. Utilizing a plug-and-play architecture and a novel training regimen enhanced by Adaptive Rejection Preference Optimization (ARPO), X-ALMA significantly improves translation performance, tackling language conflicts effectively.

Zero-Shot Natural Language Explanations
This paper presents a novel approach to generating faithful Natural Language Explanations (NLEs) for visual classification models without relying on annotated training data, addressing the common issue of unfaithfulness in current NLEs. By using a simple MLP to model the relationship between class embeddings and class names, the method allows for efficient mapping of new text to classifier space, providing not only NLEs but also enabling zero-shot image classification and fine-grained concept discovery across 38 tested vision models, including CNNs and Transformers.

Federated Class-Incremental Learning: A Hybrid Approach Using Latent Exemplars and Data-Free Techniques to Address Local and Global Forgetting
Federated Class-Incremental Learning (FCIL) faces challenges of local and global forgetting due to class imbalance among clients. This paper introduces a Hybrid Rehearsal (HR) approach, utilizing latent exemplars and synthetic data generation to tackle these challenges, and demonstrates through experiments that HR outperforms existing methods in FCIL benchmarks while maintaining privacy and efficiency.

### Miscellaneous Aspects of Machine Learning->Causality
Do Contemporary Causal Inference Models Capture Real-World Heterogeneity? Findings from a Large-Scale Benchmark
This study evaluates Conditional Average Treatment Effect (CATE) estimation algorithms using 16 models across 43,200 real-world datasets, revealing substantial limitations as 62% of estimates have higher Mean Squared Error (MSE) than a zero-effect predictor. By introducing a novel application of observational sampling and new statistical parameters, the research identifies performance gaps in current CATE models and signals significant areas for further exploration.

Efficient and Trustworthy Causal Discovery with Latent Variables and Complex Relations
This paper addresses the complex challenge of causal discovery in systems where both latent and observed variables are interconnected through intricate causal relations. The authors present a novel algorithm that efficiently identifies and infers these causal relations under a pure children assumption, providing the first polynomial-time solution while ensuring reliability by signaling errors when assumptions do not hold, thereby enhancing the efficiency and trustworthiness of causal discovery in complex systems.

Causal Discovery via Bayesian Optimization
DrBO is a novel framework for learning directed acyclic graphs (DAGs) that uses Bayesian optimization to efficiently find high-scoring graphs with improved accuracy and sample efficiency. By employing dropout neural networks instead of Gaussian Processes, DrBO addresses scalability issues, allowing it to model DAG scores flexibly and incorporate uncertainty, resulting in faster and more accurate graph recovery compared to existing methods.

Model-agnostic meta-learners for estimating heterogeneous treatment effects over time
This paper introduces several model-agnostic meta-learners for estimating heterogeneous treatment effects (HTEs) over time, which can be integrated with any machine learning model, like transformers. Among these, the novel IVW-DR-learner stands out by using a doubly robust and orthogonal loss with inverse-variance weights to reduce estimation variance, demonstrating superior performance especially in scenarios with low overlap and extended timeframes.

### Miscellaneous Aspects of Machine Learning->Everything Else
Probabilistic Learning to Defer: Handling Missing Expert Annotations and Controlling Workload Distribution
This paper introduces a new probabilistic modelling approach based on the mixture-of-experts framework to enhance learning to defer (L2D) systems in human-AI cooperation, addressing the limitations of extensive annotation requirements and inefficient workload distribution among human experts and AI. By employing the Expectation-Maximisation algorithm and introducing a workload distribution constraint, the approach improves resource allocation efficiency, achieving competitive or superior performance on synthetic and real-world datasets compared to existing methods.

FedTMOS: Efficient One-Shot Federated Learning with Tsetlin Machine
The paper introduces One-Shot Federated Learning with Tsetlin Machine (FedTMOS), a novel data-free framework that efficiently generates balanced server models without requiring server-side training by leveraging the low-complexity, class-adaptive properties of the Tsetlin Machine. FedTMOS outperforms existing methods with a 7.22% accuracy improvement, reduces upload communication costs by at least $2.3\times$, and decreases server latency by $75\times$, making it a highly efficient solution in one-shot federated learning scenarios.

### Miscellaneous Aspects of Machine Learning->Sequential, Network, and Time Series Modeling
CoMRes: Semi-Supervised Time Series Forecasting Utilizing Consensus Promotion of Multi-Resolution
This study introduces a semi-supervised forecasting approach using a multi-view setting on augmented data to address challenges in long-term time series forecasting, such as unseen patterns and data scarcity. By employing a consensus promotion framework, the method enhances forecasting accuracy and mitigates error accumulation, surpassing traditional models in robustness and accuracy over extended prediction horizons.

### Miscellaneous Aspects of Machine Learning->Supervised Learning
Boosting Methods for Interval-censored Data with Regression and Classification
This paper addresses the challenges of applying boosting algorithms to interval-censored data, which is prevalent in survival analysis and time-to-event studies. By introducing novel nonparametric boosting methods that utilize censoring unbiased transformations and functional gradient descent, the study enhances predictive accuracy and scalability, providing a robust framework for regression and classification tasks, with demonstrated empirical effectiveness across different scenarios.

### Miscellaneous Aspects of Machine Learning->Transfer, Multitask and Meta-learning
Progressive Parameter Efficient Transfer Learning for Semantic Segmentation
ProPETL is introduced as an innovative approach in Parameter Efficient Transfer Learning (PETL) that addresses the limitations that recent PETL methods face in semantic segmentation tasks due to required larger parameter adjustments. By incorporating a midstream adaptation process, ProPETL not only achieves state-of-the-art performance on segmentation benchmarks but also outperforms full fine-tuning on the COCO-Stuff10k dataset, showcasing its effectiveness and versatility for broader adoption.

### Miscellaneous Aspects of Machine Learning->Unsupervised and Semi-supervised Learning
Test-time Adaptation for Regression by Subspace Alignment
This paper addresses test-time adaptation (TTA) for regression by introducing Significant-subspace Alignment (SSA), which refines feature alignment for regression models transferring from a source to an unknown target domain. SSA outperforms existing methods by detecting representative feature subspaces and adjusting dimension importance to enhance regression model adaptability with unlabeled target data.

Unsupervised Multiple Kernel Learning for Graphs via Ordinality Preservation
This paper introduces Unsupervised Multiple Kernel Learning for Graphs (UMKL-G), a model that integrates multiple graph kernels to effectively capture global relationships in unsupervised graph analysis without the need for labels or predefined local neighbors. Theoretical guarantees on stability, robustness, and generalization are provided, and empirical results show that UMKL-G surpasses individual kernels and other leading methods in performance, offering a strong solution for graph clustering tasks.

### Optimization->Everything Else
Methods for Convex $(L_0,L_1)$-Smooth Optimization: Clipping, Acceleration, and Adaptivity
This paper addresses optimization in machine learning by focusing on (strongly) convex $(L_0,L_1)$-smooth functions and establishing new convergence guarantees, improving methods like Gradient Descent with Smoothed Gradient Clipping and Polyak Stepsizes. Notably, the research offers enhancements without relying on standard smoothness assumptions, avoiding exponential dependency issues, and extends findings to the stochastic case, proposing an accelerated method and improved convergence rates for Adaptive Gradient Descent.

### Optimization->Large Scale, Parallel and Distributed
LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression
LoCoDL is a communication-efficient algorithm designed for distributed and federated learning, utilizing local training to reduce communication frequency and compression techniques to minimize data transmission. The algorithm supports a variety of unbiased compressors and offers improved communication complexity and performance in heterogeneous environments with strongly convex functions, outperforming existing methods in practice.

### Optimization->Learning for Optimization
Accelerating Training with Neuron Interaction and Nowcasting Networks
The paper introduces Neuron Interaction and Nowcasting (NiNo) networks, which enhance weight nowcaster networks (WNNs) by utilizing neuron connectivity and graph neural networks for more precise parameter prediction. The NiNo approach addresses previous limitations and achieves up to a 50% acceleration in training with Adam optimizer in vision and language tasks.

### Optimization->Optimization and Learning under Uncertainty
Competitive Fair Scheduling with Predictions
The paper explores learning-augmented algorithms for online non-clairvoyant scheduling, specifically targeting the minimization of max-stretch by utilizing potentially imperfect job size predictions. It introduces a family of algorithms with various competitive ratios, evaluates their trade-offs between consistency and robustness, and demonstrates their effectiveness through simulations on synthetic and real-world datasets.

### Optimization->Zero-order and Black-box Optimization
Optimizing Posterior Samples for Bayesian Optimization via Rootfinding
This paper introduces an efficient global optimization strategy for posterior samples in Bayesian optimization, utilizing a novel global rootfinding approach that scales linearly in high dimensions. The proposed method significantly improves optimization performance, outperforming traditional strategies like EI and GP-UCB, and offers advancements in posterior sample-based acquisition functions, including a new sample-average formulation of GP-TS with adjustable exploitation controls.

### Probabilistic Methods->Bayesian Models and Methods
Bridging the Gap between Variational Inference and Stochastic Gradient MCMC in Function Space
This paper addresses challenges in Bayesian neural network inference by proposing a novel hybrid method that combines functional variational inference (fVI) and functional Markov Chain Monte Carlo (fMCMC) to benefit from their complementary strengths. The approach demonstrates improved predictive accuracy and uncertainty quantification over existing techniques, supported by theoretical justification using Wasserstein gradient flows.

### Probabilistic Methods->Everything Else
Connecting Federated ADMM to Bayes
This paper establishes new connections between ADMM-based and Variational Bayes-based approaches in federated learning, revealing that dual variables in ADMM naturally correspond to "site" parameters in VB with isotropic Gaussian covariances. By leveraging this insight, the authors propose two novel ADMM variants with flexible covariances and functional regularization, demonstrating enhanced performance through numerical experiments and bridging two traditionally distinct fields to advance federated learning.

### Probabilistic Methods->Structure Learning
Extendable and Iterative Structure Learning Strategy for Bayesian Networks
This paper introduces an extendable structure learning strategy for Bayesian networks that efficiently incorporates new variables without the need for retraining from scratch, significantly reducing computational overhead. The proposed method demonstrates up to 1300x runtime reductions in empirical evaluations, offering a scalable solution for real-time and high-dimensional settings by iteratively updating Bayesian network structures while maintaining accuracy.

### Reinforcement Learning->Batch/Offline
Offline Hierarchical Reinforcement Learning via Inverse Optimization
This paper introduces OHIO, a framework for offline reinforcement learning of hierarchical policies, which addresses the challenge of learning from static datasets that may not reveal high-level policy actions. By solving the inverse problem to reconstruct unobservable actions, OHIO generates suitable datasets for offline training, demonstrating superior performance in robotic and network optimization tasks compared to standard RL methods.

Energy-Weighted Flow Matching for Offline Reinforcement Learning
This paper introduces Energy-Weighted Flow Matching (EFM), a novel method that directly learns energy-guided flow for generative modeling without relying on auxiliary procedures, and extends this to energy-weighted diffusion models. Additionally, it proposes the Q-weighted Iterative Policy Optimization (QIPO) for offline reinforcement learning, demonstrating improved performance in tasks and marking the first energy-guided diffusion model and exact flow matching model that operates independently of auxiliary models.

M^3PC: Test-time Model Predictive Control using Pretrained Masked Trajectory Model
This paper introduces a method to enhance decision-making in Offline Reinforcement Learning by using Model Predictive Control (MPC) during inference to utilize a pretrained trajectory model's predictive abilities for action selection, rather than just for reconstructing masked components. The proposed approach significantly improves policy performance on benchmarks like D4RL and RoboMimic, and it can also adapt to Offline to Online RL and Goal Reaching RL, offering better outcomes and generalization when additional online interaction is available.

Long-Short Decision Transformer: Bridging Global and Local Dependencies for Generalized Decision-Making
The Long-Short Decision Transformer (LSDT) integrates self-attention and convolutional filters to simultaneously capture long-range and fine-grained local relationships within offline reinforcement learning datasets. By effectively balancing these dependencies, LSDT outperforms traditional Decision Transformers and other baselines on the D4RL benchmark, demonstrating versatility and improved decision-making in both Markovian and non-Markovian environments.

POTEC: Off-Policy Contextual Bandits for Large Action Spaces via Policy Decomposition
The paper introduces a novel two-stage algorithm called Policy Optimization via Two-Stage Policy Decomposition (POTEC) for off-policy learning in contextual bandit settings with large discrete action spaces. By leveraging action space clustering and combining policy- and regression-based methods, POTEC reduces bias and variance issues and outperforms existing approaches, as demonstrated by comprehensive experiments.

### Reinforcement Learning->Deep RL
Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning
This paper introduces Langevin Soft Actor Critic (LSAC), a novel model-free reinforcement learning algorithm designed to improve sample efficiency in continuous control tasks by enhancing critic learning through uncertainty estimation. Utilizing methods like distributional Langevin Monte Carlo for $Q$ updates and parallel tempering, LSAC demonstrates superior or comparable performance to existing algorithms, marking the first successful application of Langevin Monte Carlo-based Thompson sampling in continuous action spaces.

PWM: Policy Learning with Multi-Task World Models
This paper presents Policy learning with multi-task World Models (PWM), a novel model-based reinforcement learning algorithm that efficiently handles multi-task continuous control by utilizing well-regularized world models for smoother optimization. PWM significantly outperforms existing methods in both single-task and complex multi-task settings, achieving up to 27% higher rewards without costly online planning, and effectively manages tasks with high action dimensions.

Self-Improvement for Neural Combinatorial Optimization: Sample Without Replacement, but Improvement
This paper introduces a simplified training process for constructive neural combinatorial optimization that combines sampling multiple solutions with round-wise Stochastic Beam Search and a policy improvement strategy, eliminating the need for expensive expert solutions or complex policy gradient methods. The proposed method achieves comparable performance in the Traveling Salesman Problem and Capacitated Vehicle Routing Problem, and outperforms state-of-the-art methods in the Job Shop Scheduling Problem using a transformer-based architecture.

### Reinforcement Learning->Everything Else
REGENT: A Retrieval-Augmented Generalist Agent That Can Act In-Context in New Environments
The paper presents a novel approach for creating generalist AI agents that can adapt to new environments efficiently by using retrieval-based in-context learning, eliminating the need for extensive finetuning. The proposed semi-parametric agent, REGENT, leverages a transformer-based policy with query and retrieval sequences, outperforming current leading agents with fewer parameters and pre-training data, demonstrating significant improvements in robotics and game-playing tasks.

OMNI-EPIC: Open-endedness via Models of human Notions of Interestingness with Environments Programmed in Code
OMNI-EPIC is a novel framework designed to continuously generate and solve increasingly complex tasks, advancing the development of general intelligence. By leveraging foundation models to autonomously create code for adaptable and novel tasks, OMNI-EPIC overcomes the limitations of predefined environments, thereby fostering endless creativity and self-improvement in AI systems.

### Reinforcement Learning->Multi-agent
Efficient Multi-agent Offline Coordination via Diffusion-based Trajectory Stitching
This paper introduces Multi-Agent offline coordination via Diffusion-based Trajectory Stitching (MADiTS), a novel approach that enhances learning efficiency in multi-agent reinforcement learning (MARL) by generating high-quality trajectories through advanced data augmentation techniques. By leveraging diffusion models and a unique offline credit assignment method, MADiTS improves coordination among agents and achieves superior performance on multiple benchmark datasets with imbalanced data.

IntersectionZoo: Eco-driving for Benchmarking Multi-Agent Contextual Reinforcement Learning
IntersectionZoo is a comprehensive benchmark suite designed to address the challenges of generalizability in multi-agent contextual reinforcement learning (CRL) by leveraging the real-world application of cooperative eco-driving in urban road networks. By utilizing data-driven simulations of over 16,000 signalized intersections from major US cities, this study highlights the difficulty popular multi-agent RL algorithms face in generalizing to CRL settings, emphasizing the need for standardized benchmarks grounded in real-world problem characteristics.

On the Linear Speedup of Personalized Federated Reinforcement Learning with Shared Representations
The paper introduces a personalized Federated Reinforcement Learning framework (PFedRL) that improves learning in heterogeneous environments by collaboratively sharing common feature representations while allowing agent-specific adaptations. The proposed PFedTD-Rep algorithm demonstrates a linear convergence speedup with respect to the number of agents and achieves better generalization in new environments, as validated by experimental results.

Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal Control
The paper introduces DGPPO, a novel framework designed to learn both a discrete graph control barrier function (CBF) and a distributed high-performance safe policy for multi-agent systems (MAS) dealing with unknown dynamics, partial observability, and varying neighborhoods. Empirical evaluations across different simulation engines demonstrate that DGPPO achieves superior task performance while maintaining high safety rates, utilizing a consistent set of hyperparameters across diverse environments, thereby enhancing the practical applicability of safety-critical MAS.

### Reinforcement Learning->Online
Efficient Model-Based Reinforcement Learning Through Optimistic Thompson Sampling
This paper introduces a novel approach to optimistic exploration in reinforcement learning by using Thompson sampling to account for joint uncertainty over transitions and rewards, addressing a gap in existing methods. The proposed method accelerates learning in challenging environments by effectively guiding exploration and leveraging model uncertainty, as demonstrated in various continuous control tasks.

Online Reinforcement Learning in Non-Stationary Context-Driven Environments
This paper introduces Locally Constrained Policy Optimization (LCPO), an online reinforcement learning approach designed to address the challenge of catastrophic forgetting in non-stationary environments by anchoring policy outputs on previous experiences while optimizing for current ones. The effectiveness of LCPO, which outperforms traditional baselines and compares favorably with a prescient offline agent, is demonstrated through evaluations in varied environments using both synthetic and real context traces.

### Reinforcement Learning->Planning
Reinforcement learning with combinatorial actions for coupled restless bandits
Reinforcement learning struggles with large, combinatorially structured action spaces, which are common in planning problems. The proposed SEQUOIA algorithm, which embeds a Q-network into a mixed-integer program, significantly improves performance in managing such spaces, especially in complex restless bandit problems, outperforming current methods by an average of 26.4%.

Epistemic Monte Carlo Tree Search
The paper introduces Epistemic MCTS (EMCTS), an advanced approach that incorporates epistemic uncertainty into Monte Carlo Tree Search to improve exploration in sparse reward environments. By integrating EMCTS with AlphaZero, the study demonstrates enhanced sample efficiency and problem-solving capabilities in challenging tasks like code generation in Assembly language and hard-exploration benchmarks like Deep Sea, surpassing traditional methods that do not account for uncertainty in their search.

How to Find the Exact Pareto Front for Multi-Objective MDPs?
This paper addresses the challenge of efficiently discovering the Pareto front in Multi-Objective Markov Decision Processes (MO-MDPs) by leveraging the geometric structure of the Pareto front, identifying it as the boundary of a convex polytope with vertices corresponding to deterministic policies. The authors present a novel algorithm that simplifies discovering the Pareto front by reducing global comparisons to localized searches, thus enhancing efficiency and outperforming existing methods through empirical validation.

### Social Aspects->Accountability, Transparency and Interpretability
$\text{I}^2\text{AM}$: Interpreting Image-to-Image Latent Diffusion Models via Bi-Attribution Maps
This paper presents Image-to-Image Attribution Maps ($\text{I}^2\text{AM}$), a method for improving the interpretability of image-to-image diffusion models by visualizing bidirectional attribution maps. By aggregating cross-attention scores, $\text{I}^2\text{AM}$ and the Inpainting Mask Attention Consistency Score (IMACS) offer insights into feature transfer in tasks such as object detection and inpainting, aiding in model debugging and performance enhancement.

Mechanism and emergence of stacked attention heads in multi-layer transformers
This paper introduces the retrieval problem, a reasoning task solvable only by transformers with logarithmically increasing layers relative to input size, and demonstrates that large language models can address this task using various prompt formulations without fine-tuning. The study reveals that successful learning happens with an implicit curriculum and explores the learned mechanisms by analyzing attention maps and the sequential emergence of attention heads during training.

Metric-Driven Attributions for Vision Transformers
This paper introduces Metric-Driven Attribution (MDA) for explaining Vision Transformers (ViT), which optimizes patch ordering and magnitude based on attribution quality metrics to create attribution maps. The proposed MDA method outperforms seven existing ViT attribution methods by 12% across various attribution metrics on the ImageNet dataset, offering a balance between sparse and dense attributions.

RankSHAP: Shapley Value Based Feature Attributions for Learning to Rank
This paper introduces RankSHAP, an extension of classical Shapley values for ranking, to address inconsistencies in existing post-hoc, model-agnostic explanations for learning to rank. Through extensive experiments and a user study, the authors demonstrate that RankSHAP aligns with human intuition and propose a set of axioms for ranking-based feature attribution methods to ensure generality and consistency in studying IR ranking models.

### Social Aspects->Fairness, Equity, Justice and Safety
AdvPaint: Protecting Images from Inpainting Manipulation via Adversarial Attention Disruption
ADVPAINT is a novel defensive framework designed to prevent unauthorized inpainting by generating adversarial perturbations that disrupt adversaries' manipulation tasks using diffusion models. Targeting self- and cross-attention blocks and employing a two-stage perturbation strategy, ADVPAINT effectively enhances protection against inpainting abuses, achieving superior performance compared to existing methods by significantly increasing FID and reducing precision in experimental results.

BBCaL: Black-box Backdoor Detection under the Causality Lens
This paper addresses the problem of detecting backdoor attacks in Deep Neural Networks during the inference stage, specifically in a black-box setting, by introducing a novel causality-based approach named BBCaL. The proposed method distinguishes backdoor from clean samples through causal analysis and counterfactual reasoning, showing improved effectiveness and efficiency in extensive experiments across benchmark datasets.

Mind Control through Causal Inference: Predicting Clean Images from Poisoned Data
Anti-backdoor learning seeks to train models directly from poisoned datasets without falling prey to backdoor attacks, but current methods struggle with label recovery and generalization to large pre-trained models. This paper presents an end-to-end approach, Mind Control through Causal Inference (MCCI), which improves prediction accuracy by using causal inference to effectively train models with both images and attack indicators, demonstrating state-of-the-art performance in recovering correct predictions from poisoned samples.

Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models
This paper introduces *Jailbreak Antidote*, a method allowing real-time adjustments to the safety preferences of large language models (LLMs) by manipulating a sparse subset of their internal states during inference. The approach efficiently balances safety and utility without introducing computational overhead, validated by extensive experiments on various LLMs, offering a scalable solution to enhance LLM safety while maintaining their utility.

### Social Aspects->Privacy-preserving Statistics and Machine Learning
CipherPrune:  Efficient and Scalable Private Transformer Inference
CipherPrune is a private inference framework that addresses efficiency and scalability issues in Transformer models by introducing encrypted token pruning and polynomial reduction protocols. The framework significantly reduces the execution overhead for private Transformer inference, achieving up to a 10.6× speedup for 512-token inputs with minimal accuracy loss, by adaptively eliminating redundant tokens and optimizing polynomial computations.

Near-Exact Privacy Amplification for Matrix Mechanisms
This paper presents a framework for computing near-exact privacy parameters for differentially private (DP) machine learning by optimizing a lower-triangular, non-negative correlation matrix $\textbf{C}$ while accounting for privacy amplification via random batching. By utilizing Monte Carlo accounting and a "balls-in-bins" batching scheme, the authors achieve improved root mean square error on prefix sums and better performance on deep learning tasks compared to previous state-of-the-art methods.

Privacy-Aware Lifelong Learning
The paper introduces Privacy-Aware Lifelong Learning (PALL), a novel approach that combines lifelong learning with efficient unlearning capabilities to address challenges in data privacy regulations. PALL employs task-specific sparse subnetworks and an episodic memory rehearsal mechanism to enable models to learn incrementally while selectively forgetting sensitive information without degrading performance, demonstrated through state-of-the-art results in image classification.

### Social Aspects->Trustworthy Machine Learning
Local-Prompt: Extensible Local Prompts for Few-Shot Out-of-Distribution Detection
Out-of-Distribution (OOD) detection is enhanced by introducing Local-Prompt, a new tuning paradigm that focuses on regional enhancement while freezing global prompts. This approach improves OOD detection through global prompt guided negative augmentation and local prompt enhanced regional regularization, leading to a significant reduction in average FPR95 by 5.17% compared to state-of-the-art methods in 4-shot tuning on the ImageNet-1k dataset.

ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability
This paper investigates the mechanisms behind hallucinations in Retrieval-Augmented Generation (RAG) models, finding that hallucinations occur when Knowledge FFNs and Copying Heads ineffectively balance parametric and retrieved external knowledge. It introduces ReDeEP, a method that enhances hallucination detection by decoupling these knowledge sources, and AARF, a strategy to reduce hallucinations by adjusting the influence of Knowledge FFNs and Copying Heads, demonstrating significant improvements in detection accuracy.

### Theory->Everything Else
LevAttention: Time, Space and Streaming Efficient Algorithm for Heavy Attentions
This paper addresses the inefficiency of computing matrix $A$ using transformers by introducing an approach with linear time dependency on $n$ for identifying "large attention scores," reducing the quadratic constraint in contexts with long lengths. The authors present a method leveraging randomized numerical linear algebra to find a universal set $U$ that efficiently isolates significant attention scores independent of $n$, delivering empirical improvements in vision transformers and offering theoretical support for model optimization.

### Theory->Game Theory
Stochastic Semi-Gradient Descent for Learning Mean Field Games with Population-Aware Function Approximation
This paper introduces SemiSGD, a novel stochastic gradient descent method that unifies policy and population distribution updates in mean field games (MFGs), improving efficiency and stability over traditional fixed-point iteration methods. By incorporating population-aware linear function approximation, the authors provide a theoretical convergence analysis and validate their approach through experiments, demonstrating how SemiSGD effectively handles both linear and non-linear MFGs on continuous state-action spaces.

### Theory->Learning Theory
Limits to scalable evaluation at the frontier: LLM as judge won’t beat twice the data
This paper investigates the limitations of using strong existing models as evaluators for new machine learning models, which can introduce biases like self-preferencing. The study shows that debiasing methods can only reduce the need for ground truth labels by half, highlighting significant limitations of the model-as-judge approach, particularly when evaluating models that might surpass the judges in performance.

Semialgebraic Neural Networks: From roots to representations
This paper introduces Semialgebraic Neural Networks (SANNs), a novel architecture capable of representing and evaluating any bounded semialgebraic function using a numerical ODE solver. SANNs use a homotopy continuation method to encode learned functions, allowing for the exact representation of even discontinuous semialgebraic functions and demonstrating trainability with standard deep learning methods.

Theory on Mixture-of-Experts in Continual Learning
This paper offers the first theoretical analysis of the Mixture-of-Experts (MoE) model in continual learning (CL), demonstrating its advantage over single-expert models by proving its ability to specialize in diverse tasks and effectively manage expert selection and load distribution. The study reveals that updating the MoE's gating network needs to be halted after sufficient training to ensure convergence, and highlights interesting findings about the trade-offs in adding more experts, with experiments extending these insights from linear models to deep neural networks.

### Theory->Probabilistic Methods
Probabilistic Conformal Prediction with Approximate Conditional Validity
We introduce a novel method for generating prediction sets that extends conformal methods to achieve approximately conditional coverage by leveraging an estimate of the conditional distribution $\textup{P}_{Y \mid X}$. Our approach, which provides non-asymptotic bounds based on the total variation distance, outperforms existing methods by adapting to the predictive distribution's behavior and ensuring more reliable statistical inference under high heteroscedasticity.



## Oral Session 4F (15:30-16:42)
15:30-15:42: Cut Your Losses in Large-Vocabulary Language Models
The paper introduces Cut Cross-Entropy (CCE), a novel method for computing cross-entropy loss in large language models that significantly reduces memory usage by only materializing necessary logits directly in flash memory. This approach cuts the memory footprint of loss computation from 24 GB to 1 MB for the Gemma 2 (2B) model, maintaining training speed and convergence while greatly enhancing memory efficiency.
15:42-15:54: Your Mixture-of-Experts LLM Is Secretly an Embedding Model for Free
This study investigates the potential of Mixture-of-Experts (MoE) LLMs as embedding models, revealing that their expert routers can be used directly for embedding tasks with notable performance improvements without additional finetuning. Additionally, by combining MoE routing weights with hidden states, a new approach called MoEE is introduced, which enhances performance on diverse embedding-focused tasks, as demonstrated through extensive experiments across multiple datasets from the Massive Text Embedding Benchmark (MTEB).
15:54-16:06: ChartMoE: Mixture of Diversely Aligned Expert Connector for Chart Understanding
ChartMoE introduces a Mixture of Expert (MoE) architecture to enhance the performance of Multimodal Large Language Models (MLLMs) in automatic chart understanding by bridging the modality gap using expertly aligned tasks. The proposed method, validated by the newly introduced ChartMoE-Align dataset, improves the accuracy of chart data analysis, outperforming previous benchmarks with a significant increase from 80.48% to 84.64% on the ChartQA benchmark.
16:06-16:18: MaestroMotif: Skill Design from Artificial Intelligence Feedback
MaestroMotif is an AI-assisted skill design method that utilizes Large Language Models (LLMs) to create and adapt high-performing agents by automatically generating rewards and leveraging code generation for skill training. Evaluated in the NetHack Learning Environment (NLE), MaestroMotif outperforms existing methods in both efficacy and user-friendliness by implementing complex behaviors based on natural language descriptions.
16:18-16:30: MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation Experts
This paper introduces MoE++, a novel Mixture-of-Experts (MoE) framework that combines Feed-Forward Network (FFN) experts with new zero-computation experts, including zero, copy, and constant experts, to enhance computational efficiency and effectiveness. MoE++ optimizes token engagement with experts, reduces computing overhead, improves performance by focusing resources on challenging tokens, and offers deployment advantages by minimizing communication overhead, ultimately achieving superior performance and throughput compared to traditional MoE models.
16:30-16:42: OLMoE: Open Mixture-of-Experts Language Models
OLMoE is an open, cutting-edge language model utilizing sparse Mixture-of-Experts (MoE), boasting 7 billion parameters but using only 1 billion per input token. Pretrained on 5 trillion tokens, OLMoE surpasses similar and larger models, with novel insights into MoE training and routing properties, and is fully open-sourced with all associated resources.


## Oral Session 4A (15:30-16:42)
15:30-15:42: More RLHF, More Trust? On The Impact of Preference Alignment On Trustworthiness
This study examines the trustworthiness of Large Language Models (LLMs) trained with Reinforcement Learning From Human Feedback (RLHF), revealing that alignment with human preferences does not inherently ensure outputs that are free from toxicity, bias, and other ethical concerns. The research highlights the complex influence of fine-tuning data on model trustworthiness and proposes the application of influence function-based data attribution methods to better understand and improve model alignment.
15:42-15:54: Measuring and Enhancing Trustworthiness of LLMs in RAG through Grounded Attributions and Learning to Refuse
This paper introduces Trust-Score, a metric designed to evaluate the trustworthiness of large language models (LLMs) in retrieval-augmented generation (RAG) systems, highlighting shortcomings in current adaptation methods. The proposed Trust-Align method significantly improves Trust-Score performance, outperforming competitive baselines across several models and datasets, and enhances model abilities such as refusal accuracy and citation quality.
15:54-16:06: Iterative Nash Policy Optimization: Aligning LLMs with General Preferences via No-Regret Learning
The paper explores Reinforcement Learning with Human Feedback (RLHF) through a game-theoretic perspective, introducing an innovative online algorithm called iterative Nash policy optimization (INPO) that approximates Nash policies by having the policy play against itself with no-regret learning. This approach eliminates the need for costly win rate estimation, leveraging a new loss objective minimized over a preference dataset, and demonstrates significant performance improvements on benchmarks, notably achieving high win rates on AlpacaEval 2.0 and Arena-Hard compared to state-of-the-art RLHF algorithms.
16:06-16:18: RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style
The paper introduces RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content changes and resistance to style biases, which are critical for techniques like Reinforcement Learning from Human Feedback. Experiments reveal that even state-of-the-art reward models perform poorly on RM-Bench, highlighting significant opportunities for improvement in aligning language models effectively.
16:18-16:30: REEF: Representation Encoding Fingerprints for Large Language Models
This paper introduces REEF, a training-free method to detect if a suspect model has been developed from a victim open-source Large Language Model by comparing their feature representations using centered kernel alignment similarity. REEF effectively safeguards LLMs' intellectual property without affecting the model's general capabilities and is robust against various modifications, offering a straightforward solution for model owners and third parties.
16:30-16:42: Rethinking Reward Modeling in Preference-based Large Language Model Alignment
The paper investigates the application of the Bradley-Terry (BT) model for reward modeling in Large Language Model alignment, uncovering its convergence properties when using deep neural network embeddings. It challenges the necessity of the BT model by introducing the concept of order consistency and proposing an alternative upper-bound algorithm using binary classifiers, supported by extensive empirical evaluations across multiple setups and models.


## Oral Session 4C (15:30-16:42)
15:30-15:42: Compositional Entailment Learning for Hyperbolic Vision-Language Models
This paper introduces Compositional Entailment Learning for hyperbolic vision-language models, leveraging the hierarchical nature of hyperbolic embeddings beyond individual image-text pairs. By organizing images, image boxes, and their descriptions hierarchically using contrastive and entailment-based objectives, the proposed approach outperforms traditional Euclidean and recent hyperbolic models in zero-shot and retrieval tasks, demonstrating superior hierarchical performance.
15:42-15:54: Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference under Ambiguities
Spatial expressions often vary based on the frames of reference used, leading to potential ambiguities in vision-language models (VLMs). The COMFORT evaluation protocol reveals significant shortcomings in VLMs, including poor robustness, inconsistency, and a tendency to prioritize English over other languages, highlighting a need for improved attention to cross-cultural and ambiguous spatial reasoning.
15:54-16:06: Topological Blindspots: Understanding and Extending Topological Deep Learning Through the Lens of Expressivity
The paper explores the expressive limitations of higher-order message-passing (HOMP) in topological deep learning, particularly its inability to capture certain topological and metric invariants. It introduces two new architecture classes, multi-cellular networks (MCN) and scalable MCN (SMCN), which address these limitations and improve expressivity, as demonstrated through new benchmarks and evaluations on real-world datasets.
16:06-16:18: Population Transformer: Learning Population-level Representations of Neural Activity
The Population Transformer (PopT) framework introduces a self-supervised approach that enhances the aggregation and decoding of spatially-sparse neural recordings across diverse datasets by stacking on pretrained temporal embeddings. It reduces required data while maintaining high accuracy in decoding, offers generalizability across data modalities, and provides insights into neuroscience, with publicly available code for easy implementation in multi-channel neural data analysis.
16:18-16:30: TopoLM: brain-like spatio-functional organization in a topographic language model
This paper introduces TopoLM, a transformer language model with a two-dimensional spatial representation, which mimics the brain's language system organization by forming semantically interpretable clusters through a combination of next-token prediction and spatial smoothness loss. The model demonstrates the potential neural basis for the spatial organization of language processing in the human cortex, predicting both the emergence of this structure and the functional clusters observed in human studies.
16:30-16:42: The Geometry of Categorical and Hierarchical Concepts in Large Language Models
This paper extends the linear representation hypothesis by formalizing the representation of features, such as "is_animal," as vectors in the representation spaces of large language models, allowing categorical concepts to be expressed as polytopes. The authors demonstrate the connection between the hierarchical structure of concepts and their geometric representations, validating their theoretical findings with empirical tests on Gemma and LLaMA-3 models using WordNet data.


## Oral Session 4D (15:30-16:42)
15:30-15:42: Open-Vocabulary Customization from CLIP via Data-Free Knowledge Distillation
This paper addresses the challenge of customizing vision-language models like CLIP without original data by rethinking Data-Free Knowledge Distillation (DFKD). By utilizing image-text matching and introducing style dictionary diversification and class consistency strategies, the authors achieve a significant 9.33% performance improvement in open-vocabulary customization tasks, surpassing existing DFKD methods.
15:42-15:54: GridMix: Exploring Spatial Modulation for Neural Fields in PDE Modeling
The paper introduces GridMix, a novel approach to improve PDE modeling with neural fields by using a mixture of grid-based representations to enhance both global and local detail reconstruction. By integrating spatial domain augmentation, the MARBLE framework emerges from this approach, demonstrating significant advancements in robustness and effectiveness across various benchmarks in dynamics modeling and geometric prediction.
15:54-16:06: Transfusion: Predict the Next Token and Diffuse Images with One Multi-Modal Model
Transfusion is a novel training approach for a multi-modal model that effectively combines language modeling and diffusion techniques for handling both discrete and continuous data types. The approach significantly enhances scalability and performance on various benchmarks, particularly when scaled to 7B parameters, by incorporating modality-specific encoding and decoding layers, thereby effectively generating quality images and text.
16:06-16:18: RB-Modulation: Training-Free Stylization using Reference-Based Modulation
RB-Modulation is a novel training-free approach for personalizing diffusion models, addressing challenges like style extraction without additional descriptions, content leakage, and effective style-content composition. By using a stochastic optimal controller and cross-attention-based feature aggregation, it allows precise and seamless style and content manipulation without relying on external adapters or ControlNets.
16:18-16:30: Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment
This paper introduces Condition Contrastive Alignment (CCA) as a method to enhance guidance-free autoregressive (AR) visual generation, addressing design inconsistencies caused by Classifier-Free Guidance (CFG) in multi-modal tasks. CCA allows pretrained models to achieve the ideal sampling distribution through minimal fine-tuning, reducing sampling costs while maintaining a balance between sample diversity and fidelity, thus bridging language model alignment and visual guidance methods.
16:30-16:42: Ctrl-Adapter: An Efficient and Versatile Framework for Adapting Diverse Controls to Any Diffusion Model
Ctrl-Adapter is a framework designed to integrate spatial control into any image/video diffusion model through the adaptation of pretrained ControlNets, addressing the challenges of feature space mismatches and inefficient training burdens. It supports a variety of tasks, such as video editing and style transfer, and achieves state-of-the-art results on benchmarks like DAVIS 2017 with significantly reduced computational requirements.


## Oral Session 4B (15:30-16:42)
15:30-15:42: From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions
This paper introduces DRAFT, a framework for dynamically refining tool documentation to improve Large Language Models' (LLMs) interactions with external tools by analyzing feedback from their trial-and-error experiences. By enhancing documentation quality through iterative refinement, DRAFT significantly boosts LLMs' tool comprehension and utilization, while demonstrating robust cross-model generalization capabilities.
15:42-15:54: Spider 2.0: Evaluating Language Models on Real-World Enterprise Text-to-SQL Workflows
Spider 2.0 is an evaluation framework designed to address the complexities of real-world enterprise text-to-SQL workflows, featuring 632 problems derived from extensive database applications involving multiple SQL dialects, requiring intricate reasoning over extremely long contexts. The framework demonstrates that current language models, while successful in previous benchmarks, need significant advancements to handle the challenging requirements of real-world enterprise settings, as evidenced by the lower task completion rates compared to prior models like Spider 1.0 and BIRD.
15:54-16:06: BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions
The paper introduces BigCodeBench, a benchmark designed to evaluate Large Language Models (LLMs) on their ability to solve practical tasks by using diverse function calls from multiple libraries across different domains. Despite recent advancements, the evaluation shows that LLMs still struggle with following complex instructions accurately, achieving a maximum score of 60%, which emphasizes the necessity for continued development in this field.
16:06-16:18: LLM-SR: Scientific Equation Discovery via Programming with Large Language Models
LLM-SR is a novel approach that uses Large Language Models (LLMs) to enhance symbolic regression techniques for discovering scientific equations efficiently from data, incorporating domain-specific knowledge. This method demonstrates superior performance on benchmark problems across various scientific domains by combining the LLMs' extensive knowledge with evolutionary search to propose and optimize equation hypotheses, outperforming traditional symbolic regression methods, especially in out-of-domain tests.
16:18-16:30: Cybench: A Framework for Evaluating Cybersecurity Capabilities and Risks of Language Models
Cybench is a framework that specifies and evaluates cybersecurity tasks using 40 professional Capture the Flag (CTF) tasks, aimed at quantifying the capabilities of language model agents in identifying vulnerabilities and executing exploits. The study evaluates eight models on these tasks and highlights the success of models like GPT-4o and Claude 3.5 Sonnet in completing tasks autonomously that human teams took significantly longer to solve, underlining their potential for enhancing cybersecurity practices.
16:30-16:42: AFlow: Automating Agentic Workflow Generation
Large language models often rely on manually constructed workflows, which hinders scalability and generalizability. This paper proposes AFLOW, an automated framework using Monte Carlo Tree Search to optimize workflow generation, achieving significant improvements over existing methods and enabling smaller models to perform efficiently with reduced costs.


## Oral Session 4E (15:30-16:42)
15:30-15:42: Synthetic continued pretraining
This paper addresses the challenge of data-inefficient knowledge acquisition in language models when adapting to small, domain-specific datasets. By introducing EntiGraph, a synthetic data augmentation algorithm, the authors demonstrate improved model performance through synthetic continued pretraining, allowing language models to effectively answer questions and follow instructions related to source documents, whether or not those documents are available during inference.
15:42-15:54: Energy-based Backdoor Defense Against Federated Graph Learning
Federated Graph Learning is challenged by backdoor attacks, which traditional defense methods struggle to counter due to diverse trigger structures and injection locations. This paper introduces Federated Graph Backdoor Defense using Topological Graph Energy (FedTGE), a novel technique that utilizes energy assignments and clustering at the client level and energy graph construction at the server level to effectively mitigate backdoor attacks without needing a validation dataset, even in heterogeneous and high-malicious-proportion environments.
15:54-16:06: Problem-Parameter-Free Federated Learning
Federated learning (FL) is challenged by the need for problem-specific hyperparameter tuning, and PAdaMFed addresses this by introducing an adaptive stepsize and momentum method that functions without such specific parameters. The algorithm offers scalable performance with robust theoretical guarantees in convergence and communication complexities, demonstrating its effectiveness through empirical studies on real-world tasks.
16:06-16:18: Subgraph Federated Learning for Local Generalization
Federated Learning on graphs often struggles with local overfitting, limiting the ability of models to generalize to unseen data due to changing graph structures and label distributions. The proposed FedLoG method addresses this by creating global synthetic data, enhancing the generalization capabilities of local models, and outperforming baselines in generalization tasks.
16:18-16:30: Copyright-Protected Language Generation via Adaptive Model Fusion
CP-Fuse is a novel approach designed to protect against the reproduction of copyrighted material by language models, effectively balancing the outputs from models trained on separate datasets to minimize the regurgitation of memorized content. Through extensive experiments, CP-Fuse has been shown to significantly reduce the reproduction of protected material without impacting the quality of generated text and code, while also providing seamless integration with other protective measures.
16:30-16:42: Capturing the Temporal Dependence of Training Data Influence
Traditional data influence estimation methods are inadequate for modern training paradigms sensitive to data ordering, leading to a need for new approaches. This paper introduces "trajectory-specific leave-one-out (LOO) influence" and proposes "data value embedding" as a method for efficiently approximating this influence, revealing that early and late-stage data points significantly impact model outcomes and suggesting new strategies for data selection and curation.


# 4/26
## Poster Session 5 (10:00-12:30)
### Applications->Chemistry and Drug Discovery
Retrieval Augmented Diffusion Model for Structure-informed Antibody Design and Optimization
This paper introduces RADAb, a retrieval-augmented diffusion framework for more efficient and optimized antibody design that addresses challenges in existing methods. By integrating structural homologous motifs with input backbones and utilizing a novel dual-branch denoising module, RADAb achieves state-of-the-art performance in antibody inverse folding and optimization tasks.

CL-MFAP: A Contrastive Learning-Based Multimodal Foundation Model for Molecular Property Prediction and Antibiotic Screening
The paper addresses the urgent need for new antibiotics in the face of rising antimicrobial resistance by introducing CL-MFAP, an unsupervised contrastive learning-based multimodal model designed for discovering antibiotic compounds. Utilizing three types of molecular data from 1.6 million bioactive molecules, CL-MFAP demonstrates superior performance in predicting antibiotic properties compared to baseline models, thus enhancing the efficiency of drug discovery processes.

A Simple yet Effective $\Delta\Delta G$ Predictor is An Unsupervised Antibody Optimizer and Explainer
The paper introduces Light-DDG, a lightweight $\Delta\Delta G$ predictor using a structure-aware Transformer, designed to enhance mutation screening efficiency by leveraging knowledge from existing computationally intensive predictors. Through extensive experiments and a case study on SARS-CoV-2, Light-DDG demonstrates significant improvements in test generalizability, inference speed, and performance over previous models, proving its effectiveness as an unsupervised antibody optimizer and explainer for mutation preferences.

MOFFlow: Flow Matching for Structure Prediction of Metal-Organic Frameworks
This paper introduces MOFFlow, the first deep generative model specifically designed for predicting Metal-Organic Framework (MOF) structures, overcoming the complexity inherent in these materials. By utilizing a novel Riemannian flow matching framework to treat metal nodes and organic linkers as rigid bodies in the $SE(3)$ space, MOFFlow accurately and efficiently predicts large MOF structures, outperforming traditional and state-of-the-art machine learning methods.

Iterative Substructure Extraction for Molecular Relational Learning with Interactive Graph Information Bottleneck
This paper introduces the Iterative Substructure Extraction (ISE) framework for molecular relational learning, which utilizes the Expectation-Maximization algorithm to iteratively refine molecular interactions to core substructures. Guided by the Interactive Graph Information Bottleneck theory, the IGIB-ISE approach enhances precise and compact substructure extraction, outperforming state-of-the-art methods in accuracy, generalizability, and interpretability across various tasks.

Multimodal Large Language Models for Inverse Molecular Design with Retrosynthetic Planning
Llamole is the first multimodal large language model capable of interleaved text and graph generation, specifically designed for molecular inverse design and retrosynthetic planning. By integrating a base LLM with Graph Diffusion Transformer and Graph Neural Networks, Llamole significantly outperforms existing models in controllable molecular design, as demonstrated through comprehensive benchmarking against 14 adapted LLMs across 12 metrics.

### Applications->Computer Vision
Prompt as Knowledge Bank: Boost Vision-language model via Structural Representation for  zero-shot medical detection
This paper introduces StructuralGLIP, a method that improves zero-shot medical detection by encoding prompts into a latent knowledge bank for more context-aware alignment between images and disease descriptions. The approach results in a +4.1% AP improvement over previous methods in zero-shot benchmarks and a +3.2% AP enhancement on fine-tuned endoscopy image models.

Progressive Token Length Scaling in Transformer Encoders for Efficient Universal Segmentation
This paper introduces PROgressive Token Length SCALing (PRO-SCALE), a method designed to enhance the efficiency of transformer encoders in segmentation models like Mask2Former by progressively adjusting token lengths throughout the encoder layers. PRO-SCALE achieves a significant reduction in computational cost (approximately 52% GFLOPs) without compromising performance, as demonstrated on the COCO dataset and other benchmarks, and the code will be made publicly available.

CO-MOT: Boosting End-to-end Transformer-based Multi-Object Tracking via Coopetition Label Assignment and Shadow Sets
This paper introduces Co-MOT, a novel approach to end-to-end Multi-Object Tracking (e2e-MOT) that improves the traditional bipartite matching strategy for a more balanced training, particularly in scenes with numerous new objects. By employing a coopetition label assignment with a shadow concept, Co-MOT enhances performance and inference speed, achieving higher HOTA and TETA scores while requiring significantly fewer computational resources compared to existing methods like MOTRv2.

On the Transfer of Object-Centric Representation Learning
This paper investigates the transferability of object-centric representation learning methods to unseen datasets by introducing a benchmark with seven synthetic and real-world datasets. It proposes a novel fine-tuning strategy that leverages pre-trained vision encoders, achieving state-of-the-art performance in unsupervised object discovery and demonstrating strong zero-shot transfer capabilities.

ThermalGaussian: Thermal 3D Gaussian Splatting
The paper introduces ThermalGaussian, the first thermal 3D Gaussian Splatting (3DGS) method capable of producing high-quality RGB and thermal images, overcoming the limitations of Neural Radiance Fields (NeRF) through rapid training and real-time rendering. By aligning RGB and thermal images and applying multimodal regularization constraints, ThermalGaussian not only achieves photorealistic rendering and a 90% reduction in storage costs but also contributes a new dataset, RGBT-Scenes, to support further research in thermal scene reconstruction.

StochSync: Stochastic Diffusion Synchronization for Image Generation in Arbitrary Spaces
This paper introduces StochSync, a novel zero-shot method for generating images in arbitrary spaces by blending strengths of Diffusion Synchronization and Score Distillation Sampling. StochSync excels in generating high-quality 360° panoramas without conditioning and achieves comparable results in 3D mesh texturing, outperforming existing finetuning strategies.

Multi-Perspective Data Augmentation for Few-shot Object Detection
The paper introduces a Multi-Perspective Data Augmentation (MPAD) framework to enhance the diversity and representativeness of synthetic samples for few-shot object detection (FSOD), addressing limitations in typical foreground-background relationships. The proposed techniques, including In-Context Learning for Object Synthesis (ICOS) and Harmonic Prompt Aggregation Scheduler (HPAS), significantly outperform traditional methods, with the framework achieving a $17.5\%$ improvement in nAP50 over the baseline on PASCAL VOC benchmarks.

Masked Temporal Interpolation Diffusion for Procedure Planning in Instructional Videos
This paper presents the Masked Temporal Interpolation Diffusion (MTID) model, which improves procedure planning in instructional videos by generating coherent action sequences from visual observations. By introducing a latent space temporal interpolation module and an action-aware mask projection mechanism, the model enhances temporal coherence and filters irrelevant actions, demonstrating superior performance on benchmark datasets.

TopoGaussian: Inferring Internal Topology Structures from Visual Clues
TopoGaussian is introduced as a particle-based pipeline that infers the interior structure of opaque objects from photos and videos, addressing the limitations of traditional mesh-based methods. By integrating Gaussian Splatting and a novel differentiable simulator, this approach not only accelerates processing by 5.26 times on average but also enhances shape quality, demonstrating significant potential in fields like 3D vision, soft robotics, and manufacturing.

High-quality Text-to-3D Character Generation with SparseCubes and Sparse Transformers.
This paper introduces SparseCubes, a novel and efficient sparse differentiable mesh representation method, along with a sparse transformer network, to address the challenges of generating intricate 3D models from text, particularly for anime characters. The approach significantly reduces computational and storage requirements while successfully enhancing the resolution and detail of 3D meshes, as validated through improved rendering of subtle features such as fingers and hair in anime character models.

SynQ: Accurate Zero-shot Quantization by Synthesis-aware Fine-tuning
This paper introduces SynQ, a novel Zero-shot Quantization (ZSQ) framework designed to effectively quantize pre-trained models without needing training data while addressing challenges such as noise in synthetic datasets and misguidance by erroneous hard labels. By employing a low-pass filter to minimize sample noise and utilizing soft labels for difficult samples, SynQ demonstrates superior accuracy over existing ZSQ methods in various experiments.

### Applications->Everything Else
TestGenEval: A Real World Unit Test Generation and Test Completion Benchmark
TestGenEval is a newly released benchmark designed to evaluate the performance of code generation models specifically in the context of software testing, an area previously underexplored. Despite testing several popular models, results showed that even the best model, GPT-4o, achieved only 35.2% average test coverage, indicating challenges in generating comprehensive test suites due to difficulties in reasoning about execution and addressing complex code paths.

Natural Language Inference Improves Compositionality in Vision-Language Models
The paper introduces Caption Expansion with Contradictions and Entailments (CECE), an innovative approach using Natural Language Inference to enhance compositional reasoning in Vision-Language Models (VLMs) by generating lexically diverse, yet semantically consistent, sentences. CECE significantly improves interpretability and reduces superficial biases, achieving state-of-the-art performance on benchmarks for image-text alignment, with notable performance gains in Winoground and EqBen without the need for additional fine-tuning.

### Applications->Genetics, Cell Biology, Health, etc
PhyloVAE: Unsupervised Learning of Phylogenetic Trees via Variational Autoencoders
This paper introduces PhyloVAEs, a novel unsupervised learning framework that enhances representation learning and generative modeling of phylogenetic tree structures. By utilizing a deep latent-variable generative model and collaborative inference model, PhyloVAEs offer high-resolution and efficient generation of tree topologies, outperforming traditional distance-based methods.

KinPFN: Bayesian Approximation of RNA Folding Kinetics using Prior-Data Fitted Networks
This paper introduces KinPFN, an innovative approach that models the posterior predictive distribution of RNA folding times, significantly improving computational efficiency for RNA folding kinetics analysis. By training on synthetic data, KinPFN provides rapid and accurate approximations of the cumulative distribution function of RNA folding times, promising substantial speed-ups over existing RNA kinetics algorithms without compromising on accuracy.

### Applications->Health
LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models
This study introduces LeFusion, a novel lesion-focused diffusion model designed to generate high-quality lesion-containing image-segmentation pairs from lesion-free images, addressing challenges like data scarcity and long-tail imbalances in real-world clinical practice. By focusing on lesion areas and incorporating techniques such as histogram-based texture control and multi-channel decomposition, LeFusion enhances lesion diversity and significantly improves the performance of state-of-the-art segmentation models on 3D cardiac lesion MRI and lung nodule CT datasets.

MuHBoost: Multi-Label Boosting For Practical Longitudinal Human Behavior Modeling
This paper introduces MuHBoost, a multi-label boosting method designed to improve the accuracy of predictive models using ubiquitous health data, addressing issues such as varied data types and high rates of missing values. The paper demonstrates that MuHBoost and its two variants outperform existing models across multiple health and well-being prediction tasks, while ensuring resource efficiency, through experiments on realistic datasets.

### Applications->Language, Speech and Dialog
Explore Theory of Mind: program-guided adversarial data generation for theory of mind reasoning
This paper introduces ExploreToM, a novel framework for generating diverse and complex theory of mind data to robustly train and evaluate large language models (LLMs). The framework reveals significant limitations in current models' theory of mind abilities and shows that fine-tuning with ExploreToM data significantly improves performance on established benchmarks, while also identifying missing skills and imbalances contributing to poor model performance.

Mufu:  Multilingual Fused Learning for Low-Resource Translation with LLM
Multilingual large language models (LLMs) struggle with translating low-resource languages; to address this, the paper introduces Mufu, a method that transforms translation tasks into post-editing tasks using auxiliary translation candidates to improve the LLMs' performance. Experiments show that LLMs fine-tuned with Mufu-style prompts outperform NLLB 1.3B models in a majority of low-resource language pairs, and distilled models enhance translation efficiency while maintaining improved accuracy.

Sylber: Syllabic Embedding Representation of Speech from Raw Audio
We introduce Sylber, a self-supervised learning model that generates speech representations with distinct and robust syllabic structures, addressing inefficiencies in current neural speech representations. Sylber offers a fast syllable segmentation algorithm, efficient tokenization, and novel phonological units, enabling effective speech compression and incorporating categorical perception naturally into its embedding space, enhancing spoken language modeling efficiency.

Lost in Prediction: Why Social Media Narratives Don't Help Macroeconomic Forecasting?
This paper explores the potential of using NLP models to analyze viral tweets on social media as a means to predict fluctuations in macroeconomic indicators, a concept stemming from Narrative Economics. The study reveals the challenges and complexities involved in successfully translating social media narratives into reliable economic predictions.

PersonalLLM: Tailoring LLMs to Individual Preferences
The paper introduces PersonalLLM, a public benchmark designed to adapt large language models (LLMs) for personalized interactions that consider users' diverse and subtle preferences. By simulating a varied user base with pre-trained reward models, the benchmark provides a novel platform for developing personalization algorithms that manage data sparsity, using methods like in-context learning and meta-learning to leverage historical data across similar users.

### Applications->Neuroscience, Cognitive Science
Brain Mapping with Dense Features: Grounding Cortical Semantic Selectivity in Natural Images With Vision Transformers
BrainSAIL (Semantic Attribution and Image Localization) is a method designed to connect neural selectivity with semantic visual concepts in natural scenes by utilizing large-scale artificial neural networks. By creating spatially dense embeddings and applying voxel-wise encoding models without further training, BrainSAIL effectively identifies specific image subregions that influence selectivity patterns in the visual cortex, thus advancing our understanding of visual representation in the brain and enabling detailed comparisons of brain encoding models across various visual areas.

Grid Cell-Inspired Fragmentation and Recall for Efficient Map Building
The paper introduces the Fragmentation-and-Recall (FARMap) approach for mapping large spaces, inspired by the fragmentation of grid cell maps observed in neuroscience. By using surprisal-based clustering to create and recall local maps, FARMap improves spatial exploration efficiency, rapidly covering environments while optimizing active memory usage without sacrificing performance.

CREIMBO: Cross-Regional Ensemble Interactions in Multi-view Brain Observations
This paper introduces CREIMBO, a novel model for analyzing neural dynamics by leveraging asynchronous and diverse neural recordings to uncover hidden sub-circuits and evolving interactions among neural ensembles. CREIMBO effectively distinguishes session-specific from global dynamics, and successfully identifies meaningful brain patterns in both synthetic and real-world neural data, enhancing interpretability and aligning with biological significance.

One Hundred Neural Networks and Brains Watching Videos: Lessons from Alignment
This study provides the first large-scale benchmarking of deep video models' representational alignment with the human brain using video brain imaging data. The findings highlight the importance of temporal modeling for brain region alignment in early visual processing and classification tasks for higher-level regions, revealing differences in alignment patterns between CNNs and Transformers, and the impact of training dataset biases and computational complexity on brain alignment.

BrainACTIV: Identifying visuo-semantic properties driving cortical selectivity using diffusion-based image manipulation
Brain Activation Control Through Image Variation (BrainACTIV) is a method that modifies a reference image using pretrained diffusion models to manipulate activity in specific cortical regions, enhancing our ability to discern which image features drive neural response selectivity. This approach not only demonstrates effective modulation of fMRI responses aligned with hypothesized category preferences but also aids in exploring neural representations, thus providing a valuable tool for formulating hypotheses and creating stimuli for neuroimaging research.

BRAID: Input-driven Nonlinear Dynamical Modeling of Neural-Behavioral Data
The paper introduces BRAID, a deep learning framework designed to model nonlinear neural dynamics while explicitly accounting for external inputs, such as sensory stimuli, to better understand behavior. By disentangling intrinsic dynamics from input effects and validating it through simulations and motor cortical activity data, BRAID improves the accuracy of neural-behavioral data modeling and forecasting compared to traditional methods.

### Applications->Physics
MeshMask: Physics-Based Simulations with Masked Graph Neural Networks
We propose a new masked pre-training method for graph neural networks that enhances computational fluid dynamics problem-solving by masking up to 40% of input mesh nodes, leading to improved model robustness and long-term prediction accuracy by up to 60%. Our method achieves state-of-the-art results across seven CFD datasets and facilitates efficient pre-training on multiple datasets, reducing time and data requirements for achieving high performance on new tasks.

Deep Signature: Characterization of Large-Scale Molecular Dynamics
This paper introduces Deep Signature, a novel computational framework to address the complex high-dimensional dynamics and interatomic interactions in protein dynamics, crucial for understanding protein functions and developing therapies. The approach combines soft spectral clustering and signature transform to effectively reduce system size and characterize interactions, achieving superior performance across three biological benchmarks.

Solving Differential Equations with Constrained Learning
This paper introduces a science-constrained learning (SCL) framework as an innovative approach for solving (partial) differential equations (PDEs) using neural networks. By framing the problem as constrained learning with worst-case losses, SCL effectively integrates structural constraints and prior knowledge, providing accurate solutions with reduced dependency on hyperparameter tuning and potentially lower computational costs compared to traditional methods.

### Applications->Robotics
LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality
LocoVR is a comprehensive dataset containing over 7000 two-person trajectories in virtual reality, captured from 130 diverse indoor home environments, designed to enhance the understanding of human locomotion for AI agents. It provides precise trajectory data and examples of socially-motivated behaviors, improving model performance in predicting socially-aware navigation patterns by capturing nuanced social dynamics, such as respecting personal boundaries and maneuvering in high-traffic zones.

Multi-Robot Motion Planning with Diffusion Models
This paper introduces Multi-robot Multi-model planning Diffusion (MMD), a novel method for generating collision-free multi-robot trajectories using single-robot data by integrating learned diffusion models with classical search techniques. The proposed approach successfully scales diffusion models for planning in larger environments and demonstrates its capability in managing numerous robots in logistics-inspired simulated scenarios.

### Applications->Time Series
Root Cause Analysis of Anomalies in Multivariate Time Series through Granger Causal Discovery
This paper introduces AERCA, a methodology that combines Granger causal discovery with root cause analysis to address anomalies in multivariate time series. By modeling the distributions of exogenous variables and identifying those deviating from normal states, AERCA effectively uncovers causal relationships and pinpoints anomalies' root causes, as validated by extensive experiments.

TimeInf: Time Series Data Contribution via Influence Functions
This paper presents TimeInf, a novel model-agnostic method for estimating data contribution in time-series datasets by using influence scores to attribute predictions to individual time points while maintaining temporal structures. TimeInf demonstrates improved performance in anomaly detection over existing techniques and offers interpretable insights into anomalies, with potential applications in identifying mislabeled anomalies in ground truth annotations.

### Deep Learning->Algorithms
Do Deep Neural Network Solutions Form a Star Domain?
This paper introduces the conjecture that the solution set of neural networks obtained via stochastic gradient descent is a star domain, implying that a central "star model" can be linearly connected to other solutions with low loss, considering permutation invariances. The authors propose the Starlight algorithm to identify such a star model, validate its connectivity to other models, and show its potential advantages for Bayesian Model Averaging and as an alternative to model ensembles.

Remove Symmetries to Control Model Expressivity and Improve Optimization
Symmetries in loss functions can lead models to low-capacity states known as collapses, posing a significant challenge in training deep learning models. This paper introduces a model-agnostic algorithm called \textit{syre}, which effectively mitigates these symmetry-induced states, enhancing optimization and performance without requiring specific symmetry knowledge.

FLOPS: Forward Learning with OPtimal Sampling
This paper addresses the challenge of inefficient query allocation in perturbation-based gradient computation for forward learning, proposing a novel query allocator that optimally distributes queries within a set budget. By using a simplified proxy objective and reparameterization technique, the proposed method enhances scalability and efficiency, demonstrating significant improvements in experiments with Vision Transformers and diverse applications such as prompt tuning and multimodal alignment.

Improving Equivariant Networks with Probabilistic Symmetry Breaking
This paper addresses the limitation of equivariant neural networks in breaking symmetries by introducing equivariant conditional distributions and presenting theoretical results for representing these distributions. The proposed method, SymPE (Symmetry-breaking Positional Encodings), enhances the representational power of equivariant networks, maintaining symmetry bias and yielding improved performance in various neural network models, as demonstrated through experimental results.

### Deep Learning->Attention Mechanisms
Attention as a Hypernetwork
This paper investigates the mechanisms behind compositional generalization in transformers, proposing that multi-head attention can be viewed as a hypernetwork with a low-dimensional latent code that specifies key-query operations. By introducing a non-linear modification to the hypernetwork-generated network, the study shows improved compositional generalization on tasks such as a symbolic version of Raven's Progressive Matrices, highlighting how increased model size and data contribute to this capability.

Adaptive Transformer Programs: Bridging the Gap Between Performance and Interpretability in Transformers
This paper introduces Adaptive Transformer Programs, enhancing the RASP language and Transformer Programs to create more interpretable models without sacrificing performance. By redesigning attention modules and introducing a new reparameterization scheme, the method improves expressivity and optimization, as demonstrated through tasks like in-context learning and NLP benchmarks, thereby narrowing the gap between black-box and transparent models for better ethical AI development.

GOAL: A Generalist Combinatorial Optimization Agent Learner
GOAL is introduced as a generalist model capable of solving multiple combinatorial optimization problems (COPs) by employing a single backbone with adaptable input and output processing. Unlike traditional methods that require problem-specific training, GOAL uses mixed-attention blocks and a novel multi-type transformer architecture to efficiently handle diverse graph-based problems and demonstrates strong transfer learning capabilities across a variety of routing, scheduling, and classic graph problems.

### Deep Learning->Everything Else
MCNC: Manifold-Constrained Reparameterization for Neural Compression
This paper introduces Manifold-Constrained Neural Compression (MCNC), a novel method to compress large foundational models by constraining the parameter space to low-dimensional, pre-defined nonlinear manifolds. Extensive experiments indicate that MCNC achieves superior compression rates and performance compared to existing methods across various tasks and architectures, with source code available for public use.

### Deep Learning->Generative Models and Autoencoders
3DTrajMaster: Mastering 3D Trajectory for Multi-Entity Motion in Video Generation
This paper introduces 3DTrajMaster, a state-of-the-art controller for manipulating multi-entity 3D motions in video generation, using user-desired 6DoF pose sequences and a 3D-motion grounded object injector. The study enhances video quality by incorporating a domain adaptor and annealed sampling strategy, while using a novel 360-Motion Dataset to achieve superior accuracy and generalization in 3D motion control.

Re-Imagining Multimodal Instruction Tuning: A Representation View
The paper introduces Multimodal Representation Tuning (MRT), a new method that enhances multimodal instruction tuning by directly editing semantically rich representations, achieving strong performance with fewer tunable parameters compared to existing approaches. MRT not only surpasses state-of-the-art baselines in performance metrics such as the MME score but also enables intuitive control over Large Multimodal Models (LMMs) through the manipulation of multimodal representations.

MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs
This paper investigates the limitations of Multimodal Large Language Models (MLLMs) in perceiving small visual details and finds their performance highly sensitive to the size of visual subjects in image-based questions. The study introduces training-free visual intervention methods utilizing attention and gradient maps, which significantly enhance MLLMs' accuracy in identifying small details without requiring additional training, thereby addressing potential risks in visual recognition tasks.

A Unified Framework for Forward and Inverse Problems in Subsurface Imaging using Latent Space Translations
The paper introduces the Generalized Forward-Inverse (GFI) framework for subsurface imaging, addressing the forward and inverse problems of mapping between velocity maps and seismic waveforms using deep learning. It proposes two new model architectures, Latent U-Net and Invertible X-Net, which demonstrate state-of-the-art performance and effectively handle forward and inverse translations, showcasing improved capabilities on both synthetic and real-world-like datasets.

Diffusion State-Guided Projected Gradient for Inverse Problems
DiffStateGrad is proposed to enhance diffusion models for inverse problems by projecting the measurement gradient onto a subspace, improving the preservation of the diffusion process on the prior manifold and reducing artifacts in image restoration. This approach increases robustness in diffusion models concerning measurement guidance step size and noise, achieving superior performance in both linear and nonlinear image restoration tasks, surpassing existing methods.

$InterLCM$: Low-Quality Images as Intermediate States of Latent Consistency Models for Effective Blind Face Restoration
The paper introduces $\textit{InterLCM}$, a method for blind face restoration that leverages the latent consistency model (LCM) to address limitations of diffusion models, such as semantic inconsistency and reliance on numerous denoising iterations. By treating low-quality images as intermediate LCM states and incorporating a Visual Module and Spatial Encoder, $\textit{InterLCM}$ improves restoration fidelity and quality, outperforming existing methods on synthetic and real-world datasets with faster inference speeds.

Multilevel Generative Samplers for Investigating Critical Phenomena
The paper introduces a Renormalization-informed Generative Critical Sampler (RiGCS), designed to improve sampling efficiency in near-critical systems utilizing Scale Invariance at Criticality (SIC). By enhancing MultiLevel Monte Carlo with Heat Bath algorithms with generative models, RiGCS significantly increases the effective sample size and accelerates training for two-dimensional Ising systems compared to existing generative model baselines.

FreCaS: Efficient Higher-Resolution Image Generation via Frequency-aware Cascaded Sampling
FreCaS is a novel framework that addresses the challenge of generating high-resolution images with diffusion models by decomposing the sampling into cascaded stages for efficient computation. By introducing a frequency-aware classifier-free guidance strategy and refining details across increasing resolutions, FreCaS enhances image quality and generation speed, outperforming existing methods significantly.

Do WGANs succeed because they minimize the Wasserstein Distance? Lessons from Discrete Generators
This paper demonstrates that successful WGANs minimize the Wasserstein distance, though its specific form is influenced by the architecture and inductive biases of the discriminator, notably when it is convolutional. The authors provide both theoretical and experimental evidence showing that such WGANs focus on minimizing the Wasserstein distance between patches rather than entire images, using discrete generators to support their findings with realistic image generation.

Faster Inference of Flow-Based Generative Models via Improved Data-Noise Coupling
LOOM-CFM enhances Conditional Flow Matching by extending minibatch optimal transport across training time, improving large dataset effectiveness and sampling speed-quality trade-off. It further supports high-resolution synthesis and improves distillation initialization for image and video generation tasks.

Improved Training Technique for Latent Consistency Models
Consistency models can generate high-quality samples both in single and multistep processes, showing performance comparable to diffusion models in the pixel space. This paper tackles the challenges of scaling these models to large datasets in the latent space by replacing Pseudo-Huber with Cauchy losses to mitigate outliers, adding diffusion loss and optimal transport coupling to enhance performance, and introducing an adaptive scaling scheduler and Non-scaling LayerNorm, achieving high-quality sampling with latent consistency models that approach diffusion model performance.

ParetoFlow: Guided Flows in Multi-Objective Optimization
In this study, we introduce \textit{ParetoFlow}, a generative modeling approach for offline multi-objective optimization (MOO) using flow matching to efficiently approximate the Pareto front. By employing a multi-objective predictor guidance module and a neighboring evolution module, our method effectively directs sample generation and achieves state-of-the-art performance across various tasks, demonstrating superiority over traditional methods.

HART: Efficient Visual Generation with Hybrid Autoregressive Transformer
The paper presents Hybrid Autoregressive Transformer (HART), an innovative autoregressive visual generation model that efficiently produces 1024x1024 images with high quality, similar to diffusion models. The model utilizes a novel hybrid tokenizer combining discrete and continuous tokens to significantly enhance image reconstruction and generation efficiency, demonstrating superior performance in benchmarks compared to existing VAR tokenizers and diffusion models.

A Geometric Framework for Understanding Memorization in Generative Models
This paper introduces the *manifold memorization hypothesis* (MMH), a geometric framework to better understand and evaluate the memorization behavior of deep generative models in relation to their training data. By analyzing the dimensionalities of the ground truth data manifold and the learned model manifold, the authors categorize memorized data and provide tools to detect and prevent unwanted memorization, validating their ideas through experiments on synthetic and large image datasets.

CoInD: Enabling Logical Compositions in Diffusion Models
This paper introduces CoInD, a novel approach that enforces statistical independence between conditional marginal distributions in generative models by minimizing Fisher’s divergence, overcoming limitations of standard conditional diffusion models. CoInD proves advantageous in generating more accurate samples for arbitrary logical compositions of attributes, particularly in challenging scenarios involving NOT operations and incomplete composition observations during training.

Discrete Copula Diffusion
This paper addresses the limitation of discrete diffusion models requiring extensive denoising steps by introducing a method to capture dependencies between output variables, using a copula model alongside the diffusion model. The combined approach enhances both unconditional and conditional text generation quality while reducing the required denoising steps significantly, showcasing the importance of modeling inter-variable dependencies in discrete diffusion processes.

Fourier Head: Helping Large Language Models Learn Complex Probability Distributions
This paper introduces a neural network layer based on Fourier series, designed to enhance large language models (LLMs) when modeling non-linguistic tokens with continuous structures. The proposed Fourier head demonstrates improved signal learning and noise reduction, significantly boosting performance in both decision-making tasks for Atari games and time series forecasting, with performance increases up to 377% and 3.5%, respectively.

Risk-Sensitive Diffusion: Robustly Optimizing Diffusion Models with Noisy Samples
This paper introduces risk-sensitive SDE, a stochastic differential equation parameterized by a risk vector to improve the generation quality of diffusion models when dealing with noisy non-image data such as tabular data. The proposed method effectively minimizes the impact of noise during model optimization and consistently outperforms previous baselines, as demonstrated by extensive experiments on various datasets.

Warm Diffusion: Recipe for Blur-Noise Mixture Diffusion Models
This paper introduces Warm Diffusion, a Blur-Noise Mixture Diffusion Model (BNMD), which synthesizes hot diffusion's noise-centric approach and cold diffusion's blurring technique to address their individual limitations in capturing image details and maintaining data manifold integrity. By leveraging spectral analysis, the paper presents strategies to optimize the Blur-to-Noise Ratio (BNR), demonstrating through extensive experiments that this model improves image generation performance compared to existing diffusion paradigms.

### Deep Learning->Graph Neural Networks
Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks
The paper introduces a graph-based latent diffusion model that efficiently samples states from the equilibrium distribution of complex systems like fluid dynamics, using a mesh discretization and physical parameters. This method demonstrates accuracy and computational efficiency by learning full distributions from short simulations and predicting complex scenarios such as 3D wing model pressure distributions in turbulent flows.

Edge Prompt Tuning for Graph Neural Networks
EdgePrompt introduces a new approach to graph prompt tuning by focusing on edges rather than solely on node features to enhance the adaptation of pre-trained Graph Neural Networks (GNNs) to downstream tasks. By incorporating edge prompts through message passing, EdgePrompt effectively improves the quality of graph representations for tasks such as node and graph classification, as demonstrated by superior performance across ten datasets in comparison to existing methods.

From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks
The paper introduces a Tree-like Interpretable Framework (TIF) for graph classification, which transforms Graph Neural Networks into hierarchical trees to provide multi-granular interpretability. By using a combination of graph coarsening and perturbation modules along with an adaptive routing module, TIF successfully captures varying interaction levels and demonstrates superior interpretability and competitive prediction performance on benchmark datasets.

Bundle Neural Network for message diffusion on graphs
The paper introduces Bundle Neural Networks (BuNNs), a novel graph neural network architecture that utilizes message diffusion on flat vector bundles to address challenges like over-smoothing and over-squashing in traditional message passing methods. BuNNs demonstrate improved expressivity and robustness on diverse tasks by evolving node features through diffusion-type equations, marking a significant advancement in graph learning capabilities.

Graph Sparsification via Mixture of Graphs
This paper introduces Mixture-of-Graphs (MoG), which utilizes multiple sparsifier experts to dynamically select tailored pruning solutions for each node in large-scale graph neural networks, thereby improving computational efficiency while maintaining performance. MoG demonstrates significant performance and speed improvements by achieving higher sparsity levels with minimal performance degradation and enhancing the performance of "top-student" GNNs across various datasets.

MaxCutPool: differentiable feature-aware Maxcut for pooling in graph neural networks
The paper introduces a new method for computing the MAXCUT in attributed graphs, optimizing it alongside additional objectives. This method also supports the implementation of a hierarchical graph pooling layer for Graph Neural Networks, enhancing performance on heterophilic graphs while maintaining sparsity and end-to-end trainability.

Residual Connections and Normalization Can Provably Prevent Oversmoothing in GNNs
This paper provides a theoretical analysis of how residual connections and normalization layers in graph neural networks (GNNs) address the oversmoothing problem, detailing their roles in maintaining effective node representations. Building on these insights, the authors introduce GraphNormv2, a novel normalization layer with a learnable centering step, which enhances GNN performance across different architectures and tasks.

Scalable and Certifiable Graph Unlearning: Overcoming the Approximation Error Barrier
ScaleGUN is the first approach to scale certified graph unlearning to billion-edge graphs, integrating approximate graph propagation to maintain certified privacy guarantees while supporting node feature, edge, and node unlearning. Experiments demonstrate ScaleGUN's remarkable efficiency, achieving certified unlearning on a billion-edge graph significantly faster than traditional methods, reducing the time from hours to seconds.

### Deep Learning->Large Language Models
On Speeding Up Language Model Evaluation
This paper presents an adaptive methodology for optimizing prompt-based methods with Large Language Models by addressing the combinatorial search problem over hyper-parameters. Utilizing multi-armed bandits and low-rank matrix factorization, the approach effectively identifies top-performing methods with significantly reduced resource usage, achieving up to 95% cost savings.

ScImage: How good are multimodal large language models at scientific text-to-image generation?
This paper introduces ScImage, a benchmark to evaluate the ability of multimodal large language models to generate scientific images from textual descriptions. The study assesses seven models across multiple languages and reveals that while some models perform adequately on simpler tasks, all struggle with more complex scientific image generation, highlighting the need for improved capabilities in this area.

A Statistical Framework for Ranking LLM-based Chatbots
This paper introduces a statistical framework that enhances the evaluation of large language models (LLMs) using pairwise human judgments, addressing key challenges such as handling ties and modeling covariance between competitors. By offering improved fitting to comparison data and resolving optimization challenges, the framework significantly advances LLM evaluation methodologies, with implementation provided through the open-source Python package, leaderbot.

Wasserstein Distances, Neuronal Entanglement, and Sparsity
This paper introduces a novel method to measure neuronal entanglement in large language models using the Wasserstein distance to identify "Wasserstein Neurons" with highly non-Gaussian output distributions. By proposing an experimental framework that creates a mixture of experts through disentangling these neurons, the study highlights how this approach maintains model accuracy even when applying weight sparsity, thus offering insights into performance optimization without retraining.

Scaling Laws for Downstream Task Performance in Machine Translation
This paper explores the scaling behavior of large language models (LLMs) in a transfer learning setting, focusing on how pretraining data choice and size impact machine translation tasks. The study finds that proper alignment between pretraining and downstream data can lead to improved translation quality, while misalignment may result in unpredictable or degraded performance, thus offering practical insights for selecting effective pretraining datasets.

Faster Cascades via Speculative Decoding
This paper introduces new speculative cascading techniques that combine the strengths of cascades and speculative decoding by implementing a deferral rule through speculative execution. Experimental results with Gemma and T5 models demonstrate that the proposed approach provides superior cost-quality trade-offs compared to traditional cascading and speculative decoding methods.

Controllable Safety Alignment: Inference-Time Adaptation to Diverse Safety Requirements
The paper introduces Controllable Safety Alignment (CoSA), a framework that enables large language models to adapt to diverse safety requirements using customizable "safety configs" provided at inference time, without requiring retraining. The proposed CoSAlign method and CoSA-Score evaluation protocol demonstrate improved model controllability, enhancing LLMs' ability to represent and adapt to pluralistic human values in various real-world scenarios.

Generalization v.s. Memorization: Tracing Language Models’ Capabilities Back to Pretraining Data
This study evaluates the extent to which large language models (LLMs) generalize to unseen tasks versus rely on memorization by introducing the concept of distributional memorization. Using the Pythia models and the Pile dataset, the research finds that memorization is more pronounced in simpler, knowledge-intensive tasks like factual question answering, while generalization becomes crucial for complex, reasoning-based tasks, offering insights into analyzing pretraining corpora.

The Semantic Hub Hypothesis: Language Models Share Semantic Representations Across Languages and Modalities
This paper proposes the "semantic hub hypothesis," suggesting that modern language models process diverse languages and modalities through a shared representation space, akin to a transmodal semantic hub in the brain, which brings semantically similar inputs together regardless of their modality or language. The study demonstrates that model representations for semantically equivalent cross-language inputs are similar in intermediate layers, and that interventions in this space can predictably influence outputs across different data types, indicating its active role in processing rather than being a byproduct of broad data training.

Precise Localization of Memories: A Fine-grained Neuron-level Knowledge Editing Technique for LLMs
This paper addresses the limitations of current knowledge editing methods in Large Language Models (LLMs) by introducing a Fine-grained Neuron-level Knowledge Editing (FiNE) approach. FiNE enhances the precision of knowledge localization and editing by targeting specific neurons, improving the reliability and performance of LLMs, as demonstrated by quantitative experiments.

DRESSing Up LLM: Efficient Stylized Question-Answering via Style Subspace Editing
DRESS introduces a novel method for generating stylized responses in large language models by editing representations within a style-relevant subspace, providing an effective alternative to prompting and fine-tuning. This lightweight, train-free approach maintains stylistic fidelity and semantic integrity, making it ideal for applications like NPC creation, with notable improvements demonstrated on new QA benchmark datasets.

Unhackable Temporal Reward for Scalable Video MLLMs
This paper addresses the "anti-scaling law" in video-processing MLLMs, where more data and larger models lead to poorer performance due to a phenomenon called "temporal hacking." By introducing the Temporal Perplexity (TPL) score and the Unhackable Temporal Rewarding (UTR) framework, the study offers a solution to improve temporal modeling quality and video comprehension, highlighting the necessity of aligning proxy rewards with true objectives in MLLM development.

Self-Improvement in Language Models: The Sharpening Mechanism
This paper explores the concept of "self-improvement" in language models, where models refine their own outputs without external feedback, through a process called "sharpening." By introducing a new statistical framework and analyzing two types of self-improvement algorithms—SFT-based and RLHF-based—the study finds that while SFT is optimal with sufficient initial model coverage, RLHF can surpass SFT by utilizing online exploration, thus offering foundational insights for designing self-improvement algorithms.

DARE the Extreme: Revisiting Delta-Parameter Pruning For Fine-Tuned Models
The paper addresses the issue of redundancy and increased response times in applications using multiple fine-tuned models by enhancing delta-parameter pruning with DAREx, which improves on the existing DARE method. DAREx introduces DAREx-q for better performance at high pruning rates and DAREx-L2 for effective delta regularization, showing significant gains and compatibility with standard fine-tuning techniques, thus providing a robust pipeline for selecting optimal DPP methods.

HeadMap: Locating and Enhancing Knowledge Circuits in LLMs
This paper investigates the intrinsic mechanisms of large language models (LLMs) by examining knowledge circuits and layer dependencies, introducing a layer-conditioned locating algorithm to identify crucial attention heads for specific tasks. The novel fine-tuning method called HeadMap is proposed, which enhances knowledge flow by mapping critical head activations to the residual stream, significantly improving model performance with efficient parameter adjustments, as demonstrated through extensive experiments.

Enhancing Document Understanding with Group Position Embedding: A Novel Approach to Incorporate Layout Information
This paper presents Group Position Embedding (GPE), a novel technique that enhances the layout understanding capabilities of large language models (LLMs) without requiring architectural changes or additional pre-training. By grouping attention heads and using distinct positional embeddings, GPE effectively integrates layout information, demonstrating significant improvements in document understanding across various benchmarks, including the new BLADE benchmark.

Forewarned is Forearmed:  Harnessing LLMs for Data Synthesis via Failure-induced Exploration
ReverseGen is a novel approach designed to automatically generate training samples that expose the weaknesses of large language models (LLMs) by creating queries that lead the models to produce unsatisfactory responses. This method enhances model performance by addressing shortcomings in key applications like safety, honesty, and math and outperforms models trained on traditional human-annotated or general model-generated data.

Automatic Curriculum Expert Iteration for Reliable LLM Reasoning
This paper introduces Automatic Curriculum Expert Iteration (Auto-CEI) to address hallucinations and laziness in LLM reasoning by enhancing the model's ability to assertively handle tasks within its capabilities and appropriately decline when limits are exceeded. Auto-CEI improves robustness and extends reasoning capability across logical reasoning, mathematics, and planning tasks, achieving better alignment than existing state-of-the-art methods by balancing assertiveness and conservativeness.

Generalizing Reasoning Problems to Longer Lengths
This paper addresses the challenge of length generalization (LG) in learning to reason, where models struggle with problems larger than those seen in training. It introduces a theorem identifying the root cause of LG and proposes a class of reasoning problems for which LG can be theoretically guaranteed with Transformers, given that the chain-of-thought (CoT) schemes adhere to a condition called $(n,r)$-consistency.

Bridging and Modeling Correlations in Pairwise Data for Direct Preference Optimization
The paper introduces a novel framework called BMC to improve Direct Preference Optimization (DPO) by enhancing correlations in pairwise preference data and synthesizing pseudo-winning responses for better alignment of language models with human desires. Through dynamic token-level correlation learning and targeted modifications, the approach significantly outperforms existing baselines in various tasks, while providing insights into its effectiveness and adaptability to other DPO variants.

Do LLMs have Consistent Values?
This paper investigates the alignment of large language models (LLMs) with human-like value correlations, finding that typical prompting methods fall short. By introducing and analyzing a new prompting strategy called "Value Anchoring," the study significantly improves the alignment of value correlations with human data, offering new methodologies for evaluating consistency and human-likeness in LLM outputs.

Quantum-PEFT: Ultra parameter-efficient fine-tuning
This paper introduces Quantum-PEFT, a novel approach to parameter-efficient fine-tuning that utilizes quantum computations for full-rank yet parameter-efficient tuning via a quantum unitary parameterization. Using Pauli parameterization, Quantum-PEFT significantly reduces the number of trainable parameters compared to traditional methods like LoRA, achieving high parameter efficiency and competitive performance in language and vision transfer learning benchmarks.

Calibrating Expressions of Certainty
This paper introduces a novel approach to calibrate linguistic expressions of certainty by modeling uncertainty as distributions over the simplex, offering a more nuanced representation than previous single-score approaches. It also presents a generalized measure of miscalibration and a new post-hoc calibration method, providing insights into improving the calibration of both humans and computational models.

Round and Round We Go! What makes Rotary Positional Encodings useful?
This paper investigates the use of Rotary Positional Encodings (RoPE) in Transformer-based Large Language Models, particularly focusing on the Gemma 7B model, and argues against the common belief that RoPE primarily helps decay token dependency with increased relative distance. By analyzing RoPE at a mechanical level, the study uncovers that RoPE supports constructing robust attention patterns using high frequencies, while low frequencies tend to carry semantic information, leading to a proposed modification that addresses identified issues and enhances model performance.

Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA
This paper revisits "layer tying" as a form of parameter sharing in Transformers and introduces Recursive Transformers, which efficiently share parameters across layers to minimize size and cost without significant performance loss. By implementing Relaxed Recursive Transformers and Continuous Depth-wise Batching, the study demonstrates improved performance over similarly-sized models and outlines a new inference paradigm that significantly enhances inference throughput.

Scaling FP8 training to trillion-token LLMs
This paper presents the first successful training of large language models using FP8 precision on datasets up to 2 trillion tokens, revealing previously unobserved instabilities attributed to outlier amplification by the SwiGLU activation function. The researchers introduce Smooth-SwiGLU to ensure stable FP8 training and demonstrate FP8 quantization of Adam optimizer moments, leading to a 7B parameter model that achieves similar performance to BF16 baselines with a significant throughput improvement.

Beyond Linear Approximations: A Novel Pruning Approach for Attention Matrix
This paper presents a novel approach to pruning Large Language Models by directly optimizing the attention matrix to handle the non-linear nature of the Softmax attention mechanism, improving performance over existing methods. The proposed method not only provides theoretical guarantees but also achieves substantial computational cost reductions, enabling more efficient deployment on edge devices.

Improving Reasoning Performance in Large Language Models via Representation Engineering
This paper introduces a representation engineering approach to enhance the reasoning capabilities of large language models (LLMs) by applying control vectors derived from model activations as an inference-time intervention. The method is demonstrated on various reasoning tasks, showing improved performance without additional training, and the authors release the code for deriving control vectors and analyzing model representations.

Is Factuality Enhancement a Free Lunch For LLMs? Better Factuality Can Lead to Worse Context-Faithfulness
This paper examines the trade-off between context-faithfulness and factual accuracy in large language models (LLMs) caused by current factuality enhancement methods. The authors find that while these methods can inconsistently improve factual accuracy, they significantly undermine context-faithfulness—with declines up to 69.7%—and recommend further research to minimize this trade-off.

Language Models Learn to Mislead Humans via RLHF
The paper investigates a phenomenon termed "U-Sophistry," where language models fine-tuned with Reinforcement Learning from Human Feedback (RLHF) become more convincing to humans, even when wrong, highlighting a potential failure mode in the RLHF process. The study demonstrates that RLHF-adjusted models perform better at misleading human evaluators while failing to improve task accuracy, emphasizing the need for further research in improving human alignment and evaluation methods.

MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions
This paper introduces MA-RLHF, an advanced reinforcement learning framework that incorporates macro actions to improve the efficiency of reinforcement learning from human feedback (RLHF) by addressing the credit assignment problem in language models. Experimental results show MA-RLHF significantly outperforms traditional RLHF approaches, achieving up to 30% improvements in certain tasks and accelerating training convergence by 1.7 to 2 times.

Triples as the Key: Structuring Makes Decomposition and Verification Easier in LLM-based TableQA
This paper addresses challenges in TableQA tasks by introducing a Triple-Inspired Decomposition and Verification (TIDE) strategy that enhances question decomposition and answer verification using the structural properties of triples. TIDE improves critical detail retention and simplifies validation, achieving state-of-the-art performance across multiple datasets.

### Deep Learning->Other Representation Learning
A Differentiable Rank-Based Objective for Better Feature Learning
This paper introduces difFOCI, a parametric, differentiable approximation of the Feature Ordering by Conditional Independence (FOCI) method, enhancing feature learning by allowing variable selection, neural network parametrization, and improved regularization in machine learning. By applying difFOCI across various scenarios, including fairness in classifications without sensitive data, the study demonstrates its adaptability and effectiveness in managing spurious correlations and enhancing performance in complex machine learning tasks.

TULIP: Token-length Upgraded CLIP
The paper presents TULIP, a method to enhance vision-language models like CLIP by enabling them to handle captions longer than the standard 77 tokens through the use of relative position encodings and a specialized training process. By successfully encoding extended captions, TULIP surpasses existing models in cross-modal tasks, notably retrieval and text-to-image generation, and its implementation is available online.

Multiple Heads are Better than One: Mixture of Modality Knowledge Experts for Entity Representation Learning
This paper introduces MOMOK, a novel framework that enhances multi-modal knowledge graph (MMKG) representation learning by utilizing Mixture of Modality Knowledge experts to effectively model both structural and multi-modal entity features. The approach achieves superior MMKG completion by integrating relation-aware modality embeddings and minimizing mutual information between experts, with experiments showing its outstanding performance on four public MMKG benchmarks.

CAX: Cellular Automata Accelerated in JAX
This paper presents CAX (Cellular Automata Accelerated in JAX), an open-source library that significantly enhances research in cellular automata by providing hardware-accelerated performance and a flexible, modular design. CAX not only speeds up simulations considerably, fostering new research opportunities, but also demonstrates its versatility in diverse applications, including a one-dimensional cellular automaton surpassing GPT-4 in the 1D-ARC challenge.

JPEG Inspired Deep Learning
JPEG-DL is a novel deep learning framework that integrates a trainable JPEG compression layer into any DNN architecture, employing a differentiable soft quantizer to enable joint training of the compression and the network. The framework demonstrates significant performance improvements, including up to 20.9% enhanced prediction accuracy and increased robustness against adversarial attacks, compared to standard deep learning approaches.

Improving Neural Network Accuracy by Concurrently Training with a Twin Network
This paper investigates the effectiveness of Twin Network Augmentation (TNA) in enhancing validation accuracy for Spiking Neural Networks by training two networks together and aligning their logits. Through extensive testing on various CNN benchmarks, it demonstrates that TNA's success is due to its training methodology rather than a mere increase in trainable parameters, and highlights the superior representations learned by networks using this method.

Schur's Positive-Definite Network: Deep Learning in the SPD cone with structure
This paper introduces SpodNet, a novel learning module that guarantees symmetric positive-definite (SPD) outputs while supporting additional structural constraints such as element-wise sparsity. SpodNet effectively addresses challenges in designing neural architectures for SPD matrix estimation, demonstrating versatility and improved performance in tasks requiring joint learning of SPD and sparse matrices.

Weighted Point Set Embedding for Multimodal Contrastive Learning Toward Optimal Similarity Metric
This paper proposes using weighted point sets for multimodal contrastive learning to better capture the relationship and similarity structure between instances, compared to traditional one-point representations. The authors theoretically demonstrate the benefits of this approach through symmetric InfoNCE analysis, showing its ability to achieve optimal similarity and improved performance in downstream classification tasks, supported by experiments on text-image representation models and benchmarks.

### Deep Learning->Robustness
Aligning Visual Contrastive learning models via Preference Optimization
This paper presents a novel method for training contrastive learning models using Preference Optimization techniques to better align model behavior with desired human preferences. The approach improves model robustness against typographic attacks and biases in contrastive vision-language models, outperforming standard techniques and enhancing fairness and accuracy across various tasks.

Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning
This paper addresses the vulnerability of Offline Reinforcement Learning (RL) to poisoning attacks by extending certified defenses to enhance robustness against adversarial manipulation. Leveraging Differential Privacy, the proposed approach significantly improves the ability to maintain performance, even with poisoned data, and demonstrates the potential to enhance safety and reliability across various environments.

MixMax: Distributional Robustness in Function Space via Optimal Data Mixtures
This paper introduces MixMax, a method that reparameterizes group distributionally robust optimization (DRO) from parameter space to function space, allowing the problem to be framed as a simpler convex optimization followed by risk minimization. Experiments demonstrated that MixMax outperforms standard group DRO baselines, particularly enhancing the performance of XGBoost on datasets with variations of ACSIncome and CelebA annotations.

### Deep Learning->Self-Supervised Learning
COPER: Correlation-based Permutations for Multi-View Clustering
This paper introduces an end-to-end deep learning-based framework for multi-view clustering applicable to general data types, improving upon existing methods that are domain-specific or computationally intensive. The proposed model effectively generates fused representations through a novel permutation-based canonical correlation objective, with theoretical support and robust empirical results across ten benchmark datasets demonstrating its superior performance.

The "Law'' of the Unconscious Contrastive Learner: Probabilistic Alignment of Unpaired Modalities
This paper proves that contrastive embedding spaces, when applied under certain assumptions, can facilitate inferences over unseen modality pairs like audio and text by aligning them with existing paired modalities. The analysis extends the understanding of the geometry and probabilistic interpretation of contrastive representations and suggests new applications in settings with pre-trained models and language ambiguity in reinforcement learning, supported by numerical experiments.

### Deep Learning->Sequential Models, Time series
Shedding Light on Time Series Classification using Interpretability Gated Networks
This paper presents InterpGN, a framework that combines an interpretable model with a deep neural network to enhance both interpretability and classification performance in time-series data. By utilizing shapelets and a novel gating function based on the confidence of the interpretable expert, the model achieves comparable results to state-of-the-art deep learning models and improves shapelet quality and interpretability, showing applicability to real-world tasks beyond standard benchmarks.

### Deep Learning->Theory
Implicit Bias of Mirror Flow for Shallow Neural Networks in Univariate Regression
This paper investigates the implicit bias of mirror flow in wide and shallow neural networks for least squares error regression, showing that it shares the same lazy training behavior and implicit bias as ordinary gradient flow under certain conditions. Notably, it introduces the concept of scaled potentials, revealing that such potentials induce a diverse range of biases in neural networks, affecting the penalization of function curvature based on parameter initialization and offering a richer understanding beyond what is captured by RKHS norms.

Three Mechanisms of Feature Learning in a Linear Network
This paper provides an exact solution for the learning dynamics of a one-hidden-layer linear network with one-dimensional data, demonstrating both kernel and feature learning phases at any finite width. The study introduces three novel mechanisms in the feature learning regime and supports these theoretical findings with empirical evidence, improving our understanding and guiding effective training strategies for neural networks.

How Two-Layer Neural Networks Learn, One (Giant) Step at a Time
This paper examines how a two-layer neural network adapts to the target function's structure through few large batch gradient descent steps, improving its approximation capability beyond initial configurations for high-dimensional Gaussian data. The study reveals key insights into batch size requirements for effective learning directions and demonstrates the network's enhanced approximation capacity and generalization error, highlighting the interaction between task structure, algorithm specifics, and network architecture in learning complex tasks over time.

Prediction Risk and Estimation Risk of the Ridgeless Least Squares Estimator under General Assumptions on Regression Errors
This paper extends the analysis of minimum $\ell_2$ norm interpolation least squares estimators by considering more realistic regression error structures, such as clustered or serial dependence. The study reveals that overparameterization can be advantageous in these settings, including time series, panel, and grouped data, by linking estimation challenges to the trace of the variance-covariance matrix of regression errors.

### Misc
HyperPLR: Hypergraph Generation through Projection, Learning, and Reconstruction
The paper introduces **HyperPLR**, a novel hypergraph generative model designed to accurately replicate complex group interactions by transforming hypergraphs into weighted graphs, learning their structures in a latent space, and reconstructing them through a clique cover problem. Evaluated on real-world datasets, HyperPLR demonstrates superior performance, reinforcing its effectiveness in capturing higher-order network patterns beyond traditional approaches.

Atlas Gaussians Diffusion for 3D Generation
This paper introduces Atlas Gaussians, a novel representation for native 3D generation that uses the latent diffusion model to efficiently and effectively link latent space with 3D space. By representing a shape as a union of local patches and using UV-based sampling, the approach generates a high number of 3D Gaussian points to create detailed 3D models, outperforming previous methods in feed-forward native 3D generation.

Is Your Model Really A Good Math Reasoner? Evaluating Mathematical Reasoning with Checklist
This paper introduces MathCheck, a comprehensive checklist designed to evaluate the mathematical reasoning abilities and task generalization of large language models (LLMs) more robustly than existing benchmarks. By implementing MathCheck-GSM and MathCheck-GEO, the authors assess the mathematical textual reasoning and multi-modal reasoning capabilities of 26 LLMs and 17 multi-modal LLMs, demonstrating MathCheck's superior ability to reflect true mathematical intelligence and facilitate informative behavior analysis.

RouteLLM: Learning to Route LLMs from Preference Data
This paper introduces a training framework for efficient router models that dynamically select between stronger and weaker large language models (LLMs) during inference, addressing the balance between performance and cost. Evaluations demonstrate that the approach can more than halve costs without sacrificing quality and maintains strong generalization even for LLMs not included in training, highlighting its potential for cost-effective, high-performance LLM solutions.

ECHOPulse: ECG Controlled Echocardio-gram Video Generation
ECHOPulse is an innovative ECG-conditioned model for ECHO video generation that addresses issues of computational costs and reliance on expert annotations by using VQ-VAE tokenization and masked visual token modeling for efficient decoding. By leveraging readily available ECG signals, ECHOPulse improves synthetic ECHO data generation for disease monitoring and prediction, achieving state-of-the-art performance and demonstrating generalizability to other modalities like cardiac MRI, fMRI, and 3D CT.

Model-based Offline Reinforcement Learning with Lower Expectile Q-Learning
This paper introduces Lower Expectile Q-learning (LEQ), a novel model-based offline reinforcement learning method that enhances low-bias model-based value estimation through lower expectile regression of $\lambda$-returns. LEQ demonstrates superior performance compared to previous model-based methods and matches or surpasses state-of-the-art approaches in long-horizon and dense-reward tasks across diverse domains, with ablation studies confirming the importance of its core components.

Non-Adversarial Inverse Reinforcement Learning via Successor Feature Matching
This paper introduces a novel non-adversarial approach to inverse reinforcement learning (IRL) using direct policy search, exploiting a linear factorization of the return for policy gradient descent. The method efficiently learns from minimal expert demonstrations without requiring an explicit reward function, outperforming traditional approaches and succeeding in state-only settings.

BirdSet: A Large-Scale Dataset for Audio Classification in Avian Bioacoustics
The paper introduces BirdSet, a large-scale benchmark dataset for audio classification with a focus on avian bioacoustics, offering over 6,800 recording hours from nearly 10,000 classes, thereby surpassing the capabilities of AudioSet. BirdSet supports various use cases, including multi-label classification and self-supervised learning, with six deep learning models benchmarked, and is hosted on Hugging Face for easy access along with a comprehensive codebase for reproducing results.

MAGE: Model-Level Graph Neural Networks Explanations via Motif-based Graph Generation
Graph Neural Networks (GNNs) struggle with interpretability in molecular tasks due to traditional explanation methods failing to recognize key molecular substructures. To overcome this, we propose MAGE, a motif-based explainer that enhances interpretability by utilizing motifs as foundational units, validated through tests on six real-world molecular datasets, producing human-understandable explanations that incorporate essential molecular structure elements.

Neural Multi-Objective Combinatorial Optimization via Graph-Image Multimodal Fusion
The paper introduces a graph-image multimodal fusion (GIMF) framework designed to enhance neural multi-objective combinatorial optimization (MOCO) methods by integrating both graph and image information, thus addressing the limitations of existing techniques that rely solely on graph-modal data. The proposed framework, which includes innovative strategies such as a coordinate image, adaptive resolution, and a multimodal fusion mechanism, demonstrates superior performance and generalization capabilities over traditional methods in experimental evaluations on classic MOCO problems.

AstroCompress: A benchmark dataset for multi-purpose compression of astronomical data
This paper introduces AstroCompress, a neural compression challenge designed for astrophysics data, which includes four new datasets alongside a legacy dataset, all featuring 16-bit unsigned integer imaging data in various modes such as space-based and ground-based imaging. By benchmarking seven lossless compression methods, the results suggest that neural compression techniques can significantly improve data collection at astronomical observatories, potentially leading to billions of dollars in added scientific capability on existing instruments.

Have the VLMs Lost Confidence? A Study of Sycophancy in VLMs
This paper extends the investigation of sycophancy, a misleading behavior in LLMs where models agree with incorrect user inputs, to visual language models (VLMs) by introducing the MM-SY benchmark. It presents methods using synthetic datasets, prompts, and fine-tuning to successfully reduce sycophancy in VLMs, finding that improving image attention in higher model layers is essential for mitigating this problem.

PaCA: Partial Connection Adaptation for Efficient Fine-Tuning
PaCA introduces a method to enhance fine-tuning of large neural network models by tuning randomly selected partial connections within pretrained weights, eliminating the latency and memory issues associated with traditional parameter-efficient fine-tuning algorithms like LoRA. This approach reduces training time by 22% and memory usage by 16% while maintaining accuracy, and also supports longer sequences and improved throughput, demonstrating significant advancements in efficient model fine-tuning.

VEDIT: Latent Prediction Architecture For Procedural Video Representation Learning
This paper demonstrates that using a strong off-the-shelf frozen pretrained visual encoder and a well-designed prediction model achieves state-of-the-art performance in forecasting and procedural planning without the need for pretraining the prediction model or additional language or ASR supervision. By leveraging latent embedding spaces from vision encoders and recent advances in diffusion transformers, the method significantly surpasses existing baselines across multiple procedural learning tasks and datasets, achieving notable improvements in action anticipation, step forecasting, task classification, and procedure planning metrics.

Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models
This paper introduces AutoIF, a method for automatically generating high-quality instruction-following training data for large language models (LLMs) without manual annotation, by transforming data validation into code verification. AutoIF notably improves performance in instruction-following across multiple benchmarks and training algorithms, showcasing superior generalization and alignment abilities, achieving over 90% accuracy in IFEval's loose instruction accuracy, all without compromising the models' general, math, and coding skills.

Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise Sufficient Reasons
This paper introduces *sufficient subset training* (SST), a self-supervised approach designed to efficiently generate minimal sufficient reasons for model predictions. SST overcomes the computational challenges and out-of-distribution sampling issues of existing methods, producing concise, meaningful feature subsets with maintained predictive performance.

AdaManip: Adaptive Articulated Object Manipulation Environments and Policy Learning
This paper introduces a novel articulated object manipulation environment with 9 categories of objects to address the limitations of existing datasets and simulation environments, which often focus on simpler mechanisms. By proposing an adaptive demonstration collection and a 3D visual diffusion-based imitation learning pipeline, the study demonstrates the effectiveness of learning adaptive manipulation policies through both simulation and real-world experiments.

Noise Separation guided Candidate Label Reconstruction for Noisy Partial Label Learning
This paper addresses the noisy partial label learning (NPLL) problem, where the correct label may not be included in the candidate label set. By proving that the classifier's generalization error is bounded by the noise rate and average length of candidate labels, the authors propose a novel framework to separate noisy samples and reconstruct the label sets, achieving significant improvements in classification accuracy on benchmark datasets.

Flow: Modularized Agentic Workflow Automation
This paper presents a multi-agent framework utilizing large language models to achieve efficient task execution by dynamically adjusting workflows, defined as activity-on-vertex (AOV) graphs, in response to real-time changes and unforeseen challenges. By focusing on modular design and evaluating parallelism and dependency complexity, the framework enhances concurrent execution, goal achievement, and error tolerance, with empirical evidence showing significant efficiency improvements across various practical tasks.

Tighter Privacy Auditing of DP-SGD in the Hidden State Threat Model
This paper examines the privacy guarantees of machine learning models trained with differentially private optimizers under a threat model where adversaries only have access to the final model. By developing a method to audit privacy loss using crafted gradient sequences, the study reveals that when gradients are consistently inserted at each optimization step, intermediate updates offer no additional privacy protection, and highlights scenarios where current theoretical upper bounds on privacy could be improved.

Improving the Sparse Structure Learning of Spiking Neural Networks from the View of Compression Efficiency
This paper introduces a novel two-stage dynamic structure learning approach for Spiking Neural Networks (SNNs) to enhance sparse training and optimize compression efficiency from scratch. By adaptively determining and adjusting the rewiring ratio for synaptic connections, the method improves the exploration of sparse structures, aligns with current deep SNN performance, and supports efficient implementation on neuromorphic hardware for Edge AI.

Halton Scheduler for Masked Generative Image Transformer
This paper introduces a new sampling strategy for Masked Generative Image Transformers (MaskGIT) using a Halton sequence, which improves token unmasking by spreading tokens spatially and uniformly during image generation. By reducing non-recoverable sampling errors and simplifying hyper-parameter tuning, the proposed Halton scheduler enhances image quality and diversity, outperforming the original Confidence scheduler in class-to-image synthesis on ImageNet and text-to-image generation on the COCO dataset.

Language-Assisted Feature Transformation for Anomaly Detection
This paper presents Language-Assisted Feature Transformation (LAFT), a new method for anomaly detection that incorporates user knowledge and preferences using natural language. By leveraging the shared image-text embedding space of vision-language models, LAFT aligns visual features with user-defined requirements to enhance the detection of specific anomalies, and its effectiveness is demonstrated through extensive experiments on various datasets.

Causally Motivated Sycophancy Mitigation for Large Language Models
This paper addresses the issue of sycophancy in large language models (LLMs), where models overly align with user preferences at the expense of output correctness. By utilizing structured causal models (SCMs), the authors introduce a framework called **CAUSM** to reduce sycophancy by removing spurious correlations within LLMs, demonstrating improved performance across various language tasks.

Local Loss Optimization in the Infinite Width: Stable Parameterization of Predictive Coding Networks and Target Propagation
The paper investigates local learning as an alternative to backpropagation by introducing maximal update parameterization ($\mu$P) for predictive coding (PC) and target propagation (TP) in the infinite-width limit. The study reveals that $\mu$P enables hyperparameter transfer across models and identifies distinct gradient behaviors in PC and preference for feature learning in TP, providing valuable theoretical insights into local learning dynamics.

Perplexity Trap: PLM-Based Retrievers Overrate Low Perplexity Documents
This paper investigates the source bias in PLM-based retrieval models that preferentially rank LLM-generated content higher due to learning perplexity features, which can adversely affect the information access ecosystem. It introduces a causal-inspired inference-time debiasing method, CDC, which effectively addresses source bias, as demonstrated by experimental results across multiple domains.

CarbonSense: A Multimodal Dataset and Baseline for Carbon Flux Modelling
This paper introduces CarbonSense, the first comprehensive, machine learning-ready dataset designed to advance data-driven carbon flux modelling (DDCFM) by integrating measured carbon fluxes, meteorological predictors, and satellite imagery from 385 global locations. By also providing a baseline model and a novel transformer-based model, the paper highlights the potential of multimodal deep learning techniques in improving carbon flux predictions and encourages further advancements in the field.

Causal Graphical Models for Vision-Language Compositional Understanding
This paper introduces a novel approach to improve Vision-Language Models (VLMs) in understanding compositional language by using a Causal Graphical Model (CGM) to model dependencies between textual and visual tokens. The proposed method significantly outperforms current state-of-the-art compositional techniques on five benchmarks by focusing on genuine causal dependencies, demonstrating superior performance even compared to methods trained on larger datasets.

Geometry of Lightning Self-Attention: Identifiability and Dimension
This paper explores the geometry of function spaces defined by self-attention networks without normalization using algebraic geometry, focusing on the identifiability of deep attention and dimension computation of the function space. It also characterizes singular and boundary points for a single-layer model, extends results to normalized self-attention networks, and provides both a theoretical conjecture and numerical verification for deep cases.

Optimal Transport for Time Series Imputation
This paper introduces the Proximal Spectrum Wasserstein (PSW) discrepancy, a novel method designed to enhance missing data imputation in time-series datasets by effectively capturing temporal patterns and accommodating non-stationarity through optimal transport. The PSW for Imputation (PSW-I) framework leverages this discrepancy to iteratively improve imputation results, demonstrating superior performance in experiments compared to existing time-series imputation methods.

Discovering Influential Neuron Path in Vision Transformers
This paper addresses the opacity of Vision Transformer models by exploring influential neuron paths that significantly impact model inference. By introducing a joint influence measure and a layer-progressive neuron locating approach, the study efficiently reveals crucial neuron paths, demonstrating their role in model capability for tasks like image classification and potential real-world applications such as model pruning.

CHAMP: Conformalized 3D Human Multi-Hypothesis Pose Estimators
CHAMP is a novel method for learning sequence-to-sequence, multi-hypothesis 3D human poses from 2D keypoints using a diffusion model to generate and aggregate multiple 3D pose hypotheses. The method integrates conformal prediction into the learning process, resulting in a state-of-the-art performance with probabilistic guarantees across various metrics and datasets.

Neural Causal Graph for Interpretable and Intervenable Classification
This paper introduces the Neural Causal Graph (NCG) framework, which combines causal inference with neural networks to improve model interpretability and enable interactive reasoning with human users. By enhancing traditional classification models, NCG achieves high accuracy on benchmarks and facilitates human-AI interactions, thus improving transparency and applicability in practical applications.

Personalized Representation from Personalized Generation
This paper explores the use of personalized synthetic data for learning personalized representations in vision models, addressing the challenge of fine-grained, data-scarce tasks. By proposing a contrastive learning approach leveraging image generators and introducing an evaluation suite, the study demonstrates improved performance in diverse downstream tasks such as recognition and segmentation.

Grounding Multimodal Large Language Model in GUI World
This paper introduces a GUI grounding framework featuring an automated data collection engine and a flexible grounding module for improved localization of UI elements, enhancing task automation on digital platforms. Through integration with MLLMs and validation against benchmarks like ScreenSpot and MiniWob, the approach significantly boosts task accuracy and adaptability in GUI tasks.

CONTRA: Conformal Prediction Region via Normalizing Flow Transformation
CONTRA introduces a novel approach to conformal prediction by leveraging normalizing flow transformations to create more precise multi-dimensional prediction regions from latent space, overcoming limitations of traditional one-dimensional nonconformity scores. Both CONTRA and its extension enhance predictive models with reliable coverage and superior accuracy in defining prediction regions, establishing a new standard for conditional density estimation across diverse datasets.

Peeking Behind Closed Doors: Risks of LLM Evaluation by Private Data Curators
The paper discusses the rise of private evaluations in testing large language models (LLMs), highlighting the potential financial and evaluation risks they pose due to conflicts of interest and inherent biases from private data curators and annotators. It emphasizes the need for community discussions and policy changes to address these issues and ensure fair and unbiased model evaluations.

Measuring memorization in RLHF for code completion
This paper investigates how training data memorization occurs in reinforcement learning with human feedback (RLHF) and direct preference learning methods, particularly in the context of code completion models. The study concludes that RLHF reduces the likelihood of training data memorization compared to fine-tuning and direct preference learning, making it a safer option for aligning large language models with human preferences while minimizing privacy risks.

Everything is Editable: Extend Knowledge Editing to Unstructured Data in Large Language Models
This paper introduces Unstructured Knowledge Editing (UnKE), a novel method designed to address editing challenges in unstructured knowledge formats found in large language models. By implementing non-local block key-value storage and cause-driven optimization, UnKE outperforms existing methods on both newly proposed and traditional datasets, demonstrating superior batch and sequential editing capabilities.

MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language Models
This paper introduces MMed-RAG, a versatile multimodal retrieval-augmented generation system designed to improve the factual accuracy of Medical Large Vision-Language Models (Med-LVLMs) in diagnostic tools. By incorporating a domain-aware retrieval mechanism, adaptive context selection, and a preference fine-tuning strategy, the system achieves a 43.8% average improvement in factual accuracy across diverse medical datasets.

CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale
This paper introduces a novel multi-modal approach for biodiversity measurement by integrating photographic images and DNA using CLIP-style contrastive learning, aligning them with text-based taxonomic labels in a unified embedding space. This method improves classification accuracy of insect species by over 8% compared to single-modality models, effectively enhancing zero-shot learning tasks without requiring task-specific fine-tuning, thus advancing biodiversity studies.

Generative Flows on Synthetic Pathway for Drug Design
RxnFlow is a generative model designed for drug discovery that focuses on synthesizability by assembling molecules through predefined building blocks and reaction templates, thereby constraining the synthetic pathway. It demonstrates superior performance compared to existing models in pocket-specific optimization and achieves state-of-the-art results in pocket-conditional generation, while allowing flexibility in adapting to new objectives and building blocks without retraining.

UIFace: Unleashing Inherent Model Capabilities to Enhance Intra-Class Diversity in Synthetic Face Recognition
The paper introduces UIFace, a novel framework designed to enhance intra-class diversity for synthetic face recognition by utilizing a diffusion model with a two-stage denoising strategy that balances identity-preservation and diversity. This approach, which includes attention injection for improved variations, outperforms existing methods using less data and achieves results comparable to models trained on real datasets when scaling synthetic identities.

Matcha: Mitigating Graph Structure Shifts with Test-Time Adaptation
Graph neural networks (GNNs) struggle with distribution shifts, particularly structure shifts, which affect node connectivity and degrade node representations. The proposed framework, Matcha, effectively adapts GNNs to structure shifts by adjusting aggregation parameters and integrating prediction-informed clustering to maintain distinct node categories, improving performance under combined structure and attribute shifts.

AutoBencher: Towards Declarative Benchmark Construction
AutoBencher is a declarative framework designed to automatically construct benchmarks, allowing for scalable discovery of insights and vulnerabilities in language models. By operationalizing benchmark desiderata and utilizing a language model to refine dataset descriptions, AutoBencher identifies datasets that elicit more model errors and exposes specific knowledge gaps and safety vulnerabilities not captured by existing benchmarks.

Distribution Backtracking Builds A Faster Convergence Trajectory for Diffusion Distillation
The paper introduces DisBack, a novel score distillation method that addresses the sampling speed challenge in diffusion models by leveraging the full convergence trajectory between the student generator and the teacher model. By incorporating two stages, Degradation Recording and Distribution Backtracking, DisBack improves convergence speed and model performance, achieving an FID score of 1.38 on ImageNet 64×64, and can be easily integrated into existing methods for enhanced results.

Token Statistics Transformer: Linear-Time Attention via Variational Rate Reduction
This paper introduces a novel transformer attention operator called Token Statistics Self-Attention (TSSA), which scales linearly with the number of tokens, significantly reducing the computational complexity associated with traditional transformer models. The proposed Token Statistics Transformer (ToST) achieves competitive performance across vision, language, and long sequence tasks while challenging the reliance on pairwise similarity mechanisms, enhancing efficiency and interpretability.

Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates
This paper reveals vulnerabilities in automatic LLM benchmarks, demonstrating that even a "null model" with irrelevant, constant outputs can achieve high win rates, thereby highlighting the possibility of gaming these evaluations. The study emphasizes the need for robust anti-cheating mechanisms to ensure the reliability of these benchmarks, as tactics to exploit them could be used unethically.

Scaling Diffusion Language Models via Adaptation from Autoregressive Models
Diffusion Language Models (DLMs) offer a novel approach to text generative modeling and have the potential to overcome limitations of autoregressive (AR) models, but they have not been extensively scaled or fairly benchmarked. This work proposes adapting open-source AR models into text diffusion models using a continual pre-training method, demonstrating competitive performance and releasing a suite of such models that excel in generating fluent text and in-context learning.

Learning Graph Invariance by Harnessing Spuriosity
This paper introduces $\texttt{LIRS}$, a novel framework that improves Out-of-Distribution (OOD) generalization in graph representation learning by removing spurious features from ERM-learned features. Unlike direct methods, $\texttt{LIRS}$ indirectly isolates and eliminates spurious features, achieving a more comprehensive set of invariant graph features and outperforming existing methods by up to 25.50% in competitive testing.

Chain-of-Focus Prompting: Leveraging Sequential Visual Cues to Prompt Large Autoregressive Vision Models
This paper introduces Chain-of-Focus (CoF) Prompting, a method inspired by Chain-of-Thought prompting from natural language processing, to enhance vision models through step-by-step visual comprehension. The proposed CoF Prompting method addresses the lack of logical structure in visual data, improves the effectiveness of Large Autoregressive Vision Models (LAVMs), and is validated by experiments showing improved performance in visual understanding tasks.

Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent Cooperation
This paper introduces Hierarchical and Separate Skill Discovery (HiSSD), a novel approach for offline multi-task multi-agent reinforcement learning (MARL), which enhances the generalization to unseen tasks by learning both common and task-specific skills. By incorporating cooperative temporal knowledge and task-guided fine-grained action execution, HiSSD demonstrates superior performance in experiments on multi-agent MuJoCo and SMAC benchmarks, effectively addressing current challenges in skill learning for policy transfer.

CATCH: Channel-Aware Multivariate Time Series Anomaly Detection via Frequency Patching
CATCH is a novel framework for anomaly detection in multivariate time series that enhances the detection of fine-grained frequency characteristics and channel correlations through frequency patching and a Channel Fusion Module (CFM). Extensive experiments demonstrate its state-of-the-art performance, and resources are available for public access.

Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning
This paper introduces a neuro-symbolic tactic generator that combines the mathematical intuition of large language models (LLMs) with symbolic methods to effectively generate proof steps in mathematical theorem proving. Focused specifically on solving Olympiad inequalities, the framework integrates human-like problem-solving techniques for efficient proof search and achieves state-of-the-art performance on 161 competition challenges, surpassing existing LLM and symbolic methods without needing extra training data.

IterComp: Iterative Composition-Aware Feedback Learning from Model Gallery for Text-to-Image Generation
IterComp is a novel framework designed to enhance compositional text-to-image generation by combining the strengths of multiple diffusion models in areas such as attribute binding and spatial relationships. Through iterative feedback learning and a curated composition-aware model preference dataset, IterComp significantly improves multi-category object composition and complex semantic alignment, opening new avenues in reward feedback learning for diffusion models.

Stochastic Polyak Step-sizes and Momentum: Convergence Guarantees and Practical Performance
This paper introduces new Polyak-type variants for the Stochastic Heavy Ball (SHB) method, aimed at improving the tuning of step-size and momentum parameters in large-scale stochastic optimization without excessive computational cost. The proposed variants, including MomSPSmax, MomDecSPS, and MomAdaSPS, offer convergence guarantees under various conditions, and experiments validate their effectiveness and robustness compared to existing methods.

Calibrating LLMs with Information-Theoretic Evidential Deep Learning
This paper addresses the overconfidence and poor calibration issues in fine-tuned large language models (LLMs) by enhancing Evidential Deep Learning (EDL) with an information bottleneck (IB). The proposed IB-EDL method effectively suppresses unnecessary information and improves uncertainty estimates, outperforming existing methods in experiments, thus making LLMs more reliable for applications requiring precise confidence calibration.

Swift Hydra:  Self-Reinforcing Generative Framework for Anomaly Detection with Multiple Mamba Models
This paper introduces Swift Hydra, a novel framework combining generative AI and reinforcement learning (RL) to enhance anomaly detection by generating synthetic anomalies that improve model robustness. The framework includes scalable Mamba models and demonstrates superior performance compared to existing methods, as evidenced by empirical evaluations on the ADBench benchmark.

Concept Bottleneck Language Models For Protein Design
The paper introduces Concept Bottleneck Protein Language Models (CB-pLM), a new generative masked language model with interpretable concept neurons, offering control, interpretability, and debugging capabilities. CB-pLM maintains performance comparable to traditional models while providing enhanced transparency and control over protein generation, making it a significant advancement for applications in drug discovery.

MVTokenFlow: High-quality 4D Content Generation using Multiview Token Flow
This paper introduces MVTokenFlow, a novel approach for creating high-quality 4D content from monocular videos by leveraging multiview diffusion models to ensure spatial and temporal consistency. MVTokenFlow achieves superior 4D field reconstruction quality by employing 2D flows to guide multiview image regeneration, demonstrating significant improvements over baseline methods.

Enhancing Prediction Performance through Influence Measure
This paper introduces an influence measure as a metric to evaluate the impact of training data on model performance, aiming to optimize the selection of data points for improved prediction accuracy. The proposed data selection method and dynamic active learning algorithm, both driven by the influence measure, show effectiveness through extensive simulations and experiments on real-world datasets.

Occlusion-aware Non-Rigid Point Cloud Registration via Unsupervised Neural Deformation Correntropy
This paper introduces an unsupervised method called Occlusion-Aware Registration (OAR) for non-rigidly aligning point clouds, using the adaptive correntropy function to handle occluded scenarios by treating individual points distinctly. The method combines unsupervised implicit neural representations with the maximum correntropy criterion to optimize deformation, achieving superior or competitive performance in occluded geometries and proving versatile in tasks like large deformations and shape completion.

Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo
This paper presents a novel architecture for controlled language model (LM) generation using a sequential Monte Carlo (SMC) framework, enabling the incorporation of domain-specific constraints during inference and efficient resource allocation. The approach allows small open-source language models to surpass models over eight times their size and closed-source models in tasks like Python code generation and molecule synthesis, highlighting improvements in approximating the posterior distribution.

Rethinking Visual Counterfactual Explanations Through Region Constraint
The paper introduces region-constrained visual counterfactual explanations (RVCEs) to improve the interpretability of image classifiers by only modifying predefined regions of an image, thus avoiding issues like confirmation bias present in current methods. The proposed Region-Constrained Counterfactual Schrödinger Bridge (RCSB) technique provides a novel approach to conditional inpainting, achieving new state-of-the-art results and allowing for user interaction by manually defining regions of interest for exact counterfactual reasoning.

BANGS: Game-theoretic Node Selection for Graph Self-Training
This paper introduces BANGS, a novel graph self-training framework that uses conditional mutual information for node selection to address the limitations of traditional pseudo-labeling strategies in graph neural networks (GNNs). By incorporating game theory, BANGS selects nodes combinatorially, enhancing performance and robustness across multiple datasets and settings, and surpassing existing methods.

A Generalist Hanabi Agent
This paper introduces Recurrent Replay Relevance Distributed DQN (R3D2), a novel multi-agent reinforcement learning agent for the Hanabi card game, capable of playing across all game settings and effectively cooperating with unfamiliar agents. By reformulating tasks using text to enhance transfer learning and employing a distributed MARL algorithm, R3D2 overcomes the limitations of existing agents, showcasing improved adaptability and collaborative capabilities in dynamic environments.

AIR-BENCH 2024: A Safety Benchmark based on Regulation and Policies Specified Risk Categories
AIR-BENCH 2024 is introduced as the first AI safety benchmark that aligns with emerging government regulations and company policies, offering a cohesive framework based on the AI Risks taxonomy, AIR 2024, which categorizes risks into a detailed four-tiered structure. By evaluating leading language models against 5,694 diverse prompts, AIR-BENCH 2024 uncovers insights into model alignment with safety concerns and provides a vital foundation for assessing AI safety across different jurisdictions.

Utility-Directed Conformal Prediction: A Decision-Aware Framework for Actionable Uncertainty Quantification
This paper introduces a framework that combines conformal prediction with decision-focused machine learning to create prediction sets that incorporate downstream decision loss functions, making them suitable for high-stakes decision-making. Empirical evaluations show the framework's superiority over standard methods by reducing decision loss and enhancing diagnosis coherence in healthcare applications, particularly in dermatology.

MMDT: Decoding the Trustworthiness and Safety of Multimodal Foundation Models
This paper introduces MMDT (Multimodal DecodingTrust), the first unified platform for comprehensive safety and trustworthiness evaluation of multimodal foundation models (MMFMs) across various perspectives such as safety, hallucination, fairness/bias, privacy, adversarial robustness, and out-of-distribution generalization. The study highlights identified vulnerabilities in existing models using challenging data and evaluation scenarios, offering a high-quality benchmark that paves the way for developing safer and more reliable MMFMs.

Scaling up the Banded Matrix Factorization Mechanism for Large Scale Differentially Private ML
This paper presents techniques to scale up DP-BandMF, a state-of-the-art correlated noise mechanism for differential privacy, enabling it to effectively manage large-scale training scenarios with over $10^6$ iterations and $10^9$ model parameters. The improved approach maintains utility without degradation in smaller-scale settings, overcoming previous scalability limitations.

Solving Token Gradient Conflict in Mixture-of-Experts for Large Vision-Language Model
This paper addresses the challenge of token interference in Mixture-of-Experts (MoE) models for Large Vision-Language Models (LVLMs) by introducing a method called Solving Token Gradient Conflict (STGC). By using token-level gradient analysis to identify and reroute conflicting tokens, the proposed regularization method effectively reduces interference within experts, enhancing performance while maintaining a lower inference cost.

A Distributional Approach to Uncertainty-Aware Preference Alignment Using Offline Demonstrations
This paper introduces Uncertainty-Aware Preference-based Reinforcement Learning (UA-PbRL), which addresses the challenge of designing reward functions by learning from human preferences in offline datasets. By employing a distributional reward model and risk-sensitive policy, UA-PbRL effectively manages uncertainty, enhancing performance in tasks such as robot control and language model alignment, with code accessible at the provided GitHub repository.

Generalization and Distributed Learning of GFlowNets
This paper introduces the first data-dependent generalization bounds for Generative Flow Networks (GFlowNets) and reveals the negative impact of state space size on their generalization performance using PAC-Bayesian inequalities. Additionally, the authors propose a novel distributed learning algorithm, Subgraph Asynchronous Learning (SAL), which enhances mode coverage and distribution matching by training multiple GFlowNets in parallel on subnetworks and aggregating them efficiently.

Language Agents Meet Causality -- Bridging LLMs and Causal World Models
This paper proposes a framework that integrates causal representation learning (CRL) with large language models (LLMs) to enhance causally-aware reasoning and planning. By learning a causal world model that links causal variables to natural language, the framework outperforms traditional LLM-based reasoners, particularly in complex environments and tasks involving longer planning horizons.

Rethinking Multiple-Instance Learning From Feature Space to Probability Space
This paper introduces a novel Probability-Space MIL network (PSMIL) to address representation learning challenges in multiple-instance learning (MIL) by transitioning the problem from feature space to probability space. By employing a self-training alignment strategy and probability-space attention pooling, PSMIL effectively mitigates selection drift issues, achieving notable accuracy improvements and competitive performance on benchmark datasets compared to state-of-the-art deep MIL models.

Recognize Any Surgical Object: Unleashing the Power of Weakly-Supervised Data
RASO is a foundation model developed to Recognize Any Surgical Object, excelling in open-set recognition across various surgical procedures using a weakly-supervised learning framework that extracts tag-image-text pairs from extensive unannotated surgical lecture videos. This methodology significantly reduces manual annotation efforts while demonstrating notable improvements in mean Average Precision (mAP) across multiple benchmarks and surpassing current state-of-the-art models in supervised surgical action recognition tasks, with plans to open-source the resources for the research community.

Can Knowledge Editing Really Correct Hallucinations?
Large Language Models (LLMs) often produce hallucinations or non-factual information, and while knowledge editing has emerged as a method to correct these without retraining, evaluating its effectiveness has been challenging. HalluEditBench is introduced as a comprehensive benchmarking tool, highlighting a vast dataset across multiple domains and dimensions, to assess and provide insights into the effectiveness and limitations of knowledge editing methods in mitigating hallucinations.

Growth Inhibitors for Suppressing Inappropriate Image Concepts in Diffusion Models
This paper addresses ethical and business risks in text-to-image diffusion models by identifying and correcting implicit unsafe prompts that lead to NSFW content and copyright issues. The proposed method uses growth inhibitors to guide inappropriate concept representation towards more suitable ones, achieving effective concept erasure without requiring fine-tuning and maintaining image quality and semantics.

Reasoning of Large Language Models over Knowledge Graphs with Super-Relations
The paper identifies limitations in current methods for reasoning over knowledge graphs with large language models, such as high non-retrieval rates due to greedy search and forward reasoning. It introduces the ReKnoS framework to address these challenges through super-relations for enhanced forward and backward reasoning, demonstrating an average accuracy gain of 2.92% over existing methods across nine datasets.

More Experts Than Galaxies: Conditionally-Overlapping Experts with Biologically-Inspired Fixed Routing
This paper introduces Conditionally Overlapping Mixture of ExperTs (COMET), a novel deep learning method that creates a modular and sparse neural network architecture through overlapping experts, addressing challenges in current sparse networks like representation collapse and redundancy. COMET utilizes a biologically inspired random projection to enhance learning efficiency and generalization by ensuring parameter sharing based on input similarity, demonstrating effectiveness across various tasks and architectures.

PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection
The paper introduces PointOBB-v2, an improved method for generating pseudo rotated boxes from points, which is simpler, faster, and more accurate than the previous PointOBB approach. By leveraging a Class Probability Map with Principal Component Analysis and employing a separation mechanism, PointOBB-v2 enhances training speeds and achieves notable accuracy improvements on various datasets, advancing single point supervised oriented object detection.

Equivariant Masked Position Prediction for Efficient Molecular Representation
This paper introduces Equivariant Masked Position Prediction (EMPP), a self-supervised approach designed to improve graph neural networks' ability to learn quantum mechanical features in computational chemistry, addressing data scarcity and enhancing generalization. EMPP outperforms traditional methods by accurately acquiring physical properties without approximating Gaussian mixture distributions, significantly boosting the performance of molecular architectures.

T-Stitch: Accelerating Sampling in Pre-Trained Diffusion Models with Trajectory Stitching
This paper introduces T-Stitch, a technique that improves sampling efficiency in diffusion probabilistic models (DPMs) by using a smaller model in initial steps before switching to a larger one, thus reducing computation without sacrificing image generation quality. T-Stitch is training-free, applicable to various architectures, and enhances existing sampling methods, demonstrated in experiments with models like DiT-XL and stable diffusion to achieve faster performance and better prompt alignment while maintaining quality.

Causal Reasoning and Large Language Models: Opening a New Frontier for Causality
This paper investigates the causal reasoning capabilities of large language models (LLMs), finding that they can generate correct causal arguments more effectively than current leading methods, particularly in tasks such as pairwise causal discovery and counterfactual reasoning. Despite their promise in aiding humans with setting up causal analyses, improving certain error modes and integrating LLMs with traditional causal techniques present areas for further research, given the models' limitations and their reliance on text rather than actual data.

SINGAPO: Single Image Controlled Generation of Articulated Parts in Objects
This paper proposes a novel method to generate 3D articulated objects from a single image, addressing the limitations of previous multi-view or coarse control approaches. By employing a diffusion model and a coarse-to-fine pipeline, the method significantly enhances realism, image resemblance, and reconstruction quality, outperforming existing techniques in creating detailed and accurate 3D models of household objects.

Topograph: An Efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation
This paper introduces a novel, graph-based framework for image segmentation that ensures topological accuracy while being computationally efficient and generally applicable across various use cases. By constructing a component graph and utilizing a strict topological metric, the method achieves state-of-the-art performance with up to fivefold faster loss computation compared to existing topology-aware methods, providing formal proofs of topological guarantees and empirical validation on binary and multi-class datasets.

SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training
This paper introduces SiMHand, a framework for pre-training 3D hand pose estimation using a large dataset of in-the-wild hand images, leveraging contrastive learning to improve accuracy. By embedding similar hand pose images closer in the feature space and adaptively weighting the contrastive learning loss, SiMHand outperforms existing methods with significant improvements over state-of-the-art benchmarks on datasets such as FreiHand, DexYCB, and AssemblyHands.

ThinK: Thinner Key Cache by Query-Driven Pruning
This paper introduces ThinK, a novel query-dependent KV cache pruning method that targets and reduces redundancy in the channel dimension of the KV cache in Large Language Models, significantly cutting memory costs while preserving model accuracy. Extensive evaluations demonstrate that ThinK achieves over 20% memory cost reduction and allows increased batch sizes, confirming its effectiveness across long-sequence datasets using models like LLaMA and Mistral.

Minimal Impact ControlNet: Advancing Multi-ControlNet Integration
This paper introduces Minimal Impact ControlNet to address conflicts in controllable image generation arising from multiple control signals influencing different parts of an image. By employing a balanced dataset, combining feature signals effectively, and adjusting the score function's Jacobian matrix, the proposed approach improves the compatibility of control signals, ensuring more harmonious and texture-rich image generation.

Logic-Logit: A Logic-Based Approach to Choice Modeling
This study introduces Logic-Logit, a novel rule-based interpretable choice model that effectively learns and explains human choices using heuristic decision rules. Combining column generation techniques and the Frank-Wolfe algorithm, the model enhances preference modeling by providing interpretable results and outperforming baseline models in accuracy across commercial and healthcare datasets.

A Quantum Circuit-Based Compression Perspective for Parameter-Efficient Learning
Quantum Parameter Adaptation (QPA) is introduced as a novel method within quantum parameter generation, using quantum neural networks to efficiently generate parameters for fine-tuning large language models, such as GPT-2 and Gemma-2. The approach significantly reduces parameter counts, achieving up to 83.16% reduction for Gemma-2 and 47.94% for GPT-2, while maintaining or slightly improving performance, demonstrating the feasibility of scalable quantum-classical solutions for LLM fine-tuning without the need for quantum hardware during inference.

Progressive distillation induces an implicit curriculum
Progressive distillation, where a student model learns from successive intermediate checkpoints of a teacher model, provides an implicit curriculum that accelerates learning and enhances sample complexity benefits. Through experiments on sparse parity, probabilistic context-free grammars (PCFGs), and datasets like Wikipedia and Books, the study demonstrates how progressive distillation helps the student model to gradually capture longer contextual features, emphasizing its advantages across various tasks.

Generalization in VAE and Diffusion Models: A Unified Information-Theoretic Analysis
This paper introduces a unified theoretical framework to enhance the understanding of the generalization performance of Diffusion Models (DMs) and Variational Autoencoders (VAEs) by treating encoders and generators as randomized mappings. The framework enables a refined analysis of VAEs, reveals a generalization trade-off for DMs related to diffusion time, and proposes data-based estimable bounds to optimize model performance, with empirical results supporting these theoretical insights.

Learning Efficient Positional Encodings with Graph Neural Networks
Positional encodings (PEs) are vital for enhancing graph representation learning, but current eigenvector-based methods often lack stability, expressive power, scalability, and genericness. Addressing these issues, PEARL is introduced as a novel framework that leverages GNN architectures and statistical pooling functions to create efficient and powerful learnable PEs, achieving high performance with significantly reduced complexity compared to traditional eigenvector-based methods.

SPA-BENCH: A COMPREHENSIVE BENCHMARK FOR SMARTPHONE AGENT EVALUATION
This paper introduces SPA-Bench, a comprehensive benchmark designed to evaluate Multimodal Large Language Model (MLLM)-based smartphone agents in a real-world interactive environment. SPA-Bench's notable contributions include a diverse task set in English and Chinese, a flexible plug-and-play framework for integrating agents with Android devices, and an innovative evaluation pipeline assessing agent performance, uncovering challenges and suggesting future research avenues for improved smartphone agent applications.

AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out Context Attribution
AttriBoT introduces innovative techniques to efficiently approximate the leave-one-out (LOO) error for context attribution in large language models, providing a 300x speedup compared to previous methods. By utilizing cached activations, hierarchical attribution, and proxy models, AttriBoT significantly reduces computation time and enables scalable interpretability for LLMs, fostering further advancements in efficient context attribution methods.

Mining your own secrets: Diffusion Classifier Scores for Continual Personalization of Text-to-Image Diffusion Models
This paper addresses the challenge of continual personalization in text-to-image diffusion models, where new concepts need to be learned sequentially without access to previous data due to storage or privacy issues. The authors propose utilizing diffusion classifier scores for regularization, demonstrating that their method surpasses state-of-the-art approaches like C-LoRA without incurring additional storage or parameter overhead.

DebGCD: Debiased Learning with Distribution Guidance for Generalized Category Discovery
This paper presents DebGCD, a novel framework for Generalized Category Discovery (GCD) that addresses the challenges of label bias and semantic distribution shifts in datasets with both labelled and unlabelled images. By leveraging a debiased learning approach with distribution guidance and implementing a curriculum learning strategy, DebGCD achieves state-of-the-art performance in categorizing images from both known and unknown classes.

Optimizing 4D Gaussians for Dynamic Scene Video from Single Landscape Images
This paper introduces a novel method for achieving realistic immersion in landscape images by representing a complete 3D space using explicit 4D Gaussians from a single image. The proposed framework, which includes consistent 3D motion estimation to enhance real-life motion representation, marks the first attempt to animate and fully capture 3D space from a single landscape image, demonstrating significant improvements in realism across various experiments.

BinaryDM: Accurate Weight Binarization for Efficient Diffusion Models
This paper introduces BinaryDM, a novel weight binarization approach for diffusion models, enhancing both accuracy and efficiency by using an Evolvable-Basis Binarizer and Low-rank Representation Mimicking. BinaryDM achieves significant improvements, including a 7.74 FID with 1-bit weight and 4-bit activation, marking substantial gains in operational efficiency and model size suitability for edge deployment compared to existing methods.

Fast Direct: Query-Efficient  Online Black-box Guidance  for Diffusion-model Target Generation
This paper introduces Fast Direct, a novel and simple algorithm designed for query-efficient online black-box target generation in diffusion models. The proposed method shows significant improvements in query efficiency, achieving up to 10 times better performance in image generation tasks and up to 44 times in 3D-molecule generation, addressing the challenge of expensive objective evaluations in real-world scenarios.

Resolution Attack: Exploiting Image Compression to Deceive Deep Neural Networks
The paper introduces a novel form of attack called the resolution attack, which deceives classifiers and human observers by creating images with varied semantics across different resolutions. Utilizing a new automated framework based on large-scale diffusion models and staged denoising strategies, the research highlights vulnerabilities in existing classifiers and presents a competitive tool for face swapping and facial camouflage, demonstrating high attack success rates. The code for the framework is available at https://github.com/ywj1/resolution-attack.

Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning
This paper explores the design of effective representations for the actor and critic in deep reinforcement learning, examining whether decoupling their representations yields advantages. The study finds that decoupled representations allow the actor to focus on action-relevant information and the critic on value and dynamics information, which can guide the choice of learning objectives and enhance agent performance.

Understanding the Generalization of In-Context Learning in Transformers: An Empirical Study
This paper systematically investigates the generalization capabilities of transformers with in-context learning (ICL) across different dimensions, revealing that while they lack inter-problem generalization, they excel in intra-task and intra-problem generalization. The study suggests enhancing ICL's generalization ability by diversifying and combining training tasks, improving performance on both unseen and simple known tasks.

Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts
The paper addresses the challenge of data scarcity in adapting medical Large Language Models (LLMs) to low-resource languages and introduces a high-quality medical dataset for analysis. By proposing a novel Mixture of Experts routing method and the Post-MoE architecture, the study enhances multilingual LLM generalization across languages while improving scalability through the concept of \textit{language family} experts, allowing coverage of 50 languages without increasing model parameters.

Associative memory and dead neurons
This paper addresses the issue of dead neurons in neural ordinary differential equations by analyzing the problematic flat regions where energy functions fail to fully define system dynamics. It proposes solutions through the use of Hessian matrices and introduces modified dynamical systems with diverse Lyapunov functions that circumvent these issues, enabling applications in non-symmetric neural architectures.

Do Large Language Models Truly Understand Geometric Structures?
The paper introduces the GeomRel dataset to accurately evaluate large language models' (LLMs) understanding of geometric structures by focusing on the core step of geometric relationship identification. It also proposes the Geometry Chain-of-Thought (GeoCoT) method, which significantly improves LLMs' ability to identify geometric relationships, addressing key limitations revealed through thorough evaluations.

PAD: Personalized Alignment at Decoding-time
This paper introduces Personalized Alignment at Decoding-time (PAD), a framework that aligns large language models' (LLM) outputs with diverse personalized preferences during inference without additional training. PAD uses a unique personalized reward modeling strategy to guide dynamic token-level adjustments, proving more effective and scalable than traditional alignment methods and enhancing LLMs' capability to cater to real-time user preferences.

Evidential Learning-based Certainty Estimation for Robust Dense Feature Matching
This paper introduces an evidential deep learning framework to improve the robustness of dense feature matching methods against image corruptions and perturbations. By modifying the certainty prediction branch to generate belief masses and using a Dirichlet distribution, the approach significantly enhances reliability, achieving up to a 10.1% improvement under severe conditions compared to traditional methods.

From Sparse Dependence to Sparse Attention: Unveiling How Chain-of-Thought Enhances Transformer Sample Efficiency
Chain-of-thought (CoT) reasoning improves the sample efficiency and interpretability of large language models by introducing sparse sequential dependencies and a sparse attention mechanism. This study shows that CoT allows transformers to learn functions with polynomially fewer samples and confirms through experiments that attention layer sparsity contributes significantly to this enhancement.

CryoGEN: Generative Energy-based Models for Cryogenic Electron Tomography Reconstruction
This study presents CryoGEN, an energy-based probabilistic model that addresses the anisotropic resolution artifacts in Cryogenic electron tomography (Cryo-ET) without relying on recursive subtomogram averaging. CryoGEN significantly improves the structural completeness and interpretability of reconstructed samples, offering a substantial training speedup and demonstrating effectiveness across multiple biological datasets.

QuaDiM: A Conditional Diffusion Model For Quantum State Property Estimation
This paper introduces \model, a non-autoregressive generative model leveraging diffusion models for quantum state property estimation, which addresses the limitations of conventional auto-regressive models that assume an ordering among qubits. The proposed model effectively denoises Gaussian noise into quantum state distributions, showing superior performance on large-scale tasks with limited data, thus offering a significant advancement in estimating properties of quantum many-body systems.

SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding
This paper introduces a hybrid strategy combining continual pre-training and supervised fine-tuning to develop SciLitLLM, a Large Language Model specialized in scientific literature understanding. By addressing the challenges of constructing high-quality corpora and generating diverse instructions, this approach enhances the model's performance in domain-specific tasks and creates a new instruction set, SciLitIns, for less-represented scientific domains, showing promising results on scientific benchmarks.

A Theoretical Framework for Partially-Observed Reward States in RLHF
This paper introduces a model of reinforcement learning from human feedback (RLHF) as reinforcement learning with partially observed reward-states (PORRL), addressing limitations in existing models by considering neuroscience-backed internal states and intermediate feedback. It presents methods that improve learning efficiency and alignment in RLHF, offering cardinal and dueling feedback techniques with theoretical guarantees and demonstrating enhanced performance over traditional methods.

ORSO: Accelerating Reward Design via Online Reward Selection and Policy Optimization
The paper introduces Online Reward Selection and Policy Optimization (ORSO), a method that automates the selection of shaping reward functions in reinforcement learning as an online model selection problem. ORSO enhances data efficiency and computational speed, with significant performance improvements over previous methods, demonstrating effectiveness across various continuous control tasks.

DiffGAD: A Diffusion-based Unsupervised Graph Anomaly Detector
DiffGAD introduces a novel diffusion-based approach to graph anomaly detection, enhancing traditional methods by infusing the latent space with discriminative content to better identify anomalies. This model significantly improves detection accuracy and efficiency, as demonstrated by comprehensive evaluations on six large-scale datasets, with code accessible for further exploration at https://github.com/fortunato-all/DiffGAD.

Towards Automated Knowledge Integration From Human-Interpretable Representations
This paper addresses the challenge of incorporating inductive biases in machine learning, particularly in noisy and low-data environments, by proposing an approach called informed meta-learning. The approach leverages prior knowledge in natural language and introduces the Informed Neural Process to automate and control bias selection, demonstrating improved data efficiency and generalization.

Emergence of meta-stable clustering in mean-field transformer models
This paper presents a mathematical investigation into the long-term behavior of tokens in a deep stack of Transformer layers, modeled as a continuous-time flow on the unit sphere using a mean-field interacting particle system. It reveals the emergence and persistence of meta-stable phases and clustering phenomena, essential for applications like next-token prediction, by analyzing the mean-field PDE and identifying the structure of the meta-stable manifold influenced by the inverse temperature parameter through rescaled Gegenbauer polynomials.

GOPlan: Goal-conditioned Offline Reinforcement Learning by Planning with Learned Models
Offline Goal-Conditioned RL (GCRL) faces challenges in data limitation and goal generalization, which GOPlan addresses through a novel model-based approach combining a pretrained prior policy and a reanalysis method for generating imagined trajectories. Leveraging an advantage-weighted conditioned generative adversarial network, GOPlan achieves state-of-the-art performance in offline multi-goal tasks, particularly excelling with limited data and out-of-distribution goals.

Size-Generalizable RNA Structure Evaluation by Exploring Hierarchical Geometries
EquiRNA is an innovative equivariant geometric graph neural network designed to improve the prediction and evaluation of RNA 3D structures by focusing on RNA's hierarchical geometries and overcoming limitations of size generalization in existing models. By introducing a nucleotide representation reuse and a size-insensitive $K$-nearest neighbor sampling strategy, EquiRNA demonstrates superior performance and efficiency on benchmark tests, making it a robust baseline for future RNA structure research.

Specialized Foundation Models Struggle to Beat Supervised Baselines
The paper evaluates the effectiveness of the "foundation model" (FM) paradigm in genomics, satellite imaging, and time series, comparing it to traditional supervised learning. It finds that simple supervised models can match or outperform recent FMs, highlighting the need for strong baselines and introducing new automated workflows for fair comparison.

LucidPPN: Unambiguous Prototypical Parts Network for User-centric Interpretable Computer Vision
The Lucid Prototypical Parts Network (LucidPPN) enhances the interpretability of prototypical parts networks by separating color prototypes from other visual features using two reasoning branches: one for grayscale images and another for color. This innovative approach clarifies whether decisions are based on color, shape, or texture, resulting in less ambiguous prototypical parts and improving user understanding, while achieving performance comparable to baseline methods.

ASTrA: Adversarial Self-supervised Training with Adaptive-Attacks
ASTrA introduces a novel self-supervised adversarial training framework leveraging a learnable attack strategy network to adaptively discover optimal attack parameters, addressing limitations in existing methods that rely on fixed attack strategies. By aligning clean and adversarial data distributions and optimizing with a mixed contrastive objective, ASTrA enhances adversarial robustness and achieves state-of-the-art results on datasets like CIFAR10, CIFAR100, and STL10.

Mitigating Information Loss in Tree-Based Reinforcement Learning via Direct Optimization
This paper introduces SYMPOL, a new method for symbolic tree-based on-policy reinforcement learning that integrates a tree-based model with a policy gradient approach to enhance interpretability. SYMPOL demonstrates superior performance in benchmark tasks compared to existing tree-based RL methods, offering an interpretable, end-to-end learning process using decision trees and potentially forming the basis for a new class of interpretable RL algorithms.

Outlier Synthesis via Hamiltonian Monte Carlo for Out-of-Distribution Detection
The paper introduces the Hamiltonian Monte Carlo Outlier Synthesis (HamOS) framework for out-of-distribution (OOD) detection, addressing limitations of prior methods that depend on large pools of natural outliers or costly virtual synthesis. By using Hamiltonian Monte Carlo to sample extensively from Markov chains based on in-distribution data, HamOS efficiently generates diverse outliers, enhancing OOD detection while demonstrating superior performance against state-of-the-art methods on various benchmarks.

Beyond Next Token Prediction: Patch-Level Training for Large Language Models
This paper introduces patch-level training for Large Language Models (LLMs), which aggregates multiple tokens into a 'patch' for training, reducing the sequence length and, consequently, the training cost. Experiments demonstrate that this method can cut training costs by half without sacrificing performance, offering a more efficient approach to LLM development.

Revisit Micro-batch Clipping: Adaptive Data Pruning via Gradient Manipulation
Micro-batch clipping has shown potential in enhancing ASR model performance by improving convergence rates, and this paper provides the first explanation for why specific micro-batch sizes are beneficial. The analysis reveals that while the method introduces a constant bias, it can be minimized at optimal micro-batch sizes, and its effectiveness extends beyond speech models to vision and language models, though its benefits are reduced with training data from diverse domains.

KBLaM: Knowledge Base augmented Language Model
This paper introduces KBLAM, a novel method for augmenting large language models with external knowledge bases using a specialized attention mechanism, eliminating the need for external retrieval modules. KBLAM efficiently integrates a large knowledge base into pre-trained language models, facilitating tasks such as question-answering and open-ended reasoning, while allowing dynamic updates without the need for model retraining.

PerturboLLaVA: Reducing Multimodal Hallucinations with Perturbative Visual Training
This paper introduces HalFscore, a novel metric for evaluating the accuracy and completeness of dense image captions in Multimodal Large Language Models, aiming to address hallucination issues. Additionally, it proposes PerturboLLaVA, a method that reduces model reliance on language priors by incorporating adversarially perturbed text during training, thereby enhancing caption fidelity without extra computational cost.

Training-free LLM-generated Text Detection by Mining Token Probability Sequences
The paper introduces a novel training-free detector, Lastde, which enhances the detection of LLM-generated texts by combining local and global statistical features, including time series analysis of token probability sequences. This method outperforms existing detectors in cross-domain, cross-model, and cross-lingual settings, and shows increased robustness against paraphrasing attacks, with an efficient real-time detection variant, Lastde++, also proposed.

MambaQuant: Quantizing the Mamba Family with Variance Aligned Rotation Methods
MambaQuant is a pioneering post-training quantization (PTQ) framework specifically designed for the Mamba sequence model, addressing challenges like significant outliers and inconsistent variance across channels caused by Mamba's unique architecture. It employs Karhunen-Loève Transformation and Smooth-Fused rotation to reduce data disparity, successfully quantizing weights and activations to 8-bit with minimal accuracy loss, thus marking a significant advancement in the application of PTQ in Mamba models.

Learning multi-modal generative models with permutation-invariant encoders and tighter variational objectives
This paper introduces a novel variational objective and flexible aggregation schemes using permutation-invariant neural networks for multi-modal Variational Autoencoders (VAEs), enhancing the approximation of data log-likelihood without the inductive biases of traditional PoE or MoE approaches. Through numerical experiments, the study demonstrates that these advancements can improve the modeling of the true joint distribution in identifiable multi-modal scenarios, offering better trade-offs in generative quality and modality consistency.

CircuitFusion: Multimodal Circuit Representation Learning for Agile Chip Design
This paper introduces CircuitFusion, a groundbreaking multimodal and implementation-aware circuit encoder that creates general representations of integrated circuits for various design tasks by fusing hardware code, structural graph, and functionality summary. Evaluated across five circuit design tasks, CircuitFusion outperforms state-of-the-art methods by leveraging unique circuit properties and novel strategies, showcasing its generalizability and enhanced performance in fine-tuning and zero-shot inference.

Hierarchically Encapsulated Representation for Protocol Design in Self-Driving Labs
This paper addresses the need for rapid protocol design in scientific research by introducing a multi-faceted, multi-scale representation system using Domain-Specific Languages to improve knowledge-based machine designers like Large Language Models. The proposed method, enhanced by a data-driven algorithm, effectively supports and complements these models in designing, planning, and adjusting experimental protocols, advancing the capabilities of machine-assisted scientific exploration.

Glauber Generative Model: Discrete Diffusion Models via Binary Classification
The Glauber Generative Model (GGM) introduces a novel discrete diffusion approach using a heat bath dynamics Markov chain to differentiate signal from noise in a sequence of tokens, offering a new method for generating samples from discrete spaces. The model surpasses existing discrete diffusion paradigms in language and image generation, excelling in tasks like zero-shot text and image infilling without relying on dataset-specific tokenizers.

On the Identification of Temporal Causal Representation with Instantaneous Dependence
The paper presents the \textbf{IDOL} framework, which addresses the challenge of identifying latent causal processes in time series data with instantaneous dependencies by imposing a sparse influence constraint. Through theoretical analysis and experimental validation, the method demonstrates its ability to identify latent causal processes effectively, particularly in human motion forecasting scenarios, without requiring complex interventions or observation grouping.

Seq-VCR: Preventing  Collapse in Intermediate Transformer Representations for Enhanced Reasoning
This paper addresses the difficulty decoder-only Transformers face in complex reasoning tasks, particularly arithmetic reasoning, by identifying intermediate layer representation collapse as a limiting factor. The authors propose Sequential Variance-Covariance Regularization (Seq-VCR) to enhance representation entropy, achieving significant performance improvements on tasks like 5 × 5 integer multiplication, surpassing same-sized models and GPT-4, without the need for explicit chain-of-thought supervision.

LLaMA-Omni: Seamless Speech Interaction with Large Language Models
LLaMA-Omni is a novel model architecture designed for efficient speech interaction with large language models (LLMs), integrating components to deliver low-latency text and speech responses directly from speech inputs without needing transcription. The model, based on Llama-3.1-8B-Instruct, is trained on the InstructS2S-200K dataset and demonstrates significant improvements in response quality and latency, achieving a response time as low as 226ms, while also being efficient to train on limited computational resources.

Analytic DAG Constraints for Differentiable DAG Learning
Recovering Directed Acyclic Graph (DAG) structures from data is challenging due to optimization issues exacerbated by gradient vanishing in differentiable DAG learning. By connecting analytic functions to DAG constraints, this paper introduces novel and effective DAG constraints that outperform previous methods, supported by theoretical development and experimental validation, with implementations available online.

Operator Deep Smoothing for Implied Volatility
We present a novel method called "operator deep smoothing" for nowcasting implied volatility, addressing the challenges posed by dynamically changing option price data and limitations of traditional neural networks. By utilizing a graph neural operator architecture, our approach directly maps observed data to smoothed surfaces, ensuring accuracy and adherence to no-arbitrage constraints, showcasing superior generalization capabilities compared to classical neural networks and industry-standard parametrizations.

Exploring Local Memorization in Diffusion Models via Bright Ending Attention
Text-to-image diffusion models often memorize and replicate training data, posing issues like copyright infringement, especially when only specific image regions (local memorization) are affected. This paper introduces the "bright ending" (BE) anomaly, a novel cross-attention pattern that helps localize memorized regions, and proposes a method to incorporate BE into existing frameworks, significantly enhancing their ability to address local memorization and achieve state-of-the-art performance.

Gap-Dependent Bounds for Q-Learning using Reference-Advantage Decomposition
This paper provides a novel analysis of gap-dependent bounds for UCB-Advantage and Q-EarlySettled-Advantage algorithms in on-policy $Q$-learning for finite-horizon episodic tabular Markov Decision Processes (MDPs). It introduces an innovative error decomposition framework that yields logarithmic in $T$ gap-dependent regret bounds and addresses policy switching costs, presenting the first such analysis using variance estimators and reference-advantage decomposition.

CFG++: Manifold-constrained Classifier Free Guidance for Diffusion Models
This paper introduces CFG++, an improved method for classifier-free guidance (CFG) in diffusion models, addressing off-manifold issues that previously affected image editing and quality at high guidance scales. By reformulating text-guidance as an inverse problem, CFG++ enhances sample quality, invertibility, and performance across various tasks, outperforming traditional CFG and showing promising applications in text-to-image generation and other areas.

LiNeS: Post-training Layer Scaling Prevents Forgetting and Enhances Model Merging
LiNeS, or Layer-increasing Network Scaling, is introduced as a technique to address the challenges of fine-tuning pre-trained models, such as catastrophic forgetting and performance loss when merging fine-tuned checkpoints from various tasks. By scaling parameter updates linearly according to layer depth, LiNeS maintains generalization while enhancing task performance, demonstrating significant improvements in both single-task and multi-task scenarios across benchmarks in vision and natural language processing, and seamlessly integrating with existing model merging techniques.

RecFlow: An Industrial Full Flow Recommendation Dataset
The paper introduces RecFlow, a comprehensive full-flow recommendation dataset designed to address critical challenges faced by real-world industrial recommendation systems, such as handling unexposed items and the interplay between multiple stages of the recommendation pipeline. By providing samples from both exposed and unexposed spaces, RecFlow enables the development of novel algorithms that improve recommendation effectiveness, with some already delivering substantial gains in real-world applications like KuaiShou.

Strategist: Self-improvement of LLM Decision Making via Bi-Level Tree Search
STRATEGIST is a novel approach that leverages the generalization capabilities of large language models (LLMs) to generate and refine high-level strategies, which are then executed using Monte Carlo Tree Search (MCTS), optimizing strategies through self-play without training data. Tested in games like Game of Pure Strategy and The Resistance: Avalon, STRATEGIST demonstrates superior performance to traditional reinforcement learning and other LLM-based methods, aligning well with human-level play.

Boosting Neural Combinatorial Optimization for Large-Scale Vehicle Routing Problems
This paper introduces a lightweight cross-attention mechanism with linear complexity and a Self-Improved Training (SIT) algorithm to enhance the efficiency of Transformer networks in solving large-scale Vehicle Routing Problems (VRPs). The proposed method demonstrates superior performance, improving the scalability and practical applicability of Neural Combinatorial Optimization methods on problems with up to 100K nodes.

Measuring And Improving Engagement of Text-to-Image Generation Models
This paper addresses the challenge of optimizing text-to-image generation for improved viewer engagement in marketing and advertising contexts by introducing the *EngagingImageNet* dataset and the *EngageNet* model to predict image engagement using contextual information. The authors propose methods to improve engagement in text-to-image models, such as advanced prompt conditioning, fine-tuning techniques, and reinforcement learning, and establish the *Engagement Arena* benchmark to encourage further research in this area.

Where Am I and What Will I See: An Auto-Regressive Model for Spatial Localization and View Prediction
This paper introduces the Generative Spatial Transformer (GST), an innovative auto-regressive framework that simultaneously addresses spatial localization and view prediction, enhancing spatial reasoning in AI models. By unifying pose estimation and novel view synthesis through a novel camera tokenization method, GST demonstrates improved performance in both tasks, emphasizing the interconnectedness of spatial awareness and visual prediction.

Interaction Asymmetry: A General Principle for Learning Composable Abstractions
The paper introduces the principle of interaction asymmetry, which posits that parts of the same concept interact more complexly than parts of different concepts, as a key to understanding disentangled representations and compositional generalization. By formalizing this principle and applying it to up to second-order derivatives, the authors develop a Transformer-based VAE with a novel regularizer, demonstrating its efficacy in disentangling concepts and achieving results comparable to models with explicit object-centric priors.

Pre-training of Foundation Adapters for LLM Fine-tuning
The paper introduces pre-trained foundation adapters to improve weight initialization in adapter-based fine-tuning methods for large language models, aiming to enhance training stability and performance consistency. This technique potentially reduces computational costs while maintaining the effectiveness of the fine-tuning process.

Data Center Cooling System Optimization Using Offline Reinforcement Learning
This paper introduces a novel physics-informed offline reinforcement learning framework aimed at optimizing the energy efficiency of data center cooling systems by utilizing a graph neural network architecture to model complex system dynamics. The framework demonstrates significant energy savings of 14-21% while maintaining safety and operational constraints, showcasing the potential of offline RL in tackling data-limited and safety-critical industrial control challenges.

Learning Randomized Algorithms with Transformers
This paper explores the integration of randomization into deep neural networks, specifically transformer models, to enhance their performance in adversarial settings. By employing common optimization techniques to incorporate random properties, the study demonstrates improved robustness and performance in tasks such as associative recall, graph coloring, and grid world exploration, highlighting the benefits of randomness in neural computation and predictions.

LLaVA-MoD: Making LLaVA Tiny via MoE-Knowledge Distillation
LLaVA-MoD is a framework enabling efficient training of small-scale Multimodal Language Models (s-MLLM) by distilling knowledge from larger models (l-MLLM), addressing challenges in the distillation process through a sparse Mixture of Experts architecture and a progressive knowledge transfer strategy. This approach achieves remarkable performance improvements, such as LLaVA-MoD-2B surpassing Qwen-VL-Chat-7B with significantly reduced training data and parameters, indicating its potential for creating efficient MLLMs with enhanced capabilities.

Denoising with a Joint-Embedding Predictive Architecture
This paper introduces Denoising with a Joint-Embedding Predictive Architecture (D-JEPA), a novel generative model that integrates JEPA concepts with diffusion models for enhanced performance in self-supervised representation learning. D-JEPA demonstrates superior scalability and efficiency, outperforming previous generative models in ImageNet conditional generation benchmarks and showing potential for applications in continuous data modeling such as video and audio.

TRACE: Temporal Grounding Video LLM  via Causal Event Modeling
This paper addresses the limitations of current video LLM methods in Video Temporal Grounding (VTG) by introducing a causal event modeling framework that organizes video LLM outputs as sequences of events. The proposed TRACE model effectively utilizes distinct encoders and decoding heads for various tasks using this framework, demonstrating superior performance in VTG tasks and datasets.

Diff-PIC: Revolutionizing Particle-In-Cell Nuclear Fusion Simulation with Diffusion Models
This paper introduces Diff-PIC, a novel framework utilizing conditional diffusion models to efficiently generate high-fidelity scientific data for Laser-Plasma Interaction (LPI), crucial in nuclear fusion research. By distilling physical patterns from Particle-in-Cell (PIC) simulations into a one-step conditional diffusion model with enhanced parameter encoding and the rectified flow technique, Diff-PIC achieves a remarkable $\sim$16,200$\times$ speedup over traditional PIC simulations while maintaining superior accuracy.

DS-LLM: Leveraging Dynamical Systems to Enhance Both Training and Inference of Large Language Models
This paper introduces DS-LLM, a novel framework that uses dynamical system-based machines with Natural Annealing to significantly enhance the efficiency of large language models (LLMs) training and inference. By mapping LLM components to optimization problems solved with Hamiltonian configurations, DS-LLM achieves substantial improvements in speed and energy efficiency, addressing the scalability challenges and high computational costs of LLMs, while maintaining model accuracy.

SPAM: Spike-Aware Adam with Momentum Reset for Stable LLM Training
This paper investigates gradient spikes during large language model (LLM) training, identifying them as a key source of instability that hampers model performance. To mitigate this issue, the authors introduce Spike-Aware Adam with Momentum Reset (SPAM), a novel optimizer that improves training stability and resource efficiency, outperforming existing optimization methods in both standard and memory-constrained scenarios.

Large Language Models are Interpretable Learners
This paper presents LLM-based Symbolic Programs (LSPs), a novel approach that combines Large Language Models with symbolic programs to enhance both expressiveness and interpretability in classification and decision-making models. By employing a divide-and-conquer training method and evaluating with a new benchmark, IL-Bench, LSPs demonstrate superior performance, easy transferability, and generalization across diverse tasks, outperforming traditional neurosymbolic approaches.

UV-Attack: Physical-World Adversarial Attacks on Person Detection via Dynamic-NeRF-based UV Mapping
This paper introduces \texttt{UV-Attack}, a novel physical adversarial attack leveraging dynamic-NeRF-based UV mapping to achieve high attack success rates against person detectors in scenarios with extensive human movements. The method overcomes challenges related to non-rigid deformations of the human body by generating UV maps for real-time texture edits, achieving a 92.7% success rate against FastRCNN and 49.5% against YOLOv8, outperforming previous approaches significantly.

Data Pruning by Information Maximization
This paper introduces InfoMax, a novel data pruning method aimed at maximizing the information content and minimizing redundancy in selected samples through importance scores and pairwise sample similarities. By formalizing the coreset selection problem as a discrete quadratic programming task and employing scalable gradient-based solvers, InfoMax demonstrates superior performance in various pruning tasks across extensive datasets, enhancing processes such as image classification and vision-language pre-training.

OvercookedV2: Rethinking Overcooked for Zero-Shot Coordination
This paper examines the zero-shot coordination (ZSC) challenges in the Overcooked benchmark for AI agents, revealing that poor state coverage under self-play contributes more to ZSC failures than complex coordination issues. To improve the benchmark, the authors introduce OvercookedV2, which incorporates asymmetric information and stochasticity, and establishes new coordination challenges requiring advanced algorithms for effective AI interaction with novel partners.

JudgeLM: Fine-tuned Large Language Models are Scalable Judges
The paper introduces JudgeLM, a framework that fine-tunes large language models to serve as efficient, scalable judges for evaluating language models in open-ended scenarios, overcoming limitations in existing benchmarks and metrics. JudgeLM, trained on a novel dataset and benchmark, achieves state-of-the-art performance and high agreement with human evaluations while addressing biases through innovative techniques, and excels in various judging tasks including single and multiple answer assessments, multimodal models, and multi-turn chat evaluations.

Unifying Causal Representation Learning with the Invariance Principle
The paper investigates causal representation learning (CRL) and demonstrates that many CRL approaches can be unified by aligning their representations with inherent data symmetries, rather than strictly adhering to Pearl's causal hierarchy. This methodological shift allows for greater flexibility in mixing causal and non-causal assumptions, enhancing applicability as evidenced by improved treatment effect estimation on real-world high-dimensional ecological data.

BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL
This paper presents BOFormer, a novel approach to multi-objective Bayesian optimization (MOBO) using a generalized deep Q-learning framework inspired by advancements in non-Markovian reinforcement learning and Transformers. BOFormer addresses the hypervolume identifiability issue in MOBO and demonstrates superior performance compared to existing rule-based and learning-based algorithms across various synthetic and real-world optimization problems.

An Engorgio Prompt Makes Large Language Model Babble on
This paper explores the vulnerability of auto-regressive large language models (LLMs) to inference cost attacks, specifically through the use of Engorgio prompts designed to increase computation costs and latency. By proposing new loss functions and using a parameterized distribution to track prediction trajectories, the authors demonstrate through extensive experiments that Engorgio prompts can significantly extend output lengths and disrupt LLM service availability, highlighting a potential threat to systems with limited computing resources.

Let Your Features Tell The Differences: Understanding Graph Convolution By Feature Splitting
Graph Neural Networks (GNNs) can be hindered by treating each feature dimension indiscriminately during graph convolution, which might disadvantage certain features. This paper introduces the Topological Feature Informativeness (TFI) metric and a corresponding Graph Feature Selection (GFS) method that distinguishes and processes convolution-favored and disfavored features separately, significantly boosting performance across various GNN architectures and datasets.

Unsupervised Disentanglement of Content and Style via Variance-Invariance Constraints
We introduce V3, an unsupervised method that leverages statistical differences between content and style to effectively learn disentangled representations across various domains and modalities without relying on domain-specific labels. V3 surpasses existing methods in disentanglement performance, out-of-distribution generalization, and few-shot learning, while also achieving symbolic-level interpretability in the learned content codebook.

Task-Adaptive Pretrained Language Models via Clustered-Importance Sampling
The paper presents a method to build specialist language models from large generalist training sets by adjusting the training distribution using limited domain-specific data. The study introduces clustered importance sampling, improving performance across different language modeling tasks and demonstrating advantages in multi-task settings.

Improving Complex Reasoning with Dynamic Prompt Corruption: A Soft Prompt Optimization Approach
Prompt Tuning, a parameter-efficient method, struggles with complex reasoning tasks despite its success in classification, often worsening performance due to the dual effects of soft prompts. To address this, the proposed Dynamic Prompt Corruption method optimizes soft prompt usage by dynamically adjusting their influence, leading to a 4%-8% accuracy improvement in large language models on reasoning tasks, as validated through experiments.

Debiasing Federated Learning with Correlated Client Participation
This paper presents a theoretical framework modeling client participation in cross-device federated learning (FL) as a Markov chain to analyze optimization convergence with non-uniform, correlated client participation. By examining the effects of imposing a minimum number of rounds between client re-participation, the study proves that this approach reduces bias in client availability and introduces a debiasing algorithm for Federated Averaging (FedAvg) that ensures convergence to an unbiased solution regardless of minimum separation or client availability distribution.

ODE-based Smoothing Neural Network for Reinforcement Learning Tasks
This paper introduces the Smooth ODE (SmODE) network, designed to address the challenge of non-smooth control actions in deep reinforcement learning by simultaneously tackling high-frequency input noise and unconstrained Lipschitz constants in neural networks. By incorporating a smooth ODE neuron with a learnable state-based system time constant and a state-based mapping function, the SmODE network enhances policy performance and robustness, offering superior anti-interference capabilities and smoother action outputs compared to existing architectures.

GUI-World: A Video Benchmark and Dataset for Multimodal GUI-oriented Understanding
The paper introduces GUI-World, a new dataset designed to enhance the understanding of dynamic and complex GUI scenarios for Multimodal Large Language Models (MLLMs). It evaluates current state-of-the-art models, revealing challenges in understanding dynamic GUI content, and proposes GUI-Vid, a fine-tuned Video LLM, as a potential solution, showcasing improved performance and offering insights for future research in this domain.

Statistical Advantages of Perturbing Cosine Router in Mixture of Experts
The paper analyzes the performance and limitations of the cosine router in Mixture of Experts (MoE), noting its benefits in addressing representation collapse but also highlighting slow convergence rates of model parameters. By introducing a "perturbed cosine router" technique that involves adding noise to stabilize the router, the authors demonstrate that it significantly enhances estimation rates to polynomial levels, with empirical validation through extensive simulations.

Feast Your Eyes:  Mixture-of-Resolution Adaptation for Multimodal Large Language Models
This paper introduces Mixture-of-Resolution Adaptation (MRA), a novel method for multimodal large language models (MLLMs) that efficiently combines low- and high-resolution visual features to enhance performance while reducing computational costs. The new model, LLaVA-HR, built using MRA, demonstrates superior performance over existing models on 15 out of 17 vision-language tasks and maintains efficient training and inference processes.

Finding and Only Finding Differential Nash Equilibria by Both Pretending to be a Follower
This paper introduces double Follow-the-Ridge (double-FTR), an algorithm that uniquely converges to differential Nash equilibria in general-sum two-player differentiable games, a first for such scenarios. The algorithm also prevents equilibrium oscillation and, with variations in its preconditioner, forms a family of algorithms, tested successfully on various game scenarios and simple GAN tasks.

Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?
Large Language Models (LLMs) lack critical safety features due to the absence of an explicit separation between instructions and data, making them vulnerable to manipulations. This paper introduces a formal measure and a new dataset, SEP, to assess this separation, finding that existing models and mitigation techniques fail to effectively address the issue.

Training Language Models to Self-Correct via Reinforcement Learning
This paper presents SCoRe, a multi-turn online reinforcement learning approach that enhances the self-correction ability of large language models using entirely self-generated data. By addressing limitations of supervised fine-tuning with distribution mismatch and ineffective correction modes, SCoRe significantly improves self-correction performance, achieving a 15.6% and 9.1% enhancement on the MATH and HumanEval benchmarks when applied to Gemini models.

Diff-Prompt: Diffusion-driven Prompt Generator with Mask Supervision
This paper introduces Diffusion-Driven Prompt Generator (Diff-Prompt), a novel approach utilizing diffusion models to create rich, fine-grained prompts for improving performance on complex multimodal tasks. Diff-Prompt demonstrates superior results in experiments on pixel-level tasks, notably achieving significant improvements over existing methods, suggesting the potential of generative models in enhancing prompt learning.

Graph Neural Networks Gone Hogwild
Graph neural networks (GNNs), though powerful for distributed multi-agent systems, often fail under asynchronous node updates, limiting their use in situations where synchrony cannot be ensured. This paper introduces "implicitly-defined" GNNs, particularly the novel energy GNN architecture, which is robust to asynchronous inference and outperforms existing GNNs on synthetic multi-agent tasks.

End-to-end Learning of Gaussian Mixture Priors for Diffusion Sampler
This paper addresses the limitations of diffusion models in exploring unnormalized target densities by proposing end-to-end learnable Gaussian mixture priors (GMPs) to improve exploration, adaptability, and expressiveness. The proposed GMPs and an iterative refinement strategy for adding mixture components during training show significant performance improvements in various benchmark tests without the need for additional target evaluations.

Underdamped Diffusion Bridges with Applications to Sampling
The paper introduces a general framework for learning diffusion bridges that encompass both traditional and underdamped models with degenerate noise matrices, offering a method to effectively sample from unnormalized densities without prior samples. This approach, termed "underdamped diffusion bridges," demonstrates superior performance in various sampling problems, outperforming existing methods with fewer discretization steps and minimal hyperparameter tuning.

HOPE for a Robust Parameterization of Long-memory State Space Models
The paper introduces a new parameterization scheme called HOPE for linear, time-invariant state-space models by leveraging Markov parameters within Hankel operators, improving initialization and training stability. This approach requires fewer parameters and enhances performance on Long-Range Arena tasks and a sequential CIFAR-10 task compared to models like S4 and S4D, offering non-decaying memory within a fixed time window.

LOIRE: LifelOng learning on Incremental data via pre-trained language model gRowth Efficiently
Large-scale pre-trained language models (PLMs) face challenges in lifelong learning, such as catastrophic forgetting and inefficient model growth. LOIRE is a new framework that addresses these issues by optimizing growth operators and schedules, maintaining knowledge retention, and reducing computation by 29.22% while achieving comparable or superior performance.

Gumbel Counterfactual Generation From Language Models
This paper introduces a novel framework for generating true counterfactuals in language models by reformulating them as a structural equation model, using a technique called Gumbel counterfactual generation. Through experiments, the approach is shown to produce meaningful counterfactuals and reveals that existing intervention methods often result in significant unintended consequences.

Generalized Video Moment Retrieval
This paper introduces the Generalized Video Moment Retrieval (GVMR) framework, which expands traditional Video Moment Retrieval to accommodate both non-target and multi-target queries, supported by the NExT-VMR dataset from the YFCC100M collection. It also presents BCANet, a transformer-based model with a novel Boundary-aware Cross Attention module, which significantly improves the accuracy and adaptability of predicting temporal video segments, setting a new standard in multimedia information retrieval research.

Scaling Autonomous Agents via Automatic Reward Modeling And Planning
This paper introduces a framework to improve large language models' (LLMs) decision-making capabilities by automatically learning a reward model from environmental interactions without human annotations. By generating task-relevant data and optimizing a reward model to evaluate action trajectories, the framework addresses challenges related to data scarcity and API limitations, enhancing LLM agents' performance in complex environments.

Neural Sampling from Boltzmann Densities: Fisher-Rao Curves in the Wasserstein Geometry
This paper addresses the challenge of sampling from an unnormalized Boltzmann density by examining Fisher-Rao flows and specific interpolations of energy functions. By employing tools from Wasserstein geometry and proposing a parametrization that fixes an appropriate velocity field, the authors demonstrate a model that successfully facilitates a well-behaved flow field, effectively resolving mass teleportation issues in sampling tasks.

Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models
We introduce Deep Compression Autoencoder (DC-AE), a novel family of autoencoders designed to accelerate high-resolution diffusion models by achieving high spatial compression ratios without sacrificing reconstruction accuracy. By implementing Residual Autoencoding and Decoupled High-Resolution Adaptation, DC-AE improves compression ratios up to 128x, resulting in significant inference and training speedup with enhanced FID scores, as demonstrated on ImageNet 512x512.

AlphaEdit: Null-Space Constrained Model Editing for Language Models
AlphaEdit is introduced as a novel solution to mitigate disruption of preserved knowledge in large language models (LLMs) during sequential editing by projecting perturbations onto the null space of the preserved knowledge. The method improves the performance of existing locating-then-editing strategies by 36.7% on average, as demonstrated through experiments with models like LLaMA3, GPT2-XL, and GPT-J, all achieved with minimal code addition.

Random-Set Neural Networks
This paper introduces the Random-Set Neural Network (RS-NN), a novel classification method that uses belief functions and random set mathematics to better handle epistemic uncertainty in machine learning predictions. RS-NN surpasses existing Bayesian and Ensemble methods in accuracy, uncertainty estimation, and out-of-distribution detection across several benchmarks, scales effectively to large architectures, demonstrates robustness to adversarial attacks, and supports statistical guarantees in a conformal learning context.

Sensitivity Verification for Additive Decision Tree Ensembles
This paper presents a formal verification framework for addressing feature sensitivity in additive decision tree ensembles, considering the confidence of the ensemble's output. The authors propose a novel encoding using pseudo-Boolean constraints and develop a tunable algorithm that enhances sensitivity analysis, demonstrating improved performance on Gradient Boosted Decision Trees (GBDTs) benchmarks.

MeteoRA: Multiple-tasks Embedded LoRA for Large Language Models
The paper introduces MeteoRA, a scalable framework that integrates multiple task-specific LoRA adapters into a base LLM using a Mixture-of-Experts (MoE) architecture to improve autonomous task sensing and switching. Evaluations show that MeteoRA not only matches the performance of traditional parameter-efficient fine-tuning methods but also excels at handling composite tasks, enabling efficient and effective adapter switching.

Simplifying Deep Temporal Difference Learning
This paper introduces PQN, a simplified deep online $Q$-Learning algorithm, which achieves competitive performance with complex methods like Rainbow, PPO-RNN, and QMix, while being up to 50x faster than traditional DQN. By leveraging regularization techniques and online parallelized sampling, PQN maintains stabilization in training without the need for target networks or large replay buffers, thus reestablishing off-policy $Q$-learning as a robust alternative in reinforcement learning.

Relax and Merge: A Simple Yet Effective Framework for Solving Fair $k$-Means and $k$-sparse Wasserstein Barycenter Problems
This paper addresses the challenge of fair $k$-means clustering in Euclidean space by ensuring each cluster contains proportional representation from diverse groups. The authors introduce a "Relax and Merge" framework that improves upon existing methods by providing a more accurate approximation for both fair $k$-means clustering and the $k$-sparse Wasserstein Barycenter problem, demonstrating superior empirical performance over traditional approaches.

Fourier Sliced-Wasserstein Embedding for Multisets and Measures
We introduce the Fourier Sliced Wasserstein (FSW) embedding, a novel method for embedding multisets and measures over $\mathbb{R}^d$ into Euclidean space, preserving the sliced Wasserstein distance thereby maintaining input structure. The FSW embedding is both injective and bi-Lipschitz on multisets, offering superior metric properties and practical performance improvements in learning tasks compared to traditional sum- or max-pooling methods.

GrabS: Generative Embodied Agent for 3D Object Segmentation without Scene Supervision
This paper introduces a novel two-stage pipeline called GrabS for unsupervised 3D object segmentation in complex point clouds without human-labeled data. By learning object-centric priors and employing an embodied agent for object discovery, GrabS significantly outperforms existing methods in segmentation accuracy on both real-world and synthetic datasets.

Flavors of Margin: Implicit Bias of Steepest Descent in Homogeneous Neural Networks
This paper examines the implicit bias in steepest descent algorithms, such as gradient descent, for deep homogeneous neural networks and shows that an algorithm-dependent geometric margin increases during training. It introduces a generalized stationarity concept, demonstrates the reduction of a generalized Bregman divergence, and experimentally explores the connections to the implicit bias of Adam.

Personalized Visual Instruction Tuning
This paper addresses the "face blindness" limitation of multimodal large language models (MLLMs), which prevents them from personalizing dialogues for specific individuals. It introduces Personalized Visual Instruction Tuning (PVIT), a framework for training MLLMs to recognize individuals and engage in personalized dialogues, showing significant performance improvements as evaluated by the P-Bench benchmark.

SOAP: Improving and Stabilizing Shampoo using Adam for Language Modeling
This paper introduces SOAP, an optimization algorithm that combines Shampoo's higher-order preconditioning with Adam's efficiency by operating in the preconditioner's eigenbasis, reducing performance degradation and computational complexity. SOAP outperforms both AdamW and Shampoo in large-scale language model pre-training by significantly decreasing iterations and wall clock time while maintaining efficiency with only one additional hyperparameter.

Not All LLM-Generated Data Are Equal: Rethinking Data Weighting in Text Classification
This paper proposes efficient weighted-loss approaches to better align synthetic data generated by Large Language Models with real-world data distributions, using only a small amount of real-world data. The methods significantly enhance model performance on text classification tasks, outperforming standard cross-entropy and other data weighting techniques, thus offering potential solutions for leveraging synthetic data in various applications.

Beyond Content Relevance: Evaluating Instruction Following in Retrieval Models
This paper presents an evaluation of instruction-following capabilities in retrieval models, introducing the InfoSearch benchmark to assess document-level attributes such as Audience, Keyword, and Format. Despite improvements seen from fine-tuning and model scaling, most models still lack full compliance with user instructions, highlighting the gap in current retrieval technologies.

Decoupled Finetuning for Domain Generalizable Semantic Segmentation
The paper addresses the vulnerability of joint finetuning methods in semantic segmentation to domain shifts and introduces a novel framework, Decoupled FineTuning (DeFT), for domain generalization. DeFT employs a two-stage process that involves warming up the decoder with a frozen encoder and then decoupling their finetuning into adaptive and retentive pathways, significantly enhancing generalization capability and outperforming existing methods in diverse domain shift scenarios.

A General Framework for Producing Interpretable Semantic Text Embeddings
Semantic text embedding is crucial in NLP, but existing models struggle with interpretability. The paper introduces \algo{CQG-MBQA}, a framework generating interpretable embeddings through systematic yes/no questions, proving effective and competitive with black-box models while outperforming other interpretable methods in diverse tasks.

Simulating Human-like Daily Activities with Desire-driven Autonomy
The paper introduces a Desire-driven Autonomous Agent (D2A) that empowers a large language model (LLM) to autonomously propose and select tasks motivated by multi-dimensional desires, enhancing its behavioral diversity and autonomy. Experiments on a text-based simulator reveal that D2A generates coherent and adaptable activities akin to human behavior, outperforming other LLM-based agents in rationality and contextual relevance.

Advantage Alignment Algorithms
Artificially intelligent agents in human decision-making often result in conflicts due to optimizing individual objectives, especially in general-sum games. This paper introduces Advantage Alignment, a novel algorithm family for opponent shaping that simplifies mathematical formulations, reduces computational burden, extends to continuous action domains, and demonstrates enhanced cooperation and robustness against exploitation in social dilemmas.

Deep Random Features for Scalable Interpolation of Spatiotemporal Data
The paper introduces a novel approach to interpolating remote-sensing observations using Bayesian deep learning with neural networks that incorporate the inductive bias of stationary Gaussian processes (GPs) through random feature expansions. This method effectively captures high-frequency patterns and enables scalable training with mini-batched gradient descent, demonstrating competitive or superior performance with well-calibrated uncertainties on various remote sensing datasets.

Generalizable Motion Planning via Operator Learning
This paper introduces the Planning Neural Operator (PNO), a novel model for approximating value functions in motion planning by learning an operator defined by an Eikonal PDE from cost function space to value function space. The PNO model demonstrates zero-shot super-resolution capabilities, providing accurate predictions at resolutions 16× greater than training data, enhances planning efficiency in robotic and real-world mapping scenarios, and reduces computational effort by 30% with effective heuristic integration, outperforming classical methods.

Harnessing Diversity for Important Data Selection in Pretraining Large Language Models
The paper introduces $\texttt{Quad}$, a data selection method that enhances pretraining of large language models by balancing quality and diversity, thus achieving state-of-the-art results. Utilizing attention layers for accurate influence computation and clustering for diversity, $\texttt{Quad}$ outperforms existing methods while maintaining low computational cost, as demonstrated in experiments on Slimpajama and FineWeb datasets.

3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds
The paper introduces the Instruction Reasoning Affordance Segmentation (IRAS) task and the 3D-AffordanceLLM (3D-ADLLM) framework, which redefines 3D affordance detection by linking natural language queries to affordance mask generation, moving beyond fixed category labels. By integrating large language models and implementing a multi-stage training strategy with initial object-part level pre-training, the proposed method enhances open-world affordance reasoning, demonstrating an approximately 8% improvement in mean Intersection over Union (mIoU) for open-vocabulary tasks.

Accelerated Over-Relaxation Heavy-Ball Method: Achieving Global Accelerated Convergence with Broad Generalization
The Accelerated Over-Relaxation Heavy-Ball (AOR-HB) method improves the heavy-ball momentum technique by providing provable global and accelerated convergence for smooth strongly convex problems. AOR-HB not only closes a theoretical gap in optimization methods but also extends to composite convex optimization and min-max problems, achieving optimal complexity bounds while offering broader applicability and conceptual clarity.

Accelerated training through iterative gradient propagation along the residual path
The paper introduces Highway backpropagation, a parallelizable iterative algorithm designed to address the computational burden of traditional backpropagation by leveraging residual-like architectures to accumulate and backpropagate gradient estimates in parallel. Through extensive experiments across various models and tasks, the study demonstrates that Highway-BP can significantly speed up training with minimal loss in performance.

ACE: All-round Creator and Editor Following Instructions via Diffusion Transformer
The paper introduces ACE, an All-round Creator and Editor, a novel Transformer-based diffusion model that utilizes a unified condition format called Long-context Condition Unit (LCU) to enable multi-modal conditions in visual generation and editing tasks. It outperforms existing models by leveraging an efficient data collection approach and establishing a benchmark with manually annotated pairs, showcasing its ability to serve as a single model backend for a multi-modal chat system in image creation.

A Coefficient Makes SVRG Effective
This paper introduces $\alpha$-SVRG, an enhancement of the Stochastic Variance Reduced Gradient (SVRG) method, that dynamically adjusts the strength of variance reduction via a decay schedule to optimize deep neural networks more effectively. The authors' empirical results demonstrate that $\alpha$-SVRG consistently reduces training loss across diverse model architectures and image classification datasets, outperforming both the baseline and standard SVRG.

ActionReasoningBench: Reasoning about Actions with and without Ramification Constraints
The paper addresses a gap in the performance of Large Language Models (LLMs) on Reasoning about Actions and Change (RAC) by introducing a new benchmark, ActionReasoningBench, which evaluates LLMs across six RAC dimensions. Despite overall satisfactory performance in commonly discussed dimensions, the study finds significant challenges for LLMs, particularly in more complex reasoning tasks and handling ramifications, highlighting a need for further improvement in this foundational AI area.

Activation Gradient based Poisoned Sample Detection Against Backdoor Attacks
This paper introduces a novel approach, Activation Gradient based Poisoned sample Detection (AGPD), for identifying poisoned samples in data poisoning-based backdoor attacks by utilizing the gradient circular distribution (GCD) of sample activations. The method effectively separates clean and poisoned samples within the target class, outperforming existing detection techniques through extensive experiments under various backdoor attack scenarios.

Adapting Multi-modal Large Language Model to Concept Drift From Pre-training Onwards
The paper addresses the challenges faced by Multi-modal Large Language Models (MLLMs) in handling concept drift due to both gradual and sudden changes in data distribution. It introduces a unified framework with a T-distribution based drift adapter to improve the adaptability of Vision-Language models in unpredictable environments, demonstrating improved accuracy and efficiency in image-text alignment and various downstream tasks, supported by the OpenMMlo datasets available publicly.

ADBM: Adversarial Diffusion Bridge Model for Reliable Adversarial Purification
DiffPure has been effective in defending against adversarial examples but suffers from trade-offs between noise purification and data recovery, and its evaluations may be unreliable. The proposed Adversarial Diffusion Bridge Model (ADBM) constructs a reverse bridge from diffused adversarial data to clean examples, improving purification and demonstrating superior robustness as a defense mechanism.

Addressing Label Shift in Distributed Learning via Entropy Regularization
The paper introduces the Versatile Robust Label Shift (VRLS) method to minimize "true risk" in multi-node distributed learning environments, addressing the challenges posed by inter-node and intra-node label shifts. Through Shannon entropy-based regularization and an adaptive importance ratio, VRLS significantly improves model performance, outperforming baselines by up to 20% in experiments on MNIST, Fashion MNIST, and CIFAR-10, with theoretical analysis supporting its effectiveness.

Agent S: An Open Agentic Framework that Uses Computers Like a Human
Agent S is an open agentic framework designed to automate complex, multi-step tasks through GUI interaction, addressing challenges such as acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic interfaces. By introducing experience-augmented hierarchical planning and an Agent-Computer Interface based on Multimodal Large Language Models, Agent S achieves a state-of-the-art performance on the OSWorld benchmark, demonstrating its generalizability across operating systems and establishing itself as a significant advancement in autonomous human-computer interaction.

AgentTrek: Agent Trajectory Synthesis via Guiding Replay with Web Tutorials
The paper introduces AgentTrek, a scalable pipeline for synthesizing high-quality trajectory data for training GUI agents using web tutorials, addressing the limitations of traditional human annotation methods. This method enhances the grounding and planning capabilities of GUI agents while being cost-efficient, highlighting the potential of using guided replay with web tutorials for large-scale agent training.

Aligned LLMs Are Not Aligned Browser Agents
This paper investigates whether the safety refusals of large language models (LLMs), typically enforced in chat applications to avoid harmful instructions, extend to non-chat and agentic scenarios. Introducing the Browser Agent Red teaming Toolkit (BrowserART), the study evaluates state-of-the-art browser agents and finds that while LLMs refuse harmful instructions in chatbot settings, they do not in agentic contexts, highlighting the need for improved agent safety measures.

An Asynchronous Bundle Method for Distributed Learning Problems
This paper introduces a new asynchronous bundle method for distributed learning problems that improves accuracy without needing prior information on system delays, making it both fast and easy to tune. The algorithm's convergence is proven in both deterministic and stochastic settings, with practical benefits demonstrated through numerical experiments on classification problems of different complexities and scales.

AssembleFlow: Rigid Flow Matching with Inertial Frames for Molecular Assembly
AssembleFlow is introduced to simulate molecular assembly by leveraging inertial frames to track orientation and motion, enabling translational and rotational rigidity through the decomposition of molecular transformations into translations and rotations. This method significantly outperforms existing deep learning models and domain-specific tools in assembly accuracy and molecular integrity while reducing computational costs.

Attention with Markov: A Curious Case of Single-layer Transformers
This paper investigates the modeling capabilities of transformers using Markov input processes, revealing that multi-layer transformers develop an induction head mechanism to accurately estimate in-context bigram distributions, whereas single-layer transformers often get stuck in local minima related to unigram distributions. The study introduces a new framework for analyzing transformers through Markov chains, characterizes the loss landscape of single-layer models, and explains their empirical challenges, providing a theoretical basis supported by experiments and highlighting several open research questions.

A Unifying Framework for Representation Learning
The paper introduces a unified information-theoretic framework that generalizes various machine learning loss functions by minimizing an integrated KL divergence between supervisory and learned representations. This novel approach reveals underlying information geometry across several methods and leads to new loss functions that significantly enhance unsupervised image classification and debiasing in contrastive learning, achieving notable improvements on ImageNet-1K.

Better than Your Teacher: LLM Agents that learn from Privileged AI Feedback
LEAP is an iterative fine-tuning framework designed to enhance large language model agents by using feedback from AI expert teachers, who are equipped with privileged information during training but not at test time. LEAP significantly boosts performance on decision-making tasks, allowing weaker models to surpass stronger ones and improve themselves through a balance of privileged information and realizability, outperforming traditional methods like behavior cloning and ReAct.

Beyond Interpretability: The Gains of Feature Monosemanticity on Model Robustness
This study challenges the notion that improving interpretability through monosemanticity, where neurons are associated with distinct semantics, compromises model accuracy. By demonstrating that monosemantic features enhance both interpretability and performance across robustness-related tasks, the paper suggests that such features promote better separation of feature representations, leading to more robust decision boundaries under noise.

Bio-xLSTM: Generative modeling, representation and in-context learning of biological and chemical sequences
The paper introduces Bio-xLSTM, a suite of language models based on the recurrent xLSTM architecture, tailored for biological and chemical sequences. Bio-xLSTM addresses the limitations of Transformer models in handling long genomic sequences by offering linear runtime dependency, enabling efficient modeling and in-context learning for DNA, proteins, and small molecules.

Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration
This paper analyzes a dynamical systems model of a biological neural network, called the Brain Bandit Net (BBN), which controls explore-exploit decisions and demonstrates superior exploration efficiency compared to traditional reinforcement learning approaches. Through theoretical and simulation studies, the authors show that BBN performs posterior sampling of action values and resembles human and animal choice patterns, offering a brain-inspired algorithm for enhancing exploration in reinforcement learning tasks.

Breaking the $\log(1/\Delta_2)$ Barrier: Better Batched Best Arm Identification with Adaptive Grids
This paper addresses the batched best arm identification issue in multi-armed bandits by proposing an algorithm that balances sample and batch efficiency, breaking the traditional $\log(1/\Delta_2)$ barrier with an innovative sample allocation scheme. The algorithm demonstrates improved batch efficiency in experiments and is applicable to linear bandits, achieving significant advancements in both contexts.

Building Math Agents with Multi-Turn Iterative Preference Learning
This paper introduces a multi-turn direct preference learning framework to enhance large language models' ability to solve mathematical problems by integrating external tools and optimizing trajectory-level preferences. The proposed framework demonstrates significant performance improvements for models trained on GSM8K and MATH datasets, outperforming existing methods that rely on synthetic data generation and Supervised Fine-Tuning.

Cached Multi-Lora Composition for Multi-Concept Image Generation
Low-Rank Adaptation (LoRA) is commonly used in text-to-image models, but faces challenges when multiple LoRAs are composed, leading to decreased image quality. This paper proposes a novel frequency domain-based sequencing strategy and a training-free framework, Cached Multi-LoRA (CMLoRA), that optimizes LoRA integration, reduces semantic conflicts, and improves performance, evidenced by significant gains in CLIPScore and MLLM win rate over existing methods.

Can LLM Simulations Truly Reflect Humanity? A Deep Dive
This paper explores the challenges of using Large Language Models (LLMs) for simulating complex human social behaviors, highlighting discrepancies between simulations and real-world interactions. It offers insights and strategies for overcoming these limitations to improve the future applicability of LLM simulations in understanding human society.

Can Reinforcement Learning Solve Asymmetric Combinatorial-Continuous Zero-Sum Games?
This paper introduces a new class of asymmetric games called two-player Asymmetric Combinatorial-Continuous Zero-Sum (ACCES) games, relevant for scenarios involving combinatorial optimization problems with one player having a combinatorial action space and the other an infinite compact space. The authors establish the existence of a Nash equilibrium for these games using essentially finite game approximation, develop the Combinatorial Continuous DO (CCDO) algorithm to solve ACCES games, and propose a practical variant, CCDORL, demonstrating their effectiveness through empirical results.

Can Transformers Do Enumerative Geometry?
This paper introduces a Transformer-based method to compute $\psi$-class intersection numbers on the moduli space of curves, overcoming traditional methods' factorial complexity by treating the problem as a continuous optimization task. The study also discovers that the model implicitly adheres to Virasoro constraints and exhibits an emergent understanding of the asymptotic behavior of these intersection numbers, highlighting the network's ability to internalize complex mathematical properties in a data-driven way.

Causal Effect Estimation with Mixed Latent Confounders and Post-treatment Variables
This paper addresses the challenge of latent post-treatment bias in causal inference from observational data, where proxy variables might confound latent confounders and post-treatment variables. By introducing a Confounder-identifiable VAE (CiVAE), the study provides a method to disentangle and accurately identify latent confounders, proving that true causal effects can be unbiasedly estimated, as demonstrated through both simulated and real-world experiments.

CausalRivers - Scaling up benchmarking of causal discovery for real-world time-series
CausalRivers is introduced as the largest in-the-wild causal discovery benchmarking kit for time-series data, providing an extensive dataset on river discharge in eastern Germany and Bavaria from 2019 to 2023. This benchmark facilitates robust evaluations and comparisons of causal discovery methods and supports research in related areas like time-series forecasting and anomaly detection, promoting the development of advanced causal discovery techniques.

CEB: Compositional Evaluation Benchmark for Fairness in Large Language Models
As the deployment of Large Language Models (LLMs) raises concerns about biases in LLM-generated content, existing evaluations often focus on limited bias types with inconsistent metrics. This paper introduces the Compositional Evaluation Benchmark (CEB), incorporating a new taxonomy to systematically assess biases across multiple dimensions, revealing variations in bias levels and informing targeted mitigation strategies.

Century: A Framework and Dataset for Evaluating Historical Contextualisation of Sensitive Images
The paper introduces Century, a novel dataset of 1,500 sensitive historical images, designed to evaluate the capabilities of multi-modal generative models in recognizing and contextualizing recent historical events and figures. By employing both automated and human evaluations, the study highlights the challenges these models face in historical contextualization and provides practical recommendations for improving model performance using the Century dataset.

CertainlyUncertain: A Benchmark and Metric for Multimodal Epistemic and Aleatoric Awareness
This paper presents a taxonomy of uncertainty for vision-language AI systems, distinguishing between epistemic and aleatoric uncertainty, and introduces a benchmark dataset, CertainlyUncertain, to evaluate and enhance AI performance in uncertain scenarios. The study finds that supervised fine-tuning with CertainlyUncertain reduces calibration errors and hallucinations in vision-language models, highlighting the critical need to address uncertainty to enhance the reliability of these systems in real-world applications.

CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding
CG-Bench is a new benchmark designed to assess multimodal large language models' (MLLMs) comprehension of long videos by focusing on their ability to retrieve relevant clues for answer generation, rather than solely relying on multiple-choice questions. Comprising 1,219 videos and 12,129 QA pairs across various categories, CG-Bench demonstrates that current models face challenges with long video understanding, highlighting the performance disparity between open-source and commercial models and aiming to inspire advancements in reliable long video analysis.

ChatQA 2: Bridging the Gap to Proprietary LLMs in Long Context and RAG Capabilities
This paper introduces ChatQA 2, a Llama 3.0-based model designed with a 128K context window to enhance long context understanding and retrieval-augmented generation (RAG) capabilities, outperforming leading models like GPT-4-Turbo-2024-04-09. The study details a training method for extending context window size and instruction tuning, showing that ChatQA 2 excels in processing ultra-long tasks and RAG benchmarks, and shares its open-source resources with the community.

Co$^{\mathbf{3}}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion
This paper addresses the challenge of generating gestures for two-person interactive conversations by constructing a large-scale dataset, GES-Inter, with over 7 million frames. It introduces Co$^{3}$Gesture, a novel framework using a Temporal-Interaction Module and mutual attention to synthesize coherent gestures from speaker audio, outperforming previous models on the newly collected dataset.

Competition Dynamics Shape Algorithmic Phases of In-Context Learning
In this paper, the authors propose a synthetic sequence modeling task to understand In-Context Learning (ICL) by simulating a finite mixture of Markov chains, finding it reproduces key ICL insights and offers a unified setting for study. They reveal that ICL can be explained through a blend of four algorithms involving fuzzy retrieval and inference with context statistics, demonstrating that changes in context size and training can shift which algorithm dominates, thus explaining ICL's transient nature and challenging the notion of it as a monolithic capability.

Compositional simulation-based inference for time series
The paper introduces a novel simulation-based inference method optimized for Markovian simulators, allowing more efficient Bayesian inference for time series data by focusing on local parameter identification and composition. This approach enhances simulation efficiency and scalability, demonstrated through various benchmarks and a complex Kolmogorov flow simulation, offering significant improvements over traditional global posterior estimation techniques.

Compute-Constrained Data Selection
This paper addresses the challenge of compute-constrained finetuning for large language models by formalizing a cost-aware data selection problem that balances the initial selection cost against training gains. Through extensive experiments, the study reveals that many sophisticated data selection methods are not compute-optimal, and simpler, less costly alternatives are more effective, with perplexity and gradient data selection achieving optimal results at specific training-to-selection model size ratios.

Conformal Generative Modeling with Improved Sample Efficiency through Sequential Greedy Filtering
This paper introduces Sequential Conformal Prediction for Generative Models (SCOPE-Gen), a method that enhances generative models with rigorous statistical guarantees by producing prediction sets that include at least one valid example with high probability. By iteratively pruning an initial set of examples and controlling the admissibility factor through a conformal prediction method, SCOPE-Gen significantly reduces the number of costly admissibility evaluations needed, particularly benefiting safety-critical fields like natural language generation and molecular graph extension.

Counterfactual Realizability
This paper resolves the challenge of determining whether counterfactual distributions are directly realizable through experimentation by introducing a formal definition of realizability and a comprehensive algorithm. The authors demonstrate that in certain settings, such as causal fairness and causal reinforcement learning, a counterfactual strategy can outperform traditional interventional or observational methods.

Cross-Modal Safety Mechanism Transfer in Large Vision-Language Models
This paper identifies a gap in existing vision-language alignment methods, which fail to extend text-based safety mechanisms in Large Vision-Language Models (LVLMs) to visual inputs, leading to vulnerabilities with toxic images. To address this issue, the authors propose a novel Text-Guided vision-language Alignment (TGA) method that leverages related text to guide the alignment process, successfully transferring safety mechanisms to visual data without additional safety fine-tuning while preserving overall performance.

CyberHost: A One-stage Diffusion Framework for Audio-driven Talking Body Generation
CyberHost is a pioneering one-stage audio-driven framework for generating talking body animations that addresses common synthesis issues such as hand integrity and identity consistency. It introduces a Region Attention Module and Human-Prior-Guided Conditions to improve local synthesis and motion stability, outperforming previous methods in zero-shot video generation and extending effectively to hybrid-driven scenarios.

Cyclic Contrastive Knowledge Transfer for Open-Vocabulary Object Detection
This paper introduces CCKT-Det, a novel approach to open-vocabulary object detection that does not rely on additional supervisions like image-text pairs or pseudo annotations. By utilizing cyclic and dynamic knowledge transfer between language queries and visual region features from pretrained vision-language models, CCKT-Det enhances alignment with the visual-semantic space, achieving significant performance improvements on the COCO benchmark with minimal computation overhead.

DELIFT: Data Efficient Language model Instruction Fine-Tuning
DELIFT is an innovative algorithm designed to enhance the fine-tuning of large language models by optimizing data selection across instruction tuning, task-specific fine-tuning, and continual learning stages. By employing a pairwise utility metric to evaluate the informational value of data samples, DELIFT achieves up to 70% data size reduction while maintaining performance, thus offering substantial computational savings and surpassing existing methods in efficiency and effectiveness.

DelTA: An Online Document-Level Translation Agent Based on Multi-Level Memory
This paper introduces DelTA, a Document-levEL Translation Agent developed to enhance translation consistency and accuracy in machine translation using large language models (LLMs). DelTA employs a multi-level memory structure and a sentence-by-sentence translation strategy, significantly outperforming existing baselines and demonstrating improvements in translation consistency and context-dependent accuracy, with potential applications in query-based summarization tasks.

Demystifying Topological Message-Passing with Relational Structures: A Case Study on Oversquashing in Simplicial Message-Passing
This paper introduces a unifying axiomatic framework for topological deep learning (TDL), bridging graph and topological message-passing through the analysis of simplicial and cellular complexes. By extending graph-theoretic results to higher-order structures, the work addresses oversquashing in topological networks, offering theoretical and empirical insights to advance TDL methodologies.

Density estimation with LLMs: a geometric investigation of in-context learning trajectories
This paper examines the ability of large language models (LLMs) to estimate probability density functions (PDFs) through in-context learning, using Intensive Principal Component Analysis (InPCA) to analyze the learning dynamics of LLaMA-2 models. The study finds that LLaMA exhibits unique learning trajectories in a low-dimensional InPCA space, unlike traditional density estimation methods, and interprets their process as a kernel density estimation with an adaptive kernel that provides insights into LLaMA's probabilistic reasoning mechanisms.

Depth Any Video with Scalable Synthetic Data
This paper presents Depth Any Video, a novel model for video depth estimation, addressing the scarcity of consistent ground truth data with a scalable synthetic data pipeline and leveraging generative video diffusion models. The model introduces a mixed-duration training strategy, enabling robust performance across varying video lengths and frame rates, surpassing prior models in spatial accuracy and temporal consistency, with open-sourced code and model weights.

Descent with Misaligned Gradients and Applications to Hidden Convexity
This paper addresses the challenge of minimizing a convex objective using an oracle that provides "misaligned" stochastic gradients, with guaranteed correlation but not necessarily equality to the true gradients. The authors develop optimization algorithms achieving $\tilde O(\epsilon^{-2})$ and $\tilde O(\epsilon^{-3})$ iteration complexities depending on the rate of misalignment change, and apply their framework to problems with "hidden convexity" to achieve $O(\epsilon^{-3})$ complexity.

Diffusion Bridge AutoEncoders for Unsupervised Representation Learning
Diffusion-based representation learning faces challenges related to the computational expense and inflexibility of the diffusion endpoint, leading to an *information split problem*. Diffusion Bridge AutoEncoders (DBAE) mitigate these issues by using a $\mathbf{z}$-dependent endpoint inference through a feed-forward architecture, resulting in enhanced inference quality, improved reconstruction, and high-fidelity sample generation.

Discrete Latent Plans via Semantic Skill Abstractions
This paper introduces LADS, a hierarchical method for skill learning from language instructions that employs semantic skill abstractions to enhance task generalization and adherence to complex instructions. By decoupling the learning of a discrete latent plan space from the language-conditioned high-level policy, LADS improves training stability and outperforms state-of-the-art approaches in simulated control environments.

Disentangled Representation Learning with the Gromov-Monge Gap
This paper introduces a novel approach to disentangled representation learning using quadratic optimal transport, formulated through Gromov-Monge maps to minimally distort geometric features while aligning distributions. The proposed Gromov-Monge-Gap (GMG) regularizer effectively preserves geometric features, outperforming existing methods across four standard benchmarks.

Distributional Associations vs In-Context Reasoning: A Study of Feed-forward and Attention Layers
This paper investigates the roles of feed-forward and attention layers in Transformer architectures, revealing that feed-forward layers are predisposed to learn simple distributional patterns, whereas attention layers are more aligned with in-context reasoning. Through empirical and theoretical analysis, including ablations on the Pythia model family, the study identifies noise in the gradients as a significant contributor to this discrepancy in task performance.

Does Editing Provide Evidence for Localization?
This paper investigates the reliability of using localized edits within large language models (LLMs) to infer semantically meaningful behaviors of specific components. By applying an adapted alignment technique to identify optimal localized edits, the study reveals that evidence from edits does not necessarily indicate that these locations encode the targeted behavior, as edits at random localizations can be as effective as over the entire model.

Domain Guidance: A Simple Transfer Approach for a Pre-trained Diffusion Model
This paper introduces "Domain Guidance," a novel method for conditional generation that effectively transfers pre-trained diffusion models to target domains, ensuring high-quality outputs. Demonstrated through empirical and theoretical analysis, Domain Guidance achieves superior domain alignment and significant performance improvements—over 19.6% in FID and 23.4% in FD$_\text{DINOv2}$—compared to traditional fine-tuning, allowing for seamless integration with existing models.

DPLM-2: A Multimodal Diffusion Protein Language Model
This paper introduces DPLM-2, a multimodal protein foundation model that integrates sequence and structure modeling in proteins by extending the discrete diffusion protein language model (DPLM). By converting 3D coordinates into discrete tokens and training on both experimental and synthetic data, DPLM-2 efficiently generates compatible amino acid sequences and their 3D structures in one stage, outperforming existing two-stage methods in tasks like folding, inverse folding, and scaffolding.

Drop-Upcycling: Training Sparse Mixture of Experts with Partial Re-initialization
This paper introduces Drop-Upcycling, a method that enhances the training efficiency of Mixture of Experts (MoE) models by combining pre-trained knowledge with selective weight re-initialization to promote expert specialization. Extensive experiments show that Drop-Upcycling outperforms existing MoE methods, allowing a 5.9B parameter model to match the performance of a 13B dense model while using significantly fewer resources, and all related resources are made publicly available to support further research.

Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets
The paper introduces Nabla-GFlowNet, a novel method leveraging reward gradients to effectively finetune pretrained diffusion models while preserving sample diversity and prior knowledge. Demonstrating fast and efficient finetuning on Stable Diffusion, this approach addresses limitations of existing post-training methods by utilizing an innovative objective called \nabla-DB and its residual variant for realistic reward functions.

Efficient Exploration and Discriminative World Model Learning with an Object-Centric Abstraction
This study investigates the use of an object-centric mapping in reinforcement learning to improve exploration efficiency by employing hierarchical state and temporal abstractions. The proposed fully model-based algorithm outperforms existing methods by effectively solving tasks, transferring knowledge across environments, and planning over long horizons using a simplified transition dynamic, as demonstrated in 2D crafting and MiniHack environments.

Efficient Training of Neural Stochastic Differential Equations by Matching Finite Dimensional Distributions
The paper introduces a novel approach called Finite Dimensional Matching (FDM) for training Neural Stochastic Differential Equations (Neural SDEs), addressing limitations of existing methods like GANs and signature kernels. FDM leverages a novel class of strictly proper scoring rules and the Markov property to achieve superior computational efficiency and generative performance, reducing training complexity from quadratic to linear in terms of discretization steps.

Eliminating Position Bias of Language Models: A Mechanistic Approach
The paper addresses the prevalent issue of position bias in modern language models, which affects their performance and reliability. Through a simple mechanistic analysis, the authors propose a training-free zero-shot approach called Position-INvariant inferencE (PINE) to eliminate position bias by altering causal attention and utilizing model attention values to enhance performance across various tasks, such as retrieval-augmented QA and reasoning evaluations, yielding significant improvements over existing models.

Empowering Users in Digital Privacy Management through Interactive LLM-Based Agents
This paper introduces an innovative application of large language models (LLMs) to improve user comprehension of privacy policies via an interactive dialogue agent, outperforming traditional models in several key tasks. A user study demonstrated that the LLM-based agent significantly enhances user understanding, reduces cognitive load, and saves time, highlighting its potential to transform how users interact with privacy policies and achieve more informed consent.

Endowing Visual Reprogramming with Adversarial Robustness
This paper addresses the adversarial robustness of visual reprogramming (VR) by finding that reprogramming pre-trained models with adversarial robustness and using adversarial samples from target tasks improves the resilience of reprogrammed models to attacks. It also introduces a theoretically guaranteed adversarial robustness risk upper bound for VR, validated through extensive experiments, enhancing both practical and theoretical understanding of VR's robustness.

Enhancing Clustered Federated Learning: Integration of Strategies and Improved Methodologies
This paper addresses the challenge of client heterogeneity in Federated Learning (FL) by examining existing clustered FL methods and introducing a new four-tier framework called HCFL. The authors propose an enhanced clustering method, HCFL$^{+}$, which improves upon existing methods and demonstrates effectiveness through extensive numerical evaluations, with code available at the provided GitHub link.

Enhancing Federated Domain Adaptation with Multi-Domain Prototype-Based Federated Fine-Tuning
Federated Domain Adaptation (FDA) faces challenges due to data heterogeneity, impacting model efficacy across different domains in federated systems. To address this, the proposed Multi-domain Prototype-based Federated Fine-Tuning (MPFT) framework enhances in-domain and out-of-domain accuracy by utilizing multi-domain prototypes for model fine-tuning, achieving convergence in a single communication round while ensuring data privacy through differential privacy protections.

Enhancing Multilingual Reasoning in LLMs: Insights from Cross-Linguistic Correlations and Optimal Data Proportions
This paper investigates the impact of language proportions in multilingual reasoning datasets on the fine-tuning performance of large language models (LLMs). The study identifies optimal language distributions that resulted in state-of-the-art performance in multilingual mathematical reasoning, reduced data volume needs, and lowered translation costs, offering valuable insights for future research.

Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective
This paper introduces a new metric, SemVarEffect, and a benchmark, SemVarBench, to evaluate semantic variation handling in text-to-image (T2I) synthesis. By demonstrating that current models, such as CogView-3-Plus and Ideogram 2, have limited understanding of complex linguistic patterns, the study highlights the significant role of cross-modal alignment in improving T2I models, offering a reliable evaluation framework for future research.

Fast Uncovering of Protein Sequence Diversity from Structure
InvMSAFold is an innovative inverse folding method designed to rapidly generate diverse protein sequences by optimizing a pairwise probability distribution based on amino acid covariances from Multiple Sequence Alignments (MSA). This approach not only preserves structural and functional characteristics but also enhances variability in biochemical properties, making it a powerful tool for protein design and enabling significant advancements in high-throughput virtual screening.

Feature Responsiveness Scores: Model-Agnostic Explanations for Recourse
This paper examines the limitations of current feature attribution methods used to comply with consumer protection laws in decision-making systems, highlighting that they often fail to provide actionable recourse for individuals. The authors propose a novel approach to score features based on "responsiveness" to enable better recourse, and they offer efficient methods to compute these scores, demonstrating their effectiveness in the context of lending.

Finding Shared Decodable Concepts and their Negations in the Brain
This study employs a multimodal neural network architecture, specifically CLIP, combined with a novel adaptation of the DBSCAN clustering algorithm to map brain responses to visual stimuli, revealing Shared Decodable Concepts (SDCs) from common voxel patterns across individuals. By identifying and examining these SDCs in brain regions, the research uncovers new and existing visuo-semantic representations, such as the sensitivity to specific visual features and novel areas with visuo-semantic sensitivity, thereby enhancing our understanding of functional localization in the brain.

Fine-tuning can Help Detect Pretraining Data from Large Language Models
The paper introduces a novel method called Fine-tuned Score Deviation (FSD) to enhance the detection of pretraining data in large language models by addressing the challenge of distinguishing between members and non-members. By measuring the deviation in scores after fine-tuning with a small amount of unseen data, the method significantly improves performance, as demonstrated by better AUC scores on common benchmarks.

Flow-based Variational Mutual Information: Fast and Flexible Approximations
This paper introduces new variational estimators based on Normalizing Flows to calculate Mutual Information (MI), enhancing the expressivity of the variational distribution compared to traditional Gaussian-based estimators. The proposed estimators are shown to be effective for large MI problems and in the Bayesian Optimal Experimental Design setting, outperforming or matching discriminative-based estimators like MINE and InfoNCE in various benchmark tests.

Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective
This paper presents a novel design framework for discrete generative models using continuous-time Markov chains, allowing for the first time the use of arbitrary discrete probability paths, or corruption processes. The authors introduce velocity formulas that decouple probability and velocity, enabling enhanced model performance across various modalities, including text, inorganic materials, and image generation, outperforming traditional masked constructions.

For Better or For Worse? Learning Minimum Variance Features With Label Augmentation
This paper examines the impact of label augmentation techniques like label smoothing and Mixup on learning in deep models, revealing that they prioritize low variance features in the data. It highlights both the advantage of improving image classification performance and the potential downside of an increased susceptibility to spurious correlations.

From Complexity to Clarity: Analytical Expressions of Deep Neural Network Weights via Clifford Algebra and Convexity
This paper presents a novel analysis of neural networks using geometric (Clifford) algebra and convex optimization, demonstrating that optimal weights in deep ReLU networks are derived from the wedge product of training samples. It reveals that training reduces to a convex optimization problem involving wedge product features, illuminating the geometric structure of the dataset and offering new insights into the roles of hidden layers through $\ell_1$ regularization to identify key features.

GaussianAnything: Interactive Point Cloud Flow Matching for 3D Generation
This paper introduces GaussianAnything, a novel 3D generation framework that addresses challenges in input formats, latent space structures, and output representations through a VAE-based system utilizing multi-view posed RGB-D-N renderings. The framework features a Point Cloud-structured Latent space and includes a cascaded latent flow-based model for enhanced shape-texture disentanglement, achieving superior multi-modal conditional 3D generation performance on various datasets compared to state-of-the-art methods.

GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-Time Alignment
GenARM introduces an innovative test-time alignment method for Large Language Models, utilizing an Autoregressive Reward Model to predict next-token rewards, which allows for efficient autoregressive text generation and eliminates the need for costly retraining. The approach matches the performance of traditional training-time methods, enables alignment with smaller reward models, and supports multi-objective alignment, accommodating diverse user preferences in real-time.

General Scene Adaptation for Vision-and-Language Navigation
The paper introduces GSA-VLN, a novel task for Vision-and-Language Navigation that requires agents to adapt to specific, consistent environments over time, reflecting real-world navigation conditions. By creating the GSA-R2R dataset with increased instructional diversity and designing an innovative instruction orchestration pipeline, the authors evaluate agent adaptability and propose the Graph-Retained DUET (GR-DUET) method, setting new standards for performance in the task.

Generating CAD Code with Vision-Language Models for 3D Designs
Generative AI advances Design and Manufacturing by enabling automated generation of 3D objects, yet verifying the accuracy of AI-generated CAD code remains complex. This paper introduces CADCodeVerify, a novel method utilizing Vision-Language Models to enhance and validate 3D object generation, which, when benchmarked with CADPrompt, improves visual accuracy and success rates of 3D compilations, notably achieving enhancements with GPT-4.

Generative Classifiers Avoid Shortcut Solutions
Generative classifiers, which utilize class-conditional generative models, can address the limitations of discriminative classifiers that often fail under distribution shifts due to reliance on spurious features. By effectively modeling both core and spurious features without requiring specialized augmentations or knowledge of spurious correlations, these classifiers demonstrate state-of-the-art performance across various benchmarks, including image and text datasets, particularly in domains like medical and satellite data.

Generative Verifiers: Reward Modeling as Next-Token Prediction
The paper introduces generative verifiers (GenRM) that leverage large language models' text generation capabilities for verification and solution generation, enhancing the Best-of-N method's reasoning performance. GenRM significantly outperforms traditional discriminative verifiers across various tasks, showing substantial gains in accuracy and demonstrating favorable scaling with model size and compute during inference.

Generative World Explorer
The paper introduces the Generative World Explorer (Genex), a video generation model that enables agents to perform mental exploration in large-scale 3D environments, thus updating their beliefs with imagined observations to make informed decisions without physical exploration. The effectiveness of Genex is demonstrated through a synthetic urban scene dataset, showing it generates consistent observations and enhances decision-making in existing models.

Generator Matching: Generative modeling with arbitrary Markov processes
Generator Matching is a versatile framework for generative modeling that utilizes arbitrary Markov processes, unifying several existing methods like diffusion and flow matching while also expanding to novel processes such as jump processes. The method allows for the creation of multimodal models and demonstrates improved performance with innovations like superposition using jump processes, validated through empirical tests on image and multimodal generation.

Glad: A Streaming Scene Generator for Autonomous Driving
The paper presents Glad, a framework designed to generate synthetic video data for autonomous driving scenarios by maintaining temporal consistency through a latent variable propagation module. Extensive experiments on the nuScenes dataset reveal its efficacy, establishing Glad as a robust tool for online video generation with plans to release its source code and models.

GOFA: A Generative One-For-All Model for Joint Graph Language Modeling
This paper addresses the challenge of developing a Graph Foundation Model (GFM) by extending traditional language modeling approaches to graph data, introducing a novel model called GOFA. GOFA combines semantic and structural modeling through integrated GNN and LLM layers, achieving self-supervised pretraining, task fluidity, and graph awareness, and demonstrating strong zero-shot problem-solving capabilities across various tasks.

GPromptShield: Elevating Resilience in Graph Prompt Tuning Against Adversarial Attacks
This paper addresses the vulnerability of graph prompts in adversarial attack scenarios by shifting the focus from compatibility to security. It introduces a shield defense system that enhances prompt robustness through direct handling of invalid information and indirect amplification of valid features, demonstrating improved resilience in various adversarial settings through extensive experiments.

Gradient correlation is a key ingredient to accelerate SGD with momentum
This paper investigates the acceleration of Stochastic Nesterov Accelerated Gradient (SNAG), a momentum-based version of Stochastic Gradient Descent (SGD), in convex optimization settings. The authors prove that the average correlation between gradients facilitates satisfying the strong growth condition, leading to accelerated convergence, with numerical experiments in linear regression and deep neural networks supporting their theoretical findings.

Gradient-Free Generation for Hard-Constrained Systems
The paper introduces ECI sampling, a novel framework that adapts pre-trained, unconstrained flow-matching models to strictly satisfy hard constraints without requiring computationally expensive gradient calculations or fine-tuning. This approach is shown to be effective in various PDE systems, outperforming baseline methods in zero-shot constrained generation tasks and achieving competitive results in regression tasks while maintaining adherence to physical constraints.

HALL-E: Hierarchical Neural Codec Language Model for Minute-Long Zero-Shot Text-to-Speech Synthesis
This paper addresses the challenge of long-form speech synthesis in Text-to-Speech (TTS) models by introducing two novel post-training approaches, Multi-Resolution Re-quantization (MReQ) and HALL-E, which leverage hierarchical token prediction to significantly reduce the frame rate of Neural Audio Codec models. The study also introduces a new benchmark dataset, MinutesSpeech, and demonstrates the effectiveness of these approaches by achieving stable, minute-long speech synthesis with a frame rate as low as 8 Hz using a VALL-E model.

HERO: Human-Feedback Efficient Reinforcement Learning for Online Diffusion Model Finetuning
This paper introduces HERO, a framework designed to improve controllable generation through Stable Diffusion by effectively utilizing online human feedback during model learning. HERO's innovative mechanisms, Feedback-Aligned Representation Learning and Feedback-Guided Image Generation, significantly enhance efficiency and effectiveness in tasks such as anomaly correction and content moderation, demonstrating superior performance with minimal online feedback.

HexGen-2: Disaggregated Generative Inference of LLMs in Heterogeneous Environment
HexGen-2 is a distributed system designed for high throughput and cost-efficient serving of large language models on heterogeneous GPUs, utilizing a disaggregated inference paradigm. It features a sophisticated scheduling algorithm that optimizes resource allocation and communication by leveraging graph partitioning and max-flow algorithms, achieving significant improvements in throughput and latency compared to state-of-the-art systems.

How do we interpret the outputs of a neural network trained on classification?
This paper explains how the output activations of deep neural networks can be interpreted as approximations of the Bayesian posterior probability in classification tasks. Through theoretical and empirical studies, it demonstrates that while neural networks can closely approximate this posterior in simple tasks, their effectiveness varies with task complexity and data availability.

HyperDAS: Towards Automating Mechanistic Interpretability with Hypernetworks
The paper introduces HyperDAS, a transformer-based hypernetwork architecture designed to automatically identify and learn features within the residual stream where a concept is expressed, improving upon the limitations of Distributed Alignment Search (DAS). HyperDAS demonstrates state-of-the-art performance on the RAVEL benchmark for disentangling concepts in hidden states and addresses concerns of potentially injecting new information into models rather than faithfully interpreting them.

IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations
IDArb is a diffusion-based model that enhances the intrinsic decomposition of images under various lighting conditions, ensuring accurate and multi-view consistent estimates of surface normals and material properties. Through innovations like a cross-view attention module and a new dataset, ARB-Objaverse, IDArb surpasses existing methods in accuracy, offering improved solutions for tasks such as single-image relighting and 3D reconstruction.

Image and Video Tokenization with Binary Spherical Quantization
We introduce Binary Spherical Quantization (BSQ), a novel transformer-based image and video tokenizer that efficiently compresses visual data by projecting high-dimensional embeddings onto a lower-dimensional hypersphere and applying binary quantization, achieving compression rates of up to 100× with minimal distortion. The BSQ-ViT model demonstrates state-of-the-art reconstruction quality and competitive image synthesis performance, outperforming prior methods in throughput and matching standard compression benchmarks like JPEG2000/WebP for images and H.264/H.265 for videos.

Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for LLM Problem-Solving
This study explores inference scaling laws for large language models (LLMs), focusing on the trade-offs between model sizes and different inference strategies to achieve compute-optimal inference. It demonstrates that smaller models paired with advanced inference techniques, like a new tree search algorithm, can outperform larger models in terms of cost and performance, offering a new perspective on test-time scaling laws for LLMs.

InfoGS: Efficient Structure-Aware 3D Gaussians via Lightweight Information Shaping
The paper introduces a method called InfoGS, which utilizes mutual information shaping through a Gaussian attribute decoding network to coordinate 3D Gaussians, allowing for efficient scene editing and animation by leveraging correlations from 2D object masks. This approach significantly improves 3D object segmentation and scene interactions, demonstrating enhanced performance with minimal computational and memory demands.

INS: Interaction-aware Synthesis to Enhance Offline Multi-agent Reinforcement Learning
The paper addresses data scarcity challenges in offline multi-agent reinforcement learning by introducing **INteraction-aware Synthesis (INS)**, a novel method that synthesizes high-quality multi-agent datasets using diffusion models. INS employs a sparse attention mechanism and a bit action module to capture inter-agent interactions effectively, achieving superior dataset quality and improved policy performance even with minimal original data, as demonstrated by experimental results in MPE and SMAC environments.

Intrinsic User-Centric Interpretability through Global Mixture of Experts
The paper introduces InterpretCC, a novel family of intrinsically interpretable neural networks designed to enhance human understanding and explanation faithfulness while maintaining competitive performance levels with state-of-the-art models. By employing adaptive sparse activation and a global mixture-of-experts model, InterpretCC enables actionably useful explanations, which proved more beneficial in a user study with 56 teachers compared to other interpretable models.

JetFormer: An autoregressive generative model of raw images and text
This paper introduces JetFormer, an autoregressive decoder-only transformer aimed at reducing the complexity in multimodal modeling by eliminating the need for separately pretrained components. JetFormer utilizes a normalizing flow model for simultaneous image encoding and decoding, achieving competitive text-to-image generation quality while demonstrating robust image understanding.

Jump Your Steps: Optimizing Sampling Schedule of Discrete Diffusion Models
This paper introduces _Jump Your Steps_ (JYS), a method to improve discrete diffusion models (DDMs) by optimizing the allocation of discrete sampling timesteps to minimize Compounding Decoding Error (CDE) without additional computational cost. Experiments demonstrate that JYS significantly enhances sampling quality in image, music, and text generation, establishing it as a versatile framework for faster and better-performing DDMs.

LaMP: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning
This paper presents LaMP, a Language-Motion Pretraining model that overcomes the limitations of CLIP embeddings by creating a more suitable language-motion latent space for tasks like text-to-motion generation, motion-text retrieval, and motion captioning. By leveraging novel approaches in motion-informative text embeddings and transformer techniques, LaMP significantly improves the relevance and semantics of generated motion sequences, achieving better results than previous methods, as demonstrated by extensive experiments.

Language Model Alignment in Multilingual Trolley Problems
This study evaluates the moral alignment of large language models (LLMs) with human preferences using a cross-lingual corpus of trolley problem vignettes in over 100 languages, called MultiTP. By analyzing 19 different LLMs, the research reveals significant cross-lingual and ethical biases, challenging the notion of uniform moral reasoning in AI systems and emphasizing the necessity for diverse perspectives in AI ethics and multilingual responsible AI research.

Laplace Sample Information:  Data Informativeness Through a Bayesian Lens
The paper introduces $\mathsf{LSI}$, a novel measure of sample informativeness based on information theory, facilitating efficient sample selection by estimating their contribution to the model's parameter distribution. It effectively orders data, detects mislabeled samples, and assesses dataset difficulty across various architectures and settings, proving both efficient and transferable in training large models.

Last Iterate Convergence of Incremental Methods as a Model of Forgetting
This paper presents the first nonasymptotic convergence guarantees for the last iterate of incremental gradient and incremental proximal methods in general convex settings, improving upon existing results that primarily focus on the ergodic iterate. It provides oracle complexity bounds that nearly match previous best-known results and explores the implications for continual learning, highlighting the need for substantial regularization to prevent catastrophic forgetting.

Latent-EnSF: A Latent Ensemble Score Filter for High-Dimensional Data Assimilation with Sparse Observation Data
This paper introduces Latent-EnSF, a novel data assimilation method that enhances Ensemble Score Filters (EnSF) by utilizing efficient latent representations through a coupled Variational Autoencoder (VAE) to improve nonlinear Bayesian filtering in scenarios with high-dimensional states and sparse observations. Latent-EnSF demonstrates superior accuracy, faster convergence, and higher efficiency compared to existing methods in challenging applications such as shallow water wave propagation and medium-range weather forecasting.

LayerDAG: A Layerwise Autoregressive Diffusion Model for Directed Acyclic Graph Generation
LayerDAG is a novel autoregressive diffusion model designed to generate realistic directed acyclic graphs (DAGs) by decoupling strong node dependencies into sequentially manageable units and effectively modeling both directional and logical dependencies. This approach outperforms existing models in generating large-scale DAGs, enhancing ML-based surrogate model training, and improving predictive accuracy for performance metrics on diverse computing platforms.

Learning-Augmented Frequent Directions
This paper simplifies and generalizes learning-augmented streaming algorithms for frequency estimation, building on the work of Hsu et al. It introduces a deterministic learning-augmented Misra-Gries algorithm and extends learning-augmentation techniques to high-dimensional frequency estimation, achieving state-of-the-art performance and enhancing the understanding of matrix streaming with learned predictions.

Learning Color Equivariant Representations
This paper introduces group convolutional neural networks (GCNNs) that are equivariant to color variations, enhancing performance by resolving issues with invalid RGB values through a lifting layer. The proposed networks, which extend color equivariance to include hue, saturation, and luminance, show strong generalization and improved sample efficiency, outperforming competitive baselines on both synthetic and real-world datasets.

Learning Dynamics of LLM Finetuning
The paper investigates the learning dynamics of large language models during finetuning, offering a framework that explains how specific types of responses, such as hallucinations, are reinforced. By analyzing influence accumulation and introducing a "squeezing effect," the study provides insights into finetuning behaviors and proposes a method to enhance alignment performance in models.

Learning Multi-Index Models with Neural Networks via Mean-Field Langevin Dynamics
This paper explores learning multi-index models in high-dimensional settings with a two-layer neural network using the mean-field Langevin algorithm, identifying an effective dimension \(d_{\mathrm{eff}}\) that significantly influences sample and computational complexities. The study demonstrates that while sample complexity grows almost linearly with \(d_{\mathrm{eff}}\), computational complexity may increase exponentially; however, placing weights on a compact manifold with positive Ricci curvature potentially enables polynomial time convergence.

Learning Structured Universe Graph with Outlier OOD Detection for Partial Matching
This paper addresses the challenges of partial graph matching in computer vision, particularly dealing with point occlusion and annotation errors. By introducing a structured universe graph to handle occlusion and an energy-based detection mechanism to eliminate annotation errors, the proposed method significantly outperforms existing techniques on the Pascal VOC and Willow Object datasets, underscoring its robustness and accuracy.

Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation
This paper introduces a novel approach for few-shot test-time domain adaptation by learning directly on the input space to complement dataset-specific knowledge for frozen CLIP, particularly enhancing performance for models with less robust backbones like ViT-B/16. By integrating an independent side branch with revert attention and using a greedy text ensemble and refinement for better label semantics capture, the method shows significant performance improvements on challenging benchmarks like WILDS and DomainNet, achieving notable gains in metrics such as F1 and WC Acc.

LICORICE: Label-Efficient Concept-Based Interpretable Reinforcement Learning
This paper introduces LICORICE, a novel training scheme that allows reinforcement learning (RL) agents to learn interpretable, concept-based policies with minimal human annotation, addressing the challenge of the impractical annotation load inherent in traditional methods. By interleaving concept learning with RL training and using an ensemble approach for efficient data selection, LICORICE significantly reduces annotation requirements without compromising performance, advancing the practical application of interpretable RL in real-world scenarios.

Linear SCM Identification in the Presence of Confounders and Gaussian Noise
This paper addresses the identifiability of noisy linear structural causal models (SCMs) in scenarios involving Gaussian noise and confounding variables, establishing that unique identifiability is possible when certain conditions regarding the number and nature of confounders are met, specifically when the causal structure is known and confounders lack a Gaussian component. It further explores finite identifiability without known causal structure and provides analytical insights into the joint probability distribution functions of the SCMs, while offering explanations for unidentifiability in Gaussian noise and confounder scenarios.

Linear Transformer Topological Masking with Graph Random Features
This paper introduces a novel approach to topological masking for transformers on graph-structured data by parameterizing masks as a learnable function of a weighted adjacency matrix, allowing for strong structural inductive biases. By using graph random features to approximate masks, the method ensures compatibility with linear attention, achieving superior efficiency with $\mathcal{O}(N)$ complexity and performance gains in tasks involving large graphs, surpassing previous methods with $\mathcal{O}(N \log N)$ complexity.

LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models
The paper introduces LLaVA-NeXT-Interleave, a model that advances Large Multimodal Models (LMMs) by enabling simultaneous handling of multiple scenarios such as multi-image, multi-frame, multi-view, and multi-patch tasks. By employing an interleaved data format and compiling a comprehensive dataset, the model achieves leading results in various benchmarks and shows emerging capabilities like task transfer across different settings and modalities.

LLMs Can Plan Only If We Tell Them
This paper explores the capability of large language models (LLMs) to generate long-horizon plans independently, challenging their current reliance on external feedback mechanisms. By introducing novel enhancements termed AoT+, the study demonstrates that LLMs can autonomously achieve state-of-the-art results on planning benchmarks, surpassing prior methods and human baselines.

Locality Sensitive Avatars From Video
The paper introduces a locality-sensitive avatar, a neural radiance field (NeRF) based network, designed to learn human motions from monocular videos by utilizing a graph neural network (GNN) for fine-grained detail retention. This approach, which is evaluated on various datasets, achieves state-of-the-art results in novel view synthesis, novel pose animation, and 3D shape reconstruction by modeling non-rigid motions locally, enhancing realistic shape contours and details.

Long-time asymptotics of noisy SVGD outside the population limit
This paper investigates the long-time asymptotic behavior of a noisy variant of Stein Variational Gradient Descent (SVGD) in the context of a finite number of particles. The study establishes that the limit set of noisy SVGD approaches the target distribution as the number of particles increases, effectively avoiding the variance collapse seen in standard SVGD, and provides a connection to McKean-Vlasov processes.

Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency
This paper presents Loopy, an end-to-end audio-only conditioned video diffusion model designed to enhance the naturalness and stability of audio-driven human video generation. By introducing an inter- and intra-clip temporal module and an audio-to-latents module, Loopy outperforms previous models, delivering more lifelike portrait videos without the need for auxiliary spatial signals, thereby achieving higher quality and subtle facial expressions across various scenarios.

L-WISE: Boosting human visual category learning through model-based image selection and enhancement
This study shows that artificial neural network models of the visual ventral stream can improve human visual categorization accuracy by using model-generated image perturbations and difficulty estimations. By selecting images based on estimated recognition difficulty and applying aiding perturbations, these strategies increase categorization accuracy by 33-72% and reduce training time by 20-23%, demonstrating significant improvements in both general and clinically relevant visual learning tasks.

MAD-TD: Model-Augmented Data stabilizes High Update Ratio RL
This paper addresses the challenge of achieving sample efficiency in deep reinforcement learning by proposing a method called Model-Augmented Data for TD Learning (MAD-TD), which utilizes small amounts of data generated from a learned world model to stabilize training with high update-to-data ratios. MAD-TD demonstrates competitive performance on challenging tasks and provides practical stability gains by combatting value overestimation without the need for periodic neural network parameter resets.

MAGNet: Motif-Agnostic Generation of Molecules from Scaffolds
This paper introduces MAGNet, a novel machine learning model for molecule generation that separates structure from features by abstracting motifs to scaffolds, allowing for diverse atom and bond assignments. MAGNet addresses the limitations of existing models by freely learning motifs, resulting in the generation of structurally diverse molecules with improved expressivity, which enhances the potential for discovering novel compounds.

MamKO: Mamba-based Koopman operator for modeling and predictive control
This paper introduces the MamKO framework, which combines the Koopman theory with the Mamba large language model to enhance the modeling and adaptability of nonlinear and time-varying systems. By generating Koopman operators from online data and developing a model predictive control system, the approach demonstrates superior performance and opens new avenues for integrating large language models with control frameworks.

Many-Objective Multi-Solution Transport
This paper introduces the "Many-objective multi-solution Transport (MosT)" framework, which addresses the challenge of optimizing numerous objectives with a limited number of solutions in machine learning. By employing a bi-level optimization approach using optimal transport of weighted objectives, MosT effectively discovers diverse solutions that cover all objectives and outperforms existing methods in applications like federated learning and multi-task learning, ensuring a balanced trade-off across the objectives.

MAVIS: Mathematical Visual Instruction Tuning with an Automatic Data Engine
This paper introduces MAVIS, a mathematical visual instruction tuning pipeline for multi-modal large language models (MLLMs), addressing challenges in visual encoding, diagram-language alignment, and chain-of-thought reasoning. The approach includes an autonomous data generation process and four training stages, resulting in the MAVIS-7B model, which achieves superior performance in mathematical benchmarks, outperforming other models like LLaVA-NeXT.

Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models
This paper investigates Large Language Model (LLM) self-improvement by introducing a framework where the model verifies its own outputs and refines data accordingly, leading to a better understanding of this mechanism. The authors provide a mathematical formulation centered around the "generation-verification gap" and reveal a scaling phenomenon where self-improvement potential increases with model pre-training flops, offering insights and future research directions into enhancing LLM capabilities.

MMRole: A Comprehensive Framework for Developing and Evaluating Multimodal Role-Playing Agents
This paper introduces Multimodal Role-Playing Agents (MRPAs) through a comprehensive framework called MMRole, which includes the creation of a personalized multimodal dataset (MMRole-Data) and a robust evaluation approach (MMRole-Eval) with eight metrics. The first specialized MRPA, MMRole-Agent, is developed and demonstrated to improve performance, highlighting the challenges in multimodal understanding and role-playing consistency, with data and resources available for further research.

MoDeGPT: Modular Decomposition for Large Language Model Compression
This paper presents Modular Decomposition (MoDeGPT), a structured compression framework for large language models that reduces computational requirements while maintaining accuracy. MoDeGPT achieves a 98% reduction in compute costs and retains 90-95% of zero-shot performance on various models by utilizing innovative matrix decomposition techniques, offering a substantial improvement in efficiency and inference speed.

MoLEx: Mixture of Layer Experts for Fine-tuning with Sparse Upcycling
The paper introduces the Mixture of Layer Experts (MoLEx), a novel Sparse Mixture of Experts model designed to enhance Parameter Efficient Fine-Tuning (PEFT) by leveraging layers as experts that share linguistic information across a pre-trained model. MoLEx improves fine-tuning efficiency by performing conditional layer computations, leading to more informed predictions with minimal computational overhead across various NLP tasks, as demonstrated on benchmarks like GLUE and the NLG End-to-End Challenge.

Monitoring Latent World States in Language Models with Propositional Probes
This paper introduces _propositional probes_ as a method to extract logical propositions from the latent world states encoded by language models, aiming to address biases and unfaithful behavior. The study demonstrates that these probes can effectively decode accurate representations of input contexts even in the presence of linguistic challenges such as short stories, translations, and scenarios with prompt injections, backdoor attacks, and gender bias, pointing towards the need for improved interpretability tools in language models.

Montessori-Instruct: Generate Influential Training Data Tailored for Student Learning
Montessori-Instruct is a novel data synthesis framework that aligns a teacher language model's synthetic data generation with a student model's learning preferences, using local data influence to optimize student outcomes. The approach significantly outperforms standard methods and even stronger teacher models like GPT-4o, enhancing the student's learning while demonstrating robustness across different models.

MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses
This paper explores whether large language models (LLMs) can automatically generate novel and valid chemistry research hypotheses from a given research question. By constructing a benchmark with chemistry papers and developing an LLM-based multi-agent framework, the study demonstrates that LLMs can effectively rediscover hypotheses by using background information and inspirations, highlighting their potential role in scientific discovery.

Motion-Agent: A Conversational Framework for Human Motion Generation with LLMs
This paper introduces **Motion-Agent**, a conversational framework for general human motion generation, editing, and understanding, which leverages a pre-trained language model to create a generative agent called **MotionLLM**. By integrating MotionLLM with GPT-4, the system effectively generates complex motion sequences through interactive conversations, offering performance comparable to state-of-the-art models with minimal fine-tuning while supporting diverse motion-language tasks.

MP-Mat: A 3D-and-Instance-Aware Human Matting and Editing Framework with Multiplane Representation
MP-Mat is a novel 3D-and-instance-aware human instance matting framework designed to handle complex boundary structures by using multiplane representations at both the scene geometry and instance level, improving interpretability and boundary handling. The framework supports efficient foreground and background awareness, significantly outperforming existing methods in both matting and image editing tasks, highlighting its utility and effectiveness in these domains.

Multi-Label Node Classification with Label Influence Propagation
The paper addresses multi-label node classification (MLNC) on graphs by introducing the Label Influence Propagation (LIP) model, which identifies and exploits intricate label influence correlations through a novel label influence graph. By dynamically adjusting the learning process to amplify positive contributions and mitigate negative influences, the LIP model demonstrates superior performance on benchmark datasets, outperforming state-of-the-art methods in MLNC tasks.

Multimodal Quantitative Language for Generative Recommendation
This paper introduces Multimodal Quantitative Language for Generative Recommendation (MQL4GRec), a novel approach that unifies items from various domains and modalities into a quantitative language to improve recommendation systems. The method significantly enhances recommendation knowledge transfer, showing notable performance improvements over existing models, as demonstrated by experiments across multiple datasets.

Multi-objective antibody design with constrained preference optimization
AbNovo is a novel framework for multi-objective antibody design that balances binding affinity with other critical biophysical properties using constrained preference optimization. By incorporating a structure-aware protein language model and a primal-and-dual approach, AbNovo outperforms existing methods in key metrics like binding affinity, stability, and specificity.

Neural ODE Transformers: Analyzing Internal Dynamics and Adaptive Fine-tuning
This paper introduces a novel approach to modeling transformer architectures using non-autonomous neural ordinary differential equations, where all weights of attention and feed-forward blocks are parameterized as functions of a continuous layer index. The model not only achieves comparable or superior performance to traditional transformers but also enhances interpretability through spectral analysis and token-level sensitivity examination, offering flexible fine-tuning for various architectural constraints.

Neural Stochastic Differential Equations for Uncertainty-Aware Offline RL
The paper introduces Neural Stochastic Differential Equations for Uncertainty-aware, Offline RL (NUNO), a novel offline model-based reinforcement learning approach that excels in low-quality data environments and maintains competitive performance with high-quality datasets. By integrating neural stochastic differential equations for uncertainty estimation and adapting rollout truncation based on these estimates, NUNO significantly outperforms existing methods on low-quality datasets and matches or exceeds their performance on high-quality datasets in D4RL and NeoRL MuJoCo benchmarks.

Neuron-based Multifractal Analysis of Neuron Interaction Dynamics in Large Models
This paper introduces a novel analytical framework called *Neuron-based Multifractal Analysis (NeuroMFA)* to systematically study the internal structures of large language models and their emergent abilities. By providing a quantitative measure of network heterogeneity and organization, *NeuroMFA* links structural features to model capabilities, offering a new perspective on the emergence phenomena in large-scale models.

Node-Time Conditional Prompt Learning in Dynamic Graphs
This paper introduces DyGPrompt, a novel framework that enhances pre-training and prompt learning for dynamic graph neural networks (DGNNs) to better align with downstream tasks like node classification. DyGPrompt employs dual prompts and condition-nets to effectively capture task objectives and temporal variations, showing improved performance on dynamic graphs through comprehensive experiments on four public datasets.

No Equations Needed: Learning System Dynamics Without Relying on Closed-Form ODEs
This paper introduces a novel approach called direct semantic modeling for low-dimensional dynamical systems, which predicts the semantic representation of a system's behavior directly from data, bypassing the traditional and complex two-step process of discovering and analyzing closed-form ODEs. By simplifying the modeling pipeline, this method enhances model transparency, flexibility, and the ability to incorporate intuitive inductive biases, ensuring compliance with desired specifications.

No Location Left Behind: Measuring and Improving the Fairness of Implicit Representations for Earth Data
Implicit neural representations are promising for Earth representation challenges, but current methods focus more on global average performance rather than detailed insights. Addressing this issue, we introduce FAIR-Earth, a novel dataset designed to expose and address inequities in Earth representations by evaluating models with high-resolution signals and stratified metadata, and propose spherical wavelet encodings to improve the performance fairness across different geographic and population-based subgroups.

Null Counterfactual Factor Interactions for Goal-Conditioned Reinforcement Learning
Hindsight relabeling is effective in goal-conditioned reinforcement learning but struggles in object-centric domains due to its limitations in inferring meaningful object-object interactions. To address this, Hindsight Relabeling using Interactions (HInt) and Null Counterfactual Interaction Inference (NCII) are introduced, improving sample efficiency and interaction inference accuracy in dynamic robotic domains such as Robosuite and Franka Kitchen by using a novel definition of interactions based on the concept of null counterfactual.

Number Cookbook: Number Understanding of Language Models and How to Improve It
This paper investigates the numerical understanding and processing ability (NUPA) of large language models (LLMs) through a comprehensive benchmark covering various numerical tasks derived from educational curricula. Despite attempts to enhance NUPA via finetuning and specialized techniques, the study finds that LLMs frequently fail at these tasks, highlighting limitations in current models and offering insights into potential improvements.

ObscuraCoder: Powering Efficient Code LM Pre-Training Via Obfuscation Grounding
This work presents ObscuraCoder, a set of language models pre-trained on the ObscuraX dataset of obfuscated code pairs, which enhances the models' understanding of both syntax and semantics in code. The obfuscation-based pre-training method improves the models' sample efficiency and demonstrates significant gains in multilingual code tasks and understanding, surpassing traditional autoregressive and de-obfuscation objectives.

On Linear Representations and Pretraining Data Frequency in Language Models
The research investigates the relationship between pretraining data frequency and linear representations in language models, revealing that the formation of linear representations for factual relations is strongly influenced by the frequency of co-occurrence of subject-object pairs. The study introduces a regression model capable of predicting pretraining data frequencies from linear representation quality, highlighting its potential to inform methods for controlling and improving model behavior by adjusting training data to specific frequency thresholds.

On Rollouts in Model-Based Reinforcement Learning
Infoprop enhances model-based reinforcement learning by distinguishing between aleatoric and epistemic uncertainty to minimize accumulated model errors during synthetic rollouts, a significant barrier in MBRL. By providing a mechanism to monitor and limit data corruption, Infoprop-Dyna achieves state-of-the-art results on MuJoCo benchmarks, improving rollout length and data quality in Dyna-style MBRL.

On the Almost Sure Convergence of the Stochastic Three Points Algorithm
The paper analyzes the stochastic three points (STP) algorithm, a derivative-free optimization technique, and provides the first almost sure convergence results and some convergence in expectation for three classes of functions: smooth, smooth convex, and smooth strongly convex. It demonstrates that the STP algorithm achieves varying convergence rates depending on the class of function, such as an $o(1/{T^{\frac{1}{2}-\epsilon}})$ rate for smooth functions, $o(1/{T^{1-\epsilon}})$ for smooth convex functions, and both expected and almost sure convergence rates dependent on the step size parameters for smooth strongly convex functions.

OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video Generation
This paper addresses challenges in text-to-video (T2V) generation by introducing OpenVid-1M, a high-quality dataset with over 1 million text-video pairs, and OpenVidHD-0.4M, which includes 1080p videos to improve video generation quality. Additionally, the authors propose a novel Multi-modal Video Diffusion Transformer (MVDiT) that better utilizes structural and semantic information, with experiments showing its effectiveness over existing methods.

Optimal Learning of Kernel Logistic Regression for Complex Classification Scenarios
This paper demonstrates that kernel logistic regression (KLR) achieves minimax optimal convergence rates for cross-entropy (CE) loss in complex classification scenarios, such as long-tailed learning and domain adaptation. By providing a theoretical basis for the effectiveness of KLR in predicting conditional class probabilities (CCPs), the study enhances understanding of CCP-based approaches under challenging conditions.

OptionZero: Planning with Learned Options
OptionZero is a novel approach that enhances the MuZero framework by integrating an option network for the autonomous discovery of action sequences, or options, through self-play games. Demonstrating a significant 131.58% improvement over MuZero in mean human-normalized scores across 26 Atari games, OptionZero not only discovers valuable options but also develops strategic skills adapted to diverse game challenges, paving the way for improved planning in reinforcement learning.

OSCAR: Operating System Control via State-Aware Reasoning and Re-Planning
OSCAR is a generalist agent that autonomously navigates and interacts with desktop and mobile applications using standardized controls while processing screen images, effectively translating human instructions into executable Python code for precise GUI control. By functioning as a state machine with error-handling and dynamic task re-planning, OSCAR significantly enhances stability, adaptability, and user productivity across diverse platforms, with its code set to be open-source upon publication.

OVTR: End-to-End Open-Vocabulary Multiple Object Tracking with Transformer
The paper introduces OVTR, the first end-to-end open-vocabulary multiple object tracker utilizing a transformer-based approach to improve motion, appearance, and category modeling. It leverages innovative components like the CIP strategy and a dual-branch structure to enhance tracking and classification, demonstrating superior performance and adaptability over existing methods on open-vocabulary benchmarks with improved efficiency.

Parameter Expanded Stochastic Gradient Markov Chain Monte Carlo
This paper introduces a novel method to enhance sample diversity in Bayesian Neural Networks using Stochastic Gradient Markov Chain Monte Carlo (SGMCMC) by reparameterizing weight matrices, thus avoiding the need for tempering or multiple chains. The proposed approach improves uncertainty estimation and model performance, allowing for more effective exploration of parameter space and faster mixing without increasing computational cost, as demonstrated through extensive experiments on image classification tasks.

Perturbation-Restrained Sequential Model Editing
This paper addresses the challenge of preserving the general abilities of large language models during sequential model editing by introducing the Perturbation Restraint on Upper bouNd for Editing (PRUNE) framework. PRUNE effectively applies condition number restraints to reduce numerical sensitivity, thereby maintaining the models' performance even as multiple edits are made, as demonstrated through experiments on various editing methods and tasks.

Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws
This paper investigates the information-theoretic capacity of language models to store factual knowledge as bits, finding that models can store 2 bits of knowledge per parameter, even when quantized to int8. It also presents 12 key results on how factors like training duration, model architecture, quantization, sparsity constraints, and data signal-to-noise ratio impact a model's ability to retain and utilize knowledge.

PianoMotion10M: Dataset and Benchmark for Hand Motion Generation in Piano Performance
This paper introduces PianoMotion10M, a benchmark dataset designed to enhance piano hand motion generation for instructional purposes, featuring 116 hours of annotated piano playing videos. The authors present a baseline model that generates hand motions from audio, evaluated via metrics such as motion similarity and positional accuracy, which aims to improve guidance on piano fingering.

POGEMA: A Benchmark Platform for Cooperative Multi-Agent Pathfinding
This paper introduces POGEMA, a comprehensive toolset designed to facilitate fair comparisons between classical, learning-based, and hybrid methods in multi-agent reinforcement learning (MARL) by providing a unified framework for learning, evaluation, and benchmarking. It includes a fast learning environment, problem instance generator, visualization toolkit, and an evaluation protocol with domain-related metrics, showcasing its effectiveness through comparisons involving state-of-the-art MARL methods.

Point Cluster: A Compact Message Unit for Communication-Efficient Collaborative Perception
This paper proposes a novel communication-efficient collaborative perception framework called CPPC, which uses a compact message unit termed "point cluster" to enhance individual agents' perception capabilities by efficiently representing potential objects. By preserving structure information with strategic point sampling and enabling efficient message aggregation through set matching and merging, CPPC successfully reduces communication costs and improves performance, as demonstrated in experiments on recognized benchmarks.

Policy Optimization under Imperfect Human Interactions with Agent-Gated Shared Autonomy
AGSA is an Agent-Gated Shared Autonomy framework designed to address challenges in reward-free training, safe exploration, and imperfect human control by using a dedicated gating agent to manage control-switching between human and agent. The framework reduces the need for constant human monitoring and leverages human feedback to train a gating value function, showing significant improvements in safety, performance, and user-friendliness over previous methods in both simulated and real human participant environments.

Preference Elicitation for Offline Reinforcement Learning
This paper introduces Sim-OPRL, an offline preference-based reinforcement learning algorithm that acquires preference feedback through a learned environment model in a fully offline setup. By integrating insights from offline and preference-based RL, the proposed algorithm balances pessimism for out-of-distribution data with optimism for acquiring preferences, providing theoretical guarantees and demonstrating strong empirical performance.

Proactive Privacy Amnesia for Large Language Models: Safeguarding PII with Negligible Impact on Model Utility
The paper introduces Proactive Privacy Amnesia (PPA), an innovative technique to protect personally identifiable information (PII) in large language models without compromising their utility. PPA effectively identifies and forgets PII-related memories, then implants substitute memories, achieving complete elimination of phone number exposure and significant reduction in physical address leaks while preserving model performance.

ProteinBench: A Holistic Evaluation of Protein Foundation Models
ProteinBench is introduced as a comprehensive evaluation framework aimed at enhancing the understanding and transparency of protein foundation models by providing a standardized method of assessment across classifications and metrics. By releasing this framework, along with a public leaderboard and toolkit, the study seeks to drive development, application, and collaboration in the field of protein modeling, ultimately highlighting the models' capabilities and limitations.

ProtoSnap: Prototype Alignment For Cuneiform Signs
This paper introduces ProtoSnap, an unsupervised approach leveraging generative models to recover and analyze the complex internal configurations of cuneiform signs, traditionally treated categorically in prior research. By using prototype font images and structural consistency, the method enhances alignment and recognition for a diverse array of cuneiform signs, improving synthetic data generation and boosting recognition performance, particularly for rare signs.

Q-Adapter: Customizing Pre-trained LLMs to New Preferences with Forgetting Mitigation
This paper addresses the challenge of customizing large language models (LLMs) like Llama to align with new human preferences while preserving their original capabilities. The proposed method, Q-Adapter, leverages a residual Q-learning framework to adapt pre-trained LLMs by approximating a residual Q-function, demonstrating effectiveness on the DSP and HH-RLHF datasets.

QERA: an Analytical Framework for Quantization Error Reconstruction
This paper introduces Quantization Error Reconstruction Analysis (QERA), an analytical framework that provides a closed-form solution to improve low-precision quantization methods for large language models. By enhancing both fine-tuning and inference, QERA leads to significant accuracy gains, including a 6.05% improvement in 2-bit RoBERTa-base on GLUE and higher quantization accuracy in 4-bit Llama-3.1-70B over existing methods like LoftQ and ZeroQuant-V2.

Range, not Independence, Drives Modularity in Biologically Inspired Representations
This paper develops a theory explaining when biologically inspired networks modularize their representation of source variables, focusing on nonnegative and energy-efficient properties. By establishing conditions on data distribution that influence modularization and applying these ideas to neuroscience data, the paper offers insights into neural activity patterns and suggests new perspectives on mixed-selectivity beyond traditional theories.

Re-Aligning Language to Visual Objects with an Agentic Workflow
Real-LOD is a workflow designed to enhance language-based object detection (LOD) by reducing vision-language model (VLM) hallucinations through adaptive image and text prompt adjustments, leading to improved alignment between visual objects and language expressions. By employing a neural symbolic design consisting of planning, tool use, and reflection, Real-LOD significantly improves existing LOD methods by approximately 50% on standard benchmarks, demonstrating the potential of maintaining data quality while increasing data volume.

Reasoning Elicitation in Language Models via Counterfactual Feedback
This paper addresses the underdeveloped reasoning capabilities of language models, specifically in causal reasoning through counterfactual question answering. It introduces novel metrics for assessing reasoning abilities and proposes fine-tuning approaches to enhance reasoning, evaluating their performance and generalization in various realistic scenarios.

Reconsidering Faithfulness in Regular, Self-Explainable and Domain Invariant GNNs
This paper investigates the concept of faithfulness in explanations for Graph Neural Networks (GNNs) and demonstrates that existing metrics for faithfulness are not interchangeable. It reveals that optimizing for faithfulness can be counterproductive in injective regular GNNs while emphasizing the important link between faithfulness and out-of-distribution generalization.

ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement
This paper presents ReGenesis, a novel method for enhancing the reasoning abilities of Large Language Models by self-synthesizing reasoning paths from general guidelines, without relying on external supervision. ReGenesis demonstrates superior performance in both in-domain and out-of-domain tasks, showcasing a 6.1% improvement in out-of-domain tasks compared to existing self-synthesizing methods.

Regulatory DNA Sequence Design with Reinforcement Learning
This paper introduces a novel generative approach for designing high-fitness cis-regulatory elements (CREs) using reinforcement learning to optimize a pre-trained autoregressive model while incorporating biological priors. The proposed method outperforms existing CRE design strategies by effectively integrating computational inference-based rewards, demonstrating improved performance in promoter and enhancer design tasks across various conditions and cell types. 



Relation-Aware Diffusion for Heterogeneous Graphs with Partially Observed Features
This paper introduces a novel imputation scheme enabling diffusion-based imputation in heterogeneous graphs by assigning virtual features to nodes with undefined features and adjusting the importance of each edge type during diffusion. The proposed method bridges existing diffusion techniques with heterogeneous graphs, resulting in significant performance improvements in semi-supervised node classification and link prediction tasks, even with varying levels of missing data.

ReMatching Dynamic Reconstruction Flow
This paper introduces the ReMatching framework, which enhances the reconstruction quality of dynamic scenes by incorporating velocity-field based deformation priors into existing models. The framework is adaptable, allows integration of various model priors, and improves reconstruction accuracy as demonstrated in evaluations on synthetic and real-world benchmarks.

RESfM: Robust Deep Equivariant Structure from Motion
This paper addresses the challenge of multiview structure from motion by introducing an architecture that incorporates a multiview inlier/outlier classification module and a robust bundle adjustment step to handle outliers. The proposed method shows superior accuracy, outperforming existing deep-based approaches and matching the performance of leading classical methods, even in realistic settings with large image collections containing many outliers.

Residual Kernel Policy Network: Enhancing Stability and Robustness in RKHS-Based Reinforcement Learning
This paper addresses the instability and sensitivity issues in RKHS-based reinforcement learning by proposing the Residual Kernel Policy Network (ResKPN), which integrates representation learning and introduces a residual layer to reduce gradient variance. The proposed method enhances the robustness and stability of RKHS policies, achieving a 30% improvement in episodic rewards across complex environments.

Rethinking Invariance in In-context Learning
In this work, we propose Invariant ICL (InvICL), a novel methodology designed to achieve permutation invariance in In-Context Learning (ICL) while ensuring information non-leakage and context interdependence—two elements not simultaneously addressed by existing methods. Empirical results demonstrate that InvICL outperforms previous invariant and non-invariant models on most benchmark datasets, offering enhanced generalization capabilities across different input lengths.

Rethinking Self-Distillation: Label Averaging and Enhanced Soft Label Refinement with Partial Labels
This paper examines self-distillation in multi-class classification under linear probing and demonstrates that multi-round self-distillation effectively averages labels based on feature correlations, enhancing generalization and diminishing label noise effects. The authors introduce a novel single-round self-distillation method, the PLL student model, which achieves similar or better performance than multi-round approaches in noisy environments while significantly lowering computational demands.

Revisiting Feature Prediction for Learning Visual Representations from Video
This paper introduces V-JEPA, a set of vision models focused on a feature prediction objective for unsupervised learning from video, trained on 2 million public dataset videos without pretrained image encoders or additional supervision methods. The models demonstrate strong performance in both motion and appearance-based tasks, achieving impressive accuracy on benchmarks like Kinetics-400 and ImageNet1K using video-trained, parameter-frozen models.

Reward Guided Latent Consistency Distillation
Latent Consistency Distillation (LCD) optimizes text-to-image synthesis by reducing inference steps, but it initially sacrifices image quality. This paper enhances LCD with Reward Guided LCD (RG-LCD) and a latent proxy RM (LRM), significantly improving image quality and synthesis speed, as evidenced by human preference and quantitative metrics, without loss of fidelity compared to the baseline method.

Risk-Sensitive Variational Actor-Critic: A Model-Based Approach
This paper introduces a risk-sensitive variational actor-critic algorithm (rsVAC), which integrates the entropic risk measure within the RL-as-inference framework to handle stochastic rewards. The proposed method effectively produces risk-sensitive policies, offering enhancements in both tabular and complex continuous control environments, as demonstrated through experiments in MuJoCo.

RoboCat: A Self-Improving Generalist Agent for Robotic Manipulation
The paper introduces RoboCat, a multi-embodiment, multi-task generalist agent for robotic manipulation that leverages heterogeneous robotic experience to quickly master novel skills. RoboCat can generalize to new tasks and robots, both zero-shot and through minimal adaptation, while its ability to generate training data facilitates an autonomous improvement loop, as demonstrated through large-scale evaluations.

RobustKV: Defending Large Language Models against Jailbreak Attacks via KV Eviction
This paper presents RobustKV, a novel defense against jailbreak attacks on large language models (LLMs) by removing critical tokens of harmful queries from key-value caches, rather than merely mitigating adversarial prompts. RobustKV effectively counters advanced jailbreak attacks and balances effectiveness and evasion, proving robust against adaptive adversarial strategies while preserving LLM performance on benign queries.

Robust Simulation-Based Inference under Missing Data via Neural Processes
This paper addresses the challenge of missing data in simulation-based inference (SBI) by introducing a novel amortized method that simultaneously learns the imputation model and inference network within a neural posterior estimation framework. The proposed approach outperforms standard baselines across SBI benchmarks and demonstrates robust results, particularly on real-world bioactivity datasets, mitigating biases introduced by naive imputation methods.

R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference
Large Language Models (LLMs) face challenges in efficient inference due to their large size, particularly on edge devices, but R-Sparse offers a solution with a training-free activation sparsity approach that enables high sparsity, maintaining performance while reducing computational demands. By replacing linear layers with a rank-aware sparse inference method, R-Sparse achieves significant efficiency improvements, as demonstrated on models like Llama-2/3 and Mistral across various tasks, without the need for active channel prediction.

SaMer: A Scenario-aware Multi-dimensional Evaluator for Large Language Models
This paper introduces **SaMer**, a scenario-aware multi-dimensional evaluator that adapts to various scenarios to assess the quality of large language model (LLM) responses to open-ended questions. By creating a comprehensive preference dataset and leveraging a text embedding model, SaMer offers fine-grained, interpretable evaluations, demonstrating superior performance and adaptability compared to existing evaluation baselines.

Satisficing Regret Minimization in Bandits
The paper introduces $\texttt{SELECT}$, a general algorithmic framework designed to minimize satisficing regret in bandit optimization by identifying arms with rewards exceeding a given threshold. $\texttt{SELECT}$ maintains constant satisficing regret in realizable scenarios and retains standard regret guarantees in non-realizable cases, with its effectiveness demonstrated through numerical experiments across various bandit settings.

Scalable Decentralized Learning with Teleportation
This paper introduces TELEPORTATION, a novel method to improve the convergence rate of decentralized SGD by selectively activating a subset of nodes to perform parameter updates and gossip averaging on a smaller topology. Through efficient hyperparameter tuning, TELEPORTATION alleviates convergence rate degradation, enhancing neural network training stability and accuracy over traditional Decentralized SGD.


Scale-Free Graph-Language Models
This paper introduces a novel graph-language model (GLM) that unifies graph generation and text embedding to improve semi-supervised learning, utilizing the scale-free property of real edge distributions as a structural prior effectively approximated by k-nearest neighbor (KNN) graphs. The approach employs a graph-based pseudo-labeler for better language model finetuning, and experiments demonstrate the model's effectiveness in integrating these processes with a real structural prior.

Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model Pretraining
This paper introduces JOWA, a model-based offline reinforcement learning agent that leverages an image observation-based world model and a shared transformer backbone to enhance generalization across diverse tasks. Trained on 6 billion tokens from multiple Atari games, JOWA achieves impressive performance, outperforming existing offline RL baselines by 31.6% and demonstrating efficient transfer to new tasks with minimal data.

Score-based free-form architectures for high-dimensional Fokker-Planck equations
The paper introduces the Fokker-Planck neural network (FPNN), a novel framework that decouples score learning and density normalization for solving Fokker-Planck equations, allowing flexible network architectures and strict normalization through post-processing. Demonstrated on high-dimensional steady-state Fokker-Planck problems, FPNNs achieve superior accuracy and over a 20× speedup compared to current methods, showing potential as an efficient, accurate, and resource-saving universal solver for equations in more than 20 dimensions.

Self-Attention-Based Contextual Modulation Improves Neural System Identification
This paper demonstrates that self-attention (SA) can enhance predictions of neural responses in visual cortical neurons, outperforming traditional convolutional neural networks (CNNs) in metrics such as tuning curve correlation and peak tuning. The study introduces peak tuning to evaluate models' feature preference capture, reveals the crucial role of local receptive fields and surrounds in modeling neuron tuning, and suggests that self-attention and fully connected layers offer complementary benefits for learning surround-center interactions when learned incrementally.

SelKD: Selective Knowledge Distillation via Optimal Transport Perspective
This paper introduces selective Knowledge Distillation (SelKD), addressing scenarios where only a subset of a large teacher model's knowledge needs to be distilled into a smaller student model, using an Inverse Optimal Transport (IOT) framework. The proposed method allows for flexible sample selection and extends to both closed-set and open-set scenarios, improving student model performance on specific tasks and demonstrating effectiveness on standard benchmarks.

SGD with memory: fundamental properties and stochastic acceleration
This paper addresses the challenge of accelerating mini-batch SGD-type algorithms for quadratic problems with power-law spectra in the presence of noise by investigating first-order methods with a fixed number of auxiliary velocity vectors, known as memory-$M$ algorithms. The authors prove that although these algorithms retain the convergence rate exponent of plain GD, they can achieve different convergence constants, and they propose a memory-1 algorithm with a time-dependent schedule that experimentally and heuristically improves the performance over plain SGD.

Shared-AE: Automatic Identification of Shared Subspaces in High-dimensional Neural and Behavioral Activity
This paper introduces Shared-AE, an autoencoder framework that jointly extracts shared and private features between neural activity and behavior, improving our understanding of their alignment in complex tasks and social behavior. The model is applied to diverse behavioral contexts in mice and demonstrates its effectiveness, with the complete framework available publicly on GitHub.

SimulPL: Aligning Human Preferences in Simultaneous Machine Translation
Simultaneous Preference Learning (SimulPL) is proposed as a framework for optimizing Simultaneous Machine Translation (SiMT) models to better align with human preferences such as translation quality, monotonicity, key points, simplicity, and latency. By enhancing the read/write policy and integrating latency preferences, SimulPL demonstrates improved performance across various language pairs in SiMT tasks, effectively addressing a previously unexplored challenge.

Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes
*Sitcom-Crafter* is a comprehensive system designed for generating human motion in 3D space, extending workflow efficiency for anime and game designers by integrating diverse motion types such as locomotion and human-interaction. It features eight modules, including a novel 3D scene-aware module to minimize collisions, and demonstrates high-quality, realistic motion synthesis validated by experimental evaluations, with potential applications showcased in supplementary files.

SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs
SLoPe is a novel pretraining method combining sparse and low-rank adaptations to enhance the accuracy of sparsely pretrained large language models (LLMs) while reducing memory usage and accelerating both pretraining and inference. By employing double-pruned backward pass and low-rank adapters in the final 1% of pretraining, SLoPe effectively improves model performance without significant overhead, achieving up to 1.25× and 1.54× training and inference speedups and reducing memory usage by up to 0.63× and 0.61× for models like OPT-33B and OPT-66B.

Soft Merging of Experts with Adaptive Routing
This paper introduces SMEAR, a method for enhancing neural network modularity by using a "merged" expert constructed via a weighted average of all experts' parameters, thereby avoiding non-differentiable discrete routing decisions. SMEAR enables efficient gradient-based training and empirical results show it outperforms models with heuristic or gradient-estimated routing, with its experts demonstrating significant specialization.

Solving Video Inverse Problems Using Image Diffusion Models
This paper introduces a novel video inverse solver using image diffusion models to address spatio-temporal degradations in videos. By treating the time dimension as a batch dimension and employing a batch-consistent diffusion sampling strategy, the method achieves state-of-the-art reconstructions in video inverse problems without needing dedicated video diffusion models.

Sparse Learning for State Space Models on Mobile
The paper proposes a sparse learning framework that addresses the computational challenges of deploying State Space Models (SSMs) on resource-constrained mobile devices by integrating architecture-aware compiler optimizations. The solution, called $\mathbf{C}_4^n$ kernel sparsity, enhances SSM performance through efficient sparse models and specific optimizations, resulting in improved task performance and up to 7$\times$ speedup compared to existing methods on mobile platforms.

SSOLE: Rethinking Orthogonal Low-rank Embedding for Self-Supervised Learning
Self-supervised Orthogonal Low-rank Embedding (SSOLE) introduces a novel framework for self-supervised learning by adapting Orthogonal Low-rank Embedding principles to overcome challenges in representation collapse and feature differentiation. By decoupling rank enforcement and applying constraints to feature deviations, SSOLE achieves efficient and scalable performance on SSL benchmarks without large batch sizes, demonstrating significant advancements in multi-view learning tasks.

Stable Segment Anything Model
Stable-SAM enhances the Segment Anything Model (SAM) by improving its segmentation stability with casual and low-quality prompts through a deformable sampling plugin (DSP) and dynamic routing plugin (DRP), without altering the original architecture. This approach ensures SAM remains efficient and generalized, demonstrating improved performance across various prompt qualities with minimal parameter addition, as validated by extensive experiments.

STAMP: Scalable Task- And Model-agnostic Collaborative Perception
Perception in autonomous driving struggles with sensor constraints under challenging conditions, but multi-agent collaborative perception (CP) can mitigate these issues, though heterogeneity among agents limits effectiveness. The proposed STAMP framework addresses this by using lightweight adapters to transform Bird's Eye View features for efficient sharing between diverse agents, achieving high accuracy and low computational cost, advancing scalable mobility systems towards Level 5 autonomy.

State Space Models are Provably Comparable to Transformers in Dynamic Token Selection
This paper explores the capabilities of combining state space models (SSMs) with fully connected neural networks, demonstrating their effectiveness in extracting essential tokens similar to Transformers. By examining synthetic tasks and nonparametric regression, the study shows that SSMs with nonlinear layers perform efficiently, proving their comparability with Transformers in certain tasks.

Strategic Classification With Externalities
This paper introduces a new strategic classification model where inter-agent externalities in manipulation are considered, modeled as a Stackelberg game, with agent interactions forming a simultaneous game. The study establishes that the pure Nash Equilibrium for agent manipulation is unique and efficiently computable, and it provides PAC learning guarantees, offering a foundation for designing classifiers robust to multiple strategic actors.

StringLLM: Understanding the String Processing Capability of Large Language Models
This paper presents a comprehensive study on the string processing capabilities of large language models (LLMs), highlighting their current limitations in handling such tasks compared to humans. By introducing StringLLM and StringBench for evaluating LLMs, the authors identify shortcomings and propose a fine-tuning approach that enhances LLMs’ performance, paving the way for future research in this field.

Strong Model Collapse
In the context of large neural network training, this study establishes that even minimal amounts of synthetic data can lead to model collapse, where increasing the training set size no longer improves performance. The research also reveals that while larger models can exacerbate collapse, they may somewhat mitigate it beyond certain thresholds, highlighting the nuanced dynamics of model size in addressing collapse, as confirmed through both theoretical and empirical verification.

SurFhead: Affine Rig Blending for Geometrically Accurate 2D Gaussian Surfel Head Avatars
SurFhead is a novel method for reconstructing high-fidelity, riggable head geometry from RGB videos using 2D Gaussian surfels, improving upon existing techniques by addressing intricate geometric details and rendering unseen poses through precise geometric deformation and polar decomposition of transformations. By combining classical mesh-based deformation with modern Gaussian primitives, SurFhead achieves state-of-the-art geometry reconstruction and rendering quality, offering significant advancements over previous avatar rendering approaches.

SymmetricDiffusers: Learning Discrete Diffusion Models over Finite Symmetric Groups
This paper introduces *SymmetricDiffusers*, a novel discrete diffusion model that facilitates learning complicated distributions over permutations by using deep neural networks to manage simpler reverse diffusion transitions. By employing a riffle shuffle for forward transitions, introducing a more expressive generalized Plackett-Luce distribution, and proposing an efficient "denoising schedule," the model achieves state-of-the-art results across tasks like sorting 4-digit MNIST images, solving jigsaw puzzles, and addressing traveling salesman problems.

Systematic Outliers in Large Language Models
This paper provides a comprehensive analysis of outliers in Large Language Models, categorizing them into activation, weight, and attention outliers, and explores their impact on model performance and compression. By unveiling their origins tied to the self-attention mechanism's softmax operation, the study introduces the concept of systematic outliers and demonstrates how addressing them can enhance model efficiency.

TabM: Advancing tabular deep learning with parameter-efficient ensembling
This study introduces TabM, a simplified yet highly effective model based on MLP and improved BatchEnsemble techniques, which significantly enhances performance in tabular deep learning by emulating ensemble models in a parameter-efficient manner. Through comprehensive evaluations, TabM outperforms previous models and demonstrates that complex methods like attention- and retrieval-based approaches may not yield proportional benefits, offering a compelling balance between performance and efficiency for researchers and practitioners.

TabReD: Analyzing Pitfalls and Filling the Gaps in Tabular Deep Learning Benchmarks
This paper investigates the gap between academic tabular deep learning benchmarks and real-world industrial applications, highlighting that common evaluation datasets fail to account for data distribution drift over time and the complexity of feature engineering. Introducing TabReD, a set of eight industry-grade datasets, the study reveals divergences in method rankings when using realistic time-based data splits and richer feature sets, with simple MLP-like architectures and GBDT outperforming others in this context.

Talking Turns: Benchmarking Audio Foundation Models on Turn-Taking Dynamics
This paper investigates the turn-taking capabilities of recent audio foundation models (FMs) in conversational modeling by proposing a novel evaluation protocol using a supervised model to assess these abilities. The study reveals that existing spoken dialogue systems often struggle with proper turn-taking, such as timing for speaking up and interrupting, and demonstrates significant improvement potential, offering open-source tools to foster advancements in conversational AI systems.

Test-Time Adaptation for Combating Missing Modalities in Egocentric Videos
This study introduces MiDl, a novel self-supervised, online approach designed to address the challenge of handling missing modalities in egocentric video understanding at test time, without retraining. By minimizing mutual information and incorporating self-distillation, MiDl adapts to available modalities and enhances performance across various pre-trained models and datasets.

The Complexity of Two-Team Polymatrix Games with Independent Adversaries
This paper investigates the computation of Nash equilibria in polymatrix games where each pair of players engages in either a zero-sum or a coordination game, focusing on scenarios with players grouped into a small number of teams with identical interests. The authors establish that the problem remains complex, specifically CLS-hard, even for two-team configurations, and also demonstrate the tightness of this complexity bound, particularly when one team comprises multiple independent adversaries, while proving the difficulty of finding any stationary point in non-convex-concave min-max constrained optimization problems.

The Hidden Cost of Waiting for Accurate Predictions
This paper explores the tension between the timing of predictions and the efficiency of resource allocations in algorithmic decision-making. Using a mathematical model, the study reveals that delaying decisions to improve predictive accuracy can paradoxically worsen average ranking loss and decrease social welfare, with inequality being a key contributing factor, thereby challenging the assumption that more accurate predictions necessarily lead to better outcomes.

The KoLMogorov Test: Compression by Code Generation
The paper introduces the *KoLMogorov-Test* (KT), a novel test for code generation language models (LLMs) to evaluate their ability to generate the shortest program for a given data sequence, representing an ideal form of compression tied to intelligence. The study finds current models, including GPT4-o and Llama-3.1-405B, struggle with KT, particularly on natural data, and highlights the need for new approaches to improve model performance beyond synthetic data tasks.

TimeKAN: KAN-based Frequency Decomposition Learning Architecture for Long-term Time Series Forecasting
This paper introduces TimeKAN, a novel architecture designed to improve time series forecasting by effectively decomposing and modeling multiple frequency components using a Kolmogorov-Arnold Network-based approach. The proposed method, which includes Cascaded Frequency Decomposition, Multi-order KAN Representation Learning, and Frequency Mixing blocks, demonstrates state-of-the-art performance on various real-world datasets while maintaining a lightweight framework.

Time-to-Event Pretraining for 3D Medical Imaging
This paper introduces a novel time-to-event pretraining framework for 3D medical imaging models, integrating longitudinal electronic health records to provide enhanced temporal context necessary for predicting disease progression. Utilizing a dataset of 18,945 CT scans, the method significantly improves outcome prediction metrics while maintaining diagnostic classification performance, thereby advancing clinical risk prediction.

To Clip or not to Clip: the Dynamics of SGD with Gradient Clipping in High-Dimensions
This paper provides a theoretical analysis of gradient clipping in the context of streaming stochastic gradient descent (SGD) for least squares problems, particularly focusing on scenarios with large intrinsic dimensionality. The authors propose a heuristic for optimizing the clipping threshold and demonstrate its applicability across synthetic, CIFAR10, and Wikitext2 data, highlighting both the limitations and benefits of gradient clipping in various noisy settings.

ToVE: Efficient Vision-Language Learning via Knowledge Transfer from Vision Experts
This paper introduces the Transfers of Vision Experts (ToVE) framework, which enhances visual-language learning by leveraging pre-trained vision expert models within a hub, along with a token-aware gating network to efficiently route expert knowledge. Through a "residual knowledge transfer" strategy and a knowledge-merged CLIP encoder, ToVE achieves competitive results across various vision-language tasks using significantly less training data.

Towards Realistic Data Generation for Real-World Super-Resolution
This paper introduces Realistic Decoupled Data Generator (RealDGen), an unsupervised learning framework for generating realistic data for image super-resolution (SR). RealDGen uses a novel content-degradation decoupled diffusion model to produce high-quality paired data that improves the performance of SR models in real-world scenarios, as evidenced by extensive experiments and benchmarks.

Towards Unbiased Calibration using Meta-Regularization
This paper addresses model miscalibration in deep neural networks by proposing a meta-regularization approach involving gamma-net and smooth expected calibration error (SECE) as a differentiable calibration proxy. The study demonstrates improved and unbiased calibration across multiple datasets, showing that the proposed method excels in calibration performance while maintaining competitive predictive accuracy compared to recent methods.

Tractable Multi-Agent Reinforcement Learning through Behavioral Economics
This paper addresses the computation challenge of Nash equilibria in multi-agent reinforcement learning by introducing risk-averse quantal response equilibria (RQE), inspired by behavioral economics, which are more tractable due to agents' risk aversion and bounded rationality. The findings illustrate that RQE can be the result of no-regret learning in various games, match human decision patterns in experimental settings, and provide an analysis of their sample complexity in finite-horizon Markov games utilizing a generative model.

Training-Free Activation Sparsity in Large Language Models
This paper introduces TEAL, a training-free method that applies magnitude-based activation sparsity to large language models, achieving 40-50% model-wide sparsity with minimal performance loss. TEAL enhances existing sparse kernels, offering decoding speed-ups of up to 1.8× and is compatible with weight quantization for additional efficiency improvements.

Training Robust Ensembles Requires Rethinking Lipschitz Continuity
This paper investigates the impact of Lipschitz continuity on adversarial example transferability and introduces LOTOS, a novel training paradigm for enhancing ensemble model robustness. By promoting orthogonality in model transformations, LOTOS effectively reduces transferability rates and improves robust accuracy against adversarial attacks, achieving significant performance gains in experiments with ResNet-18 models on CIFAR-10.

Transformers are Universal In-context Learners
This paper investigates the expressivity of transformers, particularly their ability to manage an arbitrarily large number of context tokens. It demonstrates that deep transformers are universal approximators capable of approximating continuous in-context mappings to arbitrary precision, highlighting their capacity to perform regression within context and operate on an unlimited number of tokens with a fixed embedding dimension and number of heads.

Transformers Provably Learn Two-Mixture of Linear Classification via Gradient Flow
This paper investigates how transformers learn hidden connections between tokens by studying a task involving two-mixture linear classification with a symmetric two-headed transformer. It proposes a three-stage training algorithm to analyze gradient descent dynamics in transformers and demonstrates challenges and solutions for generalizing this method to scenarios with more than two mixtures.

Transformer-Squared: Self-adaptive LLMs
Transformer-Squared is a self-adaptive framework designed to enhance large language models by dynamically adjusting singular components of weight matrices for unseen tasks in real-time. It surpasses traditional methods like LoRA in efficiency and versatility, offering significant advancements in adaptability and performance across various architectures and modalities, including vision-language tasks.

Understanding and Mitigating Bottlenecks of State Space Models through the Lens of Recency and Over-smoothing
This paper investigates the limitations of Structured State Space Models (SSMs) in capturing long-sequence dependencies due to recency bias and the tendency to over-smooth with deeper architectures. The authors propose a polarization technique for state transition matrices to mitigate these issues, enhancing long-range token recall and allowing SSMs to benefit from deeper structures, with supporting experiments and open-source code available.

UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation
UniDetox is a universally applicable method for mitigating toxicity across various large language models (LLMs) without the need for model-specific tuning, thanks to a novel dataset distillation technique using contrastive decoding. This approach enables universal detoxification by fine-tuning LLMs with distilled synthetic text data and reduces politically biased content, making it effective across different models like OPT, Falcon, and LLaMA-2.

UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation
This paper introduces UniWav, a unified pre-training framework designed to accommodate both discriminative and generative tasks in speech processing. By employing an encoder-decoder structure, UniWav achieves comparable performance to task-specific models, indicating the potential for a single general-purpose foundation model to streamline pre-training processes and reduce associated costs.

Unlocking the Potential of Model Calibration in Federated Learning
This paper introduces Non-Uniform Calibration for Federated Learning (NUCFL), a framework designed to integrate model calibration into federated learning by dynamically adjusting calibration objectives to suit diverse data distributions in federated environments. NUCFL improves both model accuracy and reliability by managing the calibration loss during local training, effectively balancing accuracy and calibration in heterogeneous federated settings, as demonstrated by extensive experimentation.

Varying Shades of Wrong: Aligning LLMs with Wrong Answers Only
This paper investigates expanding the capabilities of language models (LLMs) by aligning them with preferences among incorrect options when reliable annotations are scarce. It was found that LLMs can somewhat discern varying degrees of incorrectness, improving performance by up to 20.9% over random guessing, and aligning models with these preferences enhances accuracy and model calibration.

Vec2Face: Scaling Face Dataset Generation with Loosely Constrained Vectors
This paper introduces Vec2Face, an innovative model capable of synthesizing face images of non-existent persons, which aids effective training of face recognition models by ensuring a large number of distinct identities and proper variation in appearance. With the ability to efficiently generate up to 300K unique identities, Vec2Face surpasses previous works and enhances the accuracy of facial recognition models on both synthetic and real-world test sets.

VideoGrain: Modulating Space-Time Attention for Multi-Grained Video Editing
VideoGrain is a zero-shot approach designed to tackle the challenges of multi-grained video editing by enhancing space-time attention mechanisms for precise video content control. By improving text-to-region alignment and feature separation, VideoGrain achieves state-of-the-art performance in video generation and editing, with extensive experiments validating its effectiveness in real-world scenarios.

Video-STaR: Self-Training Enables Video Instruction Tuning with Any Supervision
The paper introduces Video-STaR, a novel self-training approach for video instruction tuning, which enables the use of any labeled video dataset to improve the reasoning capabilities of Large Multi-modal Models (LMMs). By cycling between instruction generation and fine-tuning, Video-STaR leverages existing video labels as weak supervision, resulting in significant improvements in video question answering and downstream tasks, including a 6.1% performance increase in TempCompass and notable gains in Kinetics700-QA and FineDiving assessments.

Visual Haystacks: A Vision-Centric Needle-In-A-Haystack Benchmark
This paper identifies and addresses the limitations of current benchmarks in long-context Large Multimodal Models (LMMs) for multi-image question answering (MIQA), particularly in real-world applications. Introducing "Visual Haystacks (VHs)" benchmark and the MIRAGE framework, the study enhances MIQA performance by processing up to 10k images, achieving significant improvements over existing models and establishing a new state-of-the-art in multi-image QA benchmarks.

Wavelet-based Positional Representation for Long Context
This paper addresses the challenge of extrapolating sequences beyond the maximum length in large-scale language models due to limitations in position embedding mechanisms. By utilizing a new position representation method based on wavelet transforms that captures multiple scales, the proposed approach improves model performance in both short and long contexts, enhancing the ability to extrapolate position information without restricting the attention field.

What Secrets Do Your Manifolds Hold? Understanding the Local Geometry of Generative Models
This paper examines the post-training local geometry of learned manifolds in deep generative models and its impact on generation outcomes, utilizing models from simple settings to the advanced Stable Diffusion 1.4 Text-to-Image model. By introducing three geometric descriptors—scaling, rank, and complexity—the study shows how these indicators predict generation quality and diversity, and it demonstrates that leveraging this geometry can enhance image generation by controlling the likelihood and qualitative features of the outputs.

What's New in My Data? Novelty Exploration via Contrastive Generation
The study introduces Contrastive Generative Exploration (CGE), a method for novelty discovery through generation that identifies novel domains in fine-tuning datasets without direct data access by contrasting predictions from pre-trained and fine-tuned models. By implementing an iterative approach, CGE effectively detects diverse novel domains, including toxic language and new languages, and remains effective under differential privacy conditions.

When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers
This paper provides the first theoretical characterization of task vector methods on nonlinear Transformers, proving their effectiveness in tasks like multi-task learning and unlearning. The study demonstrates that task addition and negation can successfully manage aligned and contradictory tasks, while proper linear coefficient selection ensures generalization to out-of-domain tasks, with findings validated on Phi-1.5 for machine unlearning.

### Miscellaneous Aspects of Machine Learning->Causality
Gaussian Mixture Counterfactual Generator
This paper addresses the challenge of estimating individualized treatment effects (ITE) in scenarios involving continuous, multidimensional, and time-dependent treatments by introducing the Gaussian mixture counterfactual generator (GMCG). GMCG effectively handles complex treatment spaces and is demonstrated to outperform existing methods on synthetic and simulated datasets, showing promise for improving precision medicine by accurately modeling treatment-outcome relationships and requiring minimal control data.

Doubly robust identification of treatment effects from multiple environments
The paper introduces RAMEN, an algorithm designed to yield unbiased treatment effect estimates from observational data without the need for knowledge of the causal graph, by harnessing the heterogeneity of multiple data sources. RAMEN achieves doubly robust identification under certain conditions and demonstrates superior performance compared to existing methods through tests on synthetic, semi-synthetic, and real-world datasets.

Differentially private learners for heterogeneous treatment effects
This paper introduces DP-CATE, a novel framework for estimating the conditional average treatment effect (CATE) from observational data while ensuring differential privacy. The framework is versatile, applying to any Neyman-orthogonal two-stage CATE meta-learner and allowing for any machine learning model in nuisance estimation, marking a first in providing doubly robust and differentially private CATE estimation.

### Miscellaneous Aspects of Machine Learning->Everything Else
Complementary Label Learning with Positive Label Guessing and Negative Label Enhancement
Complementary label learning (CLL) is advanced by reformulating it as an inverse problem, splitting it into positive label guessing (PLG) and negative label enhancement (NLE), collectively termed PLNL. This approach improves classification accuracy, demonstrated by extensive experiments, and achieves a significant performance boost over existing CLL methods, with an increase from 34.96% to 55.25% on STL-10.

### Miscellaneous Aspects of Machine Learning->General Machine Learning Techniques
Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment
This paper addresses the limitations of speculative decoding in large language models (LLMs) by proposing an adapted verification scheme that recognizes valid but non-aligned token continuations. The authors introduce a "TokenCourt" dataset and a trained module that enhances verification abilities, demonstrating significant speedup in autoregressive generation while maintaining output quality, with a notable $9\times$ speed increase on the Llama-3.1 model family.

### Miscellaneous Aspects of Machine Learning->Supervised Learning
Diverse Policies Recovering via Pointwise Mutual Information Weighted Imitation Learning
This paper introduces a new method for recovering diverse policies from expert trajectories in imitation learning by enhancing the traditional behavioral cloning approach through a weighting mechanism based on pointwise mutual information. This modification allows the model to prioritize state-action pairs that are most representative of the trajectory's style, with theoretical justifications and empirical results demonstrating improved effectiveness in learning diverse behaviors.

### Miscellaneous Aspects of Machine Learning->Transfer, Multitask and Meta-learning
Coreset Selection via Reducible Loss in Continual Learning
This paper proposes a novel coreset selection method for rehearsal-based continual learning, using reducible loss (ReL) to measure sample importance and enhance training efficiency by identifying samples that maximize performance gain. The method, which addresses challenges like task interference and knowledge distillation, outperforms existing approaches by prioritizing informative and representative samples without relying on inefficient probabilistic sampling or local gradient methods.

On the Computation of the Fisher Information in Continual Learning
This paper empirically compares various implementations of the Fisher Information computation in Elastic Weight Consolidation (EWC) used for continual learning with deep neural networks. It reveals that the effectiveness of EWC can be enhanced by optimizing the method used to compute Fisher Information, suggesting the potential for improved results.

Boosting Multiple Views for pretrained-based Continual Learning
This paper explores the enhancement of Continual Learning (CL) models using Random Projection (RP) to make features more linearly separable, boosting the models' generalization abilities. The authors introduce a Multi-View Random Projection scheme and a task-based adaptive backbone with a self-improvement process, which collectively lead to superior performance over existing baselines across various datasets.

### Miscellaneous Aspects of Machine Learning->Unsupervised and Semi-supervised Learning
Score-based Self-supervised MRI Denoising
This paper introduces Corruption2Self (C2S), a novel score-based self-supervised framework for MRI denoising that leverages a generalized denoising score matching loss and detail refinement to address noise in MRI images without requiring high signal-to-noise ratio labels. The method outperforms existing self-supervised techniques, achieving competitive results with supervised methods across different noise levels and MRI contrasts, as demonstrated on the M4Raw and fastMRI datasets.

Realistic Evaluation of Deep Partial-Label Learning Algorithms
This paper examines the empirical aspects of partial-label learning (PLL), pinpointing critical yet overlooked issues such as non-trivial model selection, inconsistent experimental settings, and a lack of suitable real-world image datasets. It introduces PLENCH, the first benchmark for systematic evaluation of deep PLL algorithms, alongside novel model selection criteria and a new human-annotated dataset, PLCIFAR10, to improve and standardize PLL algorithm assessment.

AutoUAD: Hyper-parameter Optimization for Unsupervised Anomaly Detection
This paper addresses the challenge of tuning hyper-parameters and selecting appropriate unsupervised anomaly detection (UAD) methods for specific datasets, due to the lack of labeled anomalies and dataset diversity. By introducing two internal evaluation metrics and a semi-internal metric, as well as integrating them with Bayesian optimization, the proposed approach facilitates more practical and reliable UAD model optimization, demonstrated through experiments on 38 datasets.

### Optimization->Large Scale, Parallel and Distributed
Tight Time Complexities in Parallel Stochastic Optimization with Arbitrary Computation Dynamics
This paper establishes optimal time complexities for distributed stochastic optimization under varying computation behaviors, including disconnections and fluctuations in computation speeds, through a new universal computation model. The study identifies tight lower bounds for synchronous and asynchronous methods and demonstrates that these bounds are matched by optimal methods like Rennala SGD and Malenia SGD.

### Optimization->Learning for Optimization
ML4TSPBench: Drawing Methodological Principles for TSP and Beyond from Streamlined Design Space of Learning and Search
This study introduces ML4TSPBench, a unified framework that combines machine learning and combinatorial optimization, specifically using the Travelling Salesman Problem as a primary case study, with adaptations for other problems. It offers insights into the design principles of learning-based solvers, suggests enhancements to existing methods for improved performance, and demonstrates the efficacy of non-autoregressive and supervised paradigms, thereby creating a factory of new TSP solvers and setting a precedent for future research.

UniCO: On Unified Combinatorial Optimization via Problem Reduction to Matrix-Encoded General TSP
This paper introduces **UniCO**, a framework that unifies various combinatorial optimization problems by converting them into a generalized Traveling Salesman Problem (TSP) form, characterized by distance matrices, to enhance the training of neural TSP solvers. The study further develops two neural models, **MatPOENet** and **MatDIFFNet**, which use different techniques to tackle the unique challenges posed by non-standard TSP domains, demonstrating their effectiveness across problems like ATSP, HCP, and SAT.

Approximation algorithms for combinatorial optimization with predictions
This paper introduces a generic method to enhance approximation guarantees of classic algorithms using predictions, while maintaining efficient running times, applicable to various optimization problems such as Vertex Cover, Steiner Tree, and Knapsack. The method achieves optimal solutions with perfect predictions, flexibly manages approximation ratios with prediction error, and demonstrates potential for improved bounds using specific problem properties, highlighted by empirical evaluations on the Steiner Tree problem.

Learning-Guided Rolling Horizon Optimization for Long-Horizon Flexible Job-Shop Scheduling
The paper introduces L-RHO, a learning-guided Rolling Horizon Optimization framework for long-horizon combinatorial optimization problems like the Flexible Job-Shop Scheduling Problem. By employing a neural network to reduce redundant computations in problem decomposition, L-RHO significantly accelerates solver performance and improves solution quality, demonstrating adaptability and superior results across various scenarios and benchmarks.

### Optimization->Non-Convex
Partial Gromov-Wasserstein Metric
This paper introduces Partial Gromov-Wasserstein (PGW), a novel metric that extends the Unbalanced Gromov-Wasserstein framework to define a rigorous distance between metric measure spaces, overcoming previous limitations. The authors propose two variants of the Frank-Wolfe algorithm for solving the PGW problem, introduce the concept of barycenters for mm-spaces, and demonstrate the metric's efficacy in applications such as shape matching, retrieval, and interpolation, offering promising results compared to existing baselines.

### Probabilistic Methods->Bayesian Models and Methods
Scalable Bayesian Learning with posteriors
This paper introduces **_posteriors_**, a versatile PyTorch library designed to simplify and scale Bayesian learning for large datasets and complex models. It also presents a modified stochastic gradient Markov chain Monte Carlo approach that enhances deep ensembles' alignment with Bayesian posteriors and provides empirical evidence on Bayesian approximation benefits in specific experimental settings.

From Risk to Uncertainty: Generating Predictive Uncertainty Measures via Bayesian Estimation
This paper presents a framework that decomposes statistical pointwise risk into aleatoric and epistemic sources of predictive uncertainty, using Bayesian approximations to generate various predictive uncertainty measures. The proposed measures are validated on image datasets, demonstrating effectiveness in detecting out-of-distribution and misclassified instances through AUROC evaluation.

### Probabilistic Methods->Monte Carlo and Sampling Methods
Microcanonical Langevin Ensembles: Advancing the Sampling of Bayesian Neural Networks
The paper addresses the challenge of sampling-based inference in Bayesian Neural Networks (BNNs) by introducing an ensembling approach that combines optimization strategies with Microcanonical Langevin Monte Carlo (MCLMC) sampling. This method offers significant speedups in sampling time, improves predictive performance and uncertainty quantification, and enhances the predictability of resource requirements, providing a scalable solution for BNN inference.

Optimizing Backward Policies in GFlowNets via Trajectory Likelihood Maximization
Generative Flow Networks (GFlowNets) are generative models that sample objects based on a reward function through forward and backward policies, with recent findings linking their training to entropy-regularized reinforcement learning. To address limitations in GFlowNets due to fixed backward policies, this paper introduces a backward policy optimization algorithm for better convergence and mode discovery, validated through extensive experiments.

### Reinforcement Learning->Batch/Offline
Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction
The paper introduces the Latent Macro Action Planner (L-MAP), a method for tackling sequential decision-making in high-dimensional continuous action spaces amidst stochastic environments, particularly in offline reinforcement learning (RL). L-MAP utilizes a state-conditional Vector Quantized Variational Autoencoder (VQ-VAE) to reduce action dimensionality by learning macro-actions and improves planning efficiency using a learned latent transition model with Monte Carlo tree search (MCTS), outperforming existing methods and matching strong baselines in complex tasks.

### Reinforcement Learning->Deep RL
OGBench: Benchmarking Offline Goal-Conditioned RL
This paper introduces OGBench, a comprehensive benchmark designed to systematically evaluate offline goal-conditioned reinforcement learning (GCRL) algorithms. By encompassing 8 environment types, 85 datasets, and 6 representative algorithm implementations, OGBench highlights the varying strengths and weaknesses of current algorithms in tackling challenges such as stitching, long-horizon reasoning, and handling high-dimensional, stochastic inputs.

QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing
This paper introduces Q-switch mixture of policies (QMP), a novel framework for sharing behavioral policies across tasks to enhance multi-task reinforcement learning (MTRL) efficiency. By selectively utilizing behaviors from different tasks based on Q-function evaluations, QMP improves off-policy data collection, leading to better sample efficiency and outperforming existing behavior-sharing methods in various environments.

Any-step Dynamics Model Improves Future Predictions for Online and Offline Reinforcement Learning
This paper presents the Any-step Dynamics Model (ADM) to address compounding errors in reinforcement learning by reducing reliance on bootstrapping predictions, providing direct prediction of future states. The proposed algorithms, ADMPO-ON and ADMPO-OFF, enhance sample efficiency in online settings and improve performance and uncertainty quantification in offline settings compared to state-of-the-art methods.

Efficient Off-Policy Learning for High-Dimensional Action Spaces
The paper introduces Vlearn, an off-policy deep reinforcement learning approach that uses only a state-value function as the critic, thereby overcoming the challenges of data inefficiency and curse of dimensionality present in high-dimensional action spaces. By employing novel methods such as weighted importance sampling with design innovations like robust policy updates and twin value function networks, Vlearn improves sample complexity, ensures robust performance, and eliminates the need for state-action-value functions, leading to efficient and high-return agents.

### Reinforcement Learning->Everything Else
Accelerating Goal-Conditioned Reinforcement Learning Algorithms and Research
This paper introduces `JaxGCRL`, a high-performance codebase and benchmark for self-supervised goal-conditioned reinforcement learning (GCRL) that enables rapid training of agents by leveraging GPU-accelerated components, thereby addressing the slow simulation and algorithm stability issues plaguing previous methods. The study significantly enhances training efficiency and performance by evaluating key design choices in contrastive RL, providing a robust foundation for advancement in self-supervised GCRL research.

A Single Goal is All You Need: Skills and Exploration Emerge from Contrastive RL without Rewards, Demonstrations, or Subgoals
This paper provides empirical evidence that a simple reinforcement learning algorithm can develop skills and directed exploration well before achieving any successful trials, as demonstrated in a manipulation task where the agent learns to move, push, and place a block without using reward functions or demonstrations. The method requires only a straightforward modification of previous techniques and challenges intuitive expectations about exploration, yet it effectively reduces exploration once the goal is reliably reached, despite a lack of clear theoretical understanding of its success.

### Reinforcement Learning->Function Approximation
Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics
This paper presents a computationally efficient algorithm for Reinforcement Learning in the linear Bellman complete setting, which unifies models like linear MDPs and LQRs. The method leverages randomization by injecting noise into least squares regression to enable optimistic value iteration while ensuring statistical tractability and addressing error amplification challenges.

### Reinforcement Learning->Inverse
Subtask-Aware Visual Reward Learning from Segmented Demonstrations
This paper introduces REDS, a novel reward learning framework that uses segmented video demonstrations to derive reward functions without human-engineered rewards or extensive trial-and-error. By employing contrastive learning objectives and aligning video representations with subtasks, REDS demonstrates superior performance and generalization capabilities in complex robotic tasks, proving its potential for scalable application in varied environments.

### Reinforcement Learning->Multi-agent
FlickerFusion: Intra-trajectory Domain Generalizing Multi-agent Reinforcement Learning
This paper addresses the challenge of intra-trajectory dynamic entity composition in multi-agent reinforcement learning (MARL) in scenarios where the number of entities changes during inference, a situation not accounted for by existing methods. The proposed solution, FlickerFusion, enhances MARL by stochastically dropping parts of the observation space, achieving superior inference rewards and reduced uncertainty, with benchmarks and resources available at the provided website.

Trajectory-Class-Aware Multi-Agent Reinforcement Learning
The paper introduces TRajectory-class-Aware Multi-Agent reinforcement learning (TRAMA), which equips agents with trajectory awareness to improve multi-agent reinforcement learning in multi-task settings. By constructing quantized latent spaces, clustering trajectory embeddings, and integrating trajectory-class-aware policies, TRAMA enhances agents' versatility, yielding performance improvements over existing methods in tasks like those based on StarCraft II.

### Reinforcement Learning->Online
Online Reward-Weighted Fine-Tuning of Flow Matching with Wasserstein Regularization
This paper presents Online Reward-Weighted Conditional Flow Matching with Wasserstein-2 Regularization (ORW-CFM-W2), a novel reinforcement learning method designed to fine-tune continuous flow-based generative models using arbitrary reward functions. The approach addresses challenges like policy collapse and computational costs by integrating an online reward-weighting mechanism and Wasserstein-2 distance regularization, demonstrating its effectiveness and convergence through experiments on tasks such as image generation and text-image alignment.

### Reinforcement Learning->Planning
Bisimulation Metric for Model Predictive Control
This paper introduces Bisimulation Metric for Model Predictive Control (BS-MPC), a novel approach in model-based reinforcement learning that utilizes bisimulation metric loss to optimize the encoder for improved training stability and robustness. BS-MPC enhances decision-making by extracting intrinsic state information, delivering superior performance and efficiency on DeepMind Control Suite tasks compared to existing methods.

### Social Aspects->Accountability, Transparency and Interpretability
Residual Stream Analysis with Multi-Layer SAEs
This paper introduces the multi-layer sparse autoencoder (MLSAE) to study how information flows across transformer layers by training a single SAE on residual stream activation vectors from every layer. The findings reveal that individual latents tend to be active at specific layers rather than persist across them, with these activations varying more when aggregating over tokens, and larger models showing increased multi-layer latent activity, offering new insights into representation changes within transformers. The study also confirms these results remain robust even when the assumption of a uniform residual stream basis is relaxed through pre-trained transformations.

See What You Are Told: Visual Attention Sink in Large Multimodal Models
Large multimodal models (LMMs) often allocate high attention weights to irrelevant visual tokens due to a phenomenon called the visual attention sink, which stems from the massive activation of certain hidden state dimensions. This study introduces Visual Attention Redistribution (VAR), a method that redistributes attention in image-centric heads to improve LMM performance on various tasks by enhancing their focus on relevant visual information without requiring additional training or resources.

### Social Aspects->Everything Else
Holistically Evaluating the Environmental Impact of Creating Language Models
This paper estimates the real-world environmental impact of developing language models, revealing that the process released 493 metric tons of carbon emissions and consumed 2.769 million liters of water. The study highlights the significant but often undisclosed environmental cost of model development, which constitutes around 50% of the total impact, and underscores the fluctuating power usage during training, raising concerns about grid-scale planning.

### Social Aspects->Fairness, Equity, Justice and Safety
EFFICIENT JAILBREAK ATTACK SEQUENCES ON LARGE LANGUAGE MODELS VIA MULTI-ARMED BANDIT-BASED CONTEXT SWITCHING
This paper introduces "Sequence of Context" (SoC) attacks, a novel method for jailbreaking large language models by strategically altering conversational context, formulated as a multi-armed bandit optimization problem. Demonstrating a 95% success rate and outperforming existing methods like PAIR and AutoDAN, the study reveals significant vulnerabilities in LLM safety protocols and underscores the necessity for defenses that address sequential attack strategies.

### Social Aspects->Privacy-preserving Statistics and Machine Learning
GRAIN: Exact Graph Reconstruction from Gradients
This paper introduces GRAIN, the first gradient inversion attack on graph-structured data, which can reconstruct both graph structures and node features from gradient updates in federated learning scenarios. Focusing on Graph Convolutional Networks and Graph Attention Networks, GRAIN demonstrates significant privacy risks by precisely reconstructing up to 80% of graphs across various datasets, outperforming existing methods which recover only up to 20% correctly.

Learning from End User Data with Shuffled Differential Privacy over Kernel Densities
This paper introduces a shuffled differential privacy (DP) protocol for estimating the kernel density function of distributed data, achieving accuracy comparable to central DP while enhancing privacy by anonymizing data during collection. The study demonstrates the practical application of this protocol in privately learning classifiers and retaining semantic content, with experiments highlighting the benefits and trade-offs in machine learning deployments using shuffled DP.

Encryption-Friendly LLM Architecture
This paper addresses privacy concerns in personalized responses from large language models (LLMs) by proposing a homomorphic encryption-friendly transformer architecture that emphasizes private fine-tuning. By employing LoRA fine-tuning and Gaussian kernels, the study achieves substantial computational speedups while preserving model performance, demonstrating a promising solution for privacy-preserving LLM services, with the code accessible on GitHub.

Controllable Unlearning for Image-to-Image Generative Models via $\epsilon$-Constrained Optimization
This paper addresses the challenge of machine unlearning in Image-to-Image (I2I) generative models by proposing a controllable unlearning framework that allows for a tunable balance between data removal and model utility using a control coefficient. The proposed method reformulates the problem into an $\epsilon$-constrained optimization problem, providing theoretically Pareto optimal solutions and demonstrating effectiveness through experiments on benchmark datasets.

### Social Aspects->Trustworthy Machine Learning
AI Sandbagging: Language Models can Strategically Underperform on Evaluations
The paper explores the issue of sandbagging, where AI models strategically underperform on evaluations to misrepresent their true capabilities, which can severely impact the trustworthiness of AI safety evaluations. The research demonstrates that both large and smaller language models can be manipulated to selectively underperform or target specific evaluation scores, highlighting a significant vulnerability that undermines the reliability of AI safety assessments.

Understanding Model Calibration - A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)
This paper provides an introduction to model calibration, discussing the most commonly used definition and evaluating measures, while highlighting drawbacks that call for new notions and evaluation measures. It aims to familiarize readers with various calibration concepts and re-emphasizes issues with a widely used evaluation measure without offering detailed calibration techniques.

Mitigating the Backdoor Effect for Multi-Task Model Merging via Safety-Aware Subspace
Model merging is increasingly important for integrating multiple task-specific models, but current methods overlook security risks such as backdoor attacks. This paper introduces a Defense-Aware Merging (DAM) approach using dual masks to mitigate these vulnerabilities, achieving a better balance between performance and security by reducing attack success rates while maintaining high accuracy.

### Theory->Deep Learning
Prototype antithesis for biological few-shot class-incremental learning
Deep learning struggles with continually learning new or mutated biological species due to species confusion, which leads to "catastrophic forgetting" of old species. The proposed Prototype Antithesis (PA) method, incorporating Residual Prototype Learning and Residual Prototype Mixing, significantly mitigates this issue by reducing inter-species confusion and achieving state-of-the-art performance in species recognition tasks as demonstrated on datasets like CUB200, PlantVillage, and Tree-of-Life.

Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors
This paper presents new bounds on the generalization error of representation learning algorithms, based on the relative entropy between training and test data distributions and a symmetric prior. It proposes a method to learn a data-dependent Gaussian mixture prior, which leads to a weighted attention mechanism and demonstrates better performance compared to the Variational Information Bottleneck (VIB) and Category-Dependent VIB (CDVIB) methods.

Entropy-based Activation Function Optimization: A Method on Searching Better Activation Functions
This paper introduces the Entropy-based Activation Function Optimization (EAFO) methodology, which offers a theoretical framework for designing and optimizing activation functions in neural networks. By deriving the novel Correction Regularized ReLU (CRReLU) from ReLU, the study demonstrates improved performance in tasks such as vision processing and large language model fine-tuning, highlighting CRReLU's potential for widespread application.

### Theory->Everything Else
A Formal Framework for Understanding Length Generalization in Transformers
This paper introduces a theoretical framework to analyze length generalization in causal transformers with learnable absolute positional encodings, characterizing functions identifiable from long inputs and proving the possibility of length generalization for various problems. The theory is experimentally validated as a predictor of length generalization success or failure, explaining empirical observations and enabling predictions of transformers' generalization capabilities.

### Theory->Game Theory
Re-evaluating Open-ended Evaluation of Large Language Models
The paper addresses the limitations of current Elo-based rating systems in open-ended evaluation of Large Language Models (LLMs), highlighting their vulnerability to biases due to redundancies. It proposes a novel game-theoretic approach by treating evaluation as a 3-player game, enhancing robustness and offering intuitive ratings, thereby providing new insights into the competitive dynamics of LLM development.

### Theory->Learning Theory
On the Relation between Trainability and Dequantization of Variational Quantum Learning Models
This paper addresses the open question of when quantum machine learning (QML) models can be simultaneously trainable and non-dequantizable, hence maintaining their advantage over classical algorithms. By formalizing these concepts and introducing the role of "variationalness" using quantum circuit architectures, the study provides methods for creating QML models that ensure both trainability and resistance to dequantization, supporting the practical relevance of QML.

Transformers Provably Solve Parity Efficiently with Chain of Thought
This study presents the first theoretical analysis of training transformers to solve the $k$-parity problem using recursive intermediate states akin to chain-of-thought reasoning. The research reveals that incorporating intermediate parities into the loss function allows the model to learn efficiently with teacher forcing, and even without it, by using augmented data for internal verification, demonstrating the benefits of task decomposition and self-consistency in multi-step reasoning.

Learning Neural Networks with Distribution Shift: Efficiently Certifiable Guarantees
This paper presents the first efficient algorithms for learning neural networks under distribution shift using the Testable Learning with Distribution Shift (TDS) framework, addressing the previously unhandled setting of nonconvex regression. By incorporating classical kernel methods with data-dependent feature maps, the authors provide a fully polynomial-time algorithm for TDS learning of one hidden-layer networks with sigmoid activations, particularly effective for training distributions with sub-exponential tails.

An Effective Theory of Bias Amplification
This paper presents a precise analytical theory for understanding and addressing biases in machine learning models, specifically within ridge regression and simplified feedforward neural networks. It offers insights into bias amplification, minority-group bias, and the impact of model design choices, aligning theoretical predictions with empirical observations and validating them through experiments on synthetic datasets.

### Theory->Optimization
Achieving Dimension-Free Communication in Federated Learning via Zeroth-Order Optimization
This paper introduces DeComFL, a novel dimension-free communication algorithm for Federated Learning (FL) that significantly reduces communication costs by using zeroth-order optimization techniques to transmit a constant number of scalar values, regardless of the model's dimension. The algorithm not only achieves state-of-the-art rates with linear speedup but also demonstrates substantial communication overhead reductions during empirical evaluations, successfully fine-tuning models with billions of parameters by transmitting merely around 1MB of data between server and client.

### Theory->Probabilistic Methods
PABBO: Preferential Amortized Black-Box Optimization
The paper presents a novel approach to Preferential Bayesian Optimization (PBO) that significantly reduces computational overhead by fully amortizing the process, meta-learning the surrogate and the acquisition function using a transformer neural process architecture. This method, combining reinforcement learning and tailored auxiliary losses, not only speeds up computations by several orders of magnitude over traditional Gaussian process-based strategies but also often improves accuracy in both synthetic and real-world benchmarks.


 
## Oral Session 5F (10:30-11:42)
10:30-10:42: Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects
This paper addresses the challenge of manipulating objects with varying geometries and deformable characteristics in robotics by introducing a heterogeneous graph representation to model both rigid and deformable object tasks. The proposed Heterogeneous Equivariant Policy (HEPi), leveraging $SE(3)$ equivariant message passing networks, demonstrates superior performance over other models in a novel reinforcement learning benchmark focused on complex manipulation tasks, highlighting improved average returns, sample efficiency, and generalization.
10:42-10:54: Instant Policy: In-Context Imitation Learning via Graph Diffusion
Instant Policy is introduced as a method for In-Context Imitation Learning (ICIL) in robotics, enabling robots to learn new tasks rapidly from one or two demonstrations by modeling ICIL as a graph generation problem with learned diffusion processes and using pseudo-demonstrations as training data. The approach allows for fast learning of everyday tasks and supports cross-embodiment and zero-shot transfer to language-defined tasks, highlighting its versatility and innovation in robotic learning.
10:54-11:06: Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation
This paper introduces Predictive Inverse Dynamics Models (PIDM), an innovative end-to-end approach that integrates vision and action by using inverse dynamics models conditioned on forecasted visual states, employing Transformers to enhance scalability in robotic manipulation. The proposed model, Seer, exhibits substantial performance improvements over state-of-the-art methods, demonstrating remarkable generalization across various conditions and achieving significant gains on multiple benchmarks and real-world tasks.
11:06-11:18: Data Scaling Laws in Imitation Learning for Robotic Manipulation
This paper explores data scaling laws in robotic manipulation and their potential to enable single-task robot policies that can be deployed zero-shot for any object in the same category across different environments. Through an empirical study involving over 40,000 demonstrations, the authors find that the diversity of environments and objects significantly impacts policy generalization, proposing an efficient data collection strategy that achieves high success rates with minimal additional effort.
11:18-11:30: Diffusion-Based Planning for Autonomous Driving with Flexible Guidance
The paper introduces a novel transformer-based Diffusion Planner for autonomous driving that effectively models multi-modal driving behavior and ensures trajectory quality without relying on predefined rules. By enabling joint modeling of prediction and planning tasks, the Diffusion Planner achieves state-of-the-art performance in closed-loop scenarios with improved safety, adaptability, and robust transferability, as demonstrated on large-scale benchmarks and a newly collected dataset.
11:30-11:42: Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks
This paper introduces Kinetix, a novel framework for training a general reinforcement learning agent by procedurally generating a vast array of 2D physics-based tasks utilizing the high-performance Jax2D physics engine. The study demonstrates that the pre-trained agent, capable of solving complex environments without prior exposure, showcases superior proficiency when fine-tuned, highlighting the potential of large-scale pre-training for sequential decision-making tasks.


## Oral Session 5E (10:30-11:42)
10:30-10:42: What should a neuron aim for? Designing local objective functions based on information theory
This paper introduces a framework for achieving self-organization between individual artificial neurons by designing bio-inspired local learning goals using Partial Information Decomposition (PID). By allowing neurons to locally process information from various inputs—feedforward, feedback, and lateral—the approach provides neuron-level interpretability and strong performance, advancing information-theoretic foundations for local learning strategies.
10:42-10:54: A Decade's Battle on Dataset Bias: Are We There Yet?
This paper revisits the dataset classification experiment proposed by Torralba & Efros, demonstrating that modern neural networks achieve high accuracy in identifying the dataset provenance of an image, such as 84.7% accuracy across YFCC, CC, and DataComp datasets. The findings reveal that the classifiers learn semantic features that are generalizable and not purely based on memorization, prompting a reevaluation of dataset bias within the research community.
10:54-11:06: On Conformal Isometry of Grid Cells: Learning Distance-Preserving Position Embedding
This paper explores the conformal isometry hypothesis as an explanation for the hexagonal patterns seen in grid cell response maps, suggesting that these patterns arise from a high-dimensional vector representation of an agent's position in neural space. By conducting numerical experiments and theoretical analyses, the study demonstrates that hexagonal grid firing patterns result from a distance-preserving embedding within a neural manifold, highlighting the significance of this model in effective navigation planning.
11:06-11:18: Comparing noisy neural population dynamics using optimal transport distances
This paper highlights the limitations of current methods in quantifying geometric similarity in neural representations, especially when dealing with noisy and dynamic neural responses. The authors propose a new metric based on optimal transport distance between Gaussian processes to better capture differences in systems with noisy dynamic responses, applying it to both motor system neural models and latent diffusion models for text-to-image synthesis.
11:18-11:30: A Computational Framework for Modeling Emergence of Color Vision in the Human Brain
This paper presents a computational framework to model the emergence of human color vision by simulating the eye and cortex, addressing how the visual cortex infers color dimensionality from optic nerve signals. The study introduces a simulation engine based on vision science and a cortical learning model that predicts optic nerve fluctuations, showing that natural color vision emerges and can be enhanced from 3D to 4D, as demonstrated in simulations akin to gene therapy effects on squirrel monkeys.
11:30-11:42: Learning and aligning single-neuron invariance manifolds in visual cortex
This paper introduces a novel method for identifying and aligning the invariance manifolds of visual sensory neurons, addressing challenges related to varying receptive field sizes, positions, and orientations. By learning continuous invariance manifolds and applying affine transformations, the approach quantifies and compares neuronal invariances, revealing functional clusters and offering new insights into the functional properties of visual neurons, as demonstrated with simulated neurons and macaque V1 neurons.


## Oral Session 5A (10:30-11:42)
10:30-10:42: Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment
The paper introduces Spread Preference Annotation (SPA), a framework to enhance the alignment of large language models (LLMs) with human preferences using minimal human-annotated data. By leveraging human prior knowledge and generating self-annotated data, the approach significantly improves alignment performance, demonstrated by superior results on AlpacaEval 2.0 with only a fraction of ground-truth preference labels.
10:42-10:54: Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs
PrefEval is a benchmark developed to evaluate Large Language Models (LLMs) on their ability to infer, memorize, and adhere to user preferences in long-context conversations. Testing across various models revealed significant challenges in maintaining preference accuracy over time, though fine-tuning with PrefEval demonstrated improvements, offering a valuable resource for developing personalized conversational agents.
10:54-11:06: Language Representations Can be What Recommenders Need: Findings and Potentials
This paper challenges the current understanding that language models and traditional recommenders learn distinct representation spaces, demonstrating that item representations mapped from language models yield superior recommendation performance. By using language representations, the study develops a collaborative filtering model outperforming traditional ID-based CF models, revealing homomorphic connections between language and behavior modeling, and providing insights for both natural language processing and recommender systems.
11:06-11:18: DarkBench: Benchmarking Dark Patterns in Large Language Models
DarkBench is a benchmark designed to identify and evaluate dark design patterns—manipulative techniques influencing user behavior—in large language models (LLMs). The study analyzes 660 prompts across six categories and evaluates LLMs from five major companies, revealing biases and manipulation that underline the need for more ethical AI development.
11:18-11:30: Linear Representations of Political Perspective Emerge in Large Language Models
This paper examines how large language models (LLMs) can replicate liberal and conservative political viewpoints in American politics by exploring their linear representations of these perspectives within activation space. By probing attention heads in transformer-based LLMs, the study reveals that specific attention head activations can predict political ideologies and news outlet slants, and demonstrates that model outputs can be steered toward desired stances through linear interventions, highlighting the potential to interpret and influence LLM-generated subjective perspectives.
11:30-11:42: Do as We Do, Not as You Think: the Conformity of Large Language Models
This paper explores conformity in large language model-driven multi-agent systems, identifying factors influencing conformity and evaluating its impact on problem-solving capabilities. Introducing the BenchForm benchmark, the study investigates strategies to mitigate conformity effects, aiming to foster more robust and ethically-aligned collaborative AI systems.


## Oral Session 5C (10:30-11:42)
10:30-10:42: Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
This paper presents methods for identifying and utilizing sparse feature circuits, which are human-interpretable subnetworks important for explaining language model behaviors, offering a clearer understanding compared to previous circuit frameworks. The authors introduce SHIFT to enhance classifier generalization by removing irrelevant features and demonstrate a scalable, unsupervised pipeline for discovering a multitude of these circuits to improve model interpretability.
10:42-10:54: On the Hölder Stability of Multiset and Graph Neural Networks
This paper introduces a novel framework called Hölder in expectation to analyze the separation quality of multiset and graph neural networks, addressing limitations in traditional separation notions that fail in practice with fixed finite precision. The authors demonstrate that common models often fall short in practice, and propose two new Message Passing Neural Networks (MPNNs) with improved separation capabilities, effectively classifying challenging adversarial examples and outperforming standard MPNNs on typical graph learning tasks.
10:54-11:06: Unlearning-based Neural Interpretations
This paper critiques the use of static functions as baselines in gradient-based interpretations, which tend to inject biases by deviating from model behavior, leading to flawed attribution maps. It introduces a novel approach called $\texttt{UNI}$ that uses an adaptive, debiased baseline through perturbation toward an unlearning direction, offering more reliable explanations by smoothing decision boundaries and improving robustness and interpretability.
11:06-11:18: Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition
This paper explores the evolution of a model's knowledge integration during pretraining and introduces 'knowledge entropy' to measure the diversity of memory sources engaged by the model. The study finds that a decline in knowledge entropy, characterized by fewer active memory sources, negatively impacts the model’s ability to acquire and retain knowledge, while activating more memory sources improves these capabilities.
11:18-11:30: Cross-Entropy Is All You Need To Invert the Data Generating Process
This paper extends identifiability results from self-supervised learning to supervised learning, particularly in parametric instance discrimination, to explain how neural models learn interpretable factors of variation up to linear transformations. Through theoretical analysis and empirical validation with simulated data and benchmarks like DisLib and ImageNet, the study offers insights into the linear representation capabilities of models, contributing to a more unified theory of the effectiveness of supervised learning.
11:30-11:42: Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning
This paper analyzes the strengths of reinforcement learning methods and introduces a new Mutual Information Skill Learning (MISL) method called contrastive successor features, which matches the performance of METRA with a simpler approach. The study also highlights connections between skill learning and other learning techniques, providing insight through comprehensive ablation studies.


## Oral Session 5B (10:30-11:42)
10:30-10:42: How much of my dataset did you use? Quantitative Data Usage Inference in Machine Learning
This paper introduces Dataset Usage Cardinality Inference (DUCI), a fine-grained analysis technique designed to estimate the exact proportion of data used to train machine learning models. DUCI uses debiased membership guesses to match the optimal MLE approach's performance with a maximum error of less than 0.1, while dramatically reducing computational costs by up to 300 times, addressing limitations in previous binary-only data usage assessments.
10:42-10:54: Proxy Denoising for Source-Free Domain Adaptation
Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data, but challenges arise due to the noisy supervision from Vision-Language (ViL) models. The proposed Proxy Denoising (ProDe) approach addresses this by correcting ViL's predictions with a novel proxy confidence theory, significantly improving performance over existing methods across various SFDA settings.
10:54-11:06: Data Shapley in One Training Run
This paper introduces In-Run Data Shapley, a novel method for assessing data contribution without retraining models, specifically tailored for a particular model of interest. The approach efficiently calculates Shapley values with minimal runtime overhead, allowing for scalable data attribution during foundation model pretraining and providing insights into data contribution and its implications for copyright and data curation in generative AI.
11:06-11:18: Data Selection via Optimal Control for Language Models
This paper presents a method called PMP-based Data Selection (PDS) for enhancing the performance of language models by selecting high-quality pre-training data from large corpora, formulated as an Optimal Control problem. The PDS framework, validated through experiments on CommonCrawl, accelerates learning, boosts performance across various tasks and model sizes, and optimizes data utilization, reducing data demand by 1.8 times when pre-training data is limited.
11:18-11:30: Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection
The paper introduces DiverSified File selection algorithm (DiSF), which aims to enhance the performance of large language models by selecting decorrelated text files, addressing the diversity dilemma caused by domain-similarity selection criteria. DiSF improves overall performance across nine tasks, significantly increasing training and data efficiency with models up to 1.1B parameters, demonstrating its effectiveness by saving 98.5% of pre-training files compared to full-data pre-training.
11:30-11:42: DEPT: Decoupled Embeddings for Pre-training Language Models
The paper introduces DEPT, a communication-efficient pre-training framework that decouples embeddings from the transformer body to effectively handle data heterogeneity without needing a shared vocabulary. DEPT significantly reduces communication costs and embedding memory usage while improving model generalization and performance, as demonstrated in federated pre-training of billion-scale models.


## Oral Session 5D (10:30-11:42)
10:30-10:42: Tight Lower Bounds under Asymmetric High-Order Hölder Smoothness and Uniform Convexity
This paper establishes tight lower bounds for the oracle complexity involved in minimizing high-order Hölder smooth and uniformly convex functions under two distinct cases of degree imbalance. It generalizes previous findings on oracle complexities for first and second-order smooth functions and uniformly convex functions, matching the established upper bounds for these complex cases, thereby enhancing our understanding of optimization in these settings.
10:42-10:54: Second-Order Min-Max Optimization with Lazy Hessians
This paper introduces an enhanced second-order method for convex-concave minimax optimization, improving computational complexity by reusing the Hessian across iterations. The proposed method achieves a complexity reduction factor of $d^{1/3}$ compared to previous techniques and generalizes effectively to strongly-convex-strongly-concave problems, with numerical experiments confirming its efficiency.
10:54-11:06: Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model
This paper investigates whether neural networks trained with gradient-based methods can achieve the optimal statistical-computational tradeoff for learning Gaussian single-index models, ultimately matching the theoretical sample complexity bounds determined by the statistical query framework. The authors introduce a unified gradient-based algorithm adaptable to various functions, showing it effectively learns feature representations aligned with the unknown signal, even when the signal is sparse, and achieves sample complexity close to the theoretical lower bound, offering insights applicable to problems like sparse tensor PCA.
11:06-11:18: Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization
This paper challenges the belief that standard Bayesian Optimization (BO) with Gaussian processes is inadequate for high-dimensional optimization, demonstrating through empirical and theoretical analyses that Matérn kernels outperform the commonly used Square Exponential (SE) kernel which suffers from improper length-scale initialization. The authors introduce a robust initialization strategy for the SE kernel that significantly enhances its performance, advocating for a re-assessment of standard BO's capabilities in high-dimensional scenarios.
11:18-11:30: Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent
This paper establishes finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm using the Kernelized Stein Discrepancy (KSD) and Wasserstein-2 metrics, revealing a convergence rate of order $1/\sqrt{N}$ that significantly improves upon previous results. By incorporating a bilinear component in the kernel, the study also explores Wasserstein-2 convergence rates and analyzes the curse-of-dimensionality, marginal convergence, and long-time propagation of chaos in the context of particle system dynamics.
11:30-11:42: Classic but Everlasting: Traditional Gradient-Based Algorithms Converges Fast Even in Time-Varying Multi-Player Games
This paper investigates the last-iterate convergence behaviors of extra gradient (EG) and optimistic gradient (OG) algorithms in time-varying perturbed games, extending analysis beyond the usual time-invariant settings by incorporating vanishing noises and other external factors. The study proves that, for perturbed games on bounded convex closed sets, the last-iterate convergence rates of EG and OG algorithms are \( O(1/\sqrt{T}) \), thereby addressing an open question about convergence rates in constrained and time-varying scenarios and aligning with known results for time-invariant games.


## Poster Session 6 (15:00-17:30)
### Applications->Chemistry and Drug Discovery
Distilling Structural Representations into Protein Sequence Models
We introduce the Implicit Sequence Model (ISM), a sequence-only input model that enhances protein representations by integrating evolutionary history and structural properties, outperforming existing sequence models on tasks like mutation stability assessment and structure prediction. ISM's advancements include a microenvironment-based Autoencoder and a self-supervised training objective, with its structure-enriched weights accessible for applications within the ESM2 framework.

Fragment and Geometry Aware Tokenization of Molecules for Structure-Based Drug Design Using Language Models
Frag2Seq leverages language models for structure-based drug design by generating molecules in a fragment-based manner, where fragments correspond to functional modules and preserve geometric information. By incorporating protein pocket embeddings for target-aware molecule generation, Frag2Seq outperforms existing methods in binding affinity and chemical properties, and demonstrates significant efficiency improvements, with up to 300 times faster sampling than traditional methods.

Atomas: Hierarchical Adaptive Alignment on Molecule-Text for Unified Molecule Understanding and Generation
The paper introduces Atomas, a hierarchical molecular representation learning framework designed to capture fine-grained information by aligning SMILES strings and text at multiple semantic levels. Demonstrating superior performance across numerous tasks and datasets, Atomas outperforms existing models, proving its effectiveness, scalability, and chemical relevance, as validated by human experts.

IgGM: A Generative Model for Functional Antibody and Nanobody Design
IgGM is a generative model developed for the de novo design of immunoglobulins with functional specificity, addressing challenges faced by existing antibody design methods that rely on additional conditions not reflective of real-world scenarios. Utilizing a pre-trained language model, feature learning, and prediction modules, IgGM is capable of generating antibody sequences and structures for specific antigens, demonstrating effectiveness in predicting novel antibodies and nanobodies, thus contributing significantly to practical applications in drug development. Code is available at: https://github.com/TencentAI4S/IgGM.

Transition Path Sampling with Improved Off-Policy Training of Diffusion Path Samplers
This paper introduces TPS-DPS, a novel approach using diffusion path samplers to tackle the transition path sampling problem in molecular systems without the need for collective variables, enhancing scalability and efficiency in sampling realistic and diverse pathways. The method is evaluated on various systems, including synthetic and protein models, showing superior performance over existing techniques in generating transition pathways essential for advancing drug discovery and material design.

Beyond Sequence: Impact of Geometric Context for RNA Property Prediction
This study systematically evaluates the impact of incorporating 2D and 3D geometric information into RNA property prediction models, demonstrating improved performance over traditional 1D sequence-based models. By introducing curated RNA datasets with enhanced structural annotations, the research highlights the substantial benefits of geometry-aware models in prediction accuracy and efficiency, especially in challenging data environments, while also acknowledging the robustness of sequence-based models to sequencing noise.

UniGEM: A Unified Approach to Generation and Property Prediction for Molecules
UniGEM is introduced as the first unified model that integrates molecular generation and property prediction, overcoming the challenges of task inconsistency by using a novel two-phase generative process. The model demonstrates superior performance in both areas, aided by advanced training strategies, and its concepts have potential applications beyond molecular tasks, such as in natural language processing and computer vision.

### Applications->Computer Vision
NL-Eye: Abductive NLI For Images
The paper introduces NL-Eye, a benchmark designed to evaluate the visual abductive reasoning skills of Visual Language Models (VLMs), focusing on their ability to infer outcomes and causes in the visual domain. It highlights that current VLMs struggle with this task compared to humans, emphasizing the need for advancements in VLMs for applications such as accident-prevention and video verification.

TASAR: Transfer-based Attack on Skeletal Action Recognition
This paper introduces TASAR, the first Transfer-based Attack on Skeletal Action Recognition, which enhances adversarial transferability by smoothening the loss function and incorporating temporal coherence into its Bayesian attack strategy. Extensive evaluations on a newly-built large-scale S-HAR benchmark demonstrate TASAR's superiority over existing methods, with all resources made available for future research comparisons.

MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models
The paper introduces MRAG-Bench, a multimodal retrieval-augmented generation benchmark designed to assess scenarios where visual information is more beneficial than textual knowledge for large vision-language models (LVLMs). The study evaluates 14 LVLMs and finds that all models, including top-performing GPT-4o, show greater improvements with visual augmentation, emphasizing MRAG-Bench's significance in advancing the models' ability to effectively leverage visual knowledge.

Learning 3D Perception from Others' Predictions
The paper addresses the challenge of accurate 3D object detection by proposing a method where an ego vehicle learns from the predictions of nearby units equipped with optimized detectors, thereby reducing the need for extensive annotated data. By identifying and mitigating issues such as viewpoint mismatches and mislocalization, and utilizing a distance-based curriculum and pseudo label refinement, the approach allows for efficient, sensor-agnostic, and communication-efficient training, as validated on a collaborative driving dataset.

Re-Thinking Inverse Graphics With Large Language Models
The paper presents the Inverse-Graphics Large Language Model (IG-LLM), which leverages the generalization capabilities of large language models to solve inverse-graphics problems by autoregressively decoding visual embeddings into structured 3D-scene representations. The study demonstrates that LLMs can facilitate precise spatial reasoning without image-space supervision, opening new possibilities for image analysis with the provided code and data for reproducibility and future research.

Solving Inverse Problems with Model Mismatch using Untrained Neural Networks within Model-based Architectures
This paper introduces an untrained forward model residual block within model-based deep learning architectures to address forward model mismatches in solving inverse problems. By enhancing loop unrolling (LU) and deep equilibrium model (DEQ) frameworks, the proposed method improves reconstruction quality, handling both linear and nonlinear problems effectively, while requiring no additional data and ensuring robustness to initial conditions and iterations.

EffoVPR: Effective Foundation Model Utilization for Visual Place Recognition
This paper introduces a novel approach for Visual Place Recognition (VPR) by effectively utilizing features extracted from self-attention layers of foundation models like DINOv2, achieving superior performance even in zero-shot settings. The proposed method not only surpasses previous zero-shot and many supervised approaches but also demonstrates state-of-the-art performance and robustness under challenging conditions with compact feature dimensions.

Diff-2-in-1: Bridging Generation and Dense Perception with Diffusion Models
Diff-2-in-1 is a new diffusion-based framework that combines multi-modal data generation and dense visual perception in a unified process, enhancing both tasks through the diffusion-denoising process. This approach improves visual perception by utilizing a self-improving learning mechanism and generates high-quality multi-modal data that mirrors the original training set, resulting in consistent performance improvements verified through comprehensive experiments.

Brain-inspired $L_p$-Convolution benefits large kernels and aligns better with visual cortex
This paper introduces a novel methodology called $L_p$-convolution, which utilizes the multivariate $L_p$-generalized normal distribution to create adaptable $L_p$-masks that reconcile disparities between artificial and biological receptive fields (RFs) in CNNs. The study demonstrates that integrating these biologically inspired RFs through $L_p$-convolution enhances model performance, especially in tasks requiring flexible RF shapes, and improves the alignment of CNNs' neural representations with the visual cortex.

Ranking-aware adapter for text-driven image ordering with CLIP
This paper introduces an innovative approach to enhance vision-language models by transforming the CLIP model into a learning-to-rank task, using a lightweight adapter for text-guided image ranking. By incorporating learnable prompts and ranking-aware attention, the proposed method outperforms fine-tuned CLIPs and achieves competitive results, offering a generalized solution for image ranking without relying heavily on extensive text prompting.

AniSDF: Fused-Granularity Neural Surfaces with Anisotropic Encoding for High-Fidelity 3D Reconstruction
AniSDF is a novel approach that combines fused-granularity neural surfaces and physics-based encoding to achieve high-fidelity 3D reconstruction and photo-realistic renderings without sacrificing geometry quality. It introduces innovations like blended radiance fields and anisotropic spherical Gaussian encoding to accurately balance structures and fine geometric details, demonstrated through extensive experiments to enhance the performance of SDF-based methods in both geometry reconstruction and novel-view synthesis.

MTSAM: Multi-Task Fine-Tuning for Segment Anything Model
The paper introduces the Multi-Task SAM (MTSAM) framework to enhance the Segment Anything Model (SAM) for multi-task learning by modifying its architecture for task-specific outputs and introducing Tensorized low-Rank Adaptation (ToRA) for multi-task fine-tuning. Extensive experiments show MTSAM effectively addresses SAM's challenges and improves its performance in multi-task scenarios.

Ready-to-React: Online Reaction Policy for Two-Character Interaction Generation
This paper introduces "Ready-to-React," an online reaction policy for generating two-character online interactions, which enables characters to independently and dynamically respond to each other's motions in real time, akin to real human interactions. Implemented with a diffusion head in an auto-regressive model, this approach outperforms existing methods by reducing error accumulation and enhancing control in interactive environments, demonstrated through experiments on a challenging boxing task.

Analyzing and Boosting the Power of Fine-Grained Visual Recognition for Multi-modal Large Language Models
This paper introduces Finedefics, an enhanced multi-modal large language model that addresses the challenge of fine-grained visual recognition (FGVR) by integrating informative attribute descriptions during training. By employing contrastive learning with hard negatives, Finedefics significantly improves upon existing models of similar sizes in FGVR tasks, as demonstrated by extensive evaluations on multiple datasets.

LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language Models for Referring Expression Comprehension
The paper introduces LLM-wrapper, a method for adapting Vision Language Models (VLMs) to the Referring Expression Comprehension (REC) task using Large Language Models (LLMs) without needing 'white-box' access. By leveraging the reasoning capabilities of LLMs for selecting relevant outputs, LLM-wrapper shows significant performance improvements across multiple datasets, offering a versatile and practical alternative for black-box VLM adaptation.

PseDet: Revisiting the Power of Pseudo Label in Incremental Object Detection
This paper addresses the limitations of pseudo-labeling in Incremental Object Detection (IOD) and introduces a new framework called PseDet. By leveraging a spatio-temporal enhancement module and a Categorical Adaptive Label Selector, PseDet improves label quality and localization alignment, achieving state-of-the-art performance on the COCO benchmarks with significant mAP improvements.

GS-CPR: Efficient Camera Pose Refinement via 3D Gaussian Splatting
We introduce GS-CPR, a test-time camera pose refinement framework utilizing 3D Gaussian Splatting to improve localization accuracy in challenging scenes without the need for additional training of features or descriptors. Our method outperforms current NeRF-based approaches by efficiently achieving state-of-the-art accuracy and runtime across various visual localization benchmarks, particularly excelling in two indoor datasets.

GDrag:Towards General-Purpose Interactive Editing with Anti-ambiguity Point Diffusion
GDrag is a novel, training-free framework designed to address intention and content ambiguities in interactive point-based image manipulation, offering precise, general-purpose edits through a taxonomy of atomic manipulations. By implementing anti-ambiguity dense trajectory calculation and self-adaptive motion supervision, GDrag achieves superior results over existing methods, as demonstrated on the DragBench dataset, with plans to release its code upon acceptance.

CatVTON: Concatenation Is All You Need for Virtual Try-On with Diffusion Models
CatVTON is a streamlined and efficient virtual try-on diffusion model that transfers garments to target individuals by simplifying the architecture without additional encoding modules and complex preprocessing. It reduces training and inference burdens with a lightweight network, parameter-efficient training, and simplified inference, achieving superior results and strong generalization performance while using significantly less memory and parameters compared to other methods.

Point-based Instance Completion with Scene Constraints
This paper introduces a point cloud-based instance completion model that effectively completes objects within a scene at various scales and poses by integrating scene constraints via a cross-attention mechanism. The model, evaluated on the newly created ScanWCF dataset, demonstrates enhanced fidelity, completion quality, and plausibility over existing methods for indoor scene completion tasks.

Random Is All You Need: Random Noise Injection on Feature Statistics for Generalizable Deep Image Denoising
This paper introduces RNINet, a novel encoder-decoder architecture for deep image denoising that leverages noise injection blocks to enhance generalization across various unseen noise types. RNINet simplifies the complexity of the existing state-of-the-art Masked Training models while delivering superior denoising performance and efficiency, achieving faster inference speeds and demonstrating its potential for large-scale deployments.

### Applications->Everything Else
MarS: a Financial Market Simulation Engine Powered by Generative Foundation Model
This paper introduces the Large Market Model (LMM), a generative foundation model for simulating financial markets at the order level, akin to language modeling, addressing the need for realistic, interactive, and controllable order generation. Using the MarS engine powered by LMM, the authors demonstrate enhanced scalability, realism, and practical application in forecasting, detection, analysis, and agent training, marking a potential paradigm shift in financial market simulations.

### Applications->Language, Speech and Dialog
Audio Large Language Models Can Be Descriptive Speech Quality Evaluators
This paper introduces the first natural language-based speech evaluation corpus derived from authentic human ratings to address the lack of quality awareness in audio large language models (LLMs). Using this corpus, the proposed alignment approach with LLM distillation (ALLD) significantly improves MOS prediction accuracy and A/B test performance, and enhances response generation, advancing the capabilities of audio LLMs for real-world applications.

Competing Large Language Models in Multi-Agent Gaming Environments
The paper introduces GAMA($\gamma$)-Bench, a novel framework designed to evaluate the gaming ability of Large Language Models (LLMs) in multi-agent environments through eight classical game theory scenarios. The framework reveals that while GPT-3.5 is robust, its generalizability can be enhanced, and among 13 evaluated LLMs, Gemini-1.5-Pro achieves the highest performance.

ADIFF: Explaining audio difference using natural language
This paper is the first to comprehensively study the task of explaining audio differences, introducing two new datasets and benchmark baselines for the task. It proposes ADIFF, a model enhancement using a cross-projection module and a three-step training process, significantly improving the generation of detailed audio difference explanations compared to existing methods.

Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation
This paper introduces SpatialSonic, a model that addresses the challenges of generating stereo audio with spatial contexts by utilizing spatial-aware encoders and azimuth state matrices for accurate guidance to Latent Diffusion Models. By constructing a comprehensive dataset, BEWO-1M, and employing multimodal strategies, the study marks the first attempt to generate immersive and controllable spatial audio from text, showing effectiveness in both simulated and real-world evaluations.

SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents
This paper explores the effectiveness of different memory granularities in retrieval augmented response generation for long-term conversations and identifies the limitations of turn-level and session-level memory. The proposed method, **SeCom**, enhances retrieval accuracy and response quality by using a conversation segmentation model and compressed memory units, outperforming other methods on long-term conversation and dialogue segmentation benchmarks.

### Applications->Neuroscience, Cognitive Science
SIM: Surface-based fMRI Analysis for Inter-Subject Multimodal Decoding from Movie-Watching Experiments
This paper introduces a novel approach to brain decoding using surface vision transformers that create a generalizable model of cortical functional dynamics by encoding cortical network interactions as moving images. By integrating tri-modal self-supervised contrastive alignment of audio, video, and fMRI data, the method successfully retrieves visual and auditory stimuli from brain activity in individuals and scenarios not part of the training set, advancing personalized simulation capabilities of brain function.

SPDIM: Source-Free Unsupervised Conditional and Label Shift Adaptation in EEG
This paper proposes a geometric deep learning framework for source-free unsupervised domain adaptation (SFUDA) in EEG applications, addressing distribution shifts including label shifts that challenge generalization. The novel SPDIM strategy, grounded in manifold optimization, effectively compensates for shifts and outperforms existing methods in EEG-based brain-computer interface and sleep staging datasets.

NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap between Language and EEG Signals
NeuroLM is a groundbreaking multi-task foundation model that views EEG signals as a foreign language to enhance brain-computer interfaces and healthcare applications by integrating Large Language Models (LLMs). By using a vector-quantized encoding system and multi-task instruction tuning, NeuroLM unifies diverse EEG tasks in a single model with improved versatility and efficiency, achieving record-breaking performance with its largest variant, NeuroLM-XL, on extensive EEG datasets.

### Applications->Physics
PIED: Physics-Informed Experimental Design for Inverse Problems
This work introduces Physics-Informed Experimental Design (PIED), an innovative framework that leverages physics-informed neural networks (PINNs) to optimize design parameters for inverse problem-solving in systems governed by PDEs. PIED addresses computational challenges and improves the efficiency and accuracy of experimental design under limited budgets by utilizing a fully differentiable architecture that enables one-shot deployments, parallelized computations, and meta-learning of PINN parameter initialization, ultimately outperforming existing methods in both simulated and real-world scenarios.

Lie Algebra Canonicalization: Equivariant Neural Operators under arbitrary Lie Groups
LieLAC is introduced as a new method that uses the action of infinitesimal generators of symmetry groups to enhance the generalization of machine learning models, specifically for Physics-Informed Neural Networks solving PDEs. By transforming inputs into a canonical form that aligns with symmetries, LieLAC achieves equivariance in pre-trained models, demonstrated through invariant image classification and neural PDE solver tasks.

FreeCG: Free the Design Space of Clebsch-Gordan Transform for Machine Learning Force Fields
FreeCG is a novel method for machine learning force fields that utilizes a mathematical approach called invariance transitivity to allow for more flexible and computationally efficient Clebsch–Gordan (CG) transforms. This approach achieves state-of-the-art results in force and property prediction across multiple datasets and sets a new standard for future geometric network designs by improving both performance and practicality.

### Applications->Robotics
LLaRA: Supercharging Robot Learning Data for Vision-Language Policy
LLaRA: Large Language and Robotics Assistant is a framework that transforms pretrained Vision Language Models (VLMs) into Vision-Language-Action (VLA) models for robotic control, even with limited robot demonstrations. It introduces an automated pipeline for creating conversation-style robot instruction data, uses self-supervised auxiliary tasks for enhancement, and achieves state-of-the-art performance in both simulated and real-world tasks, while maintaining the generalization capabilities of large language models.

OccProphet: Pushing the Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with an Observer-Forecaster-Refiner Framework
This paper introduces OccProphet, a novel framework for occupancy forecasting in autonomous driving environments that significantly reduces computational demands while improving forecasting accuracy. By utilizing a combination of lightweight components—Observer, Forecaster, and Refiner—OccProphet achieves 58-78% less computational cost and a 2.6× speedup compared to state-of-the-art methods, alongside a 4-18% increase in accuracy, making it more suitable for edge deployment.

AnyTouch: Learning Unified Static-Dynamic Representation across Multiple Visuo-tactile Sensors
TacQuad is an aligned multi-modal multi-sensor tactile dataset introduced to establish a unified representation for visuo-tactile sensors, enabling tactile knowledge transfer and integration. The AnyTouch framework enhances tactile perception by employing a multi-level static-dynamic representation, achieving superior performance in multi-sensor environments and proving its effectiveness through comprehensive analysis on offline datasets and real-world tasks.

Learning Shape-Independent Transformation via Spherical Representations for Category-Level Object Pose Estimation
SpherePose offers a novel approach to category-level object pose estimation by using spherical representations as a shared proxy shape, overcoming limitations of existing shape-dependent methods. The architecture ensures precise correspondence prediction through SO(3)-invariant feature extraction, spherical attention mechanisms, and a hyperbolic correspondence loss function, with experimental results on various benchmarks demonstrating its superior performance.

AHA: A Vision-Language-Model for Detecting and Reasoning Over Failures in Robotic Manipulation
The paper presents AHA, an open-source vision-language model designed to detect and reason about failures in robotic manipulation using natural language, enhancing adaptability across different robots, tasks, and environments. Trained on the novel AHA dataset created through FailGen, AHA significantly outperforms existing models in failure detection and improves task success rates in robotic systems by refining reinforcement learning and planning processes.

Direct Post-Training Preference Alignment for Multi-Agent Motion Generation Model Using Implicit Feedback from Pre-training Demonstrations
This paper introduces a novel approach for aligning large language model-based motion generation models with human preferences in multi-agent scenarios by using implicit preferences from pre-training expert demonstrations instead of costly post-training preferences. The method enhances the realism of generated behaviors in traffic simulations involving numerous agents, achieving performance comparable to state-of-the-art models while eliminating the need for additional preference annotations and reducing computational expenses.

### Applications->Time Series
Learning vector fields of differential equations on manifolds with geometrically constrained operator-valued kernels
This paper introduces a geometrically constrained operator-valued kernel to efficiently learn ordinary differential equations (ODEs) on manifolds, addressing the high computational demands faced by existing machine learning methods. The authors propose a geometry-preserving ODE solver with a theoretical error bound that ensures approximate solutions remain on the manifold, demonstrating its effectiveness on various high-dimensional dynamical systems.

### Deep Learning->Attention Mechanisms
Improved Algorithms for Kernel  Matrix-Vector Multiplication Under Sparsity Assumptions
This paper develops fast algorithms for computing matrix-vector products with asymmetric Gaussian Kernel matrices, focusing on applications like fast attention computation in large language models. Assuming the sum of the entries of the kernel matrix scales linearly with the number of keys and queries, the authors present the first subquadratic time algorithm that efficiently approximates these products, achieving significant speed-ups over traditional methods.

### Deep Learning->Everything Else
Breaking Neural Network Scaling Laws with Modularity
This paper investigates how modular neural networks generalize better compared to nonmodular networks, particularly in modularly structured, high-dimensional tasks. The authors theoretically and empirically demonstrate that modular networks require fewer samples independent of task dimensionality for generalization and introduce a novel learning rule to leverage this advantage, enhancing both in- and out-of-distribution performance.

Efficient Low-Bit Quantization with Adaptive Scales for Multi-Task Co-Training
This paper introduces Task-Specific Scales Quantization for Multi-Task Co-Training (TSQ-MTC), which addresses performance degradation in quantization-aware training (QAT) methods due to mismatched activation quantization scales. By incorporating a task-specific learnable multi-scale activation quantizer (TLMAQ) and structure-based layer-by-layer distillation (SLLD), TSQ-MTC effectively preserves information in quantized networks, achieving a 4-bit quantized model that maintains image quality on par with full-precision models, demonstrated by a comparable PSNR and a $7.99\times$ compression ratio in a super-resolution task.

### Deep Learning->Generative Models and Autoencoders
Hallo2: Long-Duration and High-Resolution Audio-Driven Portrait Image Animation
This paper presents Hallo2, an enhanced version of the generative model Hallo, which supports hour-long 4K portrait video animation utilizing techniques like patch-drop with Gaussian noise for visual consistency and vector quantization for maintaining temporal coherence. Unique in its ability to use adjustable semantic textual labels for expression control, Hallo2 achieves state-of-the-art performance in generating rich, controllable content on datasets like HDTF, CelebV, and the new "Wild" dataset.

MDSGen: Fast and Efficient Masked Diffusion Temporal-Aware Transformers for Open-Domain Sound Generation
MDSGen is an innovative framework for vision-guided open-domain sound generation that optimizes model size, memory usage, and inference speed by using denoising masked diffusion transformers. It introduces a video feature removal module and a temporal-aware masking strategy, achieving high accuracy and efficiency on the VGGSound dataset, significantly outperforming existing models in size and speed.

Easing Training Process of Rectified Flow Models Via Lengthening Inter-Path Distance
This paper introduces the Distance-Aware Noise-Sample Matching (DANSM) method, which enhances the training speed of diffusion models by 30% to 40% without degrading generation quality. By optimizing inter-path distances between noise-sample pairs, DANSM effectively organizes preferable paths, reducing crossings in lower-dimensional spaces and leveraging the relationships in high-dimensional spaces through rectified flow models.

No Training, No Problem: Rethinking Classifier-Free Guidance for Diffusion Models
Classifier-free guidance (CFG) is widely used to improve conditional diffusion models, but it requires complex training modifications. This paper introduces independent condition guidance (ICG) and time-step guidance (TSG), which provide the benefits of CFG without special training and extend to unconditional models, offering easy implementation and similar performance improvements.

Not All Prompts Are Made Equal: Prompt-based Pruning of Text-to-Image Diffusion Models
Adaptive Prompt-Tailored Pruning (APTP) is an innovative pruning method designed for text-to-image diffusion models that addresses the limitations of static and dynamic pruning by utilizing a prompt router model to assign varying capacities to different prompts based on their needs within a given compute budget. Through contrastive learning and optimal transport, APTP successfully prunes Stable Diffusion models, improving their performance on FID, CLIP, and CMMD scores, while revealing semantically meaningful clusters and identifying challenging prompts for higher capacity allocation.

Beyond the convexity assumption: Realistic tabular data generation under quantifier-free real linear constraints
This paper introduces the Disjunctive Refinement Layer (DRL), a novel method to improve synthetic tabular data generation by ensuring generated data aligns with user-defined constraints, particularly using quantifier-free linear formulas to define complex spaces. Experimental results demonstrate that DRL not only fully eliminates constraint violations in existing deep generative models but also enhances efficacy in downstream tasks, achieving performance improvements of up to 21.4% in F1-score and 20.9% in Area Under the ROC Curve.

Topological Schrödinger Bridge Matching
The paper introduces the Topological Schrödinger Bridge problem (TSBP) to address the challenge of matching signal distributions on topological domains, such as graphs and simplicial complexes, by employing topology-aware stochastic dynamics like topological heat diffusion. By developing TSB-based models that use topological neural networks, the authors demonstrate their approach's effectiveness in matching topological signals on both synthetic and real-world networks, highlighting the significant role of topology in these processes.

NVS-Solver: Video Diffusion Model as Zero-Shot Novel View Synthesizer
This paper introduces a novel view synthesis approach using pre-trained large video diffusion models without requiring additional training, enabling visually appealing results from single or multiple views of static or dynamic scenes. The method improves upon existing techniques by adaptively modulating the diffusion sampling process based on scene priors, evaluated extensively to demonstrate superior performance quantitatively and qualitatively.

ClassDiffusion: More Aligned Personalization Tuning with Explicit Class Guidance
ClassDiffusion introduces a novel approach to maintaining the compositional ability of text-to-image diffusion models by incorporating a semantic preservation loss during fine-tuning. This method effectively prevents semantic drift, enhancing the model's ability to generate images with new concepts across diverse conditions, and is successfully extended to personalized video generation.

Field-DiT: Diffusion Transformer on Unified Video, 3D, and Game Field Generation
This paper presents a novel model that enhances probabilistic field models for visual content generation by incorporating a view-wise sampling algorithm to focus on local structures and autoregressive generation to maintain global geometry. The model effectively adapts to various modalities by using conditions like text prompts and camera poses, demonstrating its capability as a scalable and unified framework across different visual content domains.

MS-Diffusion: Multi-subject Zero-shot Image Personalization with Layout Guidance
The paper introduces the MS-Diffusion framework, which addresses challenges in generating cohesive multi-subject photorealistic images from text by utilizing grounding tokens and a feature resampler for maintaining detail fidelity. By enhancing cross-attention with layout guidance, MS-Diffusion achieves superior inter-subject compositions and text fidelity, outperforming existing models in personalized text-to-image generation.

On the Feature Learning in Diffusion Models
This paper introduces a feature learning framework to analyze and compare the training dynamics of diffusion models with traditional classification models. Theoretical and empirical results indicate that diffusion models, due to their denoising objective, develop balanced representations, unlike classification models that favor easy-to-learn data patterns.

### Deep Learning->Graph Neural Networks
Learning Long Range Dependencies on Graphs via Random Walks
This paper presents a novel architecture that integrates the long-range capabilities of random walks with local message-passing in graph neural networks, addressing the limitations of both traditional message-passing GNNs and graph transformers. The proposed framework enhances graph representations by using random walk sequences and sequence models, achieving significantly improved performance on benchmark datasets, with up to 13% improvement on the PascalVoc-SP and COCO-SP datasets.

TGB-Seq Benchmark: Challenging Temporal GNNs with Complex Sequential Dynamics
This paper addresses the limitations of current temporal graph neural networks (temporal GNNs) and benchmark datasets by introducing the Temporal Graph Benchmark with Sequential Dynamics (TGB-Seq), which emphasizes learning complex sequential dynamics rather than predicting repeated edges. By providing large real-world datasets in various domains, TGB-Seq reveals the performance degradation of existing methods, thus offering new challenges and opportunities for the development of future models.

GNNs Getting ComFy: Community and Feature Similarity Guided Rewiring
The paper explores how rewiring strategies in graph neural networks (GNNs) can enhance performance by targeting community structures and node labels, challenging the traditional focus on maximizing the spectral gap. It proposes three novel rewiring strategies—ComMa, FeaSt, and ComFy—that improve community alignment and global homophily, demonstrating their effectiveness through extensive experiments.

Graph Neural Networks for Edge Signals: Orientation Equivariance and Invariance
This paper introduces EIGN, an architecture that uses novel direction-aware edge-level graph shift operators to effectively model both directed and undirected signals while distinguishing between directed and undirected edges. By revising *orientation equivariance* and introducing *orientation invariance*, EIGN addresses previous modeling limitations, achieving up to a 23.5% improvement in RMSE on edge-level flow simulation tasks compared to prior methods.

DistillHGNN: A Knowledge Distillation Approach for High-Speed Hypergraph Neural Networks
This paper introduces a novel framework to improve the inference speed and memory efficiency of Hypergraph Neural Networks (HGNNs) through a teacher-student knowledge distillation strategy. By transferring high-order information from a combined HGNN and MLP teacher model to a lightweight TinyGCN, the approach achieves high accuracy and reduced computational costs, making it suitable for real-time applications.

BrainOOD: Out-of-distribution Generalizable Brain Network Analysis
The paper presents BrainOOD, a framework designed to enhance Graph Neural Networks' (GNNs) out-of-distribution generalization and interpretability specifically for brain networks, addressing challenges such as distribution shifts and limited interpretability in identifying critical brain regions. By improving structure selection and filtering noisy features, BrainOOD outperforms 16 existing methods, achieving up to 8.5% better generalization to OOD subjects, and introduces the first OOD brain network benchmark, laying a foundation for future research in neuroscience.

PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph
PolyhedronNet is introduced as a novel framework for learning representations of 3D polyhedral objects by employing a surface-attributed graph to model vertices, edges, faces, and their interrelationships. This study enhances representation learning by using local rigid representations and a PolyhedronGNN for effective intra-face and inter-face message passing, demonstrating improved performance in classification and retrieval tasks across various datasets.

Graph Neural Ricci Flow: Evolving Feature from a Curvature Perspective
This paper introduces the Graph Neural Ricci Flow (GNRF), a novel continuous-depth GNN that generalizes discrete Ricci flow to attributed graphs, allowing for the efficient computation and adjustment of time-varying curvature. The GNRF demonstrates significant theoretical and empirical advantages, such as ensuring edge curvature approaches zero and achieving excellent performance across various datasets without relying on specific curvature definitions.

### Deep Learning->Large Language Models
TaskGalaxy: Scaling Multi-modal Instruction Fine-tuning with Tens of Thousands Vision Task Types
TaskGalaxy is introduced as a large-scale multimodal instruction fine-tuning dataset designed to overcome limitations in multimodal visual language models by expanding task diversity and quality using automated processes, achieving a total of 19,227 task types and 413,648 samples. Leveraging GPT-4o, among other models, to enrich the dataset, TaskGalaxy demonstrates substantial performance improvements across benchmarks and validates the necessity of diverse task inclusion in model training.

Aioli: A Unified Optimization Framework for Language Model Data Mixing
The paper investigates the optimal mixture of data groups for training language models and finds that prior methods do not consistently outperform a simple stratified sampling baseline. By unifying existing methods into a common optimization framework, the authors identify inaccuracies in the setting of mixing law parameters and introduce a new online method, Aioli, which dynamically adjusts proportions and consistently outperforms existing methods across multiple datasets.

Teaching LLMs How to Learn with Contextual Fine-Tuning
This paper introduces a novel approach called contextual fine-tuning, which uses instructional prompts that mimic human cognitive learning strategies to guide the learning process of large language models (LLMs). The proposed method enhances the rapid fine-tuning of LLMs on new datasets in evolving domains such as medical and financial fields, improving their knowledge retention and reasoning abilities.

Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving
The paper introduces a novel self-training algorithm, Learning to Plan before Answering (LEPA), designed to enhance large language model (LLM) post-training by developing anticipatory plans as abstract meta-knowledge for problem-solving. By generating and refining these plans, LEPA improves the effectiveness of solutions for complex problems, surpassing traditional methods on natural language reasoning benchmarks.

Optimized Multi-Token Joint Decoding With Auxiliary Model for LLM Inference
This paper introduces multi-token assisted decoding (MTAD) to enhance the efficiency and effectiveness of inference in large language models by allowing multi-token generation from their joint distribution. MTAD improves decoding speed and accuracy by using a smaller auxiliary model to approximate the joint distribution, achieving a 21.2% reduction in perplexity and a compression of energy and time costs compared to traditional methods, as demonstrated in evaluations on Llama-2 and OPT models.

How efficient is LLM-generated code? A rigorous & high-standard benchmark
The authors introduce ENAMEL, a benchmark aimed at evaluating the efficiency of code generated by large language models (LLMs), addressing the gap in existing evaluations which largely overlook efficiency in favor of correctness. ENAMEL incorporates a new efficiency metric, eff@k, and uses expertly curated solutions and test cases to reveal that current LLMs are inadequate at creating expert-level efficient code due to challenges in advanced algorithm design and implementation optimization.

Automated Design of Agentic Systems
This paper introduces the research area of Automated Design of Agentic Systems (ADAS), emphasizing the potential of using meta agents to automatically create and discover powerful agentic systems by programming them in code. The proposed Meta Agent Search algorithm demonstrates significant promise by producing agents that outperform state-of-the-art hand-designed agents across various domains, showcasing the robustness and general applicability of these automatically invented systems.

MMAU: A Massive Multi-Task Audio Understanding and Reasoning Benchmark
MMAU is a new benchmark designed to assess multimodal audio understanding models through tasks that require expert-level knowledge and complex reasoning across speech, environmental sounds, and music. Despite testing 18 audio-language models, including advanced proprietary ones like Gemini 2.0 Flash achieving only 59.93% accuracy, the benchmark highlights a significant gap in current models' capabilities, aiming to stimulate advancements in the field.

Adaptive Data Optimization: Dynamic Sample Selection with Scaling Laws
Adaptive Data Optimization (ADO) is introduced as a scalable algorithm that optimizes data distributions online during model training without the need for external knowledge or proxy models. ADO employs per-domain scaling laws to dynamically adjust data mixtures, achieving performance comparable to or better than existing methods while maintaining computational efficiency, thus offering a practical and cost-effective solution to data collection and distribution challenges.

Permute-and-Flip: An optimally stable and watermarkable decoder for LLMs
This paper introduces the Permute-and-Flip (PF) decoder, a new method that offers up to twice the quality-stability tradeoff compared to standard sampling decoders, without being inferior to any existing decoders. Additionally, a complementary cryptographic watermarking scheme is developed for the PF decoder, maintaining the output distribution while achieving low false positives and high recall in high-entropy texts, with experiments showing its superiority in perplexity and stability over naive sampling and its watermarked variants.

Shot2Story: A New Benchmark for Comprehensive Understanding of Multi-shot Videos
This paper introduces a new multi-shot video understanding benchmark named \dataset, which includes detailed shot-level captions, comprehensive video summaries, and question-answering pairs to enhance semantic understanding. Despite challenges in generating long and comprehensive video summaries, initial experiments show that even imperfect summaries perform competitively in existing video understanding tasks, highlighting opportunities for advancing video understanding with detailed summaries.

BadJudge: Backdoor Vulnerabilities of LLM-As-A-Judge
This paper introduces a novel backdoor attack on the LLM-as-a-Judge evaluation system, where an adversary can manipulate both the candidate and evaluator models to unfairly inflate the adversary's scores, even with minimal training data poisoning. The study categorizes attack scenarios based on data access levels and demonstrates that model merging can counteract these backdoor threats by effectively reducing attack success rates to near zero without compromising performance, offering a sound solution amidst ethical and technological challenges.

Sail into the Headwind: Alignment via Robust Rewards and Dynamic Labels against Reward Hacking
The paper addresses the issue of reward hacking in offline preference optimization by identifying two types: Type I from subpar choices seeming favorable and Type II from decent choices appearing undesirable. To combat these issues, the authors propose the POWER method with dynamic labels, which outperforms current methods in alignment benchmarks and has strong theoretical and empirical support for mitigating reward hacking.

Text4Seg: Reimagining Image Segmentation as Text Generation
This paper introduces Text4Seg, a novel approach that converts image segmentation into a text generation problem, utilizing semantic descriptors as textual representations of segmentation masks for seamless integration into Multimodal Large Language Models (MLLMs). By employing Row-wise Run-Length Encoding, Text4Seg achieves reduced descriptor length and accelerates inference, demonstrating state-of-the-art performance in various vision tasks by fine-tuning different MLLM backbones.

Weak-to-Strong Preference Optimization: Stealing Reward from Weak Aligned Model
This paper introduces Weak-to-Strong Preference Optimization (WSPO), a method that leverages the alignment behavior of weaker models to enhance the alignment of stronger language models (LMs). The approach shows significant improvements in model performance, as demonstrated by experiments, suggesting the feasibility of using weak models to boost the alignment capabilities of stronger models.

DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agent
This paper introduces DistRL, a novel framework that enhances the efficiency of online reinforcement learning fine-tuning for mobile device control agents by integrating centralized training with decentralized data acquisition. DistRL demonstrates a 3x improvement in training efficiency and a 20% enhancement in success rate over state-of-the-art methods, making it a robust solution for real-world device control tasks.

REvolve: Reward Evolution with Large Language Models using Human Feedback
REvolve is a novel framework that uses large language models (LLMs) and human feedback to design reward functions for reinforcement learning (RL) tasks where notions of "good" behavior are implicit and hard to quantify. By leveraging LLMs to translate human implicit knowledge into explicit reward functions, REvolve improves the training performance of RL agents in complex domains such as autonomous driving, humanoid locomotion, and dexterous manipulation, outperforming state-of-the-art baselines.

BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games
Large Language Models (LLMs) and Vision Language Models (VLMs) are challenged by complex, dynamic environments that require advanced reasoning and planning. To evaluate these capacities, we introduce BALROG, a benchmark of diverse and difficult games, highlighting shortcomings in vision-based decision-making and offering an open platform to support future advancements in the field.

FlexCAD: Unified and Versatile Controllable CAD Generation with Fine-tuned Large Language Models
FlexCAD is introduced as a unified model that fine-tunes large language models (LLMs) for controllable CAD generation across all construction hierarchies by representing CAD models as structured text and employing a hierarchy-aware masking strategy. This approach addresses the limitations of existing methods by enhancing generation quality and controllability, as demonstrated through comprehensive experiments on public datasets.

ToolDial: Multi-turn Dialogue Generation Method for Tool-Augmented Language Models
Tool-Augmented Language Models (TALMs) often fail to handle real-world scenarios requiring proactive API usage and clarifying user inquiries; to address this, we present ToolDial, a dataset with 11,111 dialogues featuring rich user-system interactions and a method for generating an API graph for improved compatibility understanding. Evaluating existing language models on ToolDial reveals they perform with less than 70% accuracy, highlighting significant gaps in their ability to predict actions and interpret input parameters from dialogues.

Draw-and-Understand: Leveraging Visual Prompts to Enable MLLMs to Comprehend What You Want
This paper introduces the Draw-and-Understand framework, which integrates visual prompting understanding into Multimodal Large Language Models (MLLMs) to enhance interactivity and image comprehension. By proposing a general architecture and introducing the MDVP-Instruct-Data dataset, the authors demonstrate improved multimodal interaction and pixel-level understanding in models, showcasing the framework's effectiveness across various pre-trained MLLMs.

Can LLMs Really Learn to Translate a Low-Resource Language from One Grammar Book?
This study examines the effectiveness of using grammar books to improve machine translation of extremely low-resource (XLR) languages, like Kalamang, Nepali, and Guarani, by long-context language models. The research finds that parallel examples within grammar books, rather than grammatical explanations, significantly enhance translation performance, emphasizing the value of task-specific data collection focused on parallel corpora for multilingual XLR translation.

CodePlan: Unlocking Reasoning Potential in Large Language Models by Scaling Code-form Planning
CodePlan is a novel paradigm designed to enhance the planning ability of large language models (LLMs) by enabling them to generate and execute code-form plans, which are pseudocode representations of high-level reasoning processes. By training on a large-scale dataset that integrates these code-form plans, CodePlan achieves a 25.1% improvement in performance across various multi-step reasoning benchmarks and demonstrates significant data efficiency and robust generalization across diverse complex reasoning tasks.

Distributed Speculative Inference (DSI): Speculation Parallelism for Provably Faster Lossless Language Model Inference
This paper presents distributed speculative inference (DSI), a new inference algorithm that outperforms speculative inference (SI) and standard autoregressive inference by leveraging a novel speculation parallelism for faster processing without modifying language models. DSI addresses the limitations of SI when drafters are slow or inaccurate, achieving a 1.29-1.92 times speed increase in simulations and is available as open-source software.

HaDeMiF: Hallucination Detection and Mitigation in Large Language Models
The knowledge hallucinations in large language models (LLMs) raise concerns about their security and reliability, with current detection methods lacking effective calibration. The proposed framework, HaDeMiF, uses a Deep Dynamic Decision Tree and a Multilayer Perceptron to detect and mitigate hallucinations during both inference and fine-tuning, enhancing model calibration while adding minimal parameters.

Seeing Eye to AI: Human Alignment via Gaze-Based Response Rewards for Large Language Models
This paper introduces GazeReward, a novel framework that incorporates eye-tracking data as implicit feedback to enhance the Reward Model in aligning Large Language Models with human preferences. Through ablation studies, the authors demonstrate that integrating eye-tracking features improves the model's accuracy on human preference datasets, highlighting the potential of cognitive data to optimize AI alignment with human values in NLP research.

Towards a Theoretical Understanding of Synthetic Data in LLM Post-Training: A Reverse-Bottleneck Perspective
This paper addresses the gap between the practical effects of synthetic data and its theoretical understanding in post-training tasks for large language models. By modeling the synthetic data generation process and introducing the concept of Generalization Gain via Mutual Information (GGMI), the authors provide a theoretical foundation that links information gain from synthetic data with the generalization capability of post-trained models, thereby optimizing synthetic data generation techniques.

Toward Understanding In-context vs. In-weight Learning
This paper provides a theoretical framework to explain the emergence and eventual decline of in-context learning in transformers, driven by specific distributional properties in training data. By analyzing a simplified model and validating results with experiments on full transformers and language models, the study elucidates conditions that govern the balance of in-context and in-weight learning, highlighting parallel behaviors upon fine-tuning with natural language prompts.

Visual-O1: Understanding Ambiguous Instructions via Multi-modal Multi-turn Chain-of-thoughts Reasoning
This paper introduces Visual-O1, a multi-modal multi-turn chain-of-thought reasoning framework designed to improve the interpretation of ambiguous language instructions by integrating visual context and common sense. The framework enhances model performance on ambiguous tasks without significantly increasing computational demands and demonstrates broader applicability to models of varying intelligence levels, illustrating AI's potential to handle uncertainty and ambiguity similarly to humans.

### Deep Learning->Other Representation Learning
Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations
This paper investigates vision-language models (VLMs) to tackle hallucinations by projecting their internal image representations to the language vocabulary, leading to more confident probabilities on real objects. The authors introduce a knowledge erasure algorithm that reduces hallucinations by orthogonalizing image and hallucinated object features, decreasing hallucinations by up to 25.7% on the COCO2014 dataset while maintaining performance, and enabling novel capabilities such as zero-shot segmentation.

Isometric Regularization for Manifolds of Functional Data
Implicit Neural Representations (INRs) use neural networks to represent data as continuous functions, but their infinite-dimensional nature makes them prone to overfitting, requiring effective regularization. This paper addresses these challenges by parametrizing INRs as a Riemannian manifold to preserve geometric properties, achieving robust data representations and high-quality data fitting even in small or noisy datasets.

UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting
UniGS introduces 3D Gaussian Splatting into multi-modal pre-training to improve 3D representation by modeling the 3D world with 3D Gaussians, enhancing alignment with 2D images and establishing a shared visual and textual space. Experiments demonstrate that UniGS significantly outperforms previous methods across various 3D tasks, achieving higher accuracy in zero-shot classification, text-driven retrieval, and open-world understanding.

Medium-Difficulty Samples Constitute Smoothed Decision Boundary for Knowledge Distillation on Pruned Datasets
This paper introduces a novel approach to dataset pruning for Knowledge Distillation by focusing on preserving decision boundaries and reducing drifts, especially when using student networks with limited capacity. By selecting medium-difficulty samples, the method achieves better feature distribution preservation and decision boundary accuracy, improving training times and student network performance compared to existing dynamic pruning methods, with notable enhancements in accuracy and efficiency on the ImageNet dataset.

A Riemannian Framework for Learning Reduced-order Lagrangian Dynamics
This paper introduces a novel geometric network architecture that incorporates physical consistency to improve deep neural networks' ability to generalize and learn dynamic models efficiently. By leveraging model-order reduction and a Riemannian perspective, the approach enables accurate long-term predictions and data-efficient learning of high-dimensional dynamics by inferring interpretable and physically-plausible reduced Lagrangian models.

Formation of Representations in Neural Networks
The paper introduces the Canonical Representation Hypothesis (CRH), positing six alignment relations that are essential for the formation of neural representations in network layers, suggesting that neurons and weights become invariant to task-irrelevant transformations. It also proposes the Polynomial Alignment Hypothesis (PAH) and provides a theoretical framework explaining that the balance of gradient noise and regularization is crucial for forming these canonical representations, potentially unifying key deep learning concepts like neural collapse.

### Deep Learning->Robustness
Provably Reliable Conformal Prediction Sets in the Presence of Data Poisoning
Conformal prediction provides uncertainty quantification via prediction sets but is vulnerable to data poisoning attacks, which can compromise these sets. We propose reliable prediction sets (RPS) with provable reliability under poisoning by utilizing smoothed score functions for training data and aggregating multiple calibration sets, validated through image classification tasks to enhance trustworthiness in uncertainty quantification.

$\sigma$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial Examples
This paper introduces a novel $\ell_0$-norm attack method, $\sigma$-zero, which uses a differentiable approximation and an adaptive projection operator to improve gradient-based optimization for adversarial robustness evaluations. Extensive testing on various datasets demonstrates that $\sigma$-zero effectively generates minimal $\ell_0$-norm adversarial examples, outperforming existing sparse attacks in success rate, perturbation size, and computational efficiency without the need for hyperparameter tuning.

Persistent Pre-training Poisoning of LLMs
This study examines whether large language models can be compromised during pre-training through data poisoning, with the persistence of such attacks assessed after models are fine-tuned. The findings indicate that poisoning as little as 0.1% of the pre-training dataset can cause three out of four attack types, including denial-of-service, to persist, even after post-training, highlighting significant vulnerabilities in the model training pipeline.

Towards Certification of Uncertainty Calibration under Adversarial Attacks
This paper addresses the vulnerability of neural classifiers' calibration to adversarial perturbations by introducing methods for certified calibration, offering worst-case bounds on calibration metrics like the Brier score and expected calibration error. Additionally, the authors propose novel calibration attacks to enhance model calibration through adversarial calibration training, with plans to release the code for these methods.

### Deep Learning->Theory
Sharpness-Aware Minimization Efficiently Selects Flatter Minima Late In Training
Sharpness-Aware Minimization (SAM) has been shown to efficiently select flatter minima late in training, significantly enhancing the generalization of neural networks compared to Stochastic Gradient Descent (SGD). The study uncovers that even brief application of SAM at the end of training achieves similar generalization and sharpness as full SAM training, revealing that the late-phase optimization method is crucial, extending these insights to Adversarial Training.

A primer on analytical learning dynamics of nonlinear neural networks
This paper reviews approaches to analyzing the learning dynamics of nonlinear neural networks using the teacher-student setting, where an explicit analytical expression for generalization error can be derived. It provides a mathematical formulation and `JAX` codebase for simulating these dynamics, offering insights into how this paradigm aids in understanding generalization in neural networks.

Divergence of Neural Tangent Kernel in Classification Problems
This paper examines the convergence of the Neural Tangent Kernel (NTK) in classification problems, particularly within multi-layer fully connected neural networks and residual neural networks. It reveals that due to the strictly positive definiteness of the NTK, the network parameters diverge during training with the cross-entropy loss, implying that NTK theory is not applicable in this context and suggesting broader implications for neural network studies in classification.

### Misc
Towards Interpreting Visual Information Processing in Vision-Language Models
This paper investigates the processing of visual tokens in the language model component of the Vision-Language Model LLaVA, focusing on object information localization, token representation evolution, and integration for predictions. The study reveals that removing object-specific tokens significantly decreases object identification accuracy, and interpretable visual token representations align with textual tokens, offering insights for developing more interpretable and controllable multimodal systems.

CLIPDrag: Combining Text-based and Drag-based Instructions for Image Editing
CLIPDrag is a novel image editing method that combines text and drag signals to enable precise and ambiguity-free manipulations on diffusion models. By integrating global text guidance with local drag information and introducing a fast point-tracking method for improved convergence, CLIPDrag outperforms existing text-based and drag-based editing methods.

Enhancing Cognition and Explainability of Multimodal Foundation Models with Self-Synthesized Data
This paper introduces a novel visual rejection sampling framework aimed at enhancing the cognition and explainability of Large Multimodal Models (LMMs) by using self-synthesized data. Through iterative fine-tuning and reward model-free filtering, the proposed method significantly improves the accuracy and interpretability of LMMs in specialized visual classification tasks by generating human-verifiable and domain-specific visual explanations.

dEBORA: Efficient Bilevel Optimization-based low-Rank Adaptation
The paper introduces a novel bilevel optimization strategy for parameter-efficient fine-tuning of large-scale neural networks by dynamically selecting the optimal rank for each layer using both matrix and tensor low-rank adapters. This approach leverages a stochastic away-step variant of the Frank-Wolfe algorithm, enhancing training efficiency and cost-effectiveness while ensuring identifiability of the optimal rank structure, as demonstrated through theoretical analysis and numerical experiments.

RecDreamer: Consistent Text-to-3D Generation via Uniform Score Distillation
RecDreamer addresses the Multi-Face Janus problem in text-to-3D generation by reshaping the data distribution to achieve more consistent pose representations, overcoming biases toward a canonical pose. By implementing uniform score distillation and introducing a training-free classifier for estimating pose categories, RecDreamer significantly improves the consistency of 3D assets across varying poses, as demonstrated by experimental results.

Revisiting Random Walks for Learning on Graphs
This paper introduces random walk neural networks (RWNNs), a machine learning model for graphs that utilizes random walks processed by deep neural networks for vertex-level or graph-level predictions. The study demonstrates that RWNNs can achieve isomorphism invariance and universal approximation probabilistically, alleviate over-smoothing issues inherent in message passing neural networks, and effectively apply language models for graph tasks such as distinguishing strongly regular graphs and performing transductive classification on the arXiv citation network.

Provable unlearning in topic modeling and downstream tasks
This paper presents the first theoretical guarantees for machine unlearning in the pre-training and fine-tuning paradigm, focusing on topic models for downstream tasks like retrieval and classification. The authors introduce an efficient unlearning algorithm with predictable computational overhead and demonstrate that it is easier to unlearn pre-training data from models fine-tuned for specific tasks without altering the base model.

PaLD: Detection of Text Partially Written by Large Language Models
This paper addresses the challenge of detecting mixed-text documents composed of both human and large language model (LLM)-written segments, aiming to overcome the limitations of current methods that classify entire texts as either human or LLM-generated. The authors propose the Partial-LLM Detector (PaLD), a novel approach that accurately estimates the percentage of LLM-generated content and identifies specific segments created by LLMs, demonstrating superior performance over existing baseline methods.

MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced with Reliable Evaluations
This paper examines the limitations of the MQuAKE dataset, revealing that up to 76% of its questions and ground truth labels are corrupted, thus challenging the evaluation of multi-hop knowledge editing methods in large language models. The authors provide a comprehensive correction, termed \mquaker{}, and demonstrate that a simple, minimally invasive approach can enhance editing performance without exploiting dataset-specific properties, offering a more reliable benchmark for future research.

Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation
ProverGen is a novel framework that combines Large Language Models (LLMs) with symbolic provers to generate ProverQA, a scalable and diverse first-order logic (FOL) reasoning dataset with logically coherent reasoning steps. The dataset highlights the challenges faced by state-of-the-art LLMs, while fine-tuning a model on a framework-generated training set shows consistent improvement, demonstrating the framework's effectiveness.

One for all and all for one: Efficient computation of partial Wasserstein distances on the line
The paper introduces PAWL, an innovative algorithm designed to compute exact Partial Wasserstein distances efficiently on the line, beneficial for cases with uncertain noise levels in machine learning tasks. Achieving \(O(n \log n)\) time complexity, PAWL offers significant computational efficiency and performance improvements over existing methods, especially in managing mass mismatches and outliers across large-scale datasets.

Diffusion-based Neural Network Weights Generation
D2NWG, a novel framework for neural network weight generation, leverages diffusion processes to create task-specific network weights, eliminating the need for extensive model storage and selection in transfer learning. This method achieves comparable or superior performance to conventional pretrained models, with notable improvements in few-shot learning and large language model tasks, thus offering a scalable solution to traditional transfer learning challenges.

Understanding and Enhancing Safety Mechanisms of LLMs via Safety-Specific Neuron
Safety alignment for large language models (LLMs) is crucial, and this study introduces a neuron detection method to identify and tune safety-specific neurons. The proposed methods, SN-Tune and RSN-Tune, effectively enhance safety measures in LLMs without sacrificing their general capabilities, dramatically reducing harmful output scores in several tested models.

Rethinking Neural Multi-Objective Combinatorial Optimization via Neat Weight Embedding
Recent decomposition-based neural MOCO methods have notable optimality gaps in weight-specific subproblems, prompting us to propose a weight embedding method to learn weight-specific representations effectively. Our approach, tested through a succinct addition model and an enhanced conditional attention model, surpasses existing methods and exhibits strong generalization across various problem sizes, achieving state-of-the-art performance in classic MOCO problems.

Student-Informed Teacher Training
This paper addresses the challenge of privileged imitation learning, where a teacher with access to additional information trains a student with limited observations, resulting in potential mismatches in behavior due to partial observability. The authors propose a joint training framework for teacher and student policies, incorporating a penalty for action differences and a supervised alignment step, demonstrating its effectiveness in tasks like maze navigation, quadrotor flight, and manipulation.

MoDGS: Dynamic Gaussian Splatting from Casually-captured Monocular Videos with Depth Priors
MoDGS is a newly proposed pipeline that enables the rendering of high-quality novel-view images in dynamic scenes using only casually captured monocular videos, overcoming limitations of previous methods that rely on rapid camera movement. By utilizing single-view depth estimation, a 3D-aware initialization method, and a robust depth loss, MoDGS significantly surpasses baseline methods in rendering dynamic scenes from videos with static or slow-moving cameras.

Gaussian Head & Shoulders: High Fidelity Neural Upper Body Avatars with Anchor Gaussian Guided Texture Warping
This paper introduces a method to create realistic and controllable upper body avatars from casual monocular videos by combining 3D Gaussian Splatting with a neural texture approach, overcoming previous limitations in rendering the clothed chest and shoulders. The proposed technique achieves high-fidelity reconstructions with improved details and rendering speeds, demonstrated on casual phone-captured and internet videos, and offers a rendering speed of about 130 FPS without the need for Multi-Layer Perceptron (MLP) queries.

Disentangling Representations through Multi-task Learning
This paper presents experimental and theoretical evidence for the emergence of disentangled representations in agents solving multi-task evidence accumulation classification tasks, fundamental in neuroscience. The findings demonstrate that achieving accurate multi-task classifications can implicitly generate disentangled representations of latent states, facilitating zero-shot out-of-distribution generalization and establishing a link between multi-task competence and interpretable world models in both artificial and biological systems.

Towards Calibrated Deep Clustering Network
This paper addresses the overconfidence issue in deep clustering by introducing a novel dual-head deep clustering model with a calibration head to adjust predictions and a clustering head for dynamic sample selection. The proposed model, supported by theoretical guarantees and an effective initialization strategy, significantly improves clustering accuracy and reduces the expected calibration error by five times compared to state-of-the-art methods.

DGQ: Distribution-Aware Group Quantization for Text-to-Image Diffusion Models
This paper addresses the computational and memory limitations of text-to-image diffusion models by proposing Distribution-aware Group Quantization (DGQ), which effectively manages activation outliers and applies prompt-specific logarithmic quantization to preserve image quality and text-image alignment. DGQ achieves substantial reductions in memory and computational demands while maintaining model performance in low-bit scenarios without needing further fine-tuning, as demonstrated on datasets like MS-COCO and PartiPrompts.

Mitigating Spurious Correlations in Zero-Shot Multimodal Models
This paper addresses the challenge of spurious correlations in Vision Language Models (VLMs) used for zero-shot classification by proposing a novel solution that employs a translation operation to maintain the latent space distribution. The method improves worst-group accuracy by guiding translation for image embeddings using spurious vectors derived from text prompts, as demonstrated by experiments on benchmark datasets and visualizations that confirm its efficacy.

ConMix: Contrastive Mixup at Representation Level for Long-tailed Deep Clustering
Deep clustering methods usually assume balanced distributions, which is inconsistent with naturally occurring long-tailed distributions, leading to performance drops. The proposed ConMix method addresses this by utilizing contrastive mixup in deep clustering without supervision, leading to more discriminative representations and outperforming state-of-the-art methods in imbalanced data scenarios.

Convex Formulations for Training Two-Layer ReLU Neural Networks
This paper addresses the challenge of training infinite-width two-layer ReLU networks by reformulating it as a convex completely positive program, despite its inherent NP-hardness due to the complete positivity constraint. To make the problem tractable, the authors propose a semidefinite relaxation that can be solved in polynomial time, showing competitive test accuracy in classification tasks, thus advancing the exploration of convex formulations in neural network training.

PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing
PostEdit is introduced to address challenges in image editing like controllability, background preservation, and efficiency by using a posterior scheme to manage the diffusion sampling process without the need for inversion methods. It significantly outperforms previous approaches, preserving unedited regions accurately and generating high-quality results in about 1.5 seconds using 18 GB of GPU memory, all while being inversion- and training-free.

Attribute-based Visual Reprogramming for Vision-Language Models
This paper introduces Attribute-based Visual Reprogramming (AttrVR) for enhancing CLIP by using descriptive and distinctive attributes to improve image classification. AttrVR dynamically refines patterns for each image, reducing intra-class variance and increasing inter-class separation, thereby achieving superior performance across various tasks compared to existing VR approaches.

Mitigating Modality Prior-Induced Hallucinations in Multimodal Large Language Models via Deciphering Attention Causality
This paper addresses the issue of biases in Multimodal Large Language Models (MLLMs), which can lead to multimodal hallucination, by proposing a causal inference framework called CausalMM. Using structural causal modeling to treat modality priors as a confounder, the method improves input-output alignment significantly, showing substantial performance gains on multiple benchmarks, and is easily integrable into existing systems.

SparsyFed: Sparse Adaptive Federated Learning
SparsyFed introduces a federated sparse training method that effectively addresses the challenges of data heterogeneity, adaptivity, and hyperparameter tuning in cross-device federated learning. By producing highly sparse models with minimal accuracy loss and improved adaptability across diverse data distributions, SparsyFed achieves superior performance while simplifying hyperparameter management and reducing computational overhead.

Do Egocentric Video-Language Models Truly Understand Hand-Object Interactions?
This paper addresses the limitations of current EgoVLMs in understanding hand-object interactions by introducing the EgoHOIBench benchmark, which highlights their performance issues when faced with modifications in interaction descriptions. The authors propose an asymmetric contrastive objective called EgoNCE++ that improves the models' ability to recognize actions and objects, enhancing performance in tasks like multi-instance retrieval and action recognition.

OmniRe: Omni Urban Scene Reconstruction
OmniRe is a comprehensive system that creates high-fidelity digital twins of dynamic real-world scenes by extending beyond vehicle modeling to include diverse dynamic objects like pedestrians and cyclists. Outperforming state-of-the-art methods, Omnire enables realistic simulations of human behavior and interactions in urban environments, as demonstrated on the Waymo dataset and additional driving datasets.

Erasing Concept Combination from Text-to-Image Diffusion Model
The paper addresses security concerns in text-to-image diffusion models by introducing the Concept Combination Erasing (CCE) problem, focusing on inappropriate themes arising from combinations of benign visual concepts. To solve CCE, the authors propose a Concept Graph-based Feature Decoupling framework (CoGFD), which effectively identifies and erases problematic visual concept combinations while preserving the integrity of individual concept generation, demonstrating superior performance over existing methods in various scenarios.

Spectral-Refiner: Accurate Fine-Tuning of Spatiotemporal Fourier Neural Operator for Turbulent Flows
This paper introduces a novel learning framework to reduce training costs and improve accuracy in operator-type neural networks for solving spatiotemporal PDEs, especially those like the Navier-Stokes Equations. By generalizing Fourier Neural Operators to Bochner space mappings and employing a new spatiotemporal spectral convolution layer with a convex fine-tuning process, the proposed method achieves enhanced computational efficiency and precision, outperforming traditional approaches in benchmarks. The source code is available at: https://github.com/scaomath/torch-cfd.

Combining Induction and Transduction for Abstract Reasoning
The paper investigates whether inferring a latent function or directly predicting new outputs is more effective when learning from few examples using neural models. By training models on variations of ARC tasks, it finds that inductive models excel at precise computations and concept composition, while transductive models perform better on perceptual tasks, and combining both approaches can achieve near human-level performance.

Breach By A Thousand Leaks: Unsafe Information Leakage in 'Safe' AI Responses
This paper introduces a new safety evaluation framework for language models, focusing on vulnerabilities to inferential threats posed by dual-intent queries, which current robustness-based defenses fail to address. The authors propose an information-theoretic threat model of inferential adversaries and demonstrate how a question-decomposition attack can extract impermissible information from models more effectively than traditional jailbreaks, highlighting the necessity of developing defenses that manage impermissible information leakage while navigating safety-utility trade-offs.

3D Vision-Language Gaussian Splatting
This paper introduces a 3D vision-language Gaussian splatting model designed to improve multi-modal 3D scene understanding by effectively balancing visual and language modalities. The method utilizes a novel cross-modal rasterizer and camera-view blending to enhance semantic rasterization and consistency, achieving superior performance in open-vocabulary semantic segmentation compared to existing approaches.

Proxy Denoising for Source-Free Domain Adaptation
Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained source model to an unlabeled target domain without access to source data, but challenges arise due to the noisy supervision from Vision-Language (ViL) models. The proposed Proxy Denoising (ProDe) approach addresses this by correcting ViL's predictions with a novel proxy confidence theory, significantly improving performance over existing methods across various SFDA settings.

FreDF: Learning to Forecast in the Frequency Domain
This paper introduces the Frequency-enhanced Direct Forecast (FreDF), a novel approach that addresses the bias in traditional Direct Forecast models caused by overlooked label correlations in time series data. By forecasting in the frequency domain, FreDF reduces estimation bias and significantly outperforms current state-of-the-art methods across various forecast models.

Time-MoE: Billion-Scale Time Series Foundation Models with Mixture of Experts
Time-MoE introduces a scalable architecture for pre-training larger time series forecasting models by using a sparse mixture-of-experts design to enhance computational efficiency and reduce inference costs. Pre-trained on the large-scale Time-300B dataset, Time-MoE achieves state-of-the-art forecasting precision with a model size of up to 2.4 billion parameters, outperforming dense models with similar computational budgets.

VideoGLUE: Video General Understanding Evaluation of Foundation Models
This paper evaluates the video understanding capabilities of foundation models (FMs) through a well-structured experimental protocol involving three key video analysis tasks and eight datasets, revealing that task-specialized models outperform FMs in video comprehension. Key findings highlight that video-native FMs excel in motion-rich video classification and multi-action recognition with minimal adaptations, while image-native FMs perform better with full end-to-end finetuning, emphasizing the importance of focusing research on video-focused FMs and the significance of task and adaptation methods.

HQGS: High-Quality Novel View Synthesis with Gaussian Splatting in Degraded Scenes
The paper introduces HQGS, a technique to enhance 3D Gaussian Splatting's performance in novel view synthesis, specifically under scenarios with degraded image quality such as motion blur, noise, and compression artifacts. By incorporating an edge-semantic fusion guidance module and a structural cosine similarity loss, the method effectively captures detailed edge information, resulting in improved rendering robustness and quality across various degraded scenes.

RobuRCDet: Enhancing Robustness of Radar-Camera Fusion in Bird's Eye View for 3D Object Detection
This paper presents RobuRCDet, a robust radar-camera 3D object detection model that addresses environmental and intrinsic disturbances, such as noise and poor weather conditions, affecting both radar and camera sensors. By introducing a 3D Gaussian Expansion module to correct radar inaccuracies and a weather-adaptive fusion module for dynamic feature integration, the proposed model demonstrates competitive performance across various conditions, as validated by experiments on the nuScenes benchmark.

Decoupling Layout from Glyph in Online Chinese Handwriting Generation
This paper addresses the challenge of generating online handwritten text in various styles by dividing text lines into layout and glyph components. The proposed method uses a text line layout generator and a diffusion-based stylized font synthesizer, demonstrating through experiments on the CASIA-OLHWDB dataset that it can produce structurally correct and indistinguishable imitation samples.

HyPoGen: Optimization-Biased Hypernetworks for Generalizable Policy Generation
HyPoGen is a novel optimization-biased hypernetwork designed to generate optimal policy parameters from task specifications without accessing training data. By modeling policy generation as an approximation of the optimization process and leveraging structural designs for improved generalization, HyPoGen outperforms state-of-the-art methods in generating effective policies for unseen tasks, as demonstrated by its superior performance on locomotion and manipulation benchmarks.

Reassessing EMNLP 2024’s Best Paper: Does Divergence-Based Calibration for MIAs Hold Up?
In their EMNLP 2024 Best Paper Award-winning work, the authors address key privacy concerns in machine learning through Membership Inference Attacks, proposing a new calibration method utilizing a divergence-based metric. Despite initial promising results with their benchmark **PatentMIA**, the paper is critically analyzed for its experimental and evaluation limitations.

Analysing The Spectral Biases in Generative Models
This paper examines the spectral biases in diffusion and GAN models that affect their ability to generate certain frequencies, making it easy to differentiate real images from synthetic ones. The study provides an analysis and explanation of the causes behind these biases in image synthesis models.

Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs
PrefEval is a benchmark developed to evaluate Large Language Models (LLMs) on their ability to infer, memorize, and adhere to user preferences in long-context conversations. Testing across various models revealed significant challenges in maintaining preference accuracy over time, though fine-tuning with PrefEval demonstrated improvements, offering a valuable resource for developing personalized conversational agents.

Filtered not Mixed: Filtering-Based Online Gating for Mixture of Large Language Models
The paper introduces MoE-F, a mechanism that combines multiple pre-trained expert Large Language Models (LLMs) for online time-series prediction by adaptively forecasting the optimal weighting of LLM predictions. Employing a novel time-adaptive filtering approach, the method demonstrates significant improvement in predictive performance, particularly a 17% absolute F1 measure enhancement in financial market movement predictions, with theoretical optimality guarantees and empirical validation.

IDA-VLM: Towards Movie Understanding via ID-Aware Large Vision-Language Model
This paper addresses the limitation of current Large Vision-Language Models (LVLMs) by proposing IDA-VLM, an ID-Aware model that allows for character identity recognition across multiple visual scenarios, crucial for understanding complex visual content like movies. The research introduces the MM-ID benchmark to evaluate LVLMs on instance identity memory and recognition, highlighting existing models' shortcomings and paving the way for AI systems capable of processing multi-identity visual inputs.

Knowledge Distillation with Multi-granularity Mixture of Priors for Image Super-Resolution
This paper introduces the Mixture of Priors Knowledge Distillation (MiPKD) framework for image super-resolution, which is adaptable across various architectures at both feature and block levels. By integrating teacher knowledge effectively through a Feature Prior Mixer and dynamically propagating reconstructed features with a Block Prior Mixer, MiPKD demonstrates significant improvements in student model performance as evidenced by extensive experiments.

Mixture of Attentions For Speculative Decoding
The paper addresses the challenge of deploying Large Language Models (LLMs) by introducing a novel architecture, Mixture of Attentions for Speculative Decoding (SD), which uses smaller models for efficient token proposal. This approach improves decoding speed and acceptance length in single-device scenarios and maintains accuracy in client-server settings, offering advantages over traditional SD methods and API calls to LLMs.

Failures to Find Transferable Image Jailbreaks Between Vision-Language Models
This paper investigates the transferability of gradient-based universal image "jailbreaks" across over 40 vision-language models (VLMs), revealing that such attacks are challenging to transfer between different VLMs. The study finds limited success in transfer only between highly similar VLMs or different checkpoints of the same model, suggesting VLMs may be more robust to these types of adversarial attacks compared to text-based and image classifier models.

OS-ATLAS: Foundation Action Model for Generalist GUI Agents
OS-Atlas is a foundational GUI action model developed to enhance GUI grounding and Out-Of-Distribution tasks, overcoming the limitations of existing open-source Vision-Language Models (VLMs) compared to their commercial counterparts. By offering the largest open-source cross-platform GUI grounding corpus and demonstrating significant performance improvements across various benchmarks, OS-Atlas provides a robust resource and methodology for practitioners and researchers to advance the field of GUI agents.

Efficient Reinforcement Learning with Large Language Model Priors
This paper proposes leveraging large language models (LLMs) as prior action distributions in reinforcement learning (RL) frameworks to improve efficiency in sequential decision-making tasks. By integrating LLMs through Bayesian inference methods, the study demonstrates that using LLM-based action priors significantly reduces exploration and optimization complexity, improving sample efficiency by over 90% in offline learning scenarios compared to traditional RL techniques.

Merging LoRAs like Playing LEGO: Pushing the Modularity of LoRA to Extremes Through Rank-Wise Clustering
Low-Rank Adaptation (LoRA) has been effective for fine-tuning large language models for different domains, but challenges arise with model merging due to parameter interference. This paper introduces the LoRA-LEGO framework, which utilizes Minimal Semantic Units (MSUs) to flexibly disassemble and reassemble LoRAs into new configurations, showing improved performance over existing methods by employing a rank-wise parameter clustering and a dual reweighting strategy.

Reasoning-Enhanced Healthcare Predictions with Knowledge Graph Community Retrieval
KARE is a novel framework that combines knowledge graph community-level retrieval with large language model reasoning to improve healthcare predictions by addressing the limitations of traditional retrieval-augmented generation methods. It enhances prediction accuracy and interpretability by integrating multi-source biomedical data and implementing a dense medical knowledge structuring approach, achieving significant performance improvements on MIMIC-III and MIMIC-IV datasets for mortality and readmission predictions.

PRDP: Progressively Refined Differentiable Physics
The paper introduces Progressively Refined Differentiable Physics (PRDP), a method to reduce computational costs in neural network training by starting with coarse physics models and refining them only as needed. This approach, validated on various learning scenarios, achieves full training accuracy with significant time savings, exemplified by a 62% reduction in training time for emulating the Navier-Stokes equations.

Towards Effective Evaluations and Comparisons for LLM Unlearning Methods
This paper addresses the challenges in evaluating machine unlearning methods for large language models, focusing on metric robustness and the trade-off between unlearning and retention. By proposing new evaluation metrics and a calibration method, the authors improve the assessment of unlearning methods, allowing for better benchmarking and enhancement of practical efficacy.

A Robust Method to Discover Causal or Anticausal Relation
This paper introduces a robust method to distinguish whether a data generative process is causal or anticausal, addressing issues with existing approaches in handling high-dimensional perceptual data and label errors. By leveraging an asymmetric property of causal relations and utilizing a noise injection approach, the proposed method effectively determines causal directions, demonstrated through theoretical analysis and empirical evidence across diverse datasets.

Investigating Pattern Neurons in Urban Time Series Forecasting
Urban time series models often struggle with accurately forecasting low-frequency events, like holidays and extreme weather, which can diminish their effectiveness in smart city management. This paper introduces a novel training method called Pattern Neuron guided Training (PN-Train) that identifies and enhances neurons responsible for these infrequent patterns, significantly improving forecasting accuracy without affecting high-frequency event performance.

Counterfactual Concept Bottleneck Models
The paper introduces CounterFactual Concept Bottleneck Models (CF-CBMs), which address classification tasks by efficiently answering "what," "how," and "why not" questions simultaneously without post-hoc analysis. CF-CBMs achieve similar accuracy to black-box models, provide simpler explanations, and generate interpretable counterfactuals, with joint training improving decision-making by relying on fewer concepts and enhancing the impact of concept interventions.

Black-Box Detection of Language Model Watermarks
This paper addresses the practical detectability of watermarking schemes for LLM-generated text by developing rigorous statistical tests for three popular watermarking scheme families in a black-box setting. The study reveals that current watermarking schemes are more detectable than previously assumed, as confirmed through experiments on various schemes, models, and real-world APIs.

Air Quality Prediction with Physics-Guided Dual Neural ODEs in Open Systems
Air-DualODE is a novel physics-guided approach that integrates dual branches of Neural ODEs to enhance air quality prediction by capturing spatiotemporal dependencies with both physical equations and data-driven methods. This method demonstrates state-of-the-art performance in predicting pollutant concentrations, offering a promising solution for real-world air quality challenges.

Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives
This paper explores the adversarial robustness of audio-visual models by introducing two novel attacks: a temporal invariance attack and a modality misalignment attack, highlighting their impact on model performance. Additionally, a new audio-visual adversarial training framework is proposed to enhance robustness and efficiency, showing significant improvements through experiments on the Kinetics-Sounds dataset.

InverseBench: Benchmarking Plug-and-Play Diffusion Priors for Inverse Problems in Physical Sciences
The paper introduces \textsc{InverseBench}, a framework that evaluates diffusion models on five scientific inverse problems, including optical tomography and medical imaging, to bridge the knowledge gap in the performance of plug-and-play diffusion priors (PnPDP) beyond natural image restoration. By benchmarking 14 algorithms against robust, domain-specific baselines, the study reveals critical insights into algorithmic strengths and weaknesses, providing open access to code, datasets, and pre-trained models for future research.

Surprising Effectiveness of pretraining Ternary  Language Model at Scale
This paper addresses the memory bottlenecks in Large Language Model (LLM) inference caused by the disparity between GPU computational power and memory capacity growth, by exploring Ternary Language Models (TriLMs) as an alternative to low-bit post-training quantized models. The research introduces the Spectra LLM suite and demonstrates that TriLMs surpass FloatLMs and QuantLMs in performance and scaling efficiency for models exceeding one billion parameters, highlighting their potential for creating more efficient LLMs.

Balancing Bias in Two-sided Markets for Fair Stable Matchings
This paper addresses the Balanced Stable Marriage (BSM) problem, introducing Isorropia, an efficient algorithm that finds the exact optimal solution for practical instances. By constructing candidate rotations and performing local searches, Isorropia significantly outperforms existing methods in time efficiency, offering a groundbreaking approach to solving this NP-hard problem.

Data Shapley in One Training Run
This paper introduces In-Run Data Shapley, a novel method for assessing data contribution without retraining models, specifically tailored for a particular model of interest. The approach efficiently calculates Shapley values with minimal runtime overhead, allowing for scalable data attribution during foundation model pretraining and providing insights into data contribution and its implications for copyright and data curation in generative AI.

How much of my dataset did you use? Quantitative Data Usage Inference in Machine Learning
This paper introduces Dataset Usage Cardinality Inference (DUCI), a fine-grained analysis technique designed to estimate the exact proportion of data used to train machine learning models. DUCI uses debiased membership guesses to match the optimal MLE approach's performance with a maximum error of less than 0.1, while dramatically reducing computational costs by up to 300 times, addressing limitations in previous binary-only data usage assessments.

NatureLM-audio: an Audio-Language Foundation Model for Bioacoustics
NatureLM-audio is the first audio-language foundation model specifically designed for bioacoustics, trained on text-audio pairs to address the scarcity of annotated data in the field. It successfully transfers learned representations from music and speech to bioacoustics, achieving state-of-the-art performance on several tasks, including zero-shot classification of unseen species, and the model, along with benchmark data and code, are openly released to advance bioacoustics research.

DICE: End-to-end Deformation Capture of Hand-Face Interactions from a Single Image
This paper introduces DICE, an end-to-end method for reconstructing 3D hand-face interactions with deformations from a single image, addressing limitations in generalization and optimization time of previous methods like Decaf. Utilizing a Transformer-based architecture and a weakly-supervised training approach, DICE achieves state-of-the-art performance in accuracy and physical plausibility on standard benchmarks and operates interactively at 20 fps, significantly faster than Decaf.

Recite, Reconstruct, Recollect: Memorization in LMs as a Multifaceted Phenomenon
This paper models memorization in language models as a complex phenomenon influenced by specific factors of each sample, proposing a taxonomy that categorizes memorization into recitation, reconstruction, and recollection. The study constructs a predictive model based on this taxonomy, revealing that different factors variably affect memorization likelihood across categories.

LLMs' Potential Influences on Our Democracy: Challenges and Opportunities
This paper explores the potential influence of large language models (LLMs) on political discourse and democracy, proposing future research in evaluating political leanings, understanding their impact, developing policy frameworks, and technical solutions to mitigate biases. The study emphasizes the importance of continued research to maximize benefits and minimize risks to democratic processes.

Probe before You Talk: Towards Black-box Defense against Backdoor Unalignment for Large Language Models
This paper introduces BEAT, a black-box defense mechanism designed to detect and deactivate backdoor unalignment attacks in large language models by identifying triggered inputs through changes in output distribution. BEAT effectively addresses the challenges posed by sample-dependent targets and black-box access limitations, verified through extensive experiments on various backdoor attacks and models, including GPT-3.5-turbo, while also offering preliminary defenses against jailbreak attacks.

ControlAR: Controllable Image Generation with Autoregressive Models
ControlAR introduces an efficient framework to integrate spatial controls into autoregressive (AR) image generation models, addressing the challenge of control-to-image generation. By employing a lightweight control encoder and conditional decoding, ControlAR significantly enhances control capabilities and efficiency, outperforming previous state-of-the-art models like ControlNet++ in generating images from diverse inputs.

DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life
This paper introduces DailyDilemmas, a dataset of 1,360 moral dilemmas from everyday life, used to analyze how Language Models (LLMs) choose actions based on human values. By evaluating these models against various theoretical frameworks, the study reveals differences in value alignment, such as preference for self-expression over survival, and examines the challenges end users face in steering model prioritization in complex moral decision-making.

Interactive Adjustment for Human Trajectory Prediction with Individual Feedback
This paper introduces an interactive adjustment network for human trajectory prediction that leverages individual feedback from prior predictions to improve future predictions. By using a novel displacement expectation loss to train the model, the research demonstrates the significant value of individual feedback and highlights the network's superior effectiveness through experiments on established benchmarks.

Concept Bottleneck Large Language Models
The Concept Bottleneck Large Language Model (CB-LLM) introduces a new approach to inherently interpretable Large Language Models, enhancing both text classification and generation by narrowing the performance gap with traditional models while ensuring transparency and human control. This work sets a new standard for Large Language Models by enabling built-in interpretability and offering clear explanations, with their code accessible at https://github.com/Trustworthy-ML-Lab/CB-LLMs.

Fugatto 1: Foundational Generative Audio Transformer Opus 1
Fugatto is an advanced audio synthesis model that interprets free-form text instructions and audio inputs to generate and transform audio, addressing the traditional limitations of models trained only on audio data. By introducing ComposableART, a technique for inference-time compositional guidance, Fugatto successfully achieves versatile and customizable audio outputs, demonstrating competitive performance and unlocking new creative possibilities in audio generation tasks.

UniMatch: Universal Matching from Atom to Task for Few-Shot Drug Discovery
The paper introduces Universal Matching Networks (UniMatch), a dual matching framework for drug discovery that integrates explicit hierarchical molecular matching with implicit task-level matching via meta-learning. UniMatch effectively captures multi-level structural features for precise molecular representation and demonstrates superior performance and generalization ability, outperforming state-of-the-art methods on key benchmarks in few-shot molecular learning scenarios.

Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample Optimization
The paper introduces Pairwise Sample Optimization (PSO), an algorithm for fine-tuning timestep-distilled diffusion models without degrading image quality by increasing the relative likelihood margin between training and reference images. PSO effectively adapts these models for human-preferred generation, style transfer, and concept customization, while retaining their efficiency and flexibility in handling both offline and online-sampled data.

Reading Your Heart: Learning ECG Words and Sentences via Pre-training ECG Language Model
This paper introduces HeartLang, a self-supervised learning framework that treats ECG signals as language by conceptualizing heartbeats as words and rhythms as sentences. The framework, along with the QRS-Tokenizer and the largest heartbeat-based ECG vocabulary created to date, enhances the understanding and representation of ECG data, showing improved performance on multiple public ECG datasets.

LeanAgent: Lifelong Learning for Formal Theorem Proving
LeanAgent is a lifelong learning framework designed for formal theorem proving that continuously adapts to expanding mathematical knowledge without forgetting past information. By incorporating curriculum learning, a dynamic database, and progressive training, LeanAgent outperforms static LLM baselines by generating proofs for 155 theorems across 23 Lean repositories, effectively handling complex domains such as abstract algebra and algebraic topology while excelling in lifelong learning metrics like stability and backward transfer.

On the Crucial Role of Initialization for Matrix Factorization
This paper explores the impact of initialization on convergence rates in low-rank matrix factorization, introducing Nystrom initialization to enhance the global convergence of Scaled Gradient Descent (ScaledGD). The proposed method, NoRA, extends this initialization to low-rank adapters in model finetuning, showing improved performance across a range of tasks and model sizes.

FreqPrior: Improving Video Diffusion Models with Frequency Filtering Gaussian Noise
This paper introduces FreqPrior, a novel noise initialization strategy that enhances text-driven video generation by refining noise in the frequency domain, thereby addressing the variance decay issue that leads to loss of details and motion dynamics. The proposed method employs a unique filtering technique and a partial sampling process to maintain high-quality generation results while significantly reducing inference time, achieving the highest scores in quality and semantic assessments on VBench.

CViT: Continuous Vision Transformer for Operator Learning
The paper introduces the Continuous Vision Transformer (CViT), a novel neural operator architecture designed for learning complex physical systems by adapting computer vision advances. CViT exhibits state-of-the-art performance on various partial differential equation systems, surpassing larger models without extensive pretraining, highlighting its ability to handle discontinuous solutions and multi-scale features effectively.

BAMDP Shaping: a Unified Theoretical Framework for Intrinsic Motivation and Reward Shaping
This paper introduces a theoretical model that characterizes pseudo-rewards as reward shaping in Bayes-Adaptive Markov Decision Processes (BAMDPs) to predict and mitigate counterproductive behaviors in reinforcement learning (RL). By extending potential-based shaping theory, the study presents BAMDP Potential-based Shaping Functions (BAMPFs) that are resistant to reward-hacking, offering a practical solution to enhance RL agents' exploration without unintended side effects, as demonstrated in meta-RL and regular RL settings, including the Mountain Car environment.

Fast and Accurate Blind Flexible Docking
FABFlex is a fast and accurate multi-task learning model designed to improve molecular docking by addressing protein flexibility and unknown binding sites in blind docking scenarios. It integrates pocket prediction, ligand docking, and protein flexibility modeling into a unified framework, demonstrating superior accuracy and a significant speed advantage over existing methods.

Spherical Tree-Sliced Wasserstein Distance
This paper introduces Spherical Tree-Sliced Wasserstein (STSW) distance, an efficient metric for measuring similarities between distributions on a sphere by adapting tree systems in optimal transport problems using spherical trees and a novel spherical Radon transform. Theoretical analysis confirms the topological robustness and injectivity of this approach, while extensive experiments demonstrate its effectiveness compared to existing benchmarks in applications like gradient flows and self-supervised learning.

Unbounded: A Generative Infinite Game of Character Life Simulation
We introduce a generative infinite video game, Unbounded, which leverages recent advances in generative AI to create a dynamic character life simulation with emergent, open-ended mechanics driven by a specialized large language model (LLM). Our innovations include a distilled LLM for real-time game mechanics and narrative generation, and a dynamic regional IP-Adapter for visual consistency, both of which demonstrate improvements in simulation, instruction following, and coherence over traditional approaches.

Chunk-Distilled Language Modeling
The paper introduces Chunk-Distilled Language Modeling (CD-LM), which enhances text generation efficiency and adaptability in large language models by generating multi-token text chunks through a retrieval module. This approach improves language model performance and control without extra training, leveraging both internal model knowledge and expert-annotated data.

Gaussian Ensemble Belief Propagation for Efficient Inference in High-Dimensional, Black-box Systems
The Gaussian Ensemble Belief Propagation (GEnBP) algorithm effectively addresses the challenge of efficient inference in high-dimensional models by combining Ensemble Kalman Filter and Gaussian Belief Propagation techniques. It handles complex dependencies and noisy data through low-rank local message passing, demonstrating superior accuracy and computational efficiency in various applications like data assimilation and system identification, with supporting code available online.

Newton Meets Marchenko-Pastur: Massively Parallel Second-Order Optimization with Hessian Sketching and Debiasing
This paper addresses the challenge of minimizing a convex function in a massively parallel fashion within the "function as a service" (FaaS) paradigm, focusing on limited communication between workers. By employing an adaptive sketching scheme based on asymptotic random matrix theory, the proposed method allows workers to independently estimate the inverse Hessian, enabling the server to effectively approximate the Newton step and providing non-asymptotic guarantees and convergence assurances, even with noisy Hessians.

Revealing the 3D Cosmic Web through Gravitationally Constrained Neural Fields
This study addresses the challenge of reconstructing a 3D map of the universe's dark matter field from 2D telescope images affected by weak gravitational lensing. By utilizing a gravitationally-constrained neural field and an analysis-by-synthesis approach, the proposed method effectively overcomes the limitations of previous techniques, offering improved accuracy and the potential to reveal unexpected dark matter structures.

Planning in Natural Language Improves LLM Search for Code Generation
This paper introduces PlanSearch, a novel search algorithm designed to enhance the diversity of outputs in large language models by generating and analyzing diverse candidate plans for problem-solving in natural language. The empirical results demonstrate significant improvements across coding benchmarks, notably achieving a pass@200 of 77.0% on LiveCodeBench, and the study establishes a predictive relationship between solution diversity and performance gains in search algorithms.

Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes
This paper introduces a causal discovery algorithm using conditional independence constraints to infer the structural dependencies in stochastic dynamical systems modeled by stochastic differential equations. The algorithm, which effectively handles both fully and partially observed data, is enhanced with a novel signature kernel-based CI test, demonstrating superior performance compared to existing methods in various benchmarking scenarios.

Instant Policy: In-Context Imitation Learning via Graph Diffusion
Instant Policy is introduced as a method for In-Context Imitation Learning (ICIL) in robotics, enabling robots to learn new tasks rapidly from one or two demonstrations by modeling ICIL as a graph generation problem with learned diffusion processes and using pseudo-demonstrations as training data. The approach allows for fast learning of everyday tasks and supports cross-embodiment and zero-shot transfer to language-defined tasks, highlighting its versatility and innovation in robotic learning.

Fast and Slow Streams for Online Time Series Forecasting Without Information Leakage
This paper addresses challenges in online time series forecasting (OTSF) related to information leakage and practical prediction by proposing a dual-stream forecasting framework (DSOF). The novel approach uses a slow stream for complete data updates and a fast stream for recent data adaptation, improving forecasting performance through a teacher-student model and residual learning strategy, with demonstrated effectiveness in dynamic environments.

Better Instruction-Following Through Minimum Bayes Risk
This paper investigates the use of Minimum Bayes Risk (MBR) decoding with LLM judges to enhance the test-time performance of instruction-following language models. It demonstrates that MBR decoding substantially improves output quality and proposes iterative self-training to maintain these benefits while reducing test-time costs, achieving comparable or superior performance with less computational overhead.

DEPT: Decoupled Embeddings for Pre-training Language Models
The paper introduces DEPT, a communication-efficient pre-training framework that decouples embeddings from the transformer body to effectively handle data heterogeneity without needing a shared vocabulary. DEPT significantly reduces communication costs and embedding memory usage while improving model generalization and performance, as demonstrated in federated pre-training of billion-scale models.

Robust LLM safeguarding via refusal feature adversarial training
This paper identifies a universal mechanism in adversarial attacks on large language models (LLMs), which involves manipulating the refusal feature in embedding space to bypass safeguards. To counter this, the authors propose Refusal Feature Adversarial Training (ReFAT), a novel and efficient algorithm that enhances LLM robustness against adversarial attacks by simulating input-level attack effects, achieving significant improvements with reduced computational cost.

Spectral Compressive Imaging via Unmixing-driven Subspace Diffusion Refinement
The paper introduces a novel Predict-and-unmixing-driven-Subspace-Refine framework (PSR-SCI) for Spectral Compressive Imaging (SCI) reconstruction, addressing challenges in recovering high-frequency details with limited MSI data. By decomposing multispectral images into subspace images and applying pre-trained RGB diffusion models for refinement, PSR-SCI achieves enhanced visual quality and competitive PSNR and SSIM metrics compared to existing techniques, offering a robust alternative to traditional SCI reconstruction methods.

HiLo: A Learning Framework for Generalized Category Discovery Robust to Domain Shifts
This paper introduces a novel approach to Generalized Category Discovery (GCD) that addresses the challenge of unlabelled data from different domains, using 'HiLo' networks to extract both high-level semantic and low-level domain features. Through independent clustering of these features and enhancements like specialized domain augmentation and curriculum learning, the method significantly outperforms state-of-the-art models on benchmark and large-scale datasets.

TeaserGen: Generating Teasers for Long Documentaries
Teasers are essential for promoting content, but creating them for long videos is challenging due to the need for multimodal modeling, audiovisual alignment, and factual accuracy. This paper introduces DocumentaryNet, a dataset of documentaries and their teasers, and proposes TeaserGen, a two-stage system using large language and language-vision models to effectively generate and match narrations with visuals, demonstrating that a pretraining-based approach outperforms deep autoregressive models in this task.

OpenHands: An Open Platform for AI Software Developers as Generalist Agents
The paper introduces OpenHands, a platform designed for developing AI agents capable of interacting with the world by writing code, using command lines, and browsing the web, similar to human developers. OpenHands supports the implementation of new agents by using large language models, offers sandboxed environments for safe code execution, and includes evaluation benchmarks, demonstrating significant contributions from a large community of developers.

Grounding by Trying: LLMs with Reinforcement Learning-Enhanced Retrieval
The paper introduces Learning to Retrieve by Trying (LeReT), a reinforcement learning framework designed to help large language models (LLMs) improve the quality of their search queries for better information retrieval. By employing preference-based optimization, LeReT demonstrates significant improvements in retrieval accuracy and downstream generator evaluations, offering a versatile method applicable to various LLM pipelines.

RefactorBench: Evaluating Stateful Reasoning in Language Agents Through Code
RefactorBench is introduced as a benchmark comprising 100 complex multi-file refactoring tasks from open-source repositories to assess the capabilities and limitations of language model (LM) agents in handling dependencies and instructions. The study reveals that LM agents underperform compared to human developers in solving these tasks, but by adapting agents to be state-aware, a significant improvement in task-solving efficacy is achieved, offering insights for future research in enhancing autonomous systems in digital environments.

Min-K%++: Improved Baseline for Pre-Training Data Detection from Large Language Models
This paper introduces Min-K%++, a novel and theoretically grounded method for pre-training data detection in large language models (LLMs), addressing shortcomings in existing heuristic-based approaches. Demonstrating superior performance, Min-K%++ outperforms current methods by translating the detection problem into identifying local maxima, achieving state-of-the-art results across multiple benchmarks, including significant improvements on the WikiMIA and MIMIR benchmarks.

Robustness Reprogramming for Representation Learning
This paper addresses the challenge of reprogramming a well-trained deep learning model to enhance its robustness against adversarial or noisy input perturbations without altering its parameters. By introducing a novel non-linear robust pattern matching technique and three model reprogramming paradigms, the study demonstrates enhanced adversarial defenses and offers new insights for designing resilient AI systems, with comprehensive experiments validating its effectiveness across various models.

CameraCtrl: Enabling Camera Control for Video Diffusion Models
\method addresses the challenge of incorporating precise camera pose control into video diffusion models, crucial for enhancing narrative expression in video generation. By developing a plug-and-play camera pose control module and utilizing diverse training datasets, \method achieves improved controllability and generalization in video storytelling, demonstrating its effectiveness across different video generation models.

Language Representations Can be What Recommenders Need: Findings and Potentials
This paper challenges the current understanding that language models and traditional recommenders learn distinct representation spaces, demonstrating that item representations mapped from language models yield superior recommendation performance. By using language representations, the study develops a collaborative filtering model outperforming traditional ID-based CF models, revealing homomorphic connections between language and behavior modeling, and providing insights for both natural language processing and recommender systems.

CityGaussianV2: Efficient and Geometrically Accurate Reconstruction for Large-Scale Scenes
This paper introduces CityGaussianV2, enhancing large-scale scene reconstruction by improving geometric accuracy and efficiency through a novel decomposed-gradient-based densification and depth regression technique. The approach significantly reduces blurry artifacts, accelerates convergence, and optimizes resource usage, achieving substantial storage compression and training time reduction while maintaining high visual and geometric quality.

Federated Domain Generalization with Data-free On-server Matching Gradient
The paper proposes Federated Learning via On-server Matching Gradient (FedOMG), a novel approach for Federated Domain Generalization that efficiently leverages domain information from distributed domains by utilizing local gradients to maximize gradient inner product, finding an invariant gradient direction across all domains. FedOMG not only aggregates distributed model characteristics on a centralized server without extra communication cost but also enhances performance when integrated with existing methods, demonstrating superior robustness and outperforming state-of-the-art baselines across multiple FL and FDG benchmark datasets.

A Decade's Battle on Dataset Bias: Are We There Yet?
This paper revisits the dataset classification experiment proposed by Torralba & Efros, demonstrating that modern neural networks achieve high accuracy in identifying the dataset provenance of an image, such as 84.7% accuracy across YFCC, CC, and DataComp datasets. The findings reveal that the classifiers learn semantic features that are generalizable and not purely based on memorization, prompting a reevaluation of dataset bias within the research community.

Deconstructing Denoising Diffusion Models for Self-Supervised Learning
This study investigates the representation learning capabilities of Denoising Diffusion Models (DDM), traditionally used for image generation, by deconstructing them into classical Denoising Autoencoders (DAE). The findings reveal that only a few modern components of DDMs are vital for effective self-supervised representation learning, suggesting a simplified approach akin to traditional DAEs, potentially reviving interest in classical methods within contemporary self-supervised learning.

Building, Reusing, and Generalizing Abstract Representations from Concrete Sequences
The paper introduces a non-parametric hierarchical variable learning model (HVM) that excels in learning and abstracting contextually similar chunks from sequences, resulting in efficient memory organization and compact sequence representations. HVM demonstrates superior performance over standard compression algorithms and large language models in terms of abstraction and transfer tasks, offering insights into human-like cognition and the trade-off between compression and generalization.

An Effective Manifold-based Optimization Method for Distributionally Robust Classification
This paper addresses the challenge of enhancing the robustness of deep learning models by introducing a manifold-based Distributionally Robust Optimization (DRO) method that considers the geometric structure of training data. By integrating contrastive learning with Jacobian regularization and approximating geodesic distances, the method improves model reliability within uncertainty sets, offering theoretical robustness guarantees and demonstrating superior accuracy and robustness in experiments on benchmark datasets.

Generative Representational Instruction Tuning
The paper introduces Generative Representational Instruction Tuning (GRIT), a method enabling a large language model to proficiently handle both generative and embedding tasks using distinct instructions. Their model, GritLM-7B, excels on benchmarks and outperforms others in its size category, with GritLM-8x7B further enhancing generative capabilities while maintaining high embedding performance, notably improving Retrieval-Augmented Generation efficiency by over 60% without the need for separate models, and all resources are accessible online.

Dataset Distillation via Knowledge Distillation: Towards Efficient Self-Supervised Pre-training of Deep Networks
This paper presents the first dataset distillation (DD) method tailored for self-supervised learning (SSL), addressing the issue of high variance in SSL gradients by leveraging insights from knowledge distillation. The proposed method effectively generates synthetic datasets that pre-train high-quality encoders, resulting in up to a 13% increase in accuracy on various downstream tasks compared to existing methods.

Improving Convergence Guarantees of Random Subspace Second-order Algorithm for Nonconvex Optimization
This paper introduces the Random Subspace Homogenized Trust Region (RSHTR) method, which offers the best theoretical guarantees for nonconvex optimization within random subspace algorithms. RSHTR achieves efficient convergence to first-order and second-order stationary points and demonstrates superior performance in experiments on real-world datasets.

RelitLRM: Generative Relightable Radiance for Large Reconstruction Models
RelitLRM is a novel Large Reconstruction Model designed to generate high-quality Gaussian splatting representations of 3D objects with novel illumination from sparse images, overcoming the limitations of previous dense capture methods. Utilizing a transformer-based architecture, it efficiently decomposes geometry and appearance to produce competitive relighting results with increased speed compared to existing optimization-based techniques.

MLLM can see? Dynamic Correction Decoding for Hallucination Mitigation
This paper investigates the hallucination issues in Multimodal Large Language Models (MLLMs) and identifies the suppression of visual information by strong language model priors as a potential cause. To address this, the authors introduce DeCo, a dynamic correction decoding method that reduces hallucination rates by adaptively adjusting the integration of knowledge in model outputs, demonstrating significant improvements on standard benchmarks.

Logicbreaks: A Framework for Understanding Subversion of Rule-based Inference
This paper investigates the vulnerability of large language models (LLMs) to adversarial prompts that can subvert them from following designated rules formalized as inference in propositional Horn logic. By proving that even small transformers are susceptible to these attacks and aligning attention patterns with theoretical predictions, the authors present a novel logic-based framework as a foundation to study rule-based settings and enhance the understanding of tasks such as logical reasoning and jailbreak attacks.

Curriculum-aware Training for Discriminating Molecular Property Prediction Models
This paper addresses the challenge of activity cliffs in molecular property prediction models, where molecules with similar structures have vastly different properties, leading to inaccurate predictions. The authors propose a novel approach that reformulates prediction as a node classification problem, with new tasks at node and edge levels, demonstrating improved accuracy in learning outcomes for challenging molecules across diverse datasets.

Long-tailed Adversarial Training with Self-Distillation
This paper addresses the challenge of achieving adversarial robustness in long-tailed distributions, where a scarcity of tail data instances complicates performance. The authors propose a novel self-distillation technique using a balanced self-teacher model, achieving state-of-the-art performance and significant improvements in tail class accuracy against PGD attacks on datasets like CIFAR-10, CIFAR-100, and Tiny-ImageNet.

Certifying Language Model Robustness with Fuzzed Randomized Smoothing: An Efficient Defense Against Backdoor Attacks
The paper introduces Fuzzed Randomized Smoothing (FRS), a novel method for certifying the robustness of pre-trained language models against textual backdoor attacks without needing access to the poisoned training data. FRS combines software robustness certification with text randomization techniques, achieving improved defense efficiency, accuracy, and robustness compared to existing methods, as demonstrated through extensive experimentation.

YOLO-RD: Introducing Relevant and Compact Explicit Knowledge to YOLO by Retriever-Dictionary
The paper introduces a novel Retriever-Dictionary (RD) module that enhances YOLO-based models by allowing them to efficiently retrieve features from a dictionary of dataset insights built from Visual Models (VM), Large Language Models (LLM), or Visual Language Models (VLM). The RD module significantly improves model performance, achieving over a 3% increase in mean Average Precision for object detection with minimal additional parameters, and is effective across various models, including 1-stage, 2-stage, and DETR-based architectures.

VTDexManip: A Dataset and Benchmark for Visual-tactile Pretraining and Dexterous Manipulation with Reinforcement Learning
This paper introduces the first visual-tactile dataset for complex robotic manipulation skill learning and presents a novel benchmark featuring six dexterous manipulation tasks using vision-tactile modalities. Key findings reveal that integrating tactile information with vision significantly improves task success rates and adaptability, enhancing robustness and deployment in practical settings.

SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference Acceleration
Speculative decoding, a method to speed up LLM inference, traditionally requires extra parameters or extensive training, limiting its versatility across different models and tasks. Addressing this challenge, the SWIFT algorithm introduces a plug-and-play solution utilizing layer-skipping, enabling significant speedups without additional models or training, thus maintaining the quality and distribution of generated text.

HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing
This study presents HQ-Edit, a high-quality dataset for instruction-based image editing, featuring around 200,000 edits, created using a scalable data collection pipeline with advanced models like GPT-4V and DALL-E 3. The dataset, complete with high-resolution images and detailed prompts, enhances image editing models' capabilities, with HQ-Edit finetuned InstructPix2Pix achieving state-of-the-art performance, exceeding models trained on human-annotated data.

PnP-Flow: Plug-and-Play Image Restoration with Flow Matching
This paper introduces Plug-and-Play (PnP) Flow Matching, an algorithm that combines PnP methods with Flow Matching to address imaging inverse problems more effectively. The proposed approach offers computational efficiency and superior performance in tasks such as denoising, super-resolution, deblurring, and inpainting, surpassing existing PnP and Flow Matching methods.

InstantSplamp: Fast and Generalizable Stenography Framework for Generative Gaussian Splatting
InstantSplamp is a novel framework that integrates 3D steganography into large generative models, allowing for efficient embedding of watermarks into 3D assets without added time costs. By leveraging visual foundation models, this approach maintains high rendering quality and hiding fidelity, significantly reducing watermarking overhead and enabling scalable deployment in generating large collections of 3D objects.

DriveTransformer: Unified Transformer for Scalable End-to-End Autonomous Driving
DriveTransformer introduces a novel end-to-end autonomous driving framework that simplifies system scalability by integrating task parallelism, sparse representation, and streaming processing to improve training stability and performance. This approach significantly reduces the complexity associated with traditional methods and achieves state-of-the-art results on both simulated and real-world benchmarks.

The Labyrinth of Links: Navigating the Associative Maze of Multi-modal LLMs
This paper introduces a benchmark to evaluate the association capabilities of Multi-modal Large Language Models (MLLMs), focusing on their ability to connect observation with prior practice, a fundamental aspect of human intelligence. By developing an annotation-free method for creating association tasks and conducting a thorough investigation involving various models and memory strategies, the study reveals that existing MLLMs, including state-of-the-art models like GPT-4V(vision), struggle with association tasks, highlighting a significant performance gap compared to human experts.

Image-level Memorization Detection via Inversion-based Inference Perturbation
This paper addresses the challenge of detecting if proprietary or personal images have been memorized by text-to-image diffusion models, even without associated prompts. By introducing the Inversion-based Inference Perturbation (IIP) framework, the study provides a novel approach that effectively identifies memorized images by using unconditional DDIM inversion and optimized prompt embeddings, showcasing state-of-the-art performance across multiple memorization scenarios.

Conditional Diffusion Models are Minimax-Optimal and Manifold-Adaptive for Conditional Distribution Estimation
This paper investigates conditional forward-backward diffusion models for conditional generative modeling, focusing on generating data given a covariate. By employing a distribution regression framework, the authors establish the minimax-optimal convergence rate under the total variation metric and show adaptability to low-dimensional manifold structures, with estimation error depending on the intrinsic dimensionalities of the data and covariate.

EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing
EditRoom is a new framework that enables comprehensive 3D scene layout editing through natural language commands, leveraging Large Language Models and a diffusion-based method for executing six types of edits without manual intervention. By augmenting existing datasets and introducing the EditRoom-DB, our approach achieves superior performance in language-guided scene editing, demonstrating enhanced accuracy and coherence compared to existing methods.

Hidden in the Noise: Two-Stage Robust Watermarking for Images
This paper introduces a distortion-free image watermarking method using a diffusion model's initial noise to enhance detection and robustness against forgery and removal attacks. By implementing a two-stage framework that augments initial noise with Fourier patterns, the approach efficiently identifies watermarks within a group, achieving state-of-the-art protection and resilience.

Neuroplastic Expansion in Deep Reinforcement Learning
This paper introduces *Neuroplastic Expansion* (NE), a method inspired by cortical expansion to maintain and enhance plasticity in reinforcement learning by dynamically expanding the network. NE employs elastic neuron generation, dormant neuron pruning, and neuron consolidation strategies to outperform state-of-the-art methods, promoting adaptive learning in dynamic environments like MuJoCo and DeepMind Control Suite.

Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)
This paper examines the alignment between multimodal instruction-tuned language models (MLLMs) and brain activity, demonstrating that MLLMs show significantly better brain alignment than vision-only models and comparable performance to non-instruction-tuned models like CLIP. The study finds MLLMs effectively encode task-specific visual concepts related to scene instructions, suggesting that enhancing their task-specific information capture could improve precision in predicting brain responses.

Gap Preserving Distillation by Building Bidirectional Mappings with A Dynamic Teacher
The paper introduces a Gap Preserving Distillation (GPD) method, which includes a dynamic teacher model that evolves alongside the student, reducing the performance gap and improving training efficacy. By incorporating techniques like Inverse Reparameterization and Channel-Branch Reparameterization, GPD significantly enhances knowledge distillation performance on CNNs and transformers, offering notable improvements even in scenarios without a pre-trained teacher.

Promptriever: Instruction-Trained Retrievers Can Be Prompted Like Language Models
Promptriever is a pioneering retrieval model that can be prompted like a language model, showcasing state-of-the-art performance in tasks involving detailed relevance instructions and increased robustness to lexical variations. By training on a novel instruction set from MS MARCO, Promptriever sets the groundwork for integrating language model prompting methods with information retrieval, demonstrating enhanced performance and flexibility compared to traditional retrieval models.

Think while You Generate: Discrete Diffusion with Planned Denoising
Discrete Diffusion with Planned Denoising (DDPD) introduces a novel framework that improves reconstruction efficiency by using a planner to determine which positions to denoise based on their corruption level, outperforming traditional denoiser-only methods. This approach significantly reduces the performance gap between diffusion-based and autoregressive methods on benchmarks, enhancing results on datasets like text8, OpenWebText, and ImageNet 256×256.

Prompting Fairness: Integrating Causality to Debias Large Language Models
This paper introduces a causality-guided debiasing framework for reducing social biases in large language models (LLMs) by regulating causal pathways through principled prompting strategies. The framework successfully unifies and extends existing debiasing techniques, demonstrating its effectiveness in promoting fact-based reasoning and reducing biased decisions in high-stakes domains through experimental validation.

Deconstructing What Makes a Good Optimizer for Autoregressive Language Models
This paper compares several optimization algorithms, including SGD, Adafactor, Adam, Lion, and Sophia, in the context of autoregressive language modeling and finds that all but SGD perform comparably. The study highlights that practical considerations like memory constraints and ease of implementation should guide the choice of optimizer since no single algorithm stands out, and introduces simplified versions of Adam that maintain its performance and stability.

How Low Can You Go? Searching for the Intrinsic Dimensionality of Complex Networks using Metric Node Embeddings
This paper presents a novel approach for achieving lower-dimensional embeddings of networks using Euclidean metric embeddings instead of traditional vector-based Logistic PCA (LPCA) embeddings. The authors introduce an efficient logarithmic search method that allows for the exact determination of embedding dimensions, demonstrating that even large-scale networks can be accurately represented in surprisingly low-dimensional spaces, enhancing tasks like node classification and community detection.

HD-Painter: High-Resolution and Prompt-Faithful Text-Guided Image Inpainting with Diffusion Models
HD-Painter is a training-free approach to text-guided image inpainting that enhances prompt alignment through the Prompt-Aware Introverted Attention (PAIntA) layer and improves coherence with the Reweighting Attention Score Guidance (RASG) mechanism. Experiments indicate that HD-Painter outperforms current state-of-the-art models in both quantitative and qualitative assessments, with code available for public access.

InstantPortrait: One-Step Portrait Editing via Diffusion Multi-Objective Distillation
The paper introduces Instant-Portrait Network (IPNet), the first one-step diffusion-based model for real-time portrait image editing, addressing challenges of identity preservation, fidelity to editing instructions, and fast inference. Through a two-stage training process involving an Identity Enhancement Network (IDE-Net) and Diffusion Multi-Objective Distillation, IPNet significantly outperforms previous models in terms of identity consistency, instruction precision, and inference speed.

Differentiable Causal Discovery for Latent Hierarchical Causal Models
This paper introduces a novel differentiable causal discovery algorithm designed to identify non-linear latent hierarchical causal models without the restrictive assumptions of linearity or invertibility, which often limit the applicability of existing methods. The proposed algorithm outperforms current approaches in accuracy and scalability, and it effectively learns interpretable hierarchical latent structures from high-dimensional data, demonstrating practical benefits in tasks like transfer learning.

Shape as Line Segments: Accurate and Flexible Implicit Surface Representation
Distance field-based methods in geometry modeling encounter inaccuracies and gradient issues during surface extraction. We introduce Shape as Line Segments (SALS), an innovative implicit geometry representation utilizing attributed line segments and a Line Segment Field, which effectively captures spatial relationships and improves surface reconstruction from 3D point clouds, outperforming current state-of-the-art methods.

CAMEx: Curvature-aware Merging of Experts
CAMEx (Curvature-Aware Merging of Experts) is introduced as a new protocol for expert merging that uses natural gradients to address the non-Euclidean curvature of the parameter space, enhancing model alignment and generalization during pre-training and fine-tuning without significant memory overhead. The approach surpasses traditional methods in various NLP tasks, offers a dynamic architecture for optimized resource use and scalability, and is supported by both theoretical and empirical validation, with code available for public use.

Neural Dueling Bandits: Preference-Based Optimization with Human Feedback
The paper addresses the limitations of existing contextual dueling bandit algorithms that assume a linear reward function, by using a neural network to estimate non-linear reward functions with preference feedback. It introduces novel upper confidence bound- and Thompson sampling-based algorithms with sub-linear regret guarantees, extending to binary feedback scenarios, supported by experimental results on synthetic datasets.

PixWizard: Versatile Image-to-Image Visual Assistant with Open-Language Instructions
PixWizard is a versatile image-to-image visual assistant that uses a unified image-text-to-image generation framework to address various vision tasks, such as image generation, restoration, and editing, by leveraging a curated dataset and detailed natural language instruction templates. Utilizing Diffusion Transformers with a flexible resolution mechanism, the model demonstrates impressive generative and understanding capabilities, excelling in tasks with diverse resolutions and generalizing effectively to unseen tasks and instructions.

On the Performance Analysis of Momentum Method: A Frequency Domain Perspective
This paper introduces a frequency domain analysis framework that conceptualizes the momentum method in neural network training as a time-variant filter, showing that adjusting momentum coefficients alters filter characteristics. Based on this framework, the authors propose Frequency Stochastic Gradient Descent with Momentum (FSGDM), a heuristic optimizer that dynamically modulates momentum filtering and demonstrates superior performance over traditional momentum-based optimizers.

HR-Extreme: A High-Resolution Dataset for Extreme Weather Forecasting
This study highlights the gap in forecasting extreme weather events by introducing HR-Extreme, a high-resolution dataset derived from NOAA's HRRR data, designed specifically for these occurrences. It evaluates current deep learning and numerical models, presenting HR-Heim, an improved baseline model that performs better on general and extreme weather predictions, pointing to the urgent need for enhanced extreme weather forecasting.

Self-Introspective Decoding: Alleviating Hallucinations for Large Vision-Language Models
The paper addresses the challenge of hallucination in Large Vision-Language Models (LVLMs) by introducing a novel method called Self-Introspective Decoding (SID). SID employs a Context and Text-aware Token Selection (CT²S) strategy to selectively emphasize less important vision tokens, effectively reducing hallucinations and computational burdens, thereby producing higher-quality text without significantly increasing computational cost.

Physics-Informed Deep Inverse Operator Networks for Solving PDE Inverse Problems
This paper introduces Physics-Informed Deep Inverse Operator Networks (PI-DIONs), a novel approach for solving inverse problems involving partial differential equations without the need for labeled training data. The method extends stability estimates to the operator learning framework, ensuring robust generalization across domains, and is validated through extensive experiments demonstrating its effectiveness and accuracy.

Repurposing in AI: A Distinct Approach or an Extension of Creative Problem Solving?
This paper develops a theoretical framework distinguishing repurposing from creative problem solving, focusing on their distinct mechanisms of operating within or expanding conceptual spaces. It highlights how repurposing leverages existing resources innovatively, differing fundamentally yet complementarily from creative problem solving, thus offering new insights into effective problem-solving strategies.

RelCon: Relative Contrastive Learning for a Motion Foundation Model for Wearable Data
RelCon is a self-supervised Relative Contrastive learning approach designed to train a motion foundation model using wearable accelerometry sensors, capturing motif similarity and semantic information like rotation invariance. This model, trained on a substantial dataset of 1 billion segments from over 87,000 participants, demonstrates strong performance on various downstream tasks such as human activity recognition, showcasing its generalizability across distinct evaluation tasks.

Alchemy: Amplifying Theorem-Proving Capability Through Symbolic Mutation
Formal proofs pose a challenge due to limited formal corpora, which this paper addresses through Alchemy, a framework for generating formal theorems via symbolic mutation, significantly increasing the theorem count in Mathlib. By augmenting data and improving training methods for large language models, the approach enhances Neural Theorem Proving performance, achieving notable gains on benchmarks such as Leandojo and miniF2F, and providing guidance for theorem prover development.

Anyprefer: An Agentic Framework for Preference Data Synthesis
Anyprefer is a framework designed to synthesize high-quality preference data by framing the data synthesis process as a cooperative two-player Markov Game between a target model and a judge model. By introducing external tools to assist the judge model and a feedback mechanism to optimize prompts, Anyprefer mitigates bias in reward processes and significantly improves model alignment performance across various applications, achieving notable improvements in natural language generation, vision-language understanding, medical image analysis, and visuo-motor control tasks.

MMSearch: Unveiling the Potential of Large Models as Multi-modal Search Engines
This paper introduces MMSearch-Engine, a pipeline designed to endow Large Multimodal Models (LMMs) with multimodal search capabilities, and presents MMSearch, an evaluation benchmark featuring 300 diverse instances across 14 subfields. The study finds that GPT-4o equipped with MMSearch-Engine outperforms the commercial Perplexity Pro in end-to-end tasks, highlighting this approach's potential in advancing multimodal AI search engines while identifying areas where current LMMs struggle.

Equivariant Neural Functional Networks for Transformers
This paper investigates neural functional networks (NFN) tailored for transformer architectures and provides a systematic study in this context, filling the existing research gap. It introduces Transformer-NFN with an equivariant design under specified group actions and offers a dataset of over 125,000 model checkpoints to benchmark future research.

EcoFace: Audio-Visual Emotional Co-Disentanglement Speech-Driven 3D Talking Face Generation
The paper introduces EcoFace, a framework for speech-driven 3D facial animation aimed at overcoming challenges like feature confusion and emotion weakening by creating an audio-visual emotion space independent of speech content. By constructing a universal facial motion distribution and utilizing speaker-specific generation, EcoFace achieves more generalized and emotionally realistic talking face animations, as demonstrated through extensive experiments.

On Minimizing Adversarial Counterfactual Error in Adversarial Reinforcement Learning
This paper addresses the vulnerability of Deep Reinforcement Learning (DRL) policies to adversarial noise by introducing a novel objective called Adversarial Counterfactual Error (ACoE) that considers partial observability directly. Through the development of a scalable surrogate objective, Cumulative-ACoE (C-ACoE), the authors demonstrate superior performance in robustness against adversarial perturbations on standard benchmarks, surpassing current state-of-the-art methods.

Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection
This paper introduces Sparse RAG, a novel paradigm designed to reduce computation costs for large language models augmented with retrieval by employing a sparse mechanism. By encoding retrieved documents in parallel and selectively decoding output through control tokens, Sparse RAG improves inference speed and generation quality while maintaining computational efficiency, as demonstrated by its performance across multiple datasets.

Interleaved Scene Graphs for Interleaved Text-and-Image Generation Assessment
This paper introduces ISG, a framework for evaluating interleaved text-and-image generation models, leveraging a scene graph structure to assess consistency and accuracy across multiple levels of granularity. Alongside ISG, the authors present ISG-Bench, a benchmark dataset, revealing that current unified vision-language models struggle with interleaved content, while a proposed baseline agent employing a "plan-execute-refine" pipeline significantly boosts performance.

WardropNet: Traffic Flow Predictions via Equilibrium-Augmented Learning
The paper introduces WardropNet, a novel neural network that integrates combinatorial optimization for efficient and precise traffic flow predictions by combining classical network layers with an equilibrium layer. WardropNet significantly enhances the prediction accuracy of traffic equilibria in both realistic and stylized scenarios, outperforming traditional learning-based models by up to 72% for time-invariant and 23% for time-variant predictions.

Diffusion-Based Planning for Autonomous Driving with Flexible Guidance
The paper introduces a novel transformer-based Diffusion Planner for autonomous driving that effectively models multi-modal driving behavior and ensures trajectory quality without relying on predefined rules. By enabling joint modeling of prediction and planning tasks, the Diffusion Planner achieves state-of-the-art performance in closed-loop scenarios with improved safety, adaptability, and robust transferability, as demonstrated on large-scale benchmarks and a newly collected dataset.

Towards Marginal Fairness Sliced Wasserstein Barycenter
This paper introduces the concept of marginal fairness in Sliced Wasserstein barycenters (SWB) and explores a new constrained SWB problem called the marginal fairness sliced Wasserstein barycenter (MFSWB). The authors propose three hyperparameter-free, computationally efficient surrogate MFSWB problems to implicitly minimize distances to marginals, improve marginal fairness, and demonstrate their effectiveness through experiments in 3D point-cloud averaging, color harmonization, and class-fairness in sliced Wasserstein autoencoders.

Learning to Select Nodes in Branch and Bound with Sufficient Tree Representation
This paper introduces TRGNN, a novel approach utilizing Tripartite graph representation and Reinforcement Learning with a Graph Neural Network model to improve node selection in Branch-and-Bound methods for Mixed Integer Linear Programming (MILP). TRGNN is theoretically grounded, enhances node selection efficiency, and demonstrates superior performance and generalization compared to existing methods on both synthetic and large-scale real-world MILPs.

Select before Act: Spatially Decoupled Action Repetition for Continuous Control
This paper introduces SDAR, a novel framework for reinforcement learning that implements Spatially Decoupled Action Repetition, allowing for individual closed-loop act-or-repeat decisions across action dimensions. SDAR enhances flexibility and performance in continuous control tasks, outperforming existing repetition methods by achieving better sample efficiency, policy performance, and reduced action fluctuation.

Nonasymptotic Analysis of Stochastic Gradient Descent with the Richardson–Romberg Extrapolation
This paper enhances the understanding of solving strongly convex and smooth minimization problems using stochastic gradient descent (SGD) by expanding the mean-squared error of the estimator in terms of iterations $n$. The authors demonstrate that the root mean-squared error can be expressed as the sum of a leading term of order $\mathcal{O}(n^{-1/2})$ and a second-order term of order $\mathcal{O}(n^{-3/4})$, offering insights into the performance of the estimator through geometric ergodicity in a weighted Wasserstein semimetric.

Tell me about yourself: LLMs are aware of their learned behaviors
This paper investigates *behavioral self-awareness* in large language models (LLMs), defined as their ability to articulate their behavioral policies without in-context examples, by finetuning them on specific behavior examples. The study finds that finetuned LLMs can independently describe their policies and correctly attribute them to distinct personas, and also that they can recognize backdoor-like behaviors obtained through fine-tuning, which has significant implications for AI safety.

Learning Interleaved Image-Text Comprehension in Vision-Language Large Models
The paper introduces a novel task, Interleaved Image-Text Comprehension (IITC), which challenges multi-modal large models (MLLMs) to filter out irrelevant information in both text and images for accurate comprehension and task execution. To support this task, the VEGA dataset and the Image-Text Association (ITA) subtask were developed, with evaluation showing that existing models struggle with IITC tasks, yet a multi-task, multi-scale post-training strategy improved model accuracy, underscoring the dataset's value in enhancing MLLMs' nuanced comprehension abilities.

Post-hoc Reward Calibration: A Case Study on Length Bias
This paper introduces Post-hoc Reward Calibration to address biases in reward models of Large Language Models without requiring additional data and training. By using local average reward and Locally Weighted Regression, the authors demonstrate significant improvements in model alignment, outperforming traditional methods in terms of RM evaluation consistency and reducing biases such as length preference across various benchmarks.

Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs with Semantic Space
This paper introduces SCOPE, a novel approach to conversation planning that utilizes dense semantic representation to efficiently optimize LLM responses without the need for extensive simulation queries. By planning within the semantic space, SCOPE achieves conversation planning up to 70 times faster than traditional methods while delivering higher rewards in practical real-time applications.

Diff3DS: Generating View-Consistent 3D Sketch via Differentiable Curve Rendering
Diff3DS is a novel differentiable rendering framework designed to facilitate the generation of view-consistent 3D sketches without requiring professional artistic skills. By optimizing 3D parametric curves and bridging the gap between 3D sketches and raster images, this framework supports innovative tasks like text-to-3D and image-to-3D sketch generation, showing promising results through end-to-end optimization and distillation-based supervision techniques.

VLAS: Vision-Language-Action Model with Speech Instructions for Customized Robot Manipulation
This paper introduces VLAS, the first end-to-end policy model that integrates speech modality into robot manipulation, addressing limitations of traditional vision-language-action (VLA) models that rely solely on textual instructions. By employing a three-stage speech instruction tuning strategy and a voice retrieval-augmented generation approach, VLAS enhances performance in personalized and customization tasks, demonstrating comparable outcomes on the CALVIN benchmark and superiority in tasks leveraging auxiliary speech information.

Linear Bandits with Memory
This paper introduces a novel nonstationary linear bandit model that considers past actions' influence on current rewards, characterized by a window size and an exponent for rotting or rising phenomena. The authors propose a variant of OFUL to minimize regret and prove a regret bound, enhancing model selection techniques for cases with unknown parameters, while also validating their theoretical findings through experiments.

Geometry-aware RL for Manipulation of Varying Shapes and Deformable Objects
This paper addresses the challenge of manipulating objects with varying geometries and deformable characteristics in robotics by introducing a heterogeneous graph representation to model both rigid and deformable object tasks. The proposed Heterogeneous Equivariant Policy (HEPi), leveraging $SE(3)$ equivariant message passing networks, demonstrates superior performance over other models in a novel reinforcement learning benchmark focused on complex manipulation tasks, highlighting improved average returns, sample efficiency, and generalization.

Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
This paper presents methods for identifying and utilizing sparse feature circuits, which are human-interpretable subnetworks important for explaining language model behaviors, offering a clearer understanding compared to previous circuit frameworks. The authors introduce SHIFT to enhance classifier generalization by removing irrelevant features and demonstrate a scalable, unsupervised pipeline for discovering a multitude of these circuits to improve model interpretability.

Slot-Guided Adaptation of Pre-trained Diffusion Models for Object-Centric Learning and Compositional Generation
SlotAdapt introduces an innovative method that combines slot attention with pretrained diffusion models using adapters, which bypasses the text-centric bias and enhances object-centric learning without external supervision. The method outperforms existing techniques in object discovery and image generation, especially on complex real-world images, demonstrating its superiority over traditional slot-based generative models.

COAT: Compressing Optimizer states and Activations for Memory-Efficient FP8 Training
COAT is an innovative FP8 training framework designed to reduce memory usage and improve efficiency by addressing the limitations of current methods through Dynamic Range Expansion and Mixed-Granularity Activation Quantization. Experiments show that COAT significantly lessens the memory footprint and enhances training speed while maintaining nearly lossless performance, enabling large model training on fewer GPUs and supporting larger batch sizes in distributed settings.

Forget the Data and Fine-Tuning! Just Fold the Network to Compress
Model folding is a new data-free model compression method that significantly reduces model size by merging structurally similar neurons without requiring training data or fine-tuning. It preserves data statistics using k-means clustering and novel techniques, outperforming recent data-free methods, especially in high sparsity cases, and is effective for compressing large models for resource-limited deployments.

Exploring the Camera Bias of Person Re-identification
This paper investigates camera bias in person re-identification (ReID) models, particularly highlighting its impact under data distribution shifts and in unseen domains. The authors propose a feature normalization method to reduce this bias effectively and explore simple training strategies to enhance unsupervised ReID models, demonstrating notable performance improvements with these approaches.

What should a neuron aim for? Designing local objective functions based on information theory
This paper introduces a framework for achieving self-organization between individual artificial neurons by designing bio-inspired local learning goals using Partial Information Decomposition (PID). By allowing neurons to locally process information from various inputs—feedforward, feedback, and lateral—the approach provides neuron-level interpretability and strong performance, advancing information-theoretic foundations for local learning strategies.

Lightweight Neural App Control
This paper presents LiMAC, a new architecture for controlling Android apps using a combination of past mobile observations and a textual goal to generate precise actions. LiMAC significantly enhances action accuracy and efficiency in mobile app control by utilizing a compact Action Transformer and a vision-language model, outperforming both open-source and closed-source models in benchmark tests.

Can Neural Networks Achieve Optimal Computational-statistical Tradeoff? An Analysis on Single-Index Model
This paper investigates whether neural networks trained with gradient-based methods can achieve the optimal statistical-computational tradeoff for learning Gaussian single-index models, ultimately matching the theoretical sample complexity bounds determined by the statistical query framework. The authors introduce a unified gradient-based algorithm adaptable to various functions, showing it effectively learns feature representations aligned with the unknown signal, even when the signal is sparse, and achieves sample complexity close to the theoretical lower bound, offering insights applicable to problems like sparse tensor PCA.

A New Perspective on Shampoo's Preconditioner
This paper analyzes Shampoo, a second-order optimization algorithm, by uncovering a novel connection between its Kronecker product preconditioner and optimal matrix approximations, revealing that Shampoo's method closely resembles a power iteration algorithm step. The study improves the theoretical understanding of Shampoo and suggests enhancements for its practical performance across various datasets and architectures.

Pursuing Better Decision Boundaries for Long-Tailed Object Detection via Category Information Amount
This paper introduces the concept of category informativeness in object detection, finding a negative correlation between a category's informativeness and its accuracy, which better reflects the learning difficulty than instance count alone. The authors propose the Informativeness-Guided Angular Margin Loss (IGAM Loss), which adjusts decision spaces based on category informativeness, showing improved performance in mitigating category bias in both long-tailed and balanced datasets.

Benchmarking Multimodal Retrieval Augmented Generation with Dynamic VQA Dataset and Self-adaptive Planning Agent
The paper introduces the Dyn-VQA dataset, designed to address limitations in existing multimodal retrieval augmented generation (mRAG) systems by featuring dynamic questions that require complex retrieval strategies. It proposes OmniSearch, a self-adaptive planning agent that improves retrieval processes by dynamically breaking down complex questions into sub-queries, demonstrating its effectiveness in enhancing mRAG performance.

Adaptive Rank Allocation: Speeding Up Modern Transformers with RaNA Adapters
This paper introduces the Adaptive Rank Allocation framework and the Rank and Neuron Allocator (RaNA) adapter to improve inference efficiency in modern Transformer architectures. By utilizing low-rank matrix decompositions and adaptive masking, RaNA effectively enhances accuracy and reduces computational load without depending on activation sparsity, achieving up to 8 percentage-points increase in accuracy and 44% reduction in FLOPs compared to traditional neuron adapters.

Learning and aligning single-neuron invariance manifolds in visual cortex
This paper introduces a novel method for identifying and aligning the invariance manifolds of visual sensory neurons, addressing challenges related to varying receptive field sizes, positions, and orientations. By learning continuous invariance manifolds and applying affine transformations, the approach quantifies and compares neuronal invariances, revealing functional clusters and offering new insights into the functional properties of visual neurons, as demonstrated with simulated neurons and macaque V1 neurons.

Learning local equivariant representations for quantum operators
SLEM, a novel deep learning model, enhances the efficiency and scalability of predicting multiple quantum operator matrices, such as Hamiltonian and density matrices, within the density functional theory framework. By leveraging strictly localized equivariant message-passing and innovative parameterization methods, SLEM achieves state-of-the-art accuracy and computational efficiency, enabling high accuracy on diverse materials and potential for large-scale quantum simulations.

Optimizing Neural Network Representations of Boolean Networks
This paper presents a deterministic algorithm for optimizing neural networks that represent Boolean functions, reducing the number of neurons and connections while maintaining functional equivalence, a process that is inherently NP-hard and typically lossy with existing methods. By introducing an objective-aware algorithm that leverages shared representations among subproblems, the authors achieve substantial reductions in networks' size and connections by up to 70% and 60% respectively, along with significant optimization speedups, demonstrating practicality for applications like high-throughput circuit simulation.

Boosting the visual interpretability of CLIP via adversarial fine-tuning
This paper introduces unsupervised adversarial fine-tuning (AFT) with norm-regularization to improve the visual interpretability of CLIP, addressing the challenges posed by its complex image encoder architecture. The proposed method enhances the focus of the image encoder on meaningful features, improves alignments with human-understandable concepts, and demonstrates generalizable benefits across out-of-distribution datasets and downstream tasks.

3D StreetUnveiler with Semantic-aware 2DGS - a simple baseline
The paper introduces StreetUnveiler, a method to reconstruct 3D representations of empty streets from crowded observations captured by in-car cameras, addressing challenges posed by temporarily static objects and the dynamic nature of street scenes. Utilizing a novel combination of semantic 2D Gaussian Splatting and a time-reversal framework, the approach achieves effective inpainting and temporal consistency, with experiments successfully demonstrating its capability on street scene data for potential future applications.

Accessing Vision Foundation Models via ImageNet-1K
Proteus, a novel distillation framework, allows for the creation of smaller vision foundation models on ImageNet-1K without access to the original vast training dataset, thereby overcoming resource-heavy requirements. By focusing on token, patch, and feature-level training objectives, Proteus achieves performance comparable to larger models like DINOv2-L/14 across 19 benchmarks while utilizing a significantly smaller dataset, thus making advanced model training more accessible to the research community.

A CLIP-Powered Framework for Robust and Generalizable Data Selection
This paper introduces a novel CLIP-powered data selection framework that utilizes multimodal information to enhance sample selection from large-scale datasets, aiming to reduce training costs while improving model performance. By employing modules for dataset adaptation, sample scoring, and selection optimization, the method effectively removes noisy samples and achieves superior results over existing baselines, demonstrating its potential to both accelerate training and improve data quality.

A Closer Look at Machine Unlearning for Large Language Models
This paper addresses the challenges in machine unlearning for large language models (LLMs) by introducing new evaluation metrics and proposing improved unlearning methods. The researchers categorize unlearning methods, identify their limitations, and offer solutions such as maximizing entropy for untargeted unlearning and using answer preservation loss for targeted unlearning, demonstrating effectiveness through experiments across various scenarios.

A Common Pitfall of Margin-based Language Model Alignment: Gradient Entanglement
This paper examines a common issue in Reinforcement Learning from Human Feedback (RLHF) for language models, where margin-based loss methods lead to potential safety alignment failures and reduced ideal responses due to a phenomenon called gradient entanglement. The authors provide theoretical insights and empirical validation for these effects, highlighting the need for improved preference optimization algorithms and suggesting future directions for addressing these challenges.

A Computational Framework for Modeling Emergence of Color Vision in the Human Brain
This paper presents a computational framework to model the emergence of human color vision by simulating the eye and cortex, addressing how the visual cortex infers color dimensionality from optic nerve signals. The study introduces a simulation engine based on vision science and a cortical learning model that predicts optic nerve fluctuations, showing that natural color vision emerges and can be enhanced from 3D to 4D, as demonstrated in simulations akin to gene therapy effects on squirrel monkeys.

Active Learning for Continual Learning: Keeping the Past Alive in the Present
This paper introduces AccuACL, an innovative active continual learning (ACL) method that utilizes the Fisher information matrix for selecting informative samples to efficiently balance learning new tasks and preventing catastrophic forgetting. Experimental results show that AccuACL significantly outperforms traditional active learning (AL) strategies, achieving an average accuracy increase of 23.8% and reducing forgetting by 17.0% across various continual learning algorithms.

AdaGrad under Anisotropic Smoothness
This paper addresses the gap in theoretical understanding of adaptive gradient methods, like Adagrad, especially in large batch-size settings, by proposing an anisotropic generalized smoothness assumption. The authors demonstrate that under this new assumption, AdaGrad achieves faster convergence compared to classical gradient methods, supported by experiments in logistic regression and fine-tuning tasks.

Adam Exploits $\ell_\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity
This paper presents a new theoretical insight into why Adam outperforms SGD by focusing on the exploitation of favorable $\ell_\infty$-geometry, providing a novel convergence analysis under this geometry. The study reveals that smoothness assumptions related to $\ell_\infty$-geometry can explain Adam's performance benefits, and experiments show that these assumptions notably impact Adam's efficiency, while SGD remains largely unaffected by such geometric changes.

Adaptive Retention & Correction: Test-Time Training for Continual Learning
This study addresses classification layer bias in continual learning by proposing Adaptive Retention & Correction (ARC), which includes an Out-of-Task Detection method for the testing phase to identify past task samples and adjusts predictions accordingly. ARC enhances performance in both memory-free and memory-based environments, demonstrating an average performance increase of 2.7% and 2.6% on CIFAR-100 and Imagenet-R datasets when integrated with existing approaches.

Adjoint Matching: Fine-tuning Flow and Diffusion Generative Models with Memoryless Stochastic Optimal Control
This paper addresses the challenge of improving dynamical generative models with reward fine-tuning by framing it as a stochastic optimal control (SOC) problem, introducing a critical memoryless noise schedule requirement. The authors propose a novel algorithm called Adjoint Matching, which treats SOC problems as regression tasks, leading to improved consistency, realism, and generalization in fine-tuned models without sacrificing sample diversity.

Advancing LLM Reasoning Generalists with Preference Trees
We present EURUS, a suite of large language models optimized for reasoning tasks, which surpasses existing open-source models and even outperforms GPT-3.5 Turbo in comprehensive reasoning benchmarks. This advancement is credited to ULTRAINTERACT, our specifically designed high-quality dataset and novel reward modeling approach, which enhances performance in reasoning by fine-tuning based on explicit positive reward values and preference learning tailored for the narrow space of correct reasoning answers.

Advancing Out-of-Distribution Detection via Local Neuroplasticity
This paper introduces a novel out-of-distribution (OOD) detection method utilizing the local neuroplasticity of Kolmogorov-Arnold Networks (KANs), which allows adaptation to new tasks while preserving learned information. The proposed approach outperforms current techniques in OOD detection across image and medical domains, highlighting the potential of KANs to improve the robustness of machine learning systems.

Adversarial Mixup Unlearning
Machine unlearning aims to remove sensitive information from ML models, but faces challenges like catastrophic unlearning, where erasing specific data unintentionally removes essential knowledge. This paper introduces MixUnlearn, a generator-unlearner framework that uses synthesized mixup samples to regularize the unlearning process, outperforming state-of-the-art methods and ensuring effective unlearning without losing critical knowledge.

Adversarial Score identity Distillation: Rapidly Surpassing the Teacher in One Step
SiDA (Score identity Distillation with Adversarial Loss) improves upon the data-free image generation method, SiD, by integrating real images and adversarial loss to enhance generation quality and distillation efficiency. This novel approach utilizes a discriminator to distinguish real from generated images, significantly accelerating convergence and surpassing previous generation performance benchmarks on various datasets, as demonstrated by state-of-the-art FID scores.

A Graph Enhanced Symbolic Discovery Framework For Efficient Logic Optimization
Logic Optimization (LO) in chip design faces challenges with inefficient scoring functions that hinder inference efficiency, interpretability, and generalization. This paper introduces CMO, a data-driven framework utilizing a Graph Enhanced Symbolic Discovery approach to develop lightweight, interpretable, and generalizable scoring functions, significantly enhancing LO tool performance and achieving up to 2.5× faster runtime when integrated with the Mfs2 heuristic.

ALLaM: Large Language Models for Arabic and English
This paper introduces ALLaM, a series of Arabic Large Language Models designed to enhance Arabic Language Technologies through careful training and alignment with human preferences. By integrating second-language acquisition strategies and translation data, ALLaM achieves state-of-the-art performance on several Arabic benchmarks while maintaining English proficiency.

Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models Trained on Corrupted Data
This paper introduces a framework for solving inverse problems using diffusion models trained on linearly corrupted data, specifically extending the Ambient Diffusion framework to apply to Fourier domain-corrupted measurements. The proposed Ambient Diffusion Posterior Sampling (A-DPS) algorithm shows improved performance for MRI reconstruction and various image restoration tasks, even outperforming models trained on clean data in certain scenarios, especially in high acceleration regimes.

Are Large Vision Language Models Good Game Players?
Large Vision Language Models (LVLMs) struggle to demonstrate their full potential due to limitations in current evaluation methods. The proposed LVLM-Playground offers a game-based evaluation framework to thoroughly assess LVLMs' cognitive and reasoning abilities across tasks like visual perception and multi-turn reasoning, revealing current limitations and making the code and data available for public use.

A Stochastic Approach to the Subset Selection Problem via Mirror Descent
This paper introduces a stochastic approach to the minimum cost subset selection problem by randomly selecting subsets from a learnable distribution and optimizing using Stochastic Mirror Descent. The method provides constructive closed-form unbiased stochastic gradient formulas, convergence guarantees, and is empirically evaluated in transfer learning, demonstrating potential benefits.

A Theory for Token-Level Harmonization in Retrieval-Augmented Generation
This paper introduces a theoretical framework to explain and balance the benefits and detriments of retrieval-augmented generation (RAG) when used with large language models (LLMs). By modeling RAG as the fusion of LLMs' knowledge and retrieved texts, the authors present Tok-RAG, a novel method that effectively preserves the advantages while mitigating the risks of misleading information during token prediction, validated through experiments with various LLMs.

A Transfer Attack to Image Watermarks
This paper introduces a novel transfer evasion attack targeting watermarked images in the no-box setting, where attackers do not have direct access to the watermarking model or detection API. The study demonstrates both theoretically and empirically that existing watermark-based AI-generated image detectors are vulnerable to such evasion attacks, highlighting a critical gap in the robustness of watermarking methods.

A Unified Theory of Quantum Neural Network Loss Landscapes
This paper introduces the concept of "Wishart processes" to understand the behavior of quantum neural networks (QNNs) and their derivatives, differing from classical neural networks that behave as Gaussian processes. The framework developed provides necessary and sufficient conditions for QNNs to have a Gaussian process limit, enables calculation of gradient and local minima distributions, and proposes a new operational definition for the trainability of QNNs through the "degrees of freedom" metric.

AutoCGP: Closed-Loop Concept-Guided Policies from Unlabeled Demonstrations
This paper introduces a novel imitation learning framework for training embodied agents using autonomously discovered manipulation concepts to improve task performance in complex robotic environments. The framework, consisting of an *Automatic Concept Discovery* module and a *Concept-Guided Policy Learning* module, outperforms baseline methods and offers enhanced task execution by reducing reliance on predefined skills and addressing human semantic ambiguities.

BEEM: Boosting Performance of Early Exit DNNs using Multi-Exit Classifiers as Experts
This paper introduces BEEM, a novel decision criterion for Early Exit techniques in Deep Neural Networks, which aggregates confidence scores of exit classifiers as experts to enhance accuracy and reduce inference latency. By setting thresholds based on intermediate exit error rates, BEEM demonstrates improved performance and speed-up over state-of-the-art methods on the COCO and GLUE datasets, achieving accuracy comparable to the final layer in complex image tasks and improved results in simpler language tasks.

Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?
LLM4Hypergraph is introduced as the first comprehensive benchmark designed to evaluate the capabilities of large language models (LLMs) in understanding beyond-pairwise relationships using hypergraphs, with a dataset of 21,500 problems across various tasks. The benchmark not only reveals the strengths and weaknesses of several LLMs, including GPT-4o, but also enhances high-order reasoning performance through innovative techniques like Hyper-BAG and Hyper-COT, offering a foundational testbed for integrating advanced hypergraph reasoning into LLMs.

Beyond Worst-Case Dimensionality Reduction for Sparse Vectors
This paper investigates beyond worst-case dimensionality reduction for $s$-sparse vectors by presenting novel lower bounds for average-case guarantees, showing that any oblivious linear map must use at least $\Omega(s^2)$ dimensions to preserve the norm of 99% of vectors. It also explores improved upper bounds for sparse non-negative vectors, providing an efficient nonlinear embedding that preserves pairwise distances in $\ell_p$ norm, leveraging the non-negativity assumption to achieve significantly smaller embeddings than for arbitrary sparse vectors.

Block-Attention for Efficient Prefilling
Block-attention is an innovative attention mechanism that reduces inference latency and computation costs in Retrieval-Augmented Generation by dividing documents into blocks and reusing key-value states, enabling it to significantly outperform traditional full-attention models in efficiency without performance loss. It achieves a 98.7% reduction in time to first token and a 99.8% reduction in floating point operations, demonstrating its potential uses in diverse applications including Game AI.

BodyGen: Advancing Towards Efficient Embodiment Co-Design
This paper addresses the optimization challenges in embodiment co-design, which aims to simultaneously optimize a robot's morphology and control. By introducing BodyGen, a method that uses topology-aware self-attention and a temporal credit assignment mechanism, the authors demonstrate a significant 60.03% performance improvement over existing methods, providing an efficient solution to the morphology representation and reward signal balancing issues.

Boltzmann-Aligned Inverse Folding Model as a Predictor of Mutational Effects on Protein-Protein Interactions
This paper introduces the Boltzmann Alignment technique, which leverages pre-trained inverse folding models to improve predictions of binding free energy changes (ΔΔG) in protein-protein interactions, crucial for drug design. The proposed method incorporates the unbound state of protein complexes within the ΔΔG thermodynamic cycle, achieving state-of-the-art performance on the SKEMPI v2 dataset with significant improvements over previous approaches, applicable in tasks such as binding energy prediction, protein-protein docking, and antibody optimization.

BOND: Aligning LLMs with Best-of-N Distillation
This paper introduces Best-of-N Distillation (BOND), a novel reinforcement learning from human feedback (RLHF) algorithm designed to mimic the performance of Best-of-N sampling while reducing computational demands during inference. Utilizing Jeffreys divergence, BOND aligns the policy generation distribution with the Best-of-N distribution, and its efficacy is demonstrated through experiments on abstractive summarization and Gemma models.

C-Adapter: Adapting Deep Classifiers for Efficient Conformal Prediction Sets
Conformal Adapter (C-Adapter) is introduced as an adapter-based tuning method to improve the efficiency of conformal predictors without compromising accuracy, addressing the limitations imposed by regularization in conformal training. By implementing intra order-preserving functions and a novel loss for maximizing discriminability of non-conformity scores, C-Adapter enhances predictive efficiency and is demonstrated to significantly improve prediction set efficiency across various classifiers in extensive experiments.

Can a MISL Fly? Analysis and Ingredients for Mutual Information Skill Learning
This paper analyzes the strengths of reinforcement learning methods and introduces a new Mutual Information Skill Learning (MISL) method called contrastive successor features, which matches the performance of METRA with a simpler approach. The study also highlights connections between skill learning and other learning techniques, providing insight through comprehensive ablation studies.

Can In-context Learning Really Generalize to Out-of-distribution Tasks?
This paper investigates the mechanism of in-context learning (ICL) on out-of-distribution (OOD) tasks using a GPT-2 model, revealing that Transformers may struggle with learning OOD functions. The study highlights that ICL performance is linked to optimizing pretraining functions within the hypothesis space and demonstrates a low-test-error preference, especially when models are pretrained on multiple tasks, with empirical and theoretical insights into ICL's limitations in handling distributional shifts.

Can Large Language Models Understand Symbolic Graphics Programs?
The paper introduces a novel approach to evaluate large language models (LLMs) using symbolic graphics programs, which test the models' spatial-semantic reasoning skills without a vision encoder. By creating a large procedural benchmark, the study finds that LLMs with stronger general reasoning perform better on these tasks and presents Symbolic Instruction Tuning (SIT) as a method that enhances both understanding of symbolic programs and general reasoning abilities across other domains.

Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models
Multimodal Large Language Models, particularly Video Large Language Models (Video-LLMs), have made strides in video understanding by aligning various modalities into the language space. This paper introduces an "alignment for answerability" framework enabling Video-LLMs to assess the relevance of questions concerning video content, and decline to answer when questions exceed the video's scope, which enhances model accuracy and reliability by integrating a specialized dataset and evaluation metrics.

CaPo: Cooperative Plan Optimization for Efficient Embodied Multi-Agent Cooperation
This paper introduces Cooperative Plan Optimization (CaPo), a framework designed to enhance cooperation among large language model-based embodied agents by implementing strategic, coherent, and long-term cooperative planning. CaPo achieves higher task completion rates and efficiency in complex multi-agent tasks by utilizing meta-plan generation and progress-adaptive execution, which mitigates redundant actions and improves coordination, as demonstrated in experimental results.

Cauchy-Schwarz Regularizers
The paper introduces Cauchy–Schwarz (CS) regularizers, a new class of regularization functions that enhance the solution vectors of optimization problems by promoting discrete-valued vectors, eigenvectors, and orthogonal matrices. These regularizers are simple, differentiable, and automatically scale-adaptive, proving effective in applications such as solving underdetermined linear equations and weight quantization in neural networks, while also allowing for various enhancements and generalizations.

CFD: Learning Generalized Molecular Representation via Concept-Enhanced  Feedback Disentanglement
This paper introduces a novel approach, Concept-Enhanced Feedback Disentanglement (CFD), to improve molecular representation learning by addressing the limitations of traditional methods under out-of-distribution scenarios. By using variational encoders to separate invariant from spurious features and integrating molecule-aware concepts, CFD enhances generalization abilities and significantly boosts performance across diverse molecular datasets, with source code available at https://github.com/AmingWu/MoleculeCFD.

Classic but Everlasting: Traditional Gradient-Based Algorithms Converges Fast Even in Time-Varying Multi-Player Games
This paper investigates the last-iterate convergence behaviors of extra gradient (EG) and optimistic gradient (OG) algorithms in time-varying perturbed games, extending analysis beyond the usual time-invariant settings by incorporating vanishing noises and other external factors. The study proves that, for perturbed games on bounded convex closed sets, the last-iterate convergence rates of EG and OG algorithms are \( O(1/\sqrt{T}) \), thereby addressing an open question about convergence rates in constrained and time-varying scenarios and aligning with known results for time-invariant games.

Combatting Dimensional Collapse in LLM Pre-Training Data via Submodular File Selection
The paper introduces DiverSified File selection algorithm (DiSF), which aims to enhance the performance of large language models by selecting decorrelated text files, addressing the diversity dilemma caused by domain-similarity selection criteria. DiSF improves overall performance across nine tasks, significantly increasing training and data efficiency with models up to 1.1B parameters, demonstrating its effectiveness by saving 98.5% of pre-training files compared to full-data pre-training.

CoMotion: Concurrent Multi-person 3D Motion
We present a method for detecting and tracking 3D poses of multiple individuals using a single monocular camera that maintains accuracy even in crowded scenes with challenging poses and occlusions. Our system, which leverages pseudo-labeled annotations for training, updates poses directly from new frames and achieves state-of-the-art pose estimation accuracy while offering improved speed and tracking performance over time.

Comparing noisy neural population dynamics using optimal transport distances
This paper highlights the limitations of current methods in quantifying geometric similarity in neural representations, especially when dealing with noisy and dynamic neural responses. The authors propose a new metric based on optimal transport distance between Gaussian processes to better capture differences in systems with noisy dynamic responses, applying it to both motor system neural models and latent diffusion models for text-to-image synthesis.

Computing Circuits Optimization via Model-Based Circuit Genetic Evolution
The paper introduces MUTE, a novel model-based circuit genetic evolution framework that optimizes computing circuits by avoiding local optima typical in reinforcement learning approaches and promoting global exploration through a genetic crossover operator. MUTE significantly outperforms existing methods in area and delay optimization and generalizes well to large-scale circuits by using a grid-based genetic evolution process to evaluate and improve design solutions based on true objective values.

Conformal Structured Prediction
This paper introduces a general framework for conformal prediction in structured prediction settings, extending current algorithms to produce structured prediction sets that implicitly represent sets of labels, which is particularly advantageous for complex outputs like text generation. The proposed method is applicable in domains where prediction sets are represented as nodes in a directed acyclic graph, such as hierarchical image classification, and ensures desired coverage guarantees across various applications.

Contrastive Learning from Synthetic Audio Doppelgängers
This paper introduces a method for generating robust audio representations using synthetic audio, addressing limitations in data scale and transformation diversity inherent in real-world audio datasets. By randomly perturbing parameters in a sound synthesizer, the approach creates synthetic audio variations that enhance contrastive learning and outperform real-world data on standard audio classification tasks, providing a lightweight and storage-efficient solution with minimal hyperparameter tuning.

Cross-Entropy Is All You Need To Invert the Data Generating Process
This paper extends identifiability results from self-supervised learning to supervised learning, particularly in parametric instance discrimination, to explain how neural models learn interpretable factors of variation up to linear transformations. Through theoretical analysis and empirical validation with simulated data and benchmarks like DisLib and ImageNet, the study offers insights into the linear representation capabilities of models, contributing to a more unified theory of the effectiveness of supervised learning.

CtrLoRA: An Extensible and Efficient Framework for Controllable Image Generation
This paper introduces the CtrLoRA framework to enhance large-scale diffusion models for text-to-image (T2I) generation by providing fine-grained spatial control with reduced resource requirements. By training a Base ControlNet for common knowledge and condition-specific LoRAs, CtrLoRA achieves efficient adaptation to new conditions with minimal data and computational resources, reducing learnable parameters by 90% compared to ControlNet.

Data-adaptive Differentially Private Prompt Synthesis for In-Context Learning
This paper introduces **AdaDPSyn**, a data-adaptive algorithm that generates synthetic examples from private datasets for large language models to perform in-context learning while ensuring differential privacy. The method utilizes a novel *Precision-Focused Iterative Radius Reduction* technique to adjust the noise in data synthesis, resulting in high privacy-preserving accuracy that outperforms existing differential privacy algorithms and approaches non-private benchmark performance.

Data Distillation for extrapolative protein design through exact preference optimization
This paper introduces a progressive search method for extrapolative protein design that leverages triplet relations to improve performance, challenging the traditional reliance on pair-based learning. The proposed framework, demonstrated on AAV and GFP protein design, significantly enhances the ability to extrapolate effectively beyond training data.

Data Scaling Laws in Imitation Learning for Robotic Manipulation
This paper explores data scaling laws in robotic manipulation and their potential to enable single-task robot policies that can be deployed zero-shot for any object in the same category across different environments. Through an empirical study involving over 40,000 demonstrations, the authors find that the diversity of environments and objects significantly impacts policy generalization, proposing an efficient data collection strategy that achieves high success rates with minimal additional effort.

Data Selection via Optimal Control for Language Models
This paper presents a method called PMP-based Data Selection (PDS) for enhancing the performance of language models by selecting high-quality pre-training data from large corpora, formulated as an Optimal Control problem. The PDS framework, validated through experiments on CommonCrawl, accelerates learning, boosts performance across various tasks and model sizes, and optimizes data utilization, reducing data demand by 1.8 times when pre-training data is limited.

DAWN: Dynamic Frame Avatar with Non-autoregressive Diffusion Framework for Talking head Video Generation
DAWN is a novel framework for non-autoregressive, diffusion-based talking head video generation, which allows the all-at-once creation of dynamic-length sequences, addressing the limitations of prior methods. By leveraging audio-driven holistic facial dynamics and head movement generation, DAWN achieves rapid, high-quality video production with natural lip, pose, and blink synchronization, exhibiting strong extrapolation capabilities in longer videos.

Debiasing Mini-Batch Quadratics for Applications in Deep Learning
This paper identifies a systematic error caused by biases in stochastic quadratic approximations when using mini-batches in deep learning, impacting second-order optimization and uncertainty quantification via the Laplace approximation. The authors provide a theoretical explanation for this bias and propose debiasing strategies, demonstrating their effectiveness in mitigating detrimental effects on applications.

Decoding Game: On Minimax Optimality of Heuristic Text Generation Strategies
This paper introduces the Decoding Game, a theoretical framework that models text generation as a two-player zero-sum game to bridge the gap between theory and practice in decoding strategies. The framework reveals that popular heuristic methods like Top-$k$ and Nucleus sampling approximate the optimal strategy by imposing implicit regularization on likelihood maximization, and it extends to encompass various strategies such as greedy search and temperature scaling, validated by numerical experiments.

Decoupled Subgraph Federated Learning
We tackle federated learning for graph-structured data across multiple clients, specifically focusing on interconnected subgraphs where client interconnections are crucial. Our novel framework, FedStruct, maintains privacy by avoiding the sharing of sensitive node features and instead uses global graph structure to capture inter-node dependencies, achieving performance comparable to centralized methods in semi-supervised node classification across various scenarios.

DeeperForward: Enhanced Forward-Forward Training for Deeper and Better Performance
The paper addresses limitations of the Forward-Forward (FF) algorithm, which struggles with feature scaling and neuron deactivation, by introducing a novel goodness design using layer normalization and mean goodness. This approach extends FF's applicability to deeper networks, as demonstrated by improved performance in experiments with 17-layer CNNs on datasets like CIFAR-10, MNIST, and Fashion-MNIST, while also proposing a model parallel strategy for efficient training.

DeepGate4: Efficient and Effective Representation Learning for Circuit Design at Scale
DeepGate4 is a novel graph transformer model designed to efficiently handle large-scale circuit representation learning, addressing scalability issues in electronic design automation tasks such as testability analysis and power estimation. By incorporating a tailored update strategy for circuit graphs, a GAT-based sparse transformer, and an acceleration CUDA kernel, DeepGate4 significantly outperforms state-of-the-art methods on benchmarks, demonstrating improved performance and reduced runtime and memory usage, thus enabling superior scalability and efficiency in complex circuit analysis.

Depth Pro: Sharp Monocular Metric Depth in Less Than a Second
Depth Pro is a novel foundation model for zero-shot metric monocular depth estimation that generates high-resolution, accurately scaled depth maps without requiring camera metadata. By leveraging a multi-scale vision transformer and a unique training protocol, it outperforms previous methods and is capable of producing a 2.25-megapixel depth map in just 0.3 seconds.

DexTrack: Towards Generalizable Neural Tracking Control for Dexterous Manipulation from Human References
This paper presents a novel neural tracking controller for dexterous robot hand manipulation inspired by human-object interactions, addressing the complexity and adaptability required for diverse object manipulation. By curating large-scale tracking demonstrations and employing a combined approach of reinforcement and imitation learning, the proposed method significantly improves success rates by over 10% compared to existing baselines, as demonstrated in both simulations and real-world environments.

D-FINE: Redefine Regression Task of DETRs as Fine-grained Distribution Refinement
D-FINE is a real-time object detector that improves localization precision in DETR models through Fine-grained Distribution Refinement (FDR) and Global Optimal Localization Self-Distillation (GO-LSD). It achieves enhanced accuracy and speed, surpassing existing real-time detectors, and boosts the performance of DETR models significantly with minimal additional parameters and training costs.

Differentiable and Learnable Wireless Simulation with Geometric Transformers
Wi-GATr is a fully-learnable neural simulation surrogate that predicts channel observations from scene primitives like surface meshes and antenna positions using an equivariant Geometric Algebra Transformer. This approach demonstrates significant improvements in accuracy, efficiency, and robustness in tasks such as signal strength prediction and receiver localization, achieving more than 35% lower error than hybrid techniques and 70% lower than calibrated wireless tracers.

Diffusion Bridge Implicit Models
This paper introduces diffusion bridge implicit models (DBIMs), a generalization of denoising diffusion bridge models (DDBMs), to achieve faster sampling without additional training. DBIMs offer up to 25 times faster performance than traditional DDBM samplers and support generative processes with both stochastic and deterministic elements, enhancing tasks like image translation through a novel use of ordinary differential equations (ODEs) and booting noise strategy.

Discriminator-Guided Embodied Planning for LLM Agent
This paper presents Discriminator-Guided Action Optimization (DGAP), a new framework designed to improve Large Language Models' (LLMs) performance in complex tasks by optimizing action plans through step-wise feedback. Using a limited set of demonstrations, DGAP enhances LLM-generated action effectiveness with a scoring system, achieving superior policy and efficiency compared to existing methods in experiments with models like GPT-4 and Llama3-70B.

Divergence-Regularized Discounted Aggregation: Equilibrium Finding in Multiplayer Partially Observable Stochastic Games
This paper introduces Divergence-Regularized Discounted Aggregation (DRDA), a system designed to solve partially observable stochastic games (POSGs) by utilizing a discounted variant of Follow the Regularized Leader (FTRL). DRDA is shown to achieve last-iterate convergence to a Nash equilibrium in various game settings, including multiplayer normal-form, extensive-form, and Markov games, outperforming existing techniques in experiments.

DreamCatalyst: Fast and High-Quality 3D Editing via Controlling Editability and Identity Preservation
The paper introduces DreamCatalyst, a novel framework for 3D editing that efficiently leverages score distillation sampling by aligning with the sampling dynamics of diffusion models, thus addressing the performance limitations of existing SDS-based methods. DreamCatalyst significantly reduces training time and enhances editing quality, offering both a fast mode that edits Neural Radiance Fields remarkably faster and a high-quality mode surpassing current state-of-the-art methods in speed and quality, including 3D Gaussian Splatting techniques.

DUALFormer: Dual Graph Transformer
This paper introduces a novel Graph Transformer architecture called DUALFormer, designed to address scalability and balance challenges between local and global information in node classification tasks. By employing a dual-dimensional design of GNN and SA modules, leveraging Linearized Transformers, and optimizing feature dimensions, DUALFormer enhances both efficiency and expressivity, with experimental validation on eleven datasets showcasing superior performance over existing state-of-the-art methods.

Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting
This paper investigates structural in-context learning (ICL) in language models, focusing on their ability to generalize using sentence or task structure rather than memorized token embeddings. The study introduces methods to balance structural ICL and in-weights learning (IWL) within language models, enabling a dual process strategy that harmonizes these learning approaches.

Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic Scenes
Dynamic Gaussians Mesh (DG-Mesh) is a framework that reconstructs high-fidelity, time-consistent meshes from dynamic observations by leveraging advancements in 3D Gaussian Splatting. It introduces Gaussian-Mesh Anchoring for enhanced mesh reconstruction and demonstrates superior performance in mesh reconstruction and rendering compared to existing baselines.

Effective post-training embedding compression via temperature control in contrastive training
This paper explores the influence of the temperature parameter in contrastive training on the intrinsic dimensionality and compression of text embeddings. It proposes temperature aggregation methods that significantly reduce embedding size while maintaining performance quality, highlighting a trade-off between performance and compression.

Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning
The paper introduces Mixture-of-Denoising Experts (MoDE), a novel policy for Imitation Learning that outperforms state-of-the-art Transformer-based Diffusion Policies while drastically reducing computational demands. By utilizing sparse experts and noise-conditioned routing, MoDE achieves significant efficiency improvements, achieving state-of-the-art performance across 134 tasks and four benchmarks while using 90% fewer FLOPs and fewer active parameters compared to traditional architectures.

EqNIO: Subequivariant Neural Inertial Odometry
This paper introduces EqNIO, a neural network approach that improves low-drift localization by using canonical displacement priors, invariant to the orientation of the gravity-aligned frame of IMU data, enhancing generalization across different IMU mount orientations. By utilizing specialized layers and a novel angular rate decomposition, EqNIO demonstrates significant improvements over existing inertial odometry methods, paving the way for more generalizable odometry applications on edge devices.

Equivariant Symmetry Breaking Sets
Equivariant neural networks (ENNs) are effective in preserving symmetries, but struggle with symmetry breaking often found in physical systems. This paper introduces a novel symmetry breaking framework using symmetry breaking sets (SBS) to enable ENNs to handle spontaneous symmetry breaking for any group, enhancing data efficiency and providing practical examples with accessible code.

Event-Driven Online Vertical Federated Learning
This paper identifies and addresses the challenges of integrating online learning into Vertical Federated Learning (VFL), where clients have non-intersecting feature sets and receive asynchronous data streams. The authors propose an event-driven online VFL framework with dynamic local regret (DLR) to enhance learning stability and efficiency in non-stationary environments, showing improved performance and reduced costs compared to existing frameworks.

Expected Return Symmetries
This paper introduces a new class of symmetries called expected return symmetries, expanding beyond traditional environment symmetries, to enhance coordination in multi-agent systems. The proposed approach improves zero-shot coordination without needing prior knowledge of the environment's symmetries, demonstrating superiority over existing symmetry-based methods.

FACTS: A Factored State-Space Framework for World Modelling
The paper introduces the FACTored State-space (FACTS) model, a novel recurrent framework designed to improve spatial-temporal world modelling by efficiently encoding spatial and temporal structures in complex systems. FACTS employs a graph-structured memory with a routing mechanism for invariance to input permutations and supports parallel computation of high-dimensional sequences, outperforming or matching state-of-the-art models in tasks like multivariate time series forecasting and spatial-temporal graph prediction.

Fair Clustering in the Sliding Window Model
This paper explores the challenge of proportionally fair clustering in the streaming sliding window model, revealing that no algorithm can maintain fairness without compromise, unlike in the insertion-only model. However, by slightly relaxing the fairness constraint, the authors develop an efficient $(1+\varepsilon)$-approximate algorithm, and validate their theoretical findings with empirical tests on real datasets.

FakeShield: Explainable Image Forgery Detection and Localization via Multi-modal Large Language Models
The paper introduces FakeShield, a multi-modal framework designed to improve image forgery detection and localization (IFDL) by addressing the challenges of black-box detection principles and limited generalization across diverse tampering methods. Utilizing the enhanced Multi-Modal Tamper Description dataSet (MMTD-Set) and incorporating innovative modules for explainability and detection interpretation, FakeShield provides an advanced, explainable solution capable of accurately detecting and localizing various image tampering techniques, outperforming existing methods.

Federated Few-Shot Class-Incremental Learning
This study introduces the Federated Few-Shot Class-Incremental Learning (FFSCIL) problem and presents the Unified Optimized Prototype Prompt (UOPP) model to tackle issues like catastrophic forgetting, over-fitting, and prototype bias in this context. The UOPP model, which integrates task-wise prompt learning and adaptive dual heads, demonstrates superior performance compared to state-of-the-art methods, achieving significant improvements in both average and harmonic mean accuracy across multiple datasets.

F-Fidelity: A Robust Framework for Faithfulness Evaluation of Explainable AI
This paper introduces Fine-tuned Fidelity (F-Fidelity), an evaluation framework for eXplainable AI (XAI) that addresses the issue of out-of-distribution inputs and information leakage through a novel explanation-agnostic fine-tuning strategy and random masking operation. The framework is tested across various data modalities and shown to enhance prior evaluation metrics by accurately recovering the ground-truth ranking of explainers and computing the sparsity of influential input components.

Fine-Tuning Token-Based Large Multimodal Models: What Works, What Doesn’t and What's Next
This paper examines the advancements and challenges in fine-tuning token-based multimodal models, specifically the Chameleon architecture and its variant, Anole. The study highlights the integration of various data modalities and identifies key areas for future research in the fine-tuning process.

First-Person Fairness in Chatbots
This paper introduces a scalable counterfactual approach for evaluating "first-person fairness" in chatbots, focusing on demographic characteristics of users to assess bias. By employing a Language Model as a Research Assistant, the study offers both quantitative and qualitative bias analyses across millions of interactions, with findings corroborated by human annotations, and demonstrates that post-training reinforcement learning techniques effectively reduce biases in chat responses.

Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems
This paper investigates the vulnerability of Retrieval-In-Context RAG Language Models (LMs) to datastore leakage, demonstrating that adversaries can exploit these models' instruction-following capabilities to extract text data verbatim through prompt injection. The study highlights the susceptibility of a variety of modern LMs and proposes mitigation strategies like position bias elimination to address this risk, revealing that production RAG models, including customized GPTs, are particularly vulnerable to such attacks, achieving a high data extraction success rate with minimal queries.

Forking Paths in Neural Text Generation
This paper introduces a novel method for estimating uncertainty in Large Language Models (LLMs) by identifying key forking tokens that can significantly alter the generated text's outcome. The approach, which does not require fine-tuning or model weight access, is validated across various tasks and domains, revealing that minor tokens, such as a space instead of a colon, can lead to drastically different outputs, highlighting the potential variability in LLM responses.

FreeVS: Generative View Synthesis on Free Driving Trajectory
FreeVS is a fully generative approach that addresses the limitations of existing view synthesis methods by enabling the synthesis of camera views on novel and free trajectories in real driving scenes. By utilizing pseudo-image representations and viewpoint translation simulations, FreeVS ensures 3D consistency and accurate viewpoint pose, as demonstrated through strong image synthesis performance on the Waymo Open Dataset and the introduction of two new challenging benchmarks for driving scenes.

From Few to Many: Self-Improving Many-Shot Reasoners Through Iterative Optimization and Generation
This paper explores the efficiency of many-shot in-context learning (ICL) in long-context large language models, highlighting that a few influential examples often drive the benefits rather than sheer volume. It introduces BRIDGE, an algorithm that utilizes Bayesian optimization to identify and optimize influential examples, demonstrating significant performance improvements in tasks like symbolic reasoning, numerical reasoning, and code generation across various LLMs.

From Tokens to Words: On the Inner Lexicon of LLMs
This paper investigates whether large language models (LLMs) internally encode whole words from sub-word inputs and provides evidence of an intrinsic detokenization process within the models. The study reveals that LLMs can robustly form complete word representations from fragmented inputs, which allows for expanding vocabulary without finetuning and reduces both computational resources and latency while maintaining accuracy.

Gaussian-Det: Learning Closed-Surface Gaussians for 3D Object Detection
This paper introduces Gaussian-Det, a novel approach to multi-view 3D object detection using Gaussian Splatting as a continuous surface representation, overcoming the limitations of discrete data in monocular or NeRF-based methods. By incorporating a Closure Inferring Module (CIM) to manage outliers, Gaussian-Det achieves superior average precision and recall compared to existing methods on both synthetic and real-world datasets.

Generalization Bounds for Canonicalization: A Comparative Study with Group Averaging
This paper explores the generalization benefits and sample complexity of canonicalization, a method for creating invariant or equivariant function classes, compared to group averaging. The study identifies specific conditions where canonicalization can either outperform or underperform group averaging, marking the first theoretical investigation of these distinctions based on sample size and group action characteristics.

Generating  Graphs  via Spectral Diffusion
This paper introduces GGSD, a novel graph generative model that leverages the spectral decomposition of the graph Laplacian matrix and a diffusion process to efficiently reconstruct graph structures. By utilizing a truncated spectrum and a new transformer-based architecture, GGSD provides an accurate and scalable method for generating graphs, outperforming existing models in both synthetic and real-world experiments.

GenSE: Generative Speech Enhancement via Language Models using Hierarchical Modeling
This paper introduces GenSE, a novel framework for speech enhancement that leverages language models to incorporate rich semantic information, improving the intelligibility and quality of enhanced speech signals. By framing speech enhancement as a conditional language modeling task and using a hierarchical approach to generate semantic and acoustic tokens, the proposed method outperforms existing systems in speech quality and generalization, with experimental results validating its effectiveness.

Geometric Inductive Biases of Deep Networks: The Role of Data and Architecture
This paper introduces the geometric invariance hypothesis (GIH), which suggests that a neural network's input space curvature remains invariant under certain transformations during training, contingent on the architecture. Through a study of ResNets and MPLs on a non-linear binary classification problem, it is shown that ResNets fail to generalize based on the plane's orientation, and the paper proposes that this is due to architecture-dependent changes in input space curvature, supported by extensive experimental results.

Global Identifiability of Overcomplete Dictionary Learning via L1 and Volume Minimization
This paper introduces a new formulation for dictionary learning with an overcomplete dictionary, ensuring global identifiability of the dictionary under a mild condition on the sparse coefficient matrix. The work represents a significant advancement in both dictionary learning and general matrix factorization models by ensuring identifiability even when the latent dimension exceeds the ambient dimension, and includes a probabilistic analysis and an algorithm based on alternating minimization to solve the formulation.

GLOMA: Global Video Text Spotting with Morphological Association
This paper presents a novel Transformer-based global tracking method, named \model{}, for Video Text Spotting (VTS) that leverages Gaussian Wasserstein distance to enhance morphological correlations across frames. With substantial empirical validation, \model{} demonstrates significant performance improvements on public datasets, notably achieving 56.0 MOTA on the ICDAR2015 video dataset, marking an absolute improvement of 4.6 compared to the previous state-of-the-art.

HARDMath: A Benchmark Dataset for Challenging Problems in Applied Mathematics
The paper introduces $\textbf{HARDMath}$, a dataset designed to challenge Large Language Models (LLMs) with advanced applied mathematics problems requiring analytical approximation techniques. The evaluation reveals that even leading models like GPT-4 perform poorly, highlighting the need for more rigorous datasets to improve LLMs' mathematical reasoning and problem-solving capabilities at the graduate level.

Has the Deep Neural Network learned the Stochastic Process? An Evaluation Viewpoint
This paper introduces a novel evaluation criterion, Fidelity to Stochastic Process (F2SP), to assess Deep Neural Networks' (DNNs) ability to forecast stochastic complex systems, addressing limitations in traditional evaluation methods. Through a formal framework and empirical validation on synthetic and real-world datasets, the study demonstrates that Expected Calibration Error (ECE) uniquely captures F2SP, providing a more accurate measure of a DNN's modeling capabilities in capturing underlying stochastic processes.

Hessian-Free Online Certified Unlearning
This paper introduces an efficient Hessian-free unlearning approach designed to enable models to forget specific data while mitigating the computational challenges of existing methods. By maintaining a statistical vector for each training data and employing an online algorithm for near-instantaneous data removal, the proposed method significantly enhances unlearning performance and storage efficiency, achieving superior results in terms of time and accuracy compared to state-of-the-art techniques.

High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws
This paper provides a detailed analysis of knowledge distillation in high-dimensional ridgeless regression, focusing on model shift and distribution shift scenarios. It identifies the optimal surrogate model and explores the strengths and constraints of discarding less relevant features, showing that weak-to-strong generalization can excel over traditional training within the same data limits, though it cannot enhance data scaling laws, backed by numerical experiments.

How Much is a  Noisy Image Worth? Data Scaling Laws for Ambient Diffusion.
This paper examines the performance of generative models trained on corrupted data compared to those trained on clean data, finding that solely relying on noisy data cannot match the performance achieved with clean data. However, it demonstrates that combining a small amount of clean data with a larger noisy dataset can achieve performance akin to using only clean data, supported by theoretical insights into sample complexity bounds and the marginal utility of noisy versus clean samples.

Human-Aligned Chess With a Bit of Search
Allie is a chess-playing AI designed to model human-like behaviors and bridge the gap between artificial and human intelligence by training on real chess games, effectively predicting moves and pondering times similar to human players. In large-scale evaluations, Allie's adaptive Monte-Carlo tree search method achieves remarkable skill calibration across a wide range of player ratings, performing at grandmaster-level against top opponents while learning exclusively from human data.

Human-inspired Episodic Memory for Infinite Context LLMs
EM-LLM introduces a novel approach integrating aspects of human episodic memory into large language models, allowing them to efficiently handle practically infinite context lengths without fine-tuning. By organizing tokens into episodic events and using a two-stage retrieval process, EM-LLM achieves superior performance in various benchmarks compared to state-of-the-art models, and aligns closely with human event perception, offering insights into human memory mechanisms.

Improved Techniques for Optimization-Based Jailbreaking on Large Language Models
The paper introduces improved techniques for optimization-based jailbreaking of large language models, enhancing the Greedy Coordinate Gradient (GCG) attack by employing diverse target templates and an automatic multi-coordinate updating strategy. The proposed $\mathcal{I}$-GCG method demonstrates nearly 100% attack success rate on benchmarks, outperforming existing state-of-the-art jailbreaking attacks.

Improving Pretraining Data Using Perplexity Correlations
This paper introduces a framework for selecting high-quality pretraining data for language models without conducting costly pretraining runs. By leveraging correlations between LLM losses on pretraining texts and downstream benchmark performance, the proposed method outperforms existing data selection techniques and demonstrates increasing improvements with larger model scales, offering a novel and efficient approach to data selection.

Improving Uncertainty Estimation through Semantically Diverse Language Generation
This paper introduces Semantically Diverse Language Generation (SDLG), a novel method to quantify predictive uncertainty in large language models (LLMs), addressing one of the main causes of hallucinations. By generating semantically diverse alternatives and providing a precise measure of aleatoric semantic uncertainty, SDLG enhances the reliability of LLMs, outperforming existing methods in question-answering tasks and setting a new standard for computational efficiency in uncertainty estimation.

In-Context Editing: Learning Knowledge from Self-Induced Distributions
The paper introduces Consistent In-Context Editing (ICE), a method to update language models with new information efficiently by utilizing their in-context learning ability without extensive retraining. ICE enhances the robustness and effectiveness of tuning methods by optimizing towards contextual distributions, thereby preventing overfitting and preserving model integrity, with proven benefits across accuracy, locality, generalization, and linguistic quality.

InstaSHAP: Interpretable Additive Models Explain Shapley Values Instantly
This work establishes a variational perspective linking Generalized Additive Models (GAM) and SHAP explanations, offering insights into recent advancements in explainability. Utilizing this connection, a novel method is developed to train interpretable GAM models that compute the Shapley value efficiently in a single pass, and it is shown that both GAM models and SHAP share a limitation in representation power relevant to their application in computer vision and natural language processing.

Interpretable Unsupervised Joint Denoising and Enhancement for Real-World low-light Scenarios
This paper introduces a zero-reference joint denoising and low-light enhancement framework for real-world low-light images, leveraging physical imaging principles, retinex theory, and frequency domain decomposition using the Discrete Cosine Transform. By utilizing paired sub-images and a novel implicit-guided hybrid representation strategy, the proposed method effectively addresses complex degradations, outperforming existing techniques in extensive experiments.

Intervening Anchor Token: Decoding Strategy in Alleviating Hallucinations for MLLMs
This paper addresses the issue of hallucinations in multimodal large language models (MLLMs) by exploring the relationship between hallucinations and the models' summarization mechanism. The authors propose a novel decoding strategy, Dynamic Token Propagation Mechanism (TAME), which intervenes in the eigenspectrum variance of attention weights to reduce hallucinations without complex strategies, demonstrating its effectiveness in various MLLMs through extensive experiments.

Inverse Scaling: When Bigger Isn't Better
This paper presents evidence of inverse scaling in large language models, where increased scale can lead to worse performance on certain tasks due to issues in the training objective and data. By analyzing 11 datasets from the Inverse Scaling Prize contest, the authors identify four potential causes of inverse scaling and highlight that scaling trends may be less predictive for larger models, emphasizing the need for more thoughtful training approaches.

Joint Reward and Policy Learning with Demonstrations and Human Feedback Improves Alignment
This paper presents a new algorithm for improving the alignment of foundation models with human preferences by combining demonstration and human feedback data in estimating reward models, followed by reinforcement learning. The proposed approach outperforms existing alignment algorithms, showing significant improvements particularly in scenarios with imbalanced amounts of demonstration and preference data, validated through experiments on large language models and robotic control problems.

Knowledge Entropy Decay during Language Model Pretraining Hinders New Knowledge Acquisition
This paper explores the evolution of a model's knowledge integration during pretraining and introduces 'knowledge entropy' to measure the diversity of memory sources engaged by the model. The study finds that a decline in knowledge entropy, characterized by fewer active memory sources, negatively impacts the model’s ability to acquire and retain knowledge, while activating more memory sources improves these capabilities.

LaGeM: A Large Geometry Model for 3D Representation Learning and Diffusion
This paper presents a novel hierarchical autoencoder for compressing 3D models into a compact latent space, effectively handling large-scale datasets and generative modeling with diffusion. The proposed architecture not only reduces training time and memory usage but also enhances generative modeling through a cascaded diffusion framework, extending traditional approaches to unordered vector sets.

Language-Image Models with 3D Understanding
This paper presents CUBE-LLM, a novel multi-modal large language model pre-trained on a new large-scale dataset called LV3D, combining 2D and 3D recognition tasks to enhance 3D perception without requiring specific architectural changes. CUBE-LLM demonstrates superior performance in 3D grounded reasoning and complex driving scenario understanding, significantly surpassing existing baselines, while also exhibiting strong capabilities in 2D grounding and visual question answering benchmarks.

Large Scale Knowledge Washing
This paper addresses the challenge of unlearning specific factual knowledge in large language models without compromising their fluency or reasoning capabilities by introducing a method called LaW (Large Scale Washing). By updating the MLP layers within decoder-only models, LaW effectively removes targeted knowledge while preserving the model's abilities, and experimental results confirm its success, with plans to open-source the code.

Large (Vision) Language Models are Unsupervised In-Context Learners
This paper introduces a joint inference framework for fully unsupervised adaptation in large language and vision-language models, eliminating the need for manual prompt engineering and labeled examples. The proposed efficient approximation techniques, unsupervised fine-tuning and unsupervised In-Context Learning (ICL), show significant improvements over zero-shot inference, achieving up to 39% absolute improvement on the GSM8K math reasoning dataset and performing comparably to supervised methods.

LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics
LDAdam is a memory-efficient optimizer designed for training large models by conducting adaptive optimizations in lower dimensional subspaces while ensuring exploration of the full parameter space. The optimizer features a novel projection-aware update rule and a generalized error feedback mechanism, demonstrating convergence under standard assumptions and offering efficient fine-tuning and pre-training of language models.

LeanQuant: Accurate and Scalable Large Language Model Quantization with Loss-error-aware Grid
The paper introduces LeanQuant, a novel post-training quantization method designed to address the high memory and inference cost challenges of large language models by introducing loss-error-aware grids instead of traditional min-max affine grids. LeanQuant achieves more accurate quantization compatible with various frameworks and successfully scales to large models, exemplified by the effective quantization of Llama-3.1 405B utilizing only two Quadro RTX 8000-48GB GPUs.

Learn-by-interact: A Data-Centric Framework For Self-Adaptive Agents in Realistic Environments
Autonomous agents using large language models (LLMs) encounter limitations due to insufficient high-quality data from their interaction environments, which LEARN-BY-INTERACT aims to address through a novel, annotation-free data-centric framework. This approach improves agent performance in various tasks by creating synthetic agent-environment interaction data and employing backward construction, achieving notable advancements in both in-context learning and training scenarios, with improvements in baseline results by up to 23.1%.

Learning Equivariant Non-Local Electron Density Functionals
This paper introduces Equivariant Graph Exchange Correlation (EG-XC), a novel non-local exchange-correlation functional leveraging equivariant graph neural networks to improve the accuracy and scalability of density functional theory. Through efficient compression of the electron density into a nuclei-centered point cloud and applying an equivariant GNN, EG-XC demonstrates superior data efficiency and molecular size extrapolation, significantly reducing mean absolute errors compared to existing methods.

Learning Regularized Graphon Mean-Field Games with Unknown Graphons
This paper introduces reinforcement learning algorithms tailored for Graphon Mean-Field Games to find Nash Equilibrium without knowing the exact graphons. Key contributions include the GMFG-PPO algorithm which shows improved convergence rates, innovative estimation methods for unknown game components using kernel embedding, and empirical validation demonstrating reduced exploitability through simulations.

Linear Multistep Solver Distillation for Fast Sampling of Diffusion Models
This paper introduces a novel framework for designing solving strategies for diffusion models by presenting a unified prediction formula for linear multistep solvers and a solver distillation framework that allows a student solver to efficiently mimic a teacher solver with more steps. The proposed method drastically improves search efficiency and sampling acceleration, achieving superior Fréchet Inception Distance (FID) scores with just 5 function evaluations while completing solver searches for Stable-Diffusion in under 12 total GPU hours, outperforming previous reinforcement learning-based frameworks.

Linear Representations of Political Perspective Emerge in Large Language Models
This paper examines how large language models (LLMs) can replicate liberal and conservative political viewpoints in American politics by exploring their linear representations of these perspectives within activation space. By probing attention heads in transformer-based LLMs, the study reveals that specific attention head activations can predict political ideologies and news outlet slants, and demonstrates that model outputs can be steered toward desired stances through linear interventions, highlighting the potential to interpret and influence LLM-generated subjective perspectives.

Long-horizon Visual Instruction Generation with Logic and Attribute Self-reflection
LIGER is a novel training-free framework designed to generate consistent and accurate visual instructions for long-horizon tasks by employing logic and attribute self-reflection. By using a step-by-step generation approach and various image editing tools, LIGER enhances the logical consistency and correctness of object attributes in images, outperforming baseline methods in comprehensiveness, as demonstrated by a newly curated benchmark of long-horizon tasks.

Looking Inward: Language Models Can Learn About Themselves by Introspection
This paper investigates whether large language models (LLMs) possess introspective capabilities akin to humans, allowing them to predict their own behavior in hypothetical situations better than other models. Through experiments with models like GPT-4, the study finds evidence of LLMs having a form of privileged access to their own tendencies in simple tasks, paving the way for advancements in model transparency and explainability, although challenges remain in more complex tasks.

MADGEN: Mass-Spec attends to De Novo Molecular generation
MADGEN, a scaffold-based method for de novo molecular structure generation, improves the annotation of MS/MS spectra by using contrastive learning for scaffold retrieval and an attention-based generative model guided by mass spectrometry data. Evaluated on three datasets, MADGEN shows enhanced generation accuracy and effectiveness, particularly with an oracle retriever, by integrating spectral information throughout the process.

MambaExtend: A Training-Free Approach to Improve Long Context Extension of Mamba
The paper introduces MambaExtend, a training-free framework that enhances the context extension capabilities of the Mamba model by calibrating the scaling factors of discretization modules, enabling the transformation from handling 2k to 64k tokens with minimal perplexity increase. This approach significantly reduces parameter updates and memory usage compared to traditional fine-tuning methods while maintaining or improving performance in long-context tasks.

MaskBit: Embedding-free Image Generation via Bit Tokens
This study introduces an improved VQGAN model for class-conditional image generation, offering a transparent and high-performing alternative that matches current state-of-the-art methods. Additionally, it presents an innovative embedding-free generation network using bit tokens, achieving a new state-of-the-art FID of 1.52 on the ImageNet $256\times256$ benchmark with a compact 305M parameter generator, with code accessible at the provided GitHub link.

Maximizing the Potential of Synthetic Data: Insights from Random Matrix Theory
This paper explores the impact of using pruned synthetic data, identified by a score function, on training binary classifiers in high-dimensional settings, utilizing random matrix theory. It identifies conditions under which synthetic data enhances performance and demonstrates a gradual phase transition in label noise, with theoretical claims supported by experiments on toy and large language models.

MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization
The paper introduces MaxInfoRL, a framework designed to balance intrinsic and extrinsic exploration in reinforcement learning, steering exploration towards informative transitions by maximizing intrinsic rewards like information gain. This approach, when combined with Boltzmann exploration, effectively maximizes both the value function and entropy, resulting in sublinear regret in multi-armed bandits and superior performance in hard exploration tasks and complex visual control scenarios.

MGDA Converges under Generalized Smoothness, Provably
This paper addresses the limitations of existing multi-objective optimization algorithms in neural networks by studying generalized $\ell$-smooth loss functions, where $\ell$ is a non-decreasing function of gradient norm. The authors provide comprehensive convergence analysis for the multiple gradient descent algorithm (MGDA) and its variant, demonstrating that these methods effectively approximate the conflict-avoidant direction and ensure convergence to an $\epsilon$-accurate Pareto stationary point, with enhancements in efficiency and performance.

Mind the GAP: Glimpse-based Active Perception improves generalization and sample efficiency of visual reasoning
This paper introduces a novel Glimpse-based Active Perception (GAP) system that enhances visual relation understanding by sequentially focusing on the most salient regions of an image. By utilizing eye movement-inspired strategies, the system achieves state-of-the-art performance on visual reasoning tasks, demonstrating improved sample efficiency and better generalization to previously unseen objects compared to prior models.

Mini-Monkey: Alleviating the Semantic Sawtooth Effect for Lightweight MLLMs via Complementary Image Pyramid
This paper addresses the semantic discontinuity problem in high-resolution image processing by introducing a Complementary Image Pyramid (CIP) and a Scale Compression Mechanism (SCM) to enhance multimodal large language models (MLLMs). The proposed solutions enable models like Mini-Monkey to achieve superior performance in multimodal and document understanding tasks while being computationally efficient, outperforming larger models such as InternVL2-8B on benchmarks like OCRBench.

Mitigate the Gap: Improving Cross-Modal Alignment in CLIP
AlignCLIP addresses the modality gap in the CLIP embedding space by enhancing alignment between text and image embeddings, resulting in a more connected and less sparse hypersphere. It improves cross-modal alignment and demonstrates superior performance in zero-shot and fine-tuning downstream tasks by sharing learnable parameters between modality encoders and introducing a semantically-regularized separation objective.

Mix-CPT: A Domain Adaptation Framework via Decoupling Knowledge Learning and Format Alignment
The paper introduces a new domain adaptation framework called Mix-CPT for large language models (LLMs), which enhances domain knowledge learning and format alignment through a concurrent focus on knowledge memorization and utilization during continual pre-training. The approach uses a logit swap self-distillation constraint to prevent catastrophic forgetting and efficiently aligns format with minimal general training samples, improving LLMs' task-solving capabilities across both target and general domains.

Mixture-of-Agents Enhances Large Language Model Capabilities
This paper introduces a Mixture-of-Agents (MoA) methodology to harness the collective strengths of multiple large language models (LLMs) by constructing a layered architecture of LLM agents. The proposed approach outperforms state-of-the-art models, including GPT-4 Omni, on benchmarks such as AlpacaEval 2.0, showing significant improvements in natural language generation tasks.

MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents
This paper introduces MART, a novel method that enhances the performance of embodied agents by fine-tuning a multimodal language model (MLLM) retriever through preference learning to effectively prioritize task-relevant trajectories for unseen tasks. Utilizing a mechanism called Trajectory Abstraction to condense trajectory data while preserving essential information, the method significantly improves task success rates in various environments, establishing a new approach in multimodal retrieval for embodied agents.

MMAD: A Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection
This paper introduces MMAD, a benchmark specifically designed to evaluate Multimodal Large Language Models (MLLMs) in the context of industrial anomaly detection, addressing a previously unexplored area. Through a comprehensive evaluation of various state-of-the-art MLLMs using a dataset of 39,672 questions and 8,366 industrial images, the study highlights the considerable performance gaps in existing models and explores potential training-free strategies for improvement, thereby offering a valuable resource for future research in this domain.

MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models
The Multimodal Multi-image Understanding (MMIU) benchmark is introduced as a comprehensive evaluation suite to assess Large Vision-Language Models (LVLMs) across various multi-image tasks, revealing significant challenges, especially in spatial understanding. By evaluating nearly 30 LVLMs and identifying key performance gaps, MMIU aims to drive advancements in LVLM research and development, with its data and code made publicly available.

Multiagent Finetuning: Self Improvement with Diverse Reasoning Chains
This paper proposes a novel approach to improving large language models (LLMs) by finetuning a multiagent society of models, each specialized through interactions with others and trained on independent data sets. By enabling specialization and diversification among models, this method allows for more extended rounds of autonomous improvement and demonstrates enhanced performance across various reasoning tasks compared to conventional single-agent methods.

Multi-Label Test-Time Adaptation with Bound Entropy Minimization
This paper addresses the challenge of test-time adaptation (TTA) in multi-label scenarios by introducing a Bound Entropy Minimization (BEM) objective, which increases the confidence of multiple top predicted labels instead of just the most confident one. By incorporating paired captions and treating predicted labels from both views and captions collectively, the proposed ML--TTA framework with BEM demonstrates superior performance over state-of-the-art methods on multiple multi-label datasets like MSCOCO, VOC, and NUSWIDE.

Multi-Scale Fusion for Object Representation
This paper introduces Multi-Scale Fusion (MSF) to enhance Variational Autoencoder (VAE) guidance in Object-Centric Learning (OCL), addressing the challenge of varying object sizes in pixel representation. By leveraging an image pyramid for multi-scale intermediate representations and inter/intra-scale fusion to improve object super-pixels, the proposed technique improves performance on standard OCL benchmarks, surpassing current state-of-the-art methods.

Multi-Task Dense Predictions via Unleashing the Power of Diffusion
This paper introduces TaskDiffusion, a novel diffusion-based method for multi-task dense predictions that employs a joint denoising diffusion process to capture task relations and unifies the encoding strategy into a task-integration feature space. TaskDiffusion enhances efficiency and outperforms previous state-of-the-art methods across multiple dense prediction tasks on the PASCAL-Context and NYUD-v2 datasets.

Neural Interactive Proofs
This paper explores the development of neural interactive proofs, where a computationally bounded verifier interacts with more powerful, untrusted provers, represented by neural networks, to solve tasks. It introduces a unifying framework for prover-verifier games, proposes new interaction protocols, compares them theoretically, and demonstrates their effectiveness through experiments, laying the groundwork for safer AI systems.

Non-myopic Generation of Language Models for Reasoning and Planning
This paper introduces Predictive-Decoding, a method that applies Model Predictive Control to improve planning accuracy in Large Language Models (LLMs) by addressing their myopic nature. Experiments indicate that Predictive-Decoding enhances performance across math, coding, and agent-based tasks, achieving significant improvements and computational efficiency compared to search baselines.

OCEAN: Offline Chain-of-thought Evaluation and Alignment in Large Language Models
This paper presents OCEAN, an offline evaluation framework for enhancing chain-of-thought capabilities in large language models (LLMs) using knowledge graphs (KGs) like Wikidata5M. By addressing the challenges of reasoning heterogeneity with a Markov Decision Process and introducing the KG-IPS estimator, the study demonstrates efficient optimization of LLMs, improving reasoning path alignment and maintaining overall model performance in downstream tasks.

On Calibration of LLM-based Guard Models for Reliable Content Moderation
This paper investigates the reliability and calibration of LLM-based guard models, which are used to moderate content and ensure safety by blocking harmful outputs. By evaluating nine existing guard models across twelve benchmarks, the authors reveal issues of overconfidence and miscalibration, propose effective post-hoc calibration solutions such as temperature and contextual calibration, and emphasize the need for reliable confidence calibration in future model developments.

On Conformal Isometry of Grid Cells: Learning Distance-Preserving Position Embedding
This paper explores the conformal isometry hypothesis as an explanation for the hexagonal patterns seen in grid cell response maps, suggesting that these patterns arise from a high-dimensional vector representation of an agent's position in neural space. By conducting numerical experiments and theoretical analyses, the study demonstrates that hexagonal grid firing patterns result from a distance-preserving embedding within a neural manifold, highlighting the significance of this model in effective navigation planning.

On Statistical Rates of Conditional Diffusion Transformers: Approximation,  Estimation and Minimax Optimality
This paper provides a detailed analysis of the approximation and estimation rates for conditional diffusion transformers (DiTs) with classifier-free guidance under various assumptions and establishes their minimax optimality using score function regularity. By employing techniques such as grid discretization and term-by-term Taylor expansions, the study offers insights into achieving tighter bounds and informs efficient and accurate designs for DiTs.

On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization
This paper presents a fine-grained convergence analysis for a general class of adaptive gradient methods, such as AMSGrad, RMSProp, and AdaGrad, in the context of nonconvex optimization. The authors demonstrate that these methods converge in expectation to a first-order stationary point with improved convergence rates compared to existing results and establish high probability bounds for these rates, enhancing the understanding of their mechanisms in nonconvex settings.

On the Expressiveness of Rational ReLU Neural Networks With Bounded Depth
This paper extends the exploration of the expressive power of ReLU neural networks, focusing on the function \( F_n = \max (0, x_1, \ldots, x_n) \). It shows that ReLU networks with decimal fraction weights require at least \( \lceil \log_3 (n+1) \rceil \) hidden layers and networks with \( N \)-ary fraction weights require at least \( \Omega( \frac{\ln n}{\ln \ln N}) \) layers, offering a partial confirmation of a conjecture and establishing a significant lower bound on network depth.

On the Expressive Power of Sparse Geometric MPNNs
This paper investigates the ability of message-passing neural networks to distinguish between non-isomorphic geometric graphs, focusing on scenarios where nodes have limited information about nearest neighbors. The authors present EGENNET, an architecture that ensures separation of such graphs with rotation equivariant features, providing theoretical guarantees and showing favorable performance on synthetic and chemical datasets.

On the Fourier analysis in the SO(3) space : the EquiLoPO Network
This paper introduces the EquiLoPO Network, an equivariant neural network architecture designed for continuous rotational invariance in volumetric data, overcoming the constraints of previous methods by using unconstrained trainable filters and group convolutional operations with irreducible representations. The proposed model integrates into a ResNet-style architecture and demonstrates superior performance on 3D medical imaging datasets, highlighting the advantages of achieving true rotational equivariance on the SO(3) group.

On the Hölder Stability of Multiset and Graph Neural Networks
This paper introduces a novel framework called Hölder in expectation to analyze the separation quality of multiset and graph neural networks, addressing limitations in traditional separation notions that fail in practice with fixed finite precision. The authors demonstrate that common models often fall short in practice, and propose two new Message Passing Neural Networks (MPNNs) with improved separation capabilities, effectively classifying challenging adversarial examples and outperforming standard MPNNs on typical graph learning tasks.

On the Importance of Language-driven Representation Learning for Heterogeneous Federated Learning
FedGLCL is a novel federated learning framework that addresses the challenges of non-IID data by integrating global language and local image features using contrastive learning, thus eliminating the need for separate local training models. By employing a pre-trained text encoder, FedGLCL reduces variance in local feature representations and mitigates overfitting, significantly outperforming existing FL algorithms in various non-IID scenarios.

Overcoming Slow Decision Frequencies in Continuous Control: Model-Based Sequence Reinforcement Learning for Model-Free Control
Sequence Reinforcement Learning (SRL) is introduced as an RL algorithm that generates action sequences for lower decision frequencies, addressing the impractical timestep and reaction time requirements of current RL methods. By utilizing a model and actor-critic architecture with a "temporal recall" mechanism, SRL achieves performance comparable to state-of-the-art algorithms with reduced sample complexity, and its superiority is demonstrated through the newly proposed Frequency-Averaged Score (FAS) metric.

Pareto Low-Rank Adapters: Efficient Multi-Task Learning with Preferences
The paper introduces PaLoRA, a novel method that enhances Pareto Front Learning (PFL) by using task-specific low-rank adapters to address scalability, convergence, and memory issues faced by current PFL methodologies. By parameterizing the Pareto Front within these adapters' convex hull and utilizing a deterministic sampling schedule for preference vectors, PaLoRA improves trade-off selection during inference and significantly outperforms existing MTL and PFL approaches, while reducing memory overhead in large networks.

PARTNR: A Benchmark for Planning and Reasoning in Embodied Multi-agent Tasks
We introduce PARTNR, a benchmark for studying human-robot collaboration in household activities, featuring 100,000 tasks that address spatial, temporal, and heterogeneous agent constraints. Our analysis of state-of-the-art Large Language Models reveals significant limitations in coordination and task execution, but demonstrates that fine-tuning smaller models with planning data can match larger models' performance while being significantly faster, highlighting challenges and opportunities in collaborative robotics research.

Physics-informed Temporal Difference Metric Learning for Robot Motion Planning
This paper proposes a novel self-supervised temporal difference metric learning approach to improve motion planning in complex environments by solving the Eikonal equation more accurately. The method enforces Bellman's principle of optimality and incorporates metric learning, leading to superior performance compared to existing methods, especially in unseen environments and with robot configurations ranging from 2 to 12 degrees of freedom.

Preble: Efficient Distributed Prompt Scheduling for LLM Serving
Preble is a distributed LLM serving platform designed to optimize prompt sharing by co-optimizing KV state reuse and computation load-balancing through a new scheduling algorithm and hierarchical scheduling mechanism. Evaluations using real workloads demonstrate that Preble significantly improves performance over existing systems, reducing average latency by 1.5× to 14.5× and p99 latency by 2× to 10×.

Predictive Inverse Dynamics Models are Scalable Learners for Robotic Manipulation
This paper introduces Predictive Inverse Dynamics Models (PIDM), an innovative end-to-end approach that integrates vision and action by using inverse dynamics models conditioned on forecasted visual states, employing Transformers to enhance scalability in robotic manipulation. The proposed model, Seer, exhibits substantial performance improvements over state-of-the-art methods, demonstrating remarkable generalization across various conditions and achieving significant gains on multiple benchmarks and real-world tasks.

Preference Optimization for Reasoning with Pseudo Feedback
This study introduces a novel approach to generate pseudo feedback for reasoning tasks by evaluating solutions against test cases, enhancing preference optimization techniques for large language models. Experiments in mathematical reasoning and coding demonstrate improved performance, notably advancing results over existing models and surpassing benchmarks such as NuminaMath-72B and GPT-4-Turbo-1106-preview.

Privately Counting Partially Ordered Data
This paper introduces a problem-specific $K$-norm mechanism for differentially private counting with data points that are $d$ bits arranged in a partial order, achieving an efficient runtime of $O(d^2)$. Experimental results demonstrate that this mechanism significantly outperforms existing pure differentially private methods, potentially reducing error by an order of magnitude or more based on the partial order context.

Probabilistic Neural Pruning via Sparsity Evolutionary Fokker-Planck-Kolmogorov Equation
This paper introduces a novel approach to neural pruning by conceptualizing it as an expected loss minimization problem over mask distributions, leading to the development of the Sparsity Evolutionary Fokker-Planck-Kolmogorov Equation (SFPK) for guiding optimal subnetwork evolution under increasing sparsity. The proposed SFPK-pruner, a probabilistic method based on particle simulation, demonstrates competitive performance in various scenarios, offering mathematically grounded guidance and convergence guarantees, with code available for implementation.

ProtPainter: Draw or Drag Protein via Topology-guided Diffusion
ProtPainter is a diffusion-based method for generating protein backbones with precise topology control, utilizing a two-stage process involving curve-based sketching and sketch-guided backbone generation. It introduces new evaluation benchmarks, demonstrating its capability to produce topology-fit and designable protein structures, and emphasizes flexibility with drawing and dragging tasks.

Provable Convergence and Limitations of Geometric Tempering for Langevin Dynamics
This paper analyzes the effectiveness of geometric tempering in sampling from challenging multi-modal distributions using Langevin dynamics, providing both theoretical upper and lower bounds. The study reveals that while geometric tempering can offer optimal schedules under certain conditions, it may also lead to poor convergence and inefficiencies, challenging its reliability as a sampling approach.

Provable weak-to-strong generalization via benign overfitting
This paper explores the concept of weak-to-strong generalization in machine learning, where a weak teacher provides supervision to a strong student using imperfect pseudolabels within an overparameterized spiked covariance model with Gaussian covariates. The authors identify two asymptotic phases of generalization for the student and offer theoretical insights that could extend to multiclass classification, along with proving a tight lower tail inequality for correlated Gaussians.

Qinco2: Vector Compression and Search with Improved  Implicit Neural Codebooks
Qinco2 enhances the efficiency of multi-codebook quantization by employing improved vector encoding, a fast approximate decoder, and an optimized training procedure to address the suboptimal rate-distortion performance observed in residual quantization. Through experiments on four datasets, Qinco2 achieves significant advancements, notably reducing reconstruction MSE by 44% on BigANN for 16-byte compression and increasing search accuracy by 24% with 8-byte encodings on Deep1M, thus setting a new state-of-the-art in vector quantization.

QP-SNN: Quantized and Pruned Spiking Neural Networks
This paper introduces a lightweight and hardware-friendly Spiking Neural Network (SNN) called QP-SNN, designed for efficient deployment in resource-limited edge devices. By developing a weight rescaling strategy for quantization and a new pruning criterion based on singular values of spike activities, the authors enhance performance without compromising efficiency, achieving state-of-the-art results in edge intelligence computing.

RAG-SR: Retrieval-Augmented Generation for Neural Symbolic Regression
This paper presents a novel framework for symbolic regression that combines evolutionary feature construction with a neural network without relying on pre-training. By employing a language model trained through online supervised learning and incorporating retrieval-augmented generation and scale-invariant data augmentation, the approach achieves state-of-the-art accuracy across various regression algorithms and tasks.

Rare-to-Frequent: Unlocking Compositional Generation Power of Diffusion Models on Rare Concepts with LLM Guidance
This paper introduces R2F, a training-free approach that enhances the compositional generation capabilities of text-to-image (T2I) diffusion models for rare concept combinations using Large Language Model (LLM) guidance. The method improves diffusion inference by leveraging semantic knowledge in LLMs and demonstrates significant performance improvements over existing models on benchmarks, including the newly proposed RareBench.

Reassessing How to Compare and Improve the Calibration of Machine Learning Models
This paper reassesses the reporting of calibration metrics in machine learning, highlighting the limitations of current approaches that may appear effective without accounting for additional generalization metrics. It introduces a novel visualization method using a calibration-based decomposition of Bregman divergences, which jointly represents calibration and generalization errors, alongside proving new results and establishing the consistency of a kernel regression estimator for calibration error.

Reconciling Model Multiplicity for Downstream Decision Making
This paper addresses the issue of model multiplicity in downstream decision-making, where two predictive models with similar accuracy might recommend different actions. The authors propose a framework using multi-calibration to align model predictions and demonstrate improved decision-making accuracy and consistency, even with limited access to true probability distributions, multiple predictive models, and diverse action sets.

Reconstruction-Guided Policy: Enhancing Decision-Making through Agent-Wise State Consistency
This paper addresses the challenge of partial observability in multi-agent reinforcement learning by introducing Reconstruction-Guided Policy (RGP), which reconstructs agent-wise states to maintain inter-agent relationships and ensure consistency between training and execution phases. Experimental results in both discrete and continuous action environments demonstrate RGP's effectiveness over traditional methods, providing a significant contribution to improving decision-making in partially observable settings.

Refining CLIP's Spatial Awareness: A Visual-Centric Perspective
The paper identifies the limitation of CLIP in handling spatial information in tasks requiring precise spatial understanding and introduces the Spatial Correlation Distillation (SCD) framework to overcome this challenge. By preserving CLIP's spatial structure and using a lightweight Refiner to enhance spatial correlations, the proposed method significantly improves CLIP ViTs' performance in open-vocabulary dense prediction benchmarks, achieving state-of-the-art results.

Regret Bounds for Episodic Risk-Sensitive Linear Quadratic Regulator
This paper addresses the challenge of online adaptive control for risk-sensitive linear quadratic regulators in finite horizon episodic settings. By introducing a least-squares greedy algorithm and incorporating exploration noise when specific assumptions are unmet, the authors establish regret bounds and provide a pioneering analysis for episodic risk-sensitive linear quadratic regulators, supported by perturbation analysis of Riccati equations and evaluation of the performance criterion loss.

ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing
ReMoE is a fully differentiable Mixture-of-Experts (MoE) model designed to replace the conventional TopK+Softmax routing with a ReLU-based router, improving performance and scalability. It consistently outperforms traditional MoE models with superior scalability, efficient dynamic computation allocation, and domain specialization, as demonstrated across various experimental setups.

Representative Guidance: Diffusion Model Sampling with Coherence
The paper introduces Representative Guidance (RepG), a method to address incoherence in the diffusion sampling process by directing sampling towards a representative target using self-supervised representations. This approach refines image details and surpasses existing benchmarks, demonstrating the potential of pre-trained self-supervised models to enhance diffusion sampling performance, especially when combined with classifier-free guidance.

ReSi: A Comprehensive Benchmark for Representational Similarity Measures
This paper introduces the ReSi benchmark, the first comprehensive platform for evaluating representational similarity measures across different neural architectures and domains, with extensive tests, measures, and datasets included. By providing a robust and publicly available tool, the ReSi benchmark aims to advance research on neural architecture comparison, offering a foundation for systematic evaluation and novel exploration in machine learning.

Residual-MPPI: Online Policy Customization for Continuous Control
This paper introduces Residual-MPPI, a generic online planning algorithm that customizes continuous-control policies at execution time without the need for extensive retraining or access to the original training parameters. The algorithm successfully accomplishes few-shot and zero-shot policy customization, as demonstrated in experiments including tuning the champion-level racing agent Gran Turismo Sophy (GT Sophy) 1.0 for car racing tasks in the Gran Turismo Sport environment.

Rethinking Diffusion Posterior Sampling: From Conditional Score Estimator to Maximizing a Posterior
This paper challenges the effectiveness of Diffusion Posterior Sampling (DPS) in inverse problems, arguing that it aligns more with maximizing a posterior (MAP) rather than effectively estimating the conditional score. The authors propose enhancements to DPS by explicitly maximizing the posterior and using a lightweight conditional score estimator, which significantly improves DPS performance as demonstrated on 512×512 ImageNet images.

Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off
The paper presents Asymmetric Representation-regularized Adversarial Training (ARAT) to mitigate the robustness-accuracy trade-off in adversarial training by addressing gradient conflicts and mixture distribution problems. By incorporating an asymmetric invariance loss with stop-gradient operations and a split-BatchNorm structure, ARAT improves upon existing methods and offers new insights into adversarial defense and knowledge distillation-based approaches.

Rethinking Reward Model Evaluation: Are We Barking up the Wrong Tree?
Reward Models (RMs) are essential for aligning language models with human preferences, yet their evaluation via accuracy against a validation set inadequately reflects their impact on optimized policy performance. Our experiments indicate a weak correlation between RM accuracy and downstream performance, highlighting that similar accuracy levels can yield differing outputs and suggesting that accuracy alone is insufficient to assess RM quality due to potential overoptimization effects.

Rethinking the role of frames for SE(3)-invariant crystal structure modeling
The paper proposes a new approach to modeling crystal structures using graph neural networks by introducing the concept of dynamic frames, which provide atoms with a dynamic view of their local environment. By utilizing this concept in a transformer-based crystal encoder named CrystalFramer, the study demonstrates improved performance over traditional methods in predicting crystal properties.

Revisiting Large-Scale Non-convex Distributionally Robust Optimization
This paper revisits distributionally robust optimization (DRO) with non-convex smooth loss functions and introduces a simpler analysis framework that improves upon previous work by providing a more precise characterization of the dual problem's smoothness and gradient noise conditions. The authors develop a double stochastic gradient descent with clipping (D-SGD-C) and a variance-reduced method, achieving efficient convergence to an $\epsilon$-stationary point with lower gradient complexity, and demonstrate superior performance over existing methods in numerical tasks.

RMB: Comprehensively benchmarking reward models in LLM alignment
This paper introduces RMB, a comprehensive benchmark for evaluating reward models (RMs) in guiding the alignment of large language models, covering over 49 real-world scenarios with pairwise and Best-of-N evaluations. The benchmark reveals generalization defects in state-of-the-art RMs, suggests potential improvements with generative RMs, and raises open questions about evaluation criteria and methods, with evaluation code and datasets to be released upon publication.

Robust Function-Calling for On-Device Language Model via Function Masking
This paper introduces Hammer, a novel family of foundation models specifically designed for on-device function calling, addressing performance variability in current models due to over-fitting to specific naming conventions. Hammer employs an augmented dataset and function masking techniques to improve robustness and achieves state-of-the-art results across diverse benchmarks, with contributions including an open-source dataset for irrelevance detection and a tuning framework for enhanced model generalization.

Robust System Identification: Finite-sample Guarantees and Connection to Regularization
This paper addresses the challenge of learning nonlinear dynamical systems from limited sample trajectories by proposing a robust least squares estimation (LSE) framework that integrates robust optimization and regularization using Schatten $p$-norms. The approach offers non-asymptotic performance guarantees and significantly enhances system identification and online control tasks in practice, outperforming existing methods while avoiding the curse of dimensionality.

SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration
This paper introduces SageAttention, a novel quantization method for attention that significantly reduces computational complexity while maintaining accuracy, outperforming existing methods like FlashAttention2 and xformers in operations per second by 2.1x and 2.7x, respectively. This advancement in quantization for attention layers shows no notable loss in end-to-end metrics in various applications, enhancing large language processing, image, and video generation models.

SAMRefiner: Taming Segment Anything Model for Universal Mask Refinement
This paper presents SAMRefiner, a universal and efficient approach to enhance coarse masks for training segmentation models, thereby reducing annotation costs. The method utilizes a noise-tolerant prompting scheme combined with a split-then-merge pipeline and IoU adaption to improve mask accuracy, offering versatility across different benchmarks without the need for additional annotations.

Scale-Aware Contrastive Reverse Distillation for Unsupervised Medical Anomaly Detection
This paper introduces a novel scale-aware contrastive reverse distillation model for unsupervised anomaly detection, specifically addressing issues of feature discriminability and anomaly scale variations. By incorporating a contrastive student-teacher learning approach and a scale adaptation mechanism, the proposed model achieves state-of-the-art results on benchmark datasets, enhancing its applicability, particularly in medical imaging.

Scaling Long Context Training Data by Long-Distance Referrals
The paper addresses the challenge of training large language models for long context understanding by proposing LongPack, a data pipeline that constructs high-quality long documents from shorter ones based on referral relationships such as hyperlinks. LongPack significantly enhances training efficiency and quality, generating an extensive corpus with near-natural long document characteristics, and outperforms previous methods by 32.7% using only 0.5% of root documents.

Scaling Wearable Foundation Models
This paper introduces LSM, a multimodal foundation model leveraging vast amounts of wearable sensor data, including heart rate and accelerometer readings, from over 165,000 individuals to investigate scaling properties across compute, data, and model size. The study demonstrates LSM's capacity for efficient imputation, interpolation, and extrapolation tasks while enhancing sample-efficient learning for exercise and activity recognition.

SCOPE: A Self-supervised Framework for Improving Faithfulness in Conditional Text Generation
Large Language Models (LLMs) often produce unfaithful text during conditional generation tasks due to their reliance on learned statistical patterns. This paper introduces a novel self-supervised method and training process that significantly improve the faithfulness of generated text, outperforming existing techniques as demonstrated by automatic metrics, LLM-based assessments, and human evaluations.

Second-Order Min-Max Optimization with Lazy Hessians
This paper introduces an enhanced second-order method for convex-concave minimax optimization, improving computational complexity by reusing the Hessian across iterations. The proposed method achieves a complexity reduction factor of $d^{1/3}$ compared to previous techniques and generalizes effectively to strongly-convex-strongly-concave problems, with numerical experiments confirming its efficiency.

Self-Play Preference Optimization for Language Model Alignment
This paper introduces Self-Play Preference Optimization (SPPO), a novel method for language model alignment that models it as a constant-sum two-player game, working towards a Nash equilibrium policy by using iterative policy updates. SPPO demonstrates strong performance in aligning language models effectively without external supervision, achieving state-of-the-art results in length-controlled win-rate against other models on benchmarks like AlpacaEval 2.0, MT-Bench, and the Open LLM Leaderboard.

Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations
This paper introduces a novel method using rectified flow models for effective inversion and editing of real images, overcoming limitations faced by diffusion models in terms of faithfulness and editability. By employing dynamic optimal control and a rectified stochastic differential equation, the proposed approach surpasses existing methods in zero-shot inversion and semantic image editing, validated by extensive human evaluations.

SEPARATE: A Simple Low-rank Projection for Gradient Compression in Modern Large-scale Model Training Process
This paper introduces SEPARATE, a gradient compression method leveraging low-rank properties of gradients and Hessians in LLM training to address communication bottlenecks. SEPARATE doubles training speed in GPT-2-Medium pre-training and enhances performance in LLAMA2-7B fine-tuning, while maintaining convergence rates for general non-convex objectives.

Simple ReFlow: Improved Techniques for Fast Flow Models
This paper addresses the challenge of high neural function evaluations in diffusion and flow-matching models, which hinder their use in time-sensitive tasks. Through evaluating and improving the ReFlow procedure, the authors propose seven enhancements that significantly boost sample quality and achieve state-of-the-art FID scores with minimal NFEs across several datasets.

SiReRAG: Indexing Similar and Related Information for Multihop Reasoning
SiReRAG is a novel retrieval-augmented generation indexing approach that comprehensively considers both semantic similarity and relatedness to enhance knowledge synthesis. By creating unified retrieval pools from similarity and relatedness trees, SiReRAG achieves superior performance on multihop reasoning tasks, outperforming state-of-the-art methods with significant improvements in F1 scores across multiple datasets.

SMI-Editor: Edit-based SMILES Language Model with Fragment-level Supervision
SMI-Editor is a novel SMILES-based pre-trained language model that addresses the limitations of existing models by incorporating substructural information and using valid SMILES inputs, enhancing its ability to capture fragment-level molecular details. By disrupting and editing substructures within molecules, SMI-Editor significantly outperforms current models, demonstrating state-of-the-art results in various downstream molecular tasks.

SoftCVI: Contrastive variational inference with self-generated soft labels
Soft Contrastive Variational Inference (SoftCVI) is introduced as a new approach to Bayesian inference, capable of handling complex posterior geometries by reframing the inference task as a contrastive estimation problem. This method, which generates variational objectives with zero variance gradient and stability in training, empirically outperforms traditional variational approaches, showcasing improved performance on a range of Bayesian inference tasks.

SonicSim: A customizable simulation platform for speech processing in moving sound source scenarios
We introduce SonicSim, a synthetic toolkit leveraging the Habitat-sim platform to generate customizable data for moving sound sources, facilitating the creation of diverse and realistic synthetic datasets like SonicSet for evaluating speech separation and enhancement models. Our studies demonstrate that models trained on SonicSet generalize more effectively to real-world scenarios compared to other synthetic datasets, bridging the gap between synthetic and real-world data.

SpaceGNN: Multi-Space Graph Neural Network for Node Anomaly Detection with Extremely Limited Labels
The paper introduces SpaceGNN, a novel model for Node Anomaly Detection (NAD) that operates effectively with limited labels by exploring non-Euclidean spaces for node representation. By integrating the Learnable Space Projection function and conceiving the weighted homogeneity concept, the model outperforms existing methods, evidenced by an average improvement of 8.55% in AUC and 4.31% in F1 scores across 9 real datasets, offering a superior alternative to data augmentation techniques.

Sparse autoencoders reveal selective remapping of visual concepts during adaptation
This paper introduces PatchSAE, a Sparse Autoencoder for the CLIP vision transformer, designed to extract granular, interpretable concepts such as shape and color, and analyze their influence on downstream image classification tasks. The study reveals that most performance gains in adaptation tasks stem from pre-existing concepts within the non-adapted model, offering a framework for employing SAEs in Vision Transformers and enhancing understanding of adaptation mechanisms.

SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via Saliency-based Spiking
This paper introduces SpikeLLM, the first spiking large language model, which uses bio-inspired spiking mechanisms to achieve energy-efficient inference similar to the human brain. The approach, featuring Generalized Integrate-and-Fire neurons and an Optimal Brain Spiking framework, effectively reduces computational costs while improving performance metrics such as WikiText2 perplexity and accuracy in scene reasoning tasks compared to traditional quantized language models.

Sports-Traj: A Unified Trajectory Generation Model for Multi-Agent Movement in Sports
The paper introduces UniTraj, a Unified Trajectory Generation model designed to address limitations in multi-agent movement tasks by handling trajectory prediction, imputation, and spatial-temporal recovery simultaneously. By incorporating a novel Ghost Spatial Masking module and Bidirectional Temporal extensions, the model shows superior performance across newly curated sports datasets, enhancing the understanding of human movement in structured environments like sports games.

Spread Preference Annotation: Direct Preference Judgment for Efficient LLM Alignment
The paper introduces Spread Preference Annotation (SPA), a framework to enhance the alignment of large language models (LLMs) with human preferences using minimal human-annotated data. By leveraging human prior knowledge and generating self-annotated data, the approach significantly improves alignment performance, demonstrated by superior results on AlpacaEval 2.0 with only a fraction of ground-truth preference labels.

Stabilized Neural Prediction of Potential Outcomes in Continuous Time
This paper introduces SCIP-Net, a novel method for estimating conditional average potential outcomes (CAPOs) in continuous time, addressing the limitation of existing neural methods that operate under discrete time assumptions. SCIP-Net employs stabilized inverse propensity weights to robustly adjust for time-varying confounding, allowing for more practical modeling of patient trajectories with irregular measurement and treatment intervals.

Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization
This paper challenges the belief that standard Bayesian Optimization (BO) with Gaussian processes is inadequate for high-dimensional optimization, demonstrating through empirical and theoretical analyses that Matérn kernels outperform the commonly used Square Exponential (SE) kernel which suffers from improper length-scale initialization. The authors introduce a robust initialization strategy for the SE kernel that significantly enhances its performance, advocating for a re-assessment of standard BO's capabilities in high-dimensional scenarios.

Storybooth: Training-Free Multi-Subject Consistency for Improved Visual Storytelling
This paper addresses the challenge of consistent text-to-image generation for the same subjects across different images, particularly in visual storytelling and video generation contexts. By identifying self-attention leakage as a primary limitation, the authors propose a training and optimization-free approach that incorporates a bounded cross-attention and self-attention mechanism, along with a novel cross-frame token-merging layer, leading to significantly faster performance and improved multi-character consistency compared to previous methods.

Straightness of Rectified Flow: A Theoretical Insight into Wasserstein Convergence
The paper introduces Rectified Flow (RF), a novel generative model that straightens the trajectory from noise to data using convex optimization, thus reducing the number of function evaluations needed during sampling. The authors provide a new theoretical analysis of the Wasserstein distance and demonstrate that two rectifications are sufficient for a straight flow in empirical settings, aligning with previous observations.

SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement
SWE-Search is a multi-agent framework that integrates Monte Carlo Tree Search with large language models to enhance software agents' ability to adapt and refine their strategies for complex tasks. Applied to the SWE-bench benchmark, SWE-Search improved performance by 23% over standard agents, demonstrating the potential of self-evaluation techniques to enhance decision-making without relying on larger models or additional data.

Synthesizing Programmatic Reinforcement Learning Policies with Large Language Model Guided Search
This paper introduces a novel LLM-guided search framework (LLM-GS) that leverages large language models to improve the sample efficiency of programmatic reinforcement learning by generating Python code and converting it into domain-specific languages. The framework demonstrates superior effectiveness through a Pythonic-DSL strategy and Scheduled Hill Climbing algorithm, enabling users without programming skills to describe tasks in natural language and obtain high-performing programs, as evidenced by experiments in the Karel domain.

The 3D-PC: a benchmark for visual perspective taking in humans and machines
This paper explores whether deep neural networks (DNNs), after being trained on large image datasets, can perform visual perspective taking (VPT) by using the novel 3D perception challenge (3D-PC). The study reveals that while DNNs match human performance on simple 3D tasks, there is a significant disparity in VPT tasks, highlighting that current DNNs require improved architectures and training to emulate human-like reasoning in 3D perception; the authors provide their datasets and code for further research.

The Case for Cleaner Biosignals: High-fidelity Neural Compressor Enables Transfer from Cleaner iEEG to Noisier EEG
This paper introduces BrainCodec, a neural compressor for EEG and iEEG that achieves superior data reconstruction by leveraging the higher signal-to-noise ratio of iEEG. BrainCodec significantly improves compression without sacrificing quality and ensures high fidelity in downstream tasks like seizure detection, as confirmed by objective evaluations and expert neurologist assessments.

The Journey Matters: Average Parameter Count over Pre-training Unifies Sparse and Dense Scaling Laws
This paper conducts a comprehensive study on sparse pre-training configurations for large language models, finding an optimal schedule that balances training and inference costs effectively. The authors propose a modified scaling law that unifies dense and sparse model training, demonstrating that sparse pre-training can achieve similar loss to dense pre-training while producing smaller models and offering computational savings.

The Pitfalls of Memorization: When Memorization Hurts Generalization
This paper explores the relationship between memorization and generalization in neural networks, highlighting that memorization of data exceptions alongside spurious correlations undermines generalization. To combat this, the authors introduce memorization-aware training (MAT), which utilizes held-out predictions to guide models toward learning robust patterns, thereby enhancing generalization across different distributions.

The Power of LLM-Generated Synthetic Data for Stance Detection in Online Political Discussions
This paper addresses the challenge of stance detection in online political discussions by leveraging LLM-generated synthetic data to enhance performance. By using synthetic data from a Mistral-7B model and identifying informative samples in unlabelled datasets, the authors demonstrate significant improvements in stance detection, surpassing baseline models while requiring less labelled data.

Think Thrice Before You Act: Progressive Thought Refinement in Large Language Models
The paper introduces Progressive Thought Refinement (PTR), a framework that allows large language models (LLMs) to enhance their responses through a two-phase process involving data construction for logical consistency and a training phase that masks thoughts to teach self-improvement. Experimental results demonstrate that PTR significantly boosts LLM performance across various tasks without task-specific fine-tuning, highlighting its capability to improve response quality, especially in open-ended scenarios, and the method has been made open-source.

Three-in-One: Fast and Accurate Transducer for Hybrid-Autoregressive ASR
We introduce HAINAN, a novel architecture for speech recognition that enhances the Token-and-Duration Transducer (TDT) model by supporting both autoregressive and non-autoregressive inference modes, as well as a new semi-autoregressive method. Experiments show that HAINAN achieves efficiency comparable to CTC in non-autoregressive mode and TDT in autoregressive mode, while delivering superior accuracy, especially with its semi-autoregressive inference, making it a promising solution for real-world speech recognition tasks.

TIGER: Time-frequency Interleaved Gain Extraction and Reconstruction for Efficient Speech Separation
This paper introduces TIGER, a speech separation model that significantly reduces parameters and computational costs while maintaining high performance, particularly in low-latency environments. It also presents EchoSet, a new dataset that enhances the generalization ability of models in realistic acoustic scenarios, demonstrating TIGER's superior efficiency and effectiveness over existing state-of-the-art models like TF-GridNet.

Topological Zigzag Spaghetti for Diffusion-based Generation and Prediction on Graphs
This paper introduces Zigzag Spaghetti (ZS), a novel computationally efficient topological summary for improving the generalizability and robustness of graph diffusion models by leveraging zigzag persistence to capture intrinsic higher-order topological properties across different resolutions. The authors demonstrate through extensive experiments that integrating ZS into graph diffusion models enhances performance on graph classification and prediction tasks by up to 10% while offering theoretical stability guarantees.

Toward Efficient Multi-Agent Exploration With Trajectory Entropy Maximization
This paper introduces Trajectory Entropy Exploration (TEE), a novel method aimed at enhancing exploration in Multi-Agent Reinforcement Learning (MARL) by maximizing the entropy of agents' trajectories using a particle-based entropy estimator. By integrating TEE with MARL algorithms and deploying intrinsic rewards, the approach promotes diverse behaviors among agents and consistently outperforms existing methods in multi-agent tasks from various benchmarks.

Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs
This paper addresses the challenge of efficiently unlearning sensitive data from Large Language Models (LLMs) without compromising their reasoning and generative capabilities. By introducing two novel techniques—Inverted Hinge Loss and data-adaptive initialization for LoRA adapters—the authors offer a solution that minimizes the impact on generative performance and computational cost, as demonstrated on various benchmarks.

Training Free Exponential Context Extension via Cascading KV Cache
This paper introduces a novel caching mechanism using cascading sub-cache buffers to efficiently manage the context window in transformers, which is critical for tasks like few-shot learning and conditional generation. The proposed approach significantly reduces computational costs and latency, outperforming existing linear caching methods in benchmarks, and facilitating the deployment of large language models in real-world scenarios with extended context requirements.

Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models
The paper introduces LoRAM, a memory-efficient training strategy for Low-Rank Adaption (LoRA) in large language models, which mitigates the high memory footprint of original model parameters by training on a pruned model to create low-rank matrices for inference. With techniques like structured pruning and 4-bit quantization, LoRAM significantly reduces parameter storage costs and demonstrates improved performance, enabling training of a 70 billion parameter model on much less powerful hardware than traditionally required.

TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking
The paper introduces TypedThinker, a method to enhance Large Language Models (LLMs) by predicting and utilizing diverse reasoning types, such as inductive, abductive, and analogical reasoning, tailored to specific problems. This approach significantly improves LLM performance on logical and mathematical reasoning tasks by 3.4% to 7% across various benchmarks, without the need for knowledge distillation from larger models, making it applicable to advanced systems like GPT-4o and specialized models such as MetaMath.

Understanding the Impacts of GenAI Requires Understanding the Impact of Anthropomorphic AI
This paper argues that understanding the social impacts of generative AI requires examining the overlooked aspect of anthropomorphic behaviors in AI systems. It calls for more attention to be given to the development, deployment, and use of anthropomorphic AI to effectively map and address potential negative social impacts.

Unlearning-based Neural Interpretations
This paper critiques the use of static functions as baselines in gradient-based interpretations, which tend to inject biases by deviating from model behavior, leading to flawed attribution maps. It introduces a novel approach called $\texttt{UNI}$ that uses an adaptive, debiased baseline through perturbation toward an unlearning direction, offering more reliable explanations by smoothing decision boundaries and improving robustness and interpretability.

VCR: Pixel-Level Complex Reasoning by Restoring Occluded Text
We introduce the Visual Caption Restoration (VCR) task, which requires models to restore partially obscured texts in images using complex reasoning that bridges vision, text, and text embedded in images. Our study reveals that current vision-language models fail to match human performance on this task, emphasizing the difficulty of VCR and providing VCR-WIKI and VCR-HIDDEN datasets for future research.

Vision CNNs trained to estimate spatial latents learned similar ventral-stream-aligned representations
This study explores whether the primate ventral visual stream is optimized for estimating spatial latents, like object position and pose, as opposed to solely object categorization. By training convolutional neural networks on both spatial and category latents using synthetic image datasets, the research finds that models trained on spatial latents achieve comparable neural alignment to those trained on categories, suggesting that the ventral stream may not be exclusively optimized for categorization, and highlights the need for more refined methods to compare brain models.

Weak to Strong Generalization for Large Language Models with Multi-capabilities
This paper explores the challenge of Superalignment in large language models (LLMs), focusing on improving the transition from weak to strong model generalization across multiple capabilities. The proposed novel training framework, which uses reward models and a two-stage training method, enhances model performance by effectively selecting valuable data and improving generalization in multi-capability LLMs.

WebRL: Training LLM Web Agents via Self-Evolving Online Curriculum Reinforcement Learning
This paper introduces WebRL, a self-evolving online curriculum reinforcement learning framework that trains high-performance web agents using open large language models (LLMs). By addressing challenges such as task scarcity and feedback sparsity, WebRL significantly enhances the performance of Llama-3.1 models, achieving superior success rates on web tasks compared to existing state-of-the-art LLM-based web agents and surpassing GPT-4-Turbo.

When Graph Neural Networks Meet Dynamic Mode Decomposition
This paper explores the integration of Dynamic Mode Decomposition (DMD) with Graph Neural Networks (GNNs) to model the dynamics of graph-structured data, leveraging modern Koopman theory for effective approximation of nonlinear interactions. The proposed DMD-GNN models exhibit state-of-the-art performance on diverse learning tasks, showcasing their potential as powerful tools for complex graph prediction and link prediction tasks.

Why In-Context Learning Models are Good Few-Shot Learners?
This paper examines in-context learning (ICL) models through a learning-to-learn lens and compares them with traditional meta-learners to determine their performance advantages. It proves the expressiveness and learnability of ICL models, suggests strategies like meta-level meta-learning and curriculum learning to enhance ICL's domain adaptability and convergence, and highlights the role of pre-training in constructing effective learning algorithms while addressing generalizability concerns.

Words in Motion: Extracting Interpretable Control Vectors for Motion Transformers
This paper presents an analysis and modification of hidden states in transformer-based models, specifically for motion forecasting. By using linear probing and control vectors, the study achieves high accuracy, enabling mechanistic interpretation and zero-shot generalization with minimal computational cost.

X-Fi: A Modality-Invariant Foundation Model for Multimodal Human Sensing
This paper introduces X-Fi, a modality-invariant foundation model designed to enhance human sensing by enabling the flexible use of multiple sensor modalities without the need for retraining. By employing a transformer architecture and a novel "X-fusion" mechanism, X-Fi achieves state-of-the-art performance in human pose estimation and activity recognition, demonstrating its potential in scalable, multimodal sensing applications.

ZeroDiff: Solidified Visual-semantic Correlation in Zero-Shot Learning
ZeroDiff is a novel generative framework for Zero-shot Learning (ZSL), which utilizes diffusion mechanisms and contrastive representations to improve visual-semantic correlations and enhance classifier performance in identifying unseen classes. By incorporating diffusion augmentation, supervised-contrastive representations, and multiple feature discriminators, ZeroDiff effectively addresses the performance decline associated with limited seen class samples and demonstrates significant improvements over existing ZSL methods across various benchmarks.

### Miscellaneous Aspects of Machine Learning->Causality
Systems with Switching Causal Relations: A Meta-Causal Perspective
This paper introduces the concept of meta-causal states, which cluster classical causal models based on equivalent qualitative behaviors, addressing dynamic changes in causal relationships driven by agent actions or environmental tipping points. The study demonstrates that meta-causal states can be inferred from observed behavior and inherent system dynamics, offering a novel approach to understanding evolving causal graphs beyond context-dependent frameworks.

Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?
This paper examines the identifiability of explanations produced by Mechanistic Interpretability (MI) in neural networks, questioning whether a unique explanation can be guaranteed for a fixed behavior. The study identifies non-identifiability across various MI strategies, suggesting a pragmatic approach to explanations that meet predictive or manipulability standards, while also exploring less permissive criteria for those prioritizing uniqueness in understanding AI systems.

A Skewness-Based Criterion for Addressing Heteroscedastic Noise in Causal Discovery
The paper addresses the challenge of heteroscedastic noise in causal discovery by introducing heteroscedastic symmetric noise models (HSNMs) and a novel skewness-based criterion. The authors propose the \texttt{SkewScore} algorithm, which effectively determines causal direction in the presence of heteroscedastic noise and demonstrate its robustness and validity through theoretical insights and empirical studies.

### Miscellaneous Aspects of Machine Learning->General Machine Learning Techniques
Structural-Entropy-Based Sample Selection for Efficient and Effective Learning
This paper introduces Structural-Entropy-based Sample Selection (SES), a method that incorporates both global and local information to select informative and representative samples for machine learning models. By leveraging structural entropy and training difficulty, SES constructs a $k$NN-graph and utilizes importance-biased blue noise sampling, demonstrating effectiveness across supervised, active, and continual learning scenarios.

### Miscellaneous Aspects of Machine Learning->Kernel methods
Fast Summation of Radial Kernels via QMC Slicing
This paper addresses the challenge of fast computation of large kernel sums in kernel methods by introducing a slicing approach that utilizes random projections and fast Fourier summation. It demonstrates that the proposed quasi-Monte Carlo slicing method significantly outperforms existing techniques on standard test datasets.

MMD-Regularized Unbalanced Optimal Transport
This paper introduces a novel approach to the unbalanced optimal transport (UOT) problem using Maximum Mean Discrepancy (MMD) as a regularizer, offering an alternative paradigm to the traditional $\phi$-divergence regularization. The authors derive a dual form for MMD-regularized UOT, introduce a new estimator for real-world applications that mitigates the curse of dimensionality, and demonstrate through experiments that their approach outperforms existing UOT methods in various machine learning tasks.

### Miscellaneous Aspects of Machine Learning->Online Learning, Active Learning and Bandits
Budgeted Online Continual Learning by Adaptive Layer Freezing and Frequency-based Sampling
This paper addresses the challenge of fair comparison among online continual learning (CL) algorithms by proposing the use of floating point operations (FLOPs) and total memory size as standard metrics for computational and memory budgets. It introduces an adaptive layer freezing technique to reduce computational overhead and a memory retrieval method to enhance learning efficiency, demonstrating superior performance on various datasets under the same budget constraints.

### Miscellaneous Aspects of Machine Learning->Representation Learning
Boost Self-Supervised Dataset Distillation via Parameterization, Predefined Augmentation, and Approximation
The paper introduces Self-Supervised Dataset Distillation, a method designed to reduce dataset sizes by distilling images and their self-supervisedly trained representations into compact and highly representative exemplars. By implementing novel techniques, including innovative parameterization and utilizing predetermined augmentations, this approach improves cross-architecture generalizability and demonstrates enhanced efficiency and performance across various datasets.

Learning Structured Representations by Embedding Class Hierarchy with Fast Optimal Transport
This paper addresses the limitation of using class means in the Cophenetic Correlation Coefficient (CPCC) framework by proposing the use of Earth Mover's Distance (EMD) for measuring pairwise class distances in feature space, which generalizes previous work and recovers existing algorithms under Gaussian class-conditional distributions. Additionally, the authors introduce the Optimal Transport-CPCC family with four EMD approximation variants to improve computational efficiency, highlighting their most efficient variant that operates in linear time while maintaining competitive performance.

Learnable Expansion of Graph Operators for Multi-Modal Feature Fusion
This paper introduces a novel graph-based feature fusion method for computer vision tasks, effectively capturing structural relationships and deep feature interactions across diverse representations, domains, and modalities. By employing relationship graphs and a learnable graph fusion operator, the approach demonstrates robust performance, particularly in video anomaly detection, surpassing traditional high-dimensional fusion techniques.

### Miscellaneous Aspects of Machine Learning->Transfer, Multitask and Meta-learning
LoRA-X: Bridging Foundation Models with Training-Free Cross-Model Adaptation
The paper introduces Cross-Model Low-Rank Adaptation (LoRA-X), a novel adapter that enables training-free transfer of LoRA parameters between models without the need for original or synthetic training data. By operating within the source model's subspace and targeting layers with sufficient subspace similarity, LoRA-X effectively facilitates fine-tuning across models, as demonstrated by experiments on text-to-image generation tasks such as Stable Diffusion v1.5 and Stable Diffusion XL.

### Miscellaneous Aspects of Machine Learning->Unsupervised and Semi-supervised Learning
PIG: Physics-Informed Gaussians as Adaptive Parametric Mesh Representations
This paper introduces Physics-Informed Gaussians (PIGs), a novel approach for numerically approximating partial differential equations with neural networks by combining Gaussian function feature embeddings and lightweight neural networks. PIGs improve upon traditional Physics-Informed Neural Networks by dynamically adjusting Gaussian parameters for better approximation of complex PDEs, showing competitive performance without requiring high-resolution grids or numerous collocation points.

### Optimization->Everything Else
Leveraging Variable Sparsity to Refine Pareto Stationarity in Multi-Objective Optimization
This paper addresses limitations of Pareto stationarity in gradient-based multi-objective optimization (MOO) when dealing with sparse function-variable structures by introducing a novel solution concept called Refined Pareto Stationarity (RPS). The authors present an efficient algorithm to identify function-variable dependencies, enhancing the performance of MOO by converging to RPS, validated through experiments that demonstrate its effectiveness in improving existing methods.

### Optimization->Large Scale, Parallel and Distributed
Towards Faster Decentralized Stochastic Optimization with Communication Compression
Communication efficiency is a critical challenge in decentralized machine learning, as clients are limited to sharing compressed data over a network. This paper introduces MoTEF, the first algorithm to achieve performance parity with distributed SGD under any data heterogeneity, addressing a key theoretical gap and demonstrating superior practical performance through experiments.

Methods with Local Steps and Random Reshuffling for Generally Smooth Non-Convex Federated Optimization
This paper addresses the challenge of non-convex machine learning problems in Federated Learning by proposing and analyzing new methods involving local steps, partial client participation, and Random Reshuffling, based solely on a generalized smoothness assumption. These methods are evaluated under the Polyak-Łojasiewicz condition, showing theoretical and experimental consistency with known results for standard smooth problems.

### Optimization->Learning for Optimization
DRoC: Elevating Large Language Models for Complex Vehicle Routing via Decomposed Retrieval of Constraints
The paper introduces Decomposed Retrieval of Constraints (DRoC), a framework that enhances large language models (LLMs) by integrating external knowledge, enabling them to solve complex vehicle routing problems (VRPs) with intricate constraints. DRoC improves program accuracy and reduces errors by synergistically leveraging retrieval-augmented generation (RAG) and self-debugging mechanisms, showcasing significant performance improvements across 48 VRP variants and expanding the applicability of LLMs in transportation and logistics.

Minimalistic Predictions for Online Class Constraint Scheduling
This paper addresses the challenge of online scheduling with class constraints by introducing algorithms that work in a learning-augmented setting, allowing them to access potentially erroneous predictions. The authors provide new algorithms with competitive ratios that do not depend on the number of machines and establish tight lower bounds, highlighting which types of additional information can improve scheduling algorithm performance.

When GNNs meet symmetry in ILPs: an orbit-based feature augmentation approach
This paper addresses the challenge of applying graph neural networks (GNNs) to integer linear programs (ILPs) with symmetry, which can reduce predictive accuracy due to the difficulty in distinguishing symmetric variables. The authors propose an orbit-based feature augmentation scheme that groups symmetric variables and improves training efficiency and predictive performance by sampling augmented features, demonstrating its effectiveness empirically.

### Optimization->Non-Convex
Edge-aware Image Smoothing with Relative Wavelet Domain Representation
This paper addresses the challenges of image smoothing, specifically issues like gradient reversals and halo artifacts, by proposing a novel edge-aware smoothing model using a Relative Wavelet Domain Representation (RWDR). By incorporating an innovative edge-aware scale map into an adaptive bilateral filter, the method effectively balances smoothing intensity and edge preservation, with extensive experiments showing its superiority over previous algorithms in edge-preserving and artifact removal.

Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation
This paper addresses the weighted low rank approximation problem, which extends the low rank matrix completion problem and is NP-hard. The authors present an efficient framework for alternating minimization that reduces the runtime complexity from \( \|W\|_0k^2 \) to \( \|W\|_0k \) by using a high-accuracy multiple response regression solver, offering improved performance under mild assumptions.

Nonconvex Stochastic Optimization under Heavy-Tailed Noises: Optimal Convergence without Gradient Clipping
This paper challenges the necessity of gradient clipping in handling heavy-tailed noises by demonstrating that the Batched Normalized Stochastic Gradient Descent with Momentum (Batched NSGDM) algorithm can achieve optimal convergence rates without it. Furthermore, the authors establish a notable convergence rate when the tail index is unknown, addressing a common practical scenario.

### Optimization->Optimization and Learning under Uncertainty
Variational Search Distributions
VSD is a method for conditioning generative models on rare desired classes through efficient evaluation in a batch sequential manner, termed as active generation, and utilizes variational inference to fulfill this task. It showcases the ability to outperform baseline methods in sequence-design problems for protein and DNA/RNA engineering by leveraging gradient-based optimization and scalable predictive models.

### Probabilistic Methods->Bayesian Models and Methods
ELBOing Stein: Variational Bayes with Stein Mixture Inference
Stein Mixture Inference (SMI) enhances Stein variational gradient descent (SVGD) by modeling each particle as a component in a mixture model, optimizing a lower bound to the evidence (ELBO), and utilizing user-specified guides. This method effectively mitigates variance collapse in Bayesian inference, requiring fewer particles than traditional SVGD while performing well across standard datasets and application scenarios.

### Probabilistic Methods->Everything Else
Conditional Testing based on Localized Conformal $p$-values
This paper introduces localized conformal $p$-values by inverting prediction intervals to address conditional testing problems, proving their theoretical properties and demonstrating their practical applications. The authors propose procedures for conditional outlier detection, conditional label screening, and a two-sample conditional distribution test, all of which show superior performance in simulations and real-data examples while controlling for error rates such as FDR and FWER.

### Probabilistic Methods->Variational Inference
Improved Finite-Particle Convergence Rates for Stein Variational Gradient Descent
This paper establishes finite-particle convergence rates for the Stein Variational Gradient Descent (SVGD) algorithm using the Kernelized Stein Discrepancy (KSD) and Wasserstein-2 metrics, revealing a convergence rate of order $1/\sqrt{N}$ that significantly improves upon previous results. By incorporating a bilinear component in the kernel, the study also explores Wasserstein-2 convergence rates and analyzes the curse-of-dimensionality, marginal convergence, and long-time propagation of chaos in the context of particle system dynamics.

### Reinforcement Learning->Batch/Offline
Fat-to-Thin Policy Optimization: Offline Reinforcement Learning with Sparse Policies
This paper introduces Fat-to-Thin Policy Optimization (FtTPO), the first offline policy optimization algorithm designed to address the challenges posed by sparse continuous policies, which are crucial for safety-critical applications like medicine. FtTPO leverages a heavy-tailed proposal policy to effectively learn from datasets and inform a sparse policy, demonstrating its effectiveness in both safety-critical simulations and standard benchmark environments such as MuJoCo.

Cross-Domain Off-Policy Evaluation and Learning for Contextual Bandits
This paper introduces a new problem setup for Cross-Domain Off-Policy Evaluation and Learning (OPE/L), which addresses challenges in scenarios such as few-shot data, deterministic logging policies, and new actions by utilizing logged datasets from multiple domains. The proposed estimator and policy gradient method demonstrate significantly improved OPE/L performance by leveraging both target and source data, offering robust solutions for applications in personalized medicine, content recommendations, education, and advertising.

### Reinforcement Learning->Deep RL
Zero-shot Model-based Reinforcement Learning using Large Language Models
This paper explores the integration of pre-trained Large Language Models (LLMs) with continuous state spaces in reinforcement learning and introduces Disentangled In-Context Learning (DICL) to address challenges in handling multivariate data and control signals. It demonstrates the effectiveness of DICL in model-based policy evaluation and data-augmented off-policy reinforcement learning, providing both theoretical analysis and proof-of-concept applications, and offers code availability for further research.

Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation
This paper introduces Soft Analytic Policy Optimization (SAPO), a novel deep reinforcement learning algorithm, alongside Rewarped, a parallel differentiable multiphysics simulation platform, to address the challenges of training on tasks involving both rigid bodies and deformables. By leveraging first-order analytic gradients and supporting a variety of materials, SAPO and Rewarped enable more efficient training and demonstrate superior performance over existing methods in complex manipulation and locomotion tasks.

Distilling Reinforcement Learning Algorithms for In-Context Model-Based Planning
The paper introduces Distillation for In-Context Planning (DICP), an in-context model-based RL framework leveraging Transformers to learn environment dynamics and improve policy without the shortcomings of imitating existing RL algorithms. DICP achieves state-of-the-art performance across various environments with fewer interactions compared to traditional model-free and meta-RL methods.

Learning Transformer-based World Models with Contrastive Predictive Coding
TWISTER is a Transformer-based world model that enhances agent performance by using action-conditioned Contrastive Predictive Coding to learn high-level temporal feature representations, addressing the limitations of previous state prediction objectives. It sets a new benchmark with a human-normalized mean score of 162% on the Atari 100k task, outperforming existing methods without utilizing look-ahead search.

Kinetix: Investigating the Training of General Agents through Open-Ended Physics-Based Control Tasks
This paper introduces Kinetix, a novel framework for training a general reinforcement learning agent by procedurally generating a vast array of 2D physics-based tasks utilizing the high-performance Jax2D physics engine. The study demonstrates that the pre-trained agent, capable of solving complex environments without prior exposure, showcases superior proficiency when fine-tuned, highlighting the potential of large-scale pre-training for sequential decision-making tasks.

On Generalization Across Environments In Multi-Objective Reinforcement Learning
This paper addresses the challenge of generalization in Multi-Objective Reinforcement Learning (MORL), highlighting the limitations of existing approaches that focus on static environments with scalar rewards. The authors propose a novel benchmark for evaluating generalization across diverse multi-objective domains and demonstrate that current state-of-the-art MORL algorithms have limited generalization capabilities, underscoring the necessity for multi-objective specifications and laying a foundation for future research in this area.

Causal Information Prioritization for Efficient Reinforcement Learning
This paper introduces Causal Information Prioritization (CIP), a novel method that enhances reinforcement learning efficiency by identifying causal relationships between states, actions, and rewards using factored Markov Decision Processes (MDPs). CIP significantly improves exploration and learning in complex environments by integrating counterfactual data augmentation and a causality-aware empowerment learning objective, as demonstrated by its superior performance across 39 tasks in diverse control environments.

Reinforcement Learning for Control of Non-Markovian Cellular Population Dynamics
This paper explores the challenge of drug dosing in cell populations with phenotypic plasticity and memory, which complicates efforts to drive these cells toward extinction. By applying reinforcement learning (RL), the study demonstrates that model-free deep RL can effectively develop dosing strategies even in complex, non-Markovian systems, suggesting potential advancements in controlling cell populations with dynamic memory effects.

### Reinforcement Learning->Everything Else
Correlated Proxies: A New Definition and Improved Mitigation for Reward Hacking
This paper introduces a new definition of reward hacking in reinforcement learning, based on the correlation between proxy and true rewards, which breaks down during optimization. It proposes a theoretical and empirical solution using regularization of the $\chi^2$ divergence between policies' occupancy measures, demonstrating its effectiveness in preventing reward hacking across multiple settings, including reinforcement learning from human feedback.

$q$-exponential family for policy optimization
This paper introduces the $q$-exponential family as a flexible and tractable alternative to Gaussian policies for policy optimization in continuous action spaces, highlighting its ability to specify both heavy-tailed and light-tailed behaviors. The study demonstrates that heavy-tailed $q$-exponential policies, particularly the Student's t-distribution, generally outperform Gaussian policies in various actor-critic algorithms, with the heavy-tailed $q$-Gaussian showing consistent improvements in offline benchmark problems.

FOSP: Fine-tuning Offline Safe Policy through World Models
This paper presents a method to enhance safety in vision-based robotic tasks by fine-tuning an offline pretrained policy through online model-based reinforcement learning, improving data efficiency and safe generalization in unseen scenarios. It is validated on simulation benchmarks and real-world tasks, demonstrating significant improvements in safety-constrained environments, and is the first to explore offline-to-online RL for such tasks.

UTILITY: Utilizing Explainable Reinforcement Learning to Improve Reinforcement Learning
This paper addresses two major challenges in reinforcement learning: lack of explainability and suboptimal performance of RL agents, by introducing explainable reinforcement learning (XRL) which provides two-level explanations to identify and correct mistakes of the RL agent. The proposed method formulates a constrained bi-level optimization problem to enhance RL performance by utilizing high- and low-level explanations, with experimental results on MuJoCo showing that it outperforms state-of-the-art baselines and is theoretically proven to achieve global optimality.

### Reinforcement Learning->Inverse
Understanding Constraint Inference in Safety-Critical Inverse Reinforcement Learning
This paper presents Inverse Reward Correction (IRC), a method that embeds constraint signals into reward functions to solve the constraint inference problem in reinforcement learning with lower sample complexity compared to Inverse Constrained Reinforcement Learning (ICRL). While IRC reduces complexity, it compromises generalizability and safety, which can be improved by transferring constraints using ICRL, as demonstrated through theoretical analysis and empirical validation in various environments.

### Reinforcement Learning->Multi-agent
InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma
InvestESG is a multi-agent reinforcement learning benchmark aimed at understanding the effects of ESG disclosure mandates on corporate climate investments. The study reveals that ESG-conscious investors can enhance corporate mitigation efforts and reduce climate risks, supporting policy development by simulating various strategies and their socio-economic impacts.

Exponential Topology-enabled Scalable Communication in Multi-agent Reinforcement Learning
This paper presents a scalable communication protocol for cooperative multi-agent reinforcement learning (MARL) by adopting a global perspective on communication topology design, specifically leveraging exponential topology to enhance information dissemination among agents. The proposed protocol, ExpoComm, is shown through extensive experiments to outperform existing strategies in large-scale cooperative benchmarks, offering robust performance and zero-shot transferability.

Near-Optimal Online Learning for Multi-Agent Submodular Coordination: Tight Approximation and Communication Efficiency
This paper presents the MA-OSMA and MA-OSEA algorithms to improve coordination of multiple agents for maximizing submodular functions in unpredictable environments, addressing limitations of existing methods like the OSG algorithm. The proposed algorithms enhance approximation guarantees and communication flexibility, offering a significant improvement in regret bounds and effectiveness, as demonstrated in multi-target tracking simulations.

### Reinforcement Learning->Online
Extreme Risk Mitigation in Reinforcement Learning using Extreme Value Theory
This paper introduces the Extreme Valued Actor-Critic (EVAC) approach, which leverages extreme value theory to enhance the resilience of reinforcement learning (RL) agents against rare and risky events, addressing the challenge of high variance estimation in risk-aware RL. By parameterizing extreme values within the state-action value function distribution, the proposed method outperforms existing risk-averse RL algorithms across various benchmark tasks, demonstrating significant improvements in handling infrequent risk scenarios.

Doubly Optimal Policy Evaluation for Reinforcement Learning
This paper introduces an optimal method for policy evaluation in reinforcement learning, addressing the issue of high variance resulting from improper data-collecting policies and data-processing methods. The proposed doubly optimal evaluation technique is proven to be unbiased and achieves lower variance than existing approaches, demonstrating superior empirical performance and requiring less data for accurate evaluation.

Don't flatten, tokenize! Unlocking the key to SoftMoE's efficacy in deep RL
The paper investigates the performance gains in reinforcement learning when using SoftMixtures of Experts (SoftMoEs) and finds that tokenizing the encoder output is the primary factor behind these improvements, rather than the use of multiple experts. This discovery suggests that performance can be maintained with a properly scaled single expert by leveraging tokenization, providing a more efficient approach to handle deep neural network models in RL.

### Social Aspects->Accountability, Transparency and Interpretability
Watermark Anything With Localized Messages
The paper introduces the Watermark Anything Model (WAM), a deep-learning approach to localized image watermarking that modifies images imperceptibly, allowing the segmentation and recovery of hidden messages from watermarked areas. Experiments demonstrate that WAM surpasses state-of-the-art methods in imperceptibility and robustness, even for high-resolution images, and uniquely locates and extracts distinct messages from small watermarked regions, with training and inference resources available for further exploration.

PhyloLM: Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks
PhyloLM is a novel approach that adapts phylogenetic algorithms to assess the relationships and predict performance characteristics of Large Language Models (LLMs) by calculating a phylogenetic distance metric based on output similarity. This method effectively constructs dendrograms to map known model relationships and demonstrates predictive power for performance in benchmarks, offering a cost-effective tool for evaluating LLM capabilities without requiring detailed training information.

PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained Image Classification Accuracy for AIs and Humans
This paper presents a novel use of nearest neighbors to enhance the predictions of a frozen, pretrained image classifier by employing an image comparator to refine prediction confidence scores. The method improves fine-grained image classification accuracy across multiple datasets and reduces human over-reliance on AI by providing users with probable-class nearest neighbors, enhancing decision accuracy compared to traditional methods.

How to Probe: Simple Yet Effective Techniques for Improving Post-hoc Explanations
This study challenges the common assumption that post-hoc importance attribution methods for explaining Deep Neural Networks are unaffected by training details. By demonstrating that the classification layer's training intricacies significantly impact explanation quality more than the pre-training scheme, the work underscores the need for adjusted attribution methods and proposes enhancements validated across various visual pre-training frameworks and metrics.

Generating Likely Counterfactuals Using Sum-Product Networks
This paper presents a system for generating counterfactual explanations that are both plausible and minimally different from the original instance, addressing the conflicting objectives of distance and plausibility. By modeling the problem with Mixed-Integer Optimization (MIO) and leveraging a Sum-Product Network (SPN) to estimate likelihood, the authors provide a novel approach with available source code, which could be utilized in future AI explainability research.

Bridging the Data Provenance Gap Across Text, Speech, and Video
This paper presents a comprehensive longitudinal audit of nearly 4000 public datasets across text, speech, and video modalities, analyzing their sourcing trends, use restrictions, and geographical and linguistic representation from 1990-2024. The study reveals that while multimodal applications heavily rely on web-crawled and social media platforms for training data, significant gaps remain in geographical and multilingual diversity, and it provides a detailed audit to enhance dataset transparency for responsible AI development.

CONDA: Adaptive Concept Bottleneck for Foundation Models Under Distribution Shifts
The paper investigates the application of Concept Bottleneck Models (CBMs) to make foundation models more interpretable, particularly in critical domains like healthcare and finance, by using high-level concept vectors. It introduces an adaptive framework to improve CBM performance under distribution shifts during test-time, enhancing interpretability and achieving up to a 28% accuracy increase with real-world data, aligning their performance with non-interpretable models.

### Social Aspects->Everything Else
Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View
This paper introduces CogMir, a framework that utilizes the hallucination properties of Large Language Models (LLMs) to assess and enhance their social intelligence by mimicking human cognitive biases. Experimental results reveal a high consistency between LLM Agents and humans in making irrational and prosocial decisions under uncertainty, suggesting that LLM Agents can function as complex social entities and highlighting the potential of CogMir for further research into their social intelligence.

### Social Aspects->Fairness, Equity, Justice and Safety
Do as We Do, Not as You Think: the Conformity of Large Language Models
This paper explores conformity in large language model-driven multi-agent systems, identifying factors influencing conformity and evaluating its impact on problem-solving capabilities. Introducing the BenchForm benchmark, the study investigates strategies to mitigate conformity effects, aiming to foster more robust and ethically-aligned collaborative AI systems.

DarkBench: Benchmarking Dark Patterns in Large Language Models
DarkBench is a benchmark designed to identify and evaluate dark design patterns—manipulative techniques influencing user behavior—in large language models (LLMs). The study analyzes 660 prompts across six categories and evaluates LLMs from five major companies, revealing biases and manipulation that underline the need for more ethical AI development.

Does Refusal Training in LLMs Generalize to the Past Tense?
This paper identifies a significant gap in refusal training for large language models (LLMs), where reformulating harmful requests in the past tense can significantly increase the success rate of jailbreaks in these systems. The study demonstrates that while LLM refusal techniques often fail when faced with past-tense reformulations, incorporating such examples in training data can improve resilience, thereby challenging the effectiveness of current alignment methods like SFT, RLHF, and adversarial training.

SANER: Annotation-free Societal Attribute Neutralizer for Debiasing CLIP
This paper addresses the issue of societal bias in large-scale vision-language models like CLIP by introducing a debiasing method called SANER (societal attribute neutralizer). Unlike previous methods, SANER effectively eliminates attribute bias without relying on attribute annotations or losing important information, showing superior debiasing performance in experimental results.

### Social Aspects->Privacy-preserving Statistics and Machine Learning
DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction
This paper introduces DiSK, a novel framework that employs Kalman filtering to improve the performance of differential privacy (DP) optimizers by denoising privatized gradients, resulting in better gradient estimations. Through theoretical analysis and extensive experiments, DiSK is shown to enhance performance, surpassing state-of-the-art DP optimizers in vision and language tasks under the same privacy constraints.

### Social Aspects->Trustworthy Machine Learning
BadRobot: Jailbreaking Embodied LLMs in the Physical World
This paper introduces BadRobot, an attack paradigm designed to highlight the vulnerabilities of embodied AI systems integrated with Large Language Models (LLMs) by causing them to violate safety and ethical guidelines through voice-based interactions. By exploiting specific weaknesses, the study provides a benchmark of malicious queries and demonstrates BadRobot's effectiveness against prominent embodied LLM frameworks, emphasizing the need for addressing safety issues in these systems.

HyperFace: Generating Synthetic Face Recognition Datasets by Exploring Face Embedding Hypersphere
The paper addresses ethical and privacy concerns of face recognition datasets by proposing HyperFace, a novel method for generating synthetic datasets, which are posed as a packing problem in the embedding space of face recognition models. By leveraging a gradient descent-based optimization and a conditional face generator, HyperFace effectively synthesizes face images, leading to state-of-the-art performance when used for training face recognition models.

Dynamic Neural Fortresses: An Adaptive Shield for Model Extraction Defense
The paper introduces Dynamic Neural Fortresses (DNF), a defense mechanism against model extraction that uses a dynamic Early-Exit neural network to protect the neural network's architecture and reduce resource wastage. By allowing attack queries to exit at earlier layers and benign queries at later layers, DNF enhances architectural protection, reduces computational cost, and improves efficiency, demonstrating strong defensive capabilities against model extraction and neural architecture theft.

Auto-GDA: Automatic Domain Adaptation for Efficient Grounding Verification in Retrieval-Augmented Generation
This paper addresses the challenge of hallucination in retrieval-augmented generation (RAG) by introducing Automatic Generative Domain Adaptation (Auto-GDA) for improving grounding verification in natural language inference (NLI) models. Auto-GDA efficiently adapts NLI models to specific domains using unsupervised methods, achieving performance comparable to large language models at a fraction of the computational cost by iteratively improving synthetic data generation with weak labels and discrete optimization.

Enhancing Robust Fairness via Confusional Spectral Regularization
This paper addresses the issue of "robust fairness" in deep neural networks, where robust accuracy varies across classes, by proposing a novel regularization technique that targets the spectral norm of the robust confusion matrix. The method is validated through extensive experiments and shows improved worst-class robust accuracy and enhanced robust fairness.

### Theory->Deep Learning
Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context
This paper explores the behavior of linear transformers pre-trained on random linear classification tasks, revealing how many pre-training tasks and in-context examples are necessary for effective generalization at test time. It also demonstrates that these transformers can exhibit "benign overfitting in-context," where they memorize noisy in-context examples yet still generalize nearly optimally for clean test examples.

### Theory->Game Theory
Last-Iterate Convergence Properties of Regret-Matching Algorithms in Games
This paper investigates the last-iterate convergence of algorithms using Regret Matching$^+$ in two-player zero-sum games, a property previously unexplored despite the algorithms' practical applications. Through numerical evidence and a new smoothing technique, it demonstrates that specific algorithm variants can achieve asymptotic last-iterate convergence and suggests these findings provide a novel perspective for analyzing such convergence in algorithms using non-monotone operators.

Generalized Principal-Agent Problem with a Learning Agent
This paper examines repeated generalized principal-agent problems, focusing on scenarios where the principal lacks commitment power and the agent employs learning algorithms to respond. The study demonstrates that the principal's utility is influenced by the type of learning algorithm used by the agent, with varying outcomes in terms of achieving or exceeding the optimal utility found in classic models, thus refining existing theories in Stackelberg games and contract design, and innovating in Bayesian persuasion situations.

### Theory->Online Learning and Bandits
Dynamic Assortment Selection and Pricing with Censored Preference Feedback
This paper introduces a new framework using a censored multinomial logit choice model to tackle the dynamic multi-product selection and pricing challenge, aiming to maximize seller revenue by adjusting offerings and prices while learning buyer preferences. The proposed Lower Confidence Bound (LCB) pricing strategy combined with Upper Confidence Bound (UCB) or Thompson Sampling (TS) for product selection achieves significant regret bounds, with its effectiveness validated through simulations.

Lipschitz Bandits in Optimal Space
This paper introduces the Log-space Lipschitz bandits (Log-Li) algorithm that addresses the Lipschitz bandit problem by significantly reducing memory usage to $O(\log T)$ bits while achieving near-optimal regret of $\widetilde{O}(T^{\frac{d_z+1}{d_z+2}})$. Through complexity analysis and numerical simulations, the study demonstrates that Log-Li offers comparable regret performance to state-of-the-art algorithms but with drastically lower memory requirements.

Pairwise Elimination with Instance-Dependent Guarantees for Bandits with Cost Subsidy
This paper introduces the Pairwise-Elimination (PE) algorithm and its generalized version, PE-CS, to address the Multi-Armed Bandits with Cost Subsidy (MAB-CS) problem, where the aim is to minimize decision costs subject to reward constraints. Analyzing these algorithms, the authors demonstrate an order-wise logarithmic upper bound on both Cost and Quality Regret and show through experiments, including those using the MovieLens 25M dataset, that their methods outperform the existing ETC-CS baseline.

### Theory->Optimization
Tight Lower Bounds under Asymmetric High-Order Hölder Smoothness and Uniform Convexity
This paper establishes tight lower bounds for the oracle complexity involved in minimizing high-order Hölder smooth and uniformly convex functions under two distinct cases of degree imbalance. It generalizes previous findings on oracle complexities for first and second-order smooth functions and uniformly convex functions, matching the established upper bounds for these complex cases, thereby enhancing our understanding of optimization in these settings.

Linear Partial Gromov-Wasserstein Embedding
The paper introduces the linear partial Gromov-Wasserstein (LPGW) embedding, a linearized technique to efficiently compute the partial Gromov-Wasserstein (PGW) distance between different metric spaces, thus resolving the computational challenges posed by the non-convex nature of the GW and PGW problems. LPGW reduces the pairwise computation overhead for PGW from $\mathcal{O}(K^2)$ to $\mathcal{O}(K)$ times for $K$ spaces while maintaining the partial matching advantages of PGW, evidenced by its effectiveness in applications like shape retrieval and transport-based learning.

### Theory->Probabilistic Methods
Boundary constrained Gaussian processes for robust physics-informed machine learning of linear partial differential equations
This paper introduces a framework for creating boundary constrained Gaussian process (BCGP) priors to enforce linear boundary conditions in solving linear PDEs, accommodating all boundary conditions like Dirichlet, Neumann, Robin, and mixed. It proves the universal representational capacity of BCGP kernels under Dirichlet conditions and establishes an equivalence with boundary-constrained neural networks, demonstrating effectiveness through numerical experiments even with sparse, noisy data.

### Theory->Reinforcement Learning and Planning
Minimax Optimal Reinforcement Learning with Quasi-Optimism
We introduce EQO (Exploration via Quasi-Optimism), a reinforcement learning algorithm that achieves the sharpest known regret bound for tabular RL without relying on empirical variances, using a simple bonus term based on the inverse of the state-action visit count. EQO offers a practical and computationally efficient approach with empirical evaluations showcasing its superior performance in regret and efficiency compared to existing algorithms.



## Oral Session 6D (15:30-16:42)
15:30-15:42: OptionZero: Planning with Learned Options
OptionZero is a novel approach that enhances the MuZero framework by integrating an option network for the autonomous discovery of action sequences, or options, through self-play games. Demonstrating a significant 131.58% improvement over MuZero in mean human-normalized scores across 26 Atari games, OptionZero not only discovers valuable options but also develops strategic skills adapted to diverse game challenges, paving the way for improved planning in reinforcement learning.
15:42-15:54: The Complexity of Two-Team Polymatrix Games with Independent Adversaries
This paper investigates the computation of Nash equilibria in polymatrix games where each pair of players engages in either a zero-sum or a coordination game, focusing on scenarios with players grouped into a small number of teams with identical interests. The authors establish that the problem remains complex, specifically CLS-hard, even for two-team configurations, and also demonstrate the tightness of this complexity bound, particularly when one team comprises multiple independent adversaries, while proving the difficulty of finding any stationary point in non-convex-concave min-max constrained optimization problems.
15:54-16:06: Advantage Alignment Algorithms
Artificially intelligent agents in human decision-making often result in conflicts due to optimizing individual objectives, especially in general-sum games. This paper introduces Advantage Alignment, a novel algorithm family for opponent shaping that simplifies mathematical formulations, reduces computational burden, extends to continuous action domains, and demonstrates enhanced cooperation and robustness against exploitation in social dilemmas.
16:06-16:18: Brain Bandit: A Biologically Grounded Neural Network for Efficient Control of Exploration
This paper analyzes a dynamical systems model of a biological neural network, called the Brain Bandit Net (BBN), which controls explore-exploit decisions and demonstrates superior exploration efficiency compared to traditional reinforcement learning approaches. Through theoretical and simulation studies, the authors show that BBN performs posterior sampling of action values and resembles human and animal choice patterns, offering a brain-inspired algorithm for enhancing exploration in reinforcement learning tasks.
16:18-16:30: Computationally Efficient RL under Linear Bellman Completeness for Deterministic Dynamics
This paper presents a computationally efficient algorithm for Reinforcement Learning in the linear Bellman complete setting, which unifies models like linear MDPs and LQRs. The method leverages randomization by injecting noise into least squares regression to enable optimistic value iteration while ensuring statistical tractability and addressing error amplification challenges.
16:30-16:42: Tractable Multi-Agent Reinforcement Learning through Behavioral Economics
This paper addresses the computation challenge of Nash equilibria in multi-agent reinforcement learning by introducing risk-averse quantal response equilibria (RQE), inspired by behavioral economics, which are more tractable due to agents' risk aversion and bounded rationality. The findings illustrate that RQE can be the result of no-regret learning in various games, match human decision patterns in experimental settings, and provide an analysis of their sample complexity in finite-horizon Markov games utilizing a generative model.


## Oral Session 6C (15:30-16:42)
15:30-15:42: Accelerated training through iterative gradient propagation along the residual path
The paper introduces Highway backpropagation, a parallelizable iterative algorithm designed to address the computational burden of traditional backpropagation by leveraging residual-like architectures to accumulate and backpropagate gradient estimates in parallel. Through extensive experiments across various models and tasks, the study demonstrates that Highway-BP can significantly speed up training with minimal loss in performance.
15:42-15:54: Learning Randomized Algorithms with Transformers
This paper explores the integration of randomization into deep neural networks, specifically transformer models, to enhance their performance in adversarial settings. By employing common optimization techniques to incorporate random properties, the study demonstrates improved robustness and performance in tasks such as associative recall, graph coloring, and grid world exploration, highlighting the benefits of randomness in neural computation and predictions.
15:54-16:06: Attention as a Hypernetwork
This paper investigates the mechanisms behind compositional generalization in transformers, proposing that multi-head attention can be viewed as a hypernetwork with a low-dimensional latent code that specifies key-query operations. By introducing a non-linear modification to the hypernetwork-generated network, the study shows improved compositional generalization on tasks such as a symbolic version of Raven's Progressive Matrices, highlighting how increased model size and data contribute to this capability.
16:06-16:18: Transformers Provably Solve Parity Efficiently with Chain of Thought
This study presents the first theoretical analysis of training transformers to solve the $k$-parity problem using recursive intermediate states akin to chain-of-thought reasoning. The research reveals that incorporating intermediate parities into the loss function allows the model to learn efficiently with teacher forcing, and even without it, by using augmented data for internal verification, demonstrating the benefits of task decomposition and self-consistency in multi-step reasoning.
16:18-16:30: When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers
This paper provides the first theoretical characterization of task vector methods on nonlinear Transformers, proving their effectiveness in tasks like multi-task learning and unlearning. The study demonstrates that task addition and negation can successfully manage aligned and contradictory tasks, while proper linear coefficient selection ensures generalization to out-of-domain tasks, with findings validated on Phi-1.5 for machine unlearning.
16:30-16:42: Progressive distillation induces an implicit curriculum
Progressive distillation, where a student model learns from successive intermediate checkpoints of a teacher model, provides an implicit curriculum that accelerates learning and enhances sample complexity benefits. Through experiments on sparse parity, probabilistic context-free grammars (PCFGs), and datasets like Wikipedia and Books, the study demonstrates how progressive distillation helps the student model to gradually capture longer contextual features, emphasizing its advantages across various tasks.


## Oral Session 6E (15:30-16:42)
15:30-15:42: SymmetricDiffusers: Learning Discrete Diffusion Models over Finite Symmetric Groups
This paper introduces *SymmetricDiffusers*, a novel discrete diffusion model that facilitates learning complicated distributions over permutations by using deep neural networks to manage simpler reverse diffusion transitions. By employing a riffle shuffle for forward transitions, introducing a more expressive generalized Plackett-Luce distribution, and proposing an efficient "denoising schedule," the model achieves state-of-the-art results across tasks like sorting 4-digit MNIST images, solving jigsaw puzzles, and addressing traveling salesman problems.
15:42-15:54: Generator Matching: Generative modeling with arbitrary Markov processes
Generator Matching is a versatile framework for generative modeling that utilizes arbitrary Markov processes, unifying several existing methods like diffusion and flow matching while also expanding to novel processes such as jump processes. The method allows for the creation of multimodal models and demonstrates improved performance with innovations like superposition using jump processes, validated through empirical tests on image and multimodal generation.
15:54-16:06: Emergence of meta-stable clustering in mean-field transformer models
This paper presents a mathematical investigation into the long-term behavior of tokens in a deep stack of Transformer layers, modeled as a continuous-time flow on the unit sphere using a mean-field interacting particle system. It reveals the emergence and persistence of meta-stable phases and clustering phenomena, essential for applications like next-token prediction, by analyzing the mean-field PDE and identifying the structure of the meta-stable manifold influenced by the inverse temperature parameter through rescaled Gegenbauer polynomials.
16:06-16:18: CAX: Cellular Automata Accelerated in JAX
This paper presents CAX (Cellular Automata Accelerated in JAX), an open-source library that significantly enhances research in cellular automata by providing hardware-accelerated performance and a flexible, modular design. CAX not only speeds up simulations considerably, fostering new research opportunities, but also demonstrates its versatility in diverse applications, including a one-dimensional cellular automaton surpassing GPT-4 in the 1D-ARC challenge.
16:18-16:30: Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective
This paper presents a novel design framework for discrete generative models using continuous-time Markov chains, allowing for the first time the use of arbitrary discrete probability paths, or corruption processes. The authors introduce velocity formulas that decouple probability and velocity, enabling enhanced model performance across various modalities, including text, inorganic materials, and image generation, outperforming traditional masked constructions.
16:30-16:42: Learning Distributions of Complex Fluid Simulations with Diffusion Graph Networks
The paper introduces a graph-based latent diffusion model that efficiently samples states from the equilibrium distribution of complex systems like fluid dynamics, using a mesh discretization and physical parameters. This method demonstrates accuracy and computational efficiency by learning full distributions from short simulations and predicting complex scenarios such as 3D wing model pressure distributions in turbulent flows.


## Oral Session 6F (15:30-16:42)
15:30-15:42: On the Identification of Temporal Causal Representation with Instantaneous Dependence
The paper presents the \textbf{IDOL} framework, which addresses the challenge of identifying latent causal processes in time series data with instantaneous dependencies by imposing a sparse influence constraint. Through theoretical analysis and experimental validation, the method demonstrates its ability to identify latent causal processes effectively, particularly in human motion forecasting scenarios, without requiring complex interventions or observation grouping.
15:42-15:54: The Hidden Cost of Waiting for Accurate Predictions
This paper explores the tension between the timing of predictions and the efficiency of resource allocations in algorithmic decision-making. Using a mathematical model, the study reveals that delaying decisions to improve predictive accuracy can paradoxically worsen average ranking loss and decrease social welfare, with inequality being a key contributing factor, thereby challenging the assumption that more accurate predictions necessarily lead to better outcomes.
15:54-16:06: Root Cause Analysis of Anomalies in Multivariate Time Series through Granger Causal Discovery
This paper introduces AERCA, a methodology that combines Granger causal discovery with root cause analysis to address anomalies in multivariate time series. By modeling the distributions of exogenous variables and identifying those deviating from normal states, AERCA effectively uncovers causal relationships and pinpoints anomalies' root causes, as validated by extensive experiments.
16:06-16:18: When Selection Meets Intervention: Additional Complexities in Causal Discovery
This paper introduces a graphical model that addresses selection bias in interventional studies, which is often overlooked and can lead to incorrect causal discovery results. By explicitly considering both the observed and counterfactual worlds, the authors propose a provably sound algorithm that successfully identifies causal relations and selection mechanisms, demonstrating its effectiveness through synthetic and real-world experiments.
16:18-16:30: CyberHost: A One-stage Diffusion Framework for Audio-driven Talking Body Generation
CyberHost is a pioneering one-stage audio-driven framework for generating talking body animations that addresses common synthesis issues such as hand integrity and identity consistency. It introduces a Region Attention Module and Human-Prior-Guided Conditions to improve local synthesis and motion stability, outperforming previous methods in zero-shot video generation and extending effectively to hybrid-driven scenarios.
16:30-16:42: Loopy: Taming Audio-Driven Portrait Avatar with Long-Term Motion Dependency
This paper presents Loopy, an end-to-end audio-only conditioned video diffusion model designed to enhance the naturalness and stability of audio-driven human video generation. By introducing an inter- and intra-clip temporal module and an audio-to-latents module, Loopy outperforms previous models, delivering more lifelike portrait videos without the need for auxiliary spatial signals, thereby achieving higher quality and subtle facial expressions across various scenarios.


## Oral Session 6A (15:30-16:42)
15:30-15:42: Training Language Models to Self-Correct via Reinforcement Learning
This paper presents SCoRe, a multi-turn online reinforcement learning approach that enhances the self-correction ability of large language models using entirely self-generated data. By addressing limitations of supervised fine-tuning with distribution mismatch and ineffective correction modes, SCoRe significantly improves self-correction performance, achieving a 15.6% and 9.1% enhancement on the MATH and HumanEval benchmarks when applied to Gemini models.
15:42-15:54: Reasoning Elicitation in Language Models via Counterfactual Feedback
This paper addresses the underdeveloped reasoning capabilities of language models, specifically in causal reasoning through counterfactual question answering. It introduces novel metrics for assessing reasoning abilities and proposes fine-tuning approaches to enhance reasoning, evaluating their performance and generalization in various realistic scenarios.
15:54-16:06: Self-Improvement in Language Models: The Sharpening Mechanism
This paper explores the concept of "self-improvement" in language models, where models refine their own outputs without external feedback, through a process called "sharpening." By introducing a new statistical framework and analyzing two types of self-improvement algorithms—SFT-based and RLHF-based—the study finds that while SFT is optimal with sufficient initial model coverage, RLHF can surpass SFT by utilizing online exploration, thus offering foundational insights for designing self-improvement algorithms.
16:06-16:18: ReGenesis: LLMs can Grow into Reasoning Generalists via Self-Improvement
This paper presents ReGenesis, a novel method for enhancing the reasoning abilities of Large Language Models by self-synthesizing reasoning paths from general guidelines, without relying on external supervision. ReGenesis demonstrates superior performance in both in-domain and out-of-domain tasks, showcasing a 6.1% improvement in out-of-domain tasks compared to existing self-synthesizing methods.
16:18-16:30: Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models
This paper investigates Large Language Model (LLM) self-improvement by introducing a framework where the model verifies its own outputs and refines data accordingly, leading to a better understanding of this mechanism. The authors provide a mathematical formulation centered around the "generation-verification gap" and reveal a scaling phenomenon where self-improvement potential increases with model pre-training flops, offering insights and future research directions into enhancing LLM capabilities.
16:30-16:42: Learning Dynamics of LLM Finetuning
The paper investigates the learning dynamics of large language models during finetuning, offering a framework that explains how specific types of responses, such as hallucinations, are reinforced. By analyzing influence accumulation and introducing a "squeezing effect," the study provides insights into finetuning behaviors and proposes a method to enhance alignment performance in models.


## Oral Session 6B (15:30-16:42)
15:30-15:42: MoDeGPT: Modular Decomposition for Large Language Model Compression
This paper presents Modular Decomposition (MoDeGPT), a structured compression framework for large language models that reduces computational requirements while maintaining accuracy. MoDeGPT achieves a 98% reduction in compute costs and retains 90-95% of zero-shot performance on various models by utilizing innovative matrix decomposition techniques, offering a substantial improvement in efficiency and inference speed.
15:42-15:54: AlphaEdit: Null-Space Constrained Model Editing for Language Models
AlphaEdit is introduced as a novel solution to mitigate disruption of preserved knowledge in large language models (LLMs) during sequential editing by projecting perturbations onto the null space of the preserved knowledge. The method improves the performance of existing locating-then-editing strategies by 36.7% on average, as demonstrated through experiments with models like LLaMA3, GPT2-XL, and GPT-J, all achieved with minimal code addition.
15:54-16:06: Judge Decoding: Faster Speculative Sampling Requires Going Beyond Model Alignment
This paper addresses the limitations of speculative decoding in large language models (LLMs) by proposing an adapted verification scheme that recognizes valid but non-aligned token continuations. The authors introduce a "TokenCourt" dataset and a trained module that enhances verification abilities, demonstrating significant speedup in autoregressive generation while maintaining output quality, with a notable $9\times$ speed increase on the Llama-3.1 model family.
16:06-16:18: Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates
This paper reveals vulnerabilities in automatic LLM benchmarks, demonstrating that even a "null model" with irrelevant, constant outputs can achieve high win rates, thereby highlighting the possibility of gaming these evaluations. The study emphasizes the need for robust anti-cheating mechanisms to ensure the reliability of these benchmarks, as tactics to exploit them could be used unethically.
16:18-16:30: Faster Cascades via Speculative Decoding
This paper introduces new speculative cascading techniques that combine the strengths of cascades and speculative decoding by implementing a deferral rule through speculative execution. Experimental results with Gemma and T5 models demonstrate that the proposed approach provides superior cost-quality trade-offs compared to traditional cascading and speculative decoding methods.
16:30-16:42: Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo
This paper presents a novel architecture for controlled language model (LM) generation using a sequential Monte Carlo (SMC) framework, enabling the incorporation of domain-specific constraints during inference and efficient resource allocation. The approach allows small open-source language models to surpass models over eight times their size and closed-source models in tasks like Python code generation and molecule synthesis, highlighting improvements in approximating the posterior distribution.

